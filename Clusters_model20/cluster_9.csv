X1,file_name,category,text,recommendations
50,training-dataset/engineering/1404.txt,engineering,Relocatable vs  Position Independent Code  or  Virtual Memory isn t Just For Swap Myth   Virtual memory  is the mechanism that a kernel uses to make more memory available than is actually physically installed  by setting aside a disk partition for the overflow and copying pages between memory and disk as needed   I acquired this belief very early in my programming career  but it turns out that swapping pages to disk is merely one of the many things that  virtual memory  makes possible   Fact   Virtual memory  is a hardware  CPU  mechanism  which  every single time memory is accessed  references a kernel specified data structure called a  page table  to arbitrarily frobnicate the high bits of the address  which is called  translating  from a  linear address  to a  physical address    The page table gets cached by a translation lookaside buffer  so the lookup is usually quite efficient    This fact became very real to me this week as I made a kernel from scratch  I was moderately surprised that I needed to set up a page table  when I had always thought of virtual memory as a somewhat advanced kernel feature  Today  I learned how  relocatable  and  PIC    terms I d encountered in the past and never really understood   suddenly make sense in this context   Here s another fact that surprised me  in conventional operating systems  every process has its own page table  The pointer 0x7fff8000 does not necessarily translate to the same physical address in one process as it does in another    Now  let s talk about libraries  Libraries are code  but they don t run as processes of their own  They re going to wind up under someone else s page table  There s two ways that can happen  static linking and dynamic linking    If a library is statically linked  the linker finds some place in a code segment of the executable to situate the library  The loader will then place this segment in virtual memory  wherever it s explicitly specified to go  when the executable is run   If a library is dynamically linked  then when the loader sets up the executable  it will invoke the dynamic linker to make sure that the required library shows up some place in the process s virtual memory    Whether static or dynamic  a linked library is going to be situated in virtual memory somewhere that the library can t predict   which is problematic for accessing its own memory  Fortunately  the linker  whether static or dynamic  can help us out by relocating the library s code  so that it knows where it is  Unfortunately  library writers have to help the linker out by specifying  in the object file  the set of instructions or initialized data that need to be modified to properly relocate it  As long as all that  relocation information  is present  the object file is said to be relocatable   On the other hand  position independent code  PIC   as the name suggests  doesn t even need to be relocated  None of its instructions or initialized data encode any assumptions about the region of virtual memory the program will be loaded into  it figures out where it is  usually by referencing the instruction pointer  and makes all memory accesses based on what it finds out   So why do all that work when the linker can relocate for us   Here s the kicker  The whole motivation for dynamic linking was shared libraries  Shared doesn t just mean that multiple programs reference the same library file on disk  It means those processes share that library in physical memory   Since every process has its own page table  the exact same library code winds up executing as if it were loaded into multiple  inconsistent virtual memory locations  If we relocated it for one process  it wouldn t necessarily be valid for another  This is why weird things sometimes happen where the solution is  recompile blah with  fPIC     Perhaps the most interesting thing about all this is that in today s 64 bit age  position independent code may not even be necessary  The available virtual memory address space with 64 bits is so large that an OS may be able to afford blocking off a region of every process s virtual memory space to host every shared library on the system  so that their linear locations are guaranteed to be consistent from process to process  That means shared libraries would still have to be relocatable  but they wouldn t have to be PIC   On the other hand  x86_64 makes it significantly easier to write position independent code  by referring addresses to the current program counter  so no matter what virtual memory offset the code is at  it s internally consistent   If we adopt a policy that all libraries  static and dynamic  are PIC  then libraries don t ever have to worry about being relocated and the linker gets a lot simpler,"[50 1177 1310 954 1051 720 463 957 933 805 136]"
136,training-dataset/engineering/220.txt,engineering,Consistent Hashing   carlosgaldinoConsistent Hashing  Introduction  Traditional hash tables map keys to an array index using the following process   hash   hashFunc key  index   hash   arraySize  When the arraySize changes  all keys need to be remapped because the index is calculated by a modular operation   The same technique can be used to partition the data from some application into a number of databases1 by calculating the hash of a key for the data modulo the number of databases  and you could have a situation like the following   If a new database is added to the cluster  or one existing db is removed  or fails   all keys would need to be remapped just like in the hash table example  Now  if you are dealing with a lot of data you can imagine that remapping all keys would take quite some time which is not very attractive   Consistent Hashing  That s when consistent hashing comes as an alternative  First  let s consider the output range of a function f    If we connect both ends we end up with a ring   Using the same function f   we can map each node to a point in the ring   The interval between two nodes in the ring form a partition  If we use the same function f 2 over the key we will end up with a projection of that key in the ring   With this notion we can define the server responsible for our key as the first node in a clockwise direction after the key projection3   So in this case   Mars  would be stored in the server 10 9 5 1   The same process is applied for any other key   Note that f  Venus   mapped to a point after the last node and before the maximum value of function f   Since both ends are connected  there is no problem in walking clockwise to find the responsible node which in this case is 10 1 2 3    Adding a new node to the ring does not mean that all keys will need to be remapped  Only a fraction4 of keys will need to move to a different node   A fraction of the keys are also remapped when a node leaves the ring   And that s the essence of Consistent Hashing  The idea was presented in a paper by Karger et al  in 1997  1   Nodes are mapped to a ring  forming partitions  which are then used to find the node responsible for a key by mapping the key to the same ring  and finding the first node in a clockwise direction after the key position   Some examples of systems that use consistent hashing are  Amazon s Dynamo  2   Riak  3   Akka  4   and Chord  5    With consistent hashing is easier to avoid hotspots by using a function f that mixes well  so even if the keys are very similar they end up projected in different and distant points in the ring  causing them to be stored at different nodes  Another benefit is the smoothness for moving keys when nodes join or leave the ring  only the immediate neighbors of a node are impacted and other nodes remain unaffected   A system using consistent hashing can apply other techniques to reduce even more the impact of changes in the ring structure  If nodes are data stores  like the initial example in this post  the system could replicate the data in the next N nodes after the original node  N1   that is responsible for that data  This gives the advantage that if N1 leaves the ring  its immediate neighbors will already have the data that was stored at N1   preventing an increase of network traffic after a node  in this case  N1   departs  The same technique helps avoiding hotspots even more since requests for the data can be handled by N1 or any of its next N neighbors in the ring   Usually  systems using consistent hashing construct their rings as the output range of a hash function like SHA 1  or SHA 2  for example  The range for SHA 1 goes from 0 to 2160  and SHA 2 has different output ranges  SHA 256 is 0 to 2256  SHA 512 is 0 to 2512  etc  Using any of these functions to map the nodes in the ring will have the effect of placing the nodes in random positions  The partitions will very likely have different sizes which means nodes will be responsible for different amounts of data  It may not be attractive to have this characteristic and since the range is well defined  the ring could be split into partitions of equal size and then each node could claim ownership of a number of these partitions  guaranteeing that each node handles more or less the same amount of data5   Another important technique is the usage of virtual nodes  which we will see next   Virtual Nodes  The ring shown above had a one to one mapping between physical nodes and nodes in the ring  This approach presents a challenge that randomly placing the nodes in the ring might lead to a non uniform distribution of data between nodes   This problem becomes more evident when a node leaves the ring which requires that all the data handled by that node need to be moved entirely to a single other node   To avoid overloading a single node when another one leaves the ring  and to distribute the keys more evenly  the system can create a different mapping between physical nodes and nodes in the ring  Instead of having a one to one mapping  the system creates virtual nodes  creating a M to N mapping between physical nodes and virtual nodes in the ring   With virtual nodes  each physical node becomes responsible for multiple partitions in the ring  Then if a node leaves the ring  the load handled by this node is evenly dispersed across the remaining nodes in the ring   And similarly when a node joins the ring  it receives a roughly equivalent amount of data from the other nodes in the ring  The virtual nodes scheme also helps when the system is comprised of heterogeneous nodes in terms of resources such as CPU cores  RAM  disk space  etc  With an heterogeneous cluster the number of virtual nodes for each physical node can be chosen considering the characteristics of each physical node   concha   A consistent hashing library in Erlang  concha is a consistent hashing library in Erlang that I built  The ring represents the output range of SHA 256  and the same function is used for mapping nodes and keys to the ring  It provides lookup of nodes based on the given keys  creating the ring with virtual nodes  adding and removing nodes  etc  For more information and examples of usage you can visit its repository on GitHub   References   1  D  Karger  E  Lehman  T  Leighton  R  Panigrahy  M  Levine  and D  Lewin  Consistent hashing and random trees  distributed caching protocols for relieving hot spots on the World Wide Web  New York  New York  USA  ACM  1997  pp  654 663    2  G  DeCandia  D  Hastorun  M  Jampani  G  Kakulapati  A  Lakshman  A  Pilchin  S  Sivasubramanian  P  Vosshall  and W  Vogels  Dynamo  Amazon s Highly Available Key value Store  2007  pp  1 16    3  Riak Vnodes  https   docs basho com riak kv 2 1 4 learn concepts vnodes    4  Akka Routing  http   doc akka io docs akka 2 4 scala routing html ConsistentHashingPool_and_ConsistentHashingGroup   5  I  Stoica  R  Morris  D  Karger  M  F  Kaashoek  and H  Balakrishnan  Chord  A Scalable Peer to peer Lookup Service for Internet Applications  ACM SIGCOMM    pp  149 160  2001    6  Consistent hashing  Wikipedia  https   en wikipedia org wiki Consistent_hashing  Notes,"[136 933 1177 1199 50 805 463 720 1051 1310 1130]"
383,training-dataset/engineering/1106.txt,engineering,Performance   Usage at Instagram   Instagram EngineeringPerformance   Usage at Instagram  At Instagram  we treat performance as a feature  and as we build products  we constantly think about how we can make things faster and more efficient  We ve found that improving performance can actually drive usage  and by making small changes we can also improve user experience with the service  Here  we explore one case study where identifying and fixing low hanging fruit reduced network usage  improved client side reliability  reduced server side latency  and in turn we saw an improvement in app wide user experience   Background  Before a mobile client downloads actual media  i e  photos or videos  from our CDNs  it must first fetch JSON metadata   media bundles   from our Django webserver endpoints  Depending on the endpoint  the compressed response is typically 10 60 kB  Each bundle contains information like media id  metadata about the author  the number of likes  the caption  and the most recent comments  called  preview  or  summary  comments        status   ok     items        id            author       id            profile_pic_url           like_count   500    caption    tbt     comments        id            text    Great pic      user       id            profile_pic_url            another comment                another media bundle              Simplified illustrative example of a media bundle JSON  When you open Instagram to the main feed  you will notice that you only see up to three preview comments below each photo  in addition to the caption   In grid view  e g  in the Search Explore tab or user profiles   no preview comments are visible at all   3 preview comments per item are visible in feed  and no preview comments are visible in grid view   However  we had been sending up to 20 comments to the client with each bundle  Originally  this was intended as an optimization to make the  View all comments  screen load faster  But when viewed holistically  this now seems like a poor trade off for these reasons    Media are viewed more commonly than their comments  and we should optimize for the common case    Comment bundles are particularly heavy   They contain the comment text itself as well as its id  timestamp  and author metadata  including profile picture URL    The authors and content of the comments are usually unique from comment to comment  so compression performs poorly    Generating profile picture URLs is a CPU inefficient operation because we must dynamically compute the correct CDN URL  The more comments we load  the more profile picture URLs we need to generate    When a user clicks on  View all   comments  we ask the server for new comments anyway   For these reasons  reducing the maximum number of summary comments in each media bundle seemed like an obvious thing to do  But it was still unclear how much of a user facing impact it would have  After all  this only makes a difference in the order of tens of kilobytes per payload  a difference that is dominated by the size of photo or video files  We hypothesized that the impact on network latency should be negligent   if a user were using a connection slow enough where downloading a few more kilobytes matters  just about any Internet service would probably be too difficult to use anyway  But  considering the possible bandwidth and CPU savings  we decided to do an experiment to see if there were in fact any user facing effects   The experiment  We ran an A B experiment that reduced the maximum number of summary comments in each bundle from 20 to 5  This dropped the median response size of the main newsfeed endpoint from 15 KB to 10KB  while the median response size size of the  Explore Posts  endpoint dropped from 46 KB to 23 KB  This drop is even more pronounced when considering response sizes at higher percentiles  at the 95th percentile  median response size of the main feed endpoint dropped from 32 KB to 16 KB   Results  As expected  reducing the size of the payload by a few kilobytes had no perceptible impact on network latency  But it had a surprising impact on memory usage  reducing the average memory usage for each feed screen ended up significantly improving the stability of the entire app  Android out of memory  OOM  errors dropped 30   We hypothesize that the difference between platforms results from the Android market  some Android phones come with very low amounts of RAM  and correspondingly high memory pressure   Median CPU usage on our most popular endpoints  like the main feed endpoint  dropped 20   This translated into a median savings of 30ms in server side wall time  and thus reduced end to end latency   and at the 95th percentile  we saved 70ms in server side wall time  That makes a difference   Infra improvements  When we launched this across all our users  CPU across our entire Django fleet dropped about 8  and egress dropped about 25   Egress is a measure of site health  and such a drop would normally be alarming  But in this case  it s a good sign that we re reducing the load on our infrastructure   Results  During the A B test  we saw app wide impressions across all mobile platforms increase by 0 7   and likes increase by 0 4   This was driven by increases of impressions on all surfaces   for instance   Explore Photos  impressions increased over 3   and user profile scrolls increased 2 7   These trends continued over time  confirming that good performance brings users back   Percent increase in user profile scrolls over 3 month period  Takeaways  Question baked in assumptions  In this case  we asked   Why do we send 20 comments per media bundle   Sometimes  questioning baked in assumptions can lead to identifying low hanging fruit   In this case  we asked   Why do we send 20 comments per media bundle   Sometimes  questioning baked in assumptions can lead to identifying low hanging fruit  Measure  Before optimizing  take time to understand the potential impact  We saw how heavy comments were when we inspected the payload  and profiling led us to the realization that we could save CPU   Before optimizing  take time to understand the potential impact  We saw how heavy comments were when we inspected the payload  and profiling led us to the realization that we could save CPU  Optimize for the most common case  A cardinal rule of optimization  Here  we consciously chose to optimize media loads rather than comment loads   A cardinal rule of optimization  Here  we consciously chose to optimize media loads rather than comment loads  Do the simple thing   Do the simple thing first  is dogma at Instagram  After identifying the potential problem  we chose the simplest and most obvious course of action  And despite its simplicity  it yielded big results    Do the simple thing first  is dogma at Instagram  After identifying the potential problem  we chose the simplest and most obvious course of action  And despite its simplicity  it yielded big results  Empathize  This is oft repeated at Facebook  We use powerful phones on powerful networks  so this change was personally imperceptible  Yet it still impacted many people  Again  it s worth noting that the observed improvements on Android outpaced those on iOS  This makes sense   Android phones tend to be cheaper and less powerful   This is oft repeated at Facebook  We use powerful phones on powerful networks  so this change was personally imperceptible  Yet it still impacted many people  Again  it s worth noting that the observed improvements on Android outpaced those on iOS  This makes sense   Android phones tend to be cheaper and less powerful  Follow your nose  Here at Instagram NY  we re chiefly responsible for ranking media  for instance  we personalize and rank media for  Explore Photos    So a performance optimization like this wasn t directly related to our work  But our intuition told us that this would be worth pursuing  and one of the best things about working at Instagram is that joining a specific team doesn t constrain which parts of the code base we can touch  Within reason  we have the latitude to pursue anything we think is worthwhile   Thanks to Lisa Guo  Hao Chen  Tyler Kieft  Jimmy Zhang  Kang Zhang  and William Liu,"[383 1310 1353 1130 463 933 1177 1051 954 1060 720]"
463,training-dataset/engineering/1499.txt,engineering,Code Generation  The Inner Sanctum of Database PerformanceWednesday  September 7  2016 at 9 14AM  This is guest post by Drew Paroski  architect and engineering manager at MemSQL  Previously he worked at Facebook and developed HHVM  the popular real time PHP compiler used across the company s web scale application   Achieving maximum software efficiency through native code generation can bring superior scaling and performance to any database  And making code generation a first class citizen of the database  from the beginning  enables a rich set of speed improvements that provide benefits throughout the software architecture and end user experience   If you decide to build a code generation system you need to clearly understand the costs and benefits  which we detail in this article  If you are willing to go all the way in the name of performance  we also detail an approach to save you time leveraging existing compiler tools and frameworks such as LLVM in a proven and robust way   Code Generation Basics  In the database world  human readable SQL queries can be compiled to native code  which executes faster and is more efficient when compared to the alternative of interpreted execution  Code generation helps streamline operations by populating a plan cache with a natively compiled query plan  as shown in the following diagram  While this can take a bit more time at the outset  the performance benefit for a compiling query plans to native code in a database can be significant   Efficiency and Performance  Software has been one of the greatest forces affecting the world in the last 50 years  and virtually all software is powered by compilers  Compilers massively impact the efficiency and performance of software  and even influence how people use and experience software  as well as how programmers develop software   There are both qualitative and quantitative benefits of code compilation   QUALITATIVE BENEFITS  big enough differences in execution performance that change what user experiences are even possible  br   QUANTITATIVE BENEFITS  improvements in execution efficiency help reduce CPU and resource usage  which often reduces costs and increases profits  Implementing Code Generation as a First Class Citizen  Making code generation a first class citizen sets a foundation for high performance software systems  Some databases such as MemSQL assume code generation as a core competency and ensure broad native compilation capabilities from day one  Other databases implement support for code generation later after the initial release  usually with some partial coverage that expands over time   Of course it is possible to improve query performance by various means without compiling query plans to native code  However  the alternative requires a layer of software called an  interpreter  to continually act as a go between  where the interpreter runs directly on the CPU and executes lower level machine instructions on behalf of the SQL query  Regardless of whether you run your software on bare metal  in a container  or in a VM  generating native code and passing it directly to the CPU allows for greater potential performance   Taking a compilation first mentality when building a database means being able to fully compile all SQL query plans to native code  This is an aggressive stance for databases  as other approaches that delay support for code generation frequently end up with a subset of functionality   Database Performance When Disk Is No Longer The Bottleneck  With the rise of in memory databases  the performance landscape for databases has changed substantially  There is also a growing demand for real time analytics on large  constantly changing datasets  These developments present new and interesting performance challenges to tackle   When optimizing any software system  it is useful to focus on three broad areas   Intelligent I O scheduling  data movement  and load balancing Memory use on individual machines How computation is actually performed on the CPU  I O scheduling  data movement  and load balancing  For many applications  intelligent I O scheduling  data movement  and load balancing directly impact performance  Poor I O scheduling can cause an application to stall and leave the CPU idle while waiting for I O to finish  wasting time that potentially could have been spent doing productive CPU work  Poor planning with respect data movement and load balancing can cause an application to waste precious I O bandwidth unnecessarily or to overload some nodes in a cluster while leaving other nodes idle  Optimizing I O scheduling  data movement  and load balancing is critical for reducing an application s latency and maximizing its overall throughput   Memory Use  Memory use is another important area of focus  Modern hardware can achieve lightning fast memory access most of the time through the use of sophisticated multi level hardware caches that reside on the same die as the CPU  The cache is a limited resource  however  and so the CPU still needs to read from main memory every so often when there is a cache miss  A high cache miss rate can slow down an application by introducing millions of  micro stalls  per second  where the CPU idles for a few dozen of nanoseconds at a time waiting to retrieve a value from memory  Like I O  optimizing memory usage and memory access patterns is important both for increasing throughput and reducing latency   CPU Computation  The last area of focus  how computation performs on the CPU  is perhaps the most obvious  In the simplest terms  optimizing computation consists of changing the application to reduce the total number of CPU executed machine instructions while still performing all of the essential application functions  The CPU only performs so many operations per second  so naturally reducing the CPU burden maximizes application performance   New Challenges  For traditional databases  efforts to maximize throughput were bottlenecked on I O  Native code generation can have a substantial impact on CPU and memory usage  but when I O is the limiting factor  improvements to CPU and memory usage by themselves often have little effect on an application s maximum potential throughput   With I O no longer the bottleneck for in memory databases  users now push databases systems to extremes  using data to improve business efficiency and yield real time insights  To support these demands  investing in native code generation has become an essential part of building modern high performance databases   Building a Compilation Pipeline  Building a compiler from scratch that generates high quality native code requires a great deal of engineering effort before it pays off  Fortunately  software engineers have devised a number of techniques that leverage existing technologies to reduce the amount of upfront work needed  albeit while accepting some trade offs and limitations in the longer term   The  transpilation  approach is a common technique  which works by first translating the desired gsource language  ex  SQL  into a second high  or mid level programming language  ex  Java  C    and then leveraging an existing compiler to convert this second language to native code  Notable examples of this technique include Microsoft SQL Server s natively compiled stored procedures  Hekaton  and earlier versions of MemSQL  prior to MemSQL 5    Depending on the needs of a business  the trade offs and limitations imposed by the transpilation approach can eventually become obstacles to supporting new use cases and expanding into new markets  As MemSQL adoption continued to grow larger and more diverse over time  it became apparent that the original approach to native code generation would need an overhaul to deliver on ever increasing performance requirements  During the development of MemSQL 5  the original compilation pipeline was replaced by a new high level language virtual machine architecture powered by LLVM  delivering a 20x increase in the speed of code generation itself while still producing high quality native code  Boosting the speed of code generation was an important step towards creating an immersive  interactive experience working with huge datasets   The New MemSQL Code Generation Architecture  Let s briefly look at some of the internal details of the new MemSQL code generation architecture   The compilation pipeline starts with a parametrized SQL query  The query is transformed into an imperative query plan expressed in a high level imperative language called MPL  MemSQL Plan Language   which in turn is lowered down to mid  and low level intermediate languages  MemSQL Bit Code and LLVM bitcode respectively   and finally the last stage uses LLVM to convert the LLVM bitcode to high quality native code   The  explain mpl  and  explain mbc  commands give the user some introspection capabilities to see how the MemSQL native compiler works  The compiler does its best to intelligently choose the optimal execution plan  so typically users don t need to pay attention to these internal details   Basic example  create database db1  use db1  create table t  a bigint primary key not null  b bigint not null   insert into t values  1 2   explain select a a   b from t                                               EXPLAIN                                                Project  t a   t a   t b AS  a a   b       TableScan db1 t  PRIMARY KEY  a                                                explain mpl select a a   b from t   declare NetworkProcessFn  function exprs  exprs_t1   context  context_t1   bool    with  buffer   ProtocolRowBuffer           context   state2  buffer   with  bufferPartition   int32_t    0    with  outRowBase   ProtocolRowBase     cast ProtocolRowBase   ProtocolRowBufferGetNextRow buffer  bufferPartition    OutRowSigned  outRowBase    AddSignedSigned  MulSignedSigned      exprs   state1   a        exprs   state1   a         exprs   state1   b     ProtocolRowBufferFlushIfNeeded buffer  return 0   declare ScanTable  function exprs  exprs_t1   context  context_t1   xact  Transaction   foundRowParam  bool      bool   foreach  rowPtr in GetSL rcIndex 0  indexOrdinal 0  takeRowLocks 0   using transaction xact     exprs  state1    rowPtr if  call NetworkProcessFn exprs  context   return 1   return 0    As shown above  the sample SQL query gets transformed into an imperative plan written in MPL  The  foreach  block inside the  ScanTable  function loops over all the rows in the table and calls the  NetworkProcessFn  function for each row  The  NetworkProcessFn  computes  a a b  for each row and queues the results to be sent to the client   explain mbc select a a   b from t   Function 2  NetworkOutputFn   0x0000 Lea local  local2 local context i32 8 0x0010 Lea local  local1 local local2 i32 0 0x0020 ProtocolRowBufferFlush local local1 target 0x0040 0x002c Literal8 local  rvToReturn i64 1 0x003c Return     Function 3  ScanTable   0x0000  VSIterInit local  vsIter i32 0 i32 0 i32 0 local xact 0x0018  Literal1 local  hasMore i32 0 0x0024  GetQueryContext local  queryContext 0x002c  VSIterHasMore local  hasMore local  vsIter target 0x0140 0x0048  JumpIfFalse local hasMore target 0x0108 0x0054  VSIterGetRowPtr local  rowPtr local  vsIter 0x0060  Lea local  local1 local exprs i32 0 0x0070  Deref8 local local1 local  rowPtr 0x007c  CallManaged func 0  NetworkProcessFn  target 0x0138 local  local2 local exprs local context     0x00cc  VSIterNext local  vsIter 0x00d4  CheckQueryKilled local queryContext target 0x0138 0x00e0  VSIterHasMore local  hasMore local  vsIter target 0x0130 0x00fc  JumpIfTrue local hasMore target 0x0054 0x0108  VSLIteratorFree local  vsIter 0x0110  Literal8 local  rvToReturn i64 1 0x0120  Literal1 local hiddenRvParam i32 0 0x012c  Return      The MBC generated for the sample SQL query shows the same imperative plan as the MPL  but at a greater level of detail with larger and more complex operations broken down into smaller steps  For example  the  foreach  loop from the MPL has been converted to a sequence of lower level MBC instructions  specifically  VSIterHasMore    JumpIfTrue   and  JumpIfFalse     Once the query plan has been converted to MBC  the next steps involve translating the MBC into LLVM bitcode and leveraging LLVM to convert this bitcode to high quality native code  Once LLVM generates native code  we pass it directly to the CPU for execution  All SELECT queries and all data modifying queries in MemSQL execute in this manner   Landscape Disruption with First Class Code Generation  Building a high performance database with first class code generation enables new and qualitatively different user experiences that were not possible before  Across industries  areas like personalization  risk management  anomaly detection  billing  and supply chain optimization stand to gain from better database performance  Real time business analytics provide companies an instant view of their operations through ad hoc and ongoing queries on rapidly changing data sets  This could be manually at the command line or via and visualization tools like Tableau or Zoomdata   Most importantly  fast performance enabled by native code generation allows developers to write sophisticated capabilities that were previously challenging to integrate into applications  For example  it is impossible to incorporate live query results to an application if they take minutes to calculate  However  results that return in under a second can be presented live to an interactive application  This could range from serving the right content  ecommerce item  or ad to an end user  to fraud alerts with real time monitoring   The Path Forward with Natively Compiled Functions  Taking an aggressive stance on native code generation sets a foundation for performance that pushes the bounds of data processing  Having broad native compilation capabilities is imperative to maximizing the efficiency of modern in memory database systems   With this foundation in place  database systems can further extend many dynamic and highly programmable features  such as user defined functions  and ensure that they can take full advantage of the underlying hardware  Given the benefits  we are likely to see several database products striving to deliver the best possible code compilation performance to deliver on low latency queries and more sophisticated functions as the market evolves,"[463 1051 805 50 1310 933 1177 383 1353 1130 1199]"
720,training-dataset/engineering/1481.txt,engineering,Efficient storage  how we went down from 50 PB to 32 PBMonday  January 2  2017 at 8 56AM  As the Russian rouble exchange rate slumped two years ago  it drove us to think of cutting hardware and hosting costs for the Mail Ru email service  To find ways of saving money  let s first take a look at what emails consist of   Indexes and bodies account for only 15  of the storage size  whereas 85  is taken up by files  So  files  that is attachments  are worth exploring in more detail in terms of optimization  At that time  we didn t have file deduplication in place  but we estimated that it could shrink the total storage size by 36  since many users receive the same messages  such as price lists from online stores or newsletters from social networks containing images and so on  In this article  I m going to describe how we implemented a deduplication system under the supervision of Albert Galimov   Metadata storage  We re dealing with a stream of files  and we need to quickly recognize a duplicate  An easy solution would be to name files based on their contents  We re using SHA 1 for this purpose  The initial name of the file is stored in the email itself  so we don t need to worry about it   Once a new email arrives  we retrieve the files  compute their hashes and add the result to the email  It s a necessary step to be able to easily locate the files belonging to a particular email in our future storage when this email is being sent   Now  let s upload a file to our storage and find out if another file with the same hash already exists there  It means we ll need to store all hashes in memory  Let s call this storage for hashes FileDB   One and the same file can be attached to different emails  so we ll need a counter that keeps track of all emails containing this file   The counter gets incremented with every new uploaded file  About 40  of all files are deleted  so if a user deletes an email that contains files uploaded to the cloud  the counter must be decremented  If it reaches zero  the file can be deleted   Here we face the first issue  information about an email  indexes  is stored in one system  whereas information about a file is stored in another one  This may lead to an error   for example  consider the following workflow   The system receives a request to delete an email  The system checks the email indexes  The system can see there s an attachment  SHA 1   The system sends a request to delete a file  A crash occurs  so the email doesn t get deleted   In this case  the email stays in the system  but the counter gets decremented by one  When the system receives a second request to delete this email  the counter gets decremented again   and we may encounter the following situation  the file is still attached to an email  but the counter is already at zero   It s essential not to lose data  We can t have a situation where a user opens an email and discovers there s no attachment there  That said  storing some redundant files on disks is no big deal  All we need is a mechanism that unambiguously decides if the counter was correctly set to zero  That s why we have one more field   magic   The algorithm is simple  Along with the hash of a file  we store a randomly generated number in an email  All requests to upload or delete a file take this random number into account  in case of an upload request  this number is added to the current value of the magic number  if it s a delete request  this random number is subtracted from the current value of the magic number   Thus  if all the emails have incremented and decremented the counter the correct number of times  the magic number will also equal zero  Otherwise  the file must not be deleted   Let s consider an example  We have a file named sha1  It s uploaded once  and the email generates a random  magic  number for it  which is equal to 345   Then a new email arrives with the same file  It generates its own magic number  123  and uploads the file  The new magic number is added to the current value of the magic number  345   and the counter gets incremented by one  As a result  what we have now in FileDB is the magic number with the value of 468 and the counter set to 2   After the user deletes the second email  the magic number generated for this email is subtracted from the current value of the magic number  468   and the counter gets decremented by one   Let s first take a look at the normal course of events  The user deletes the first email  and the magic number and the counter both become zero  It means the data is consistent  and the file can be deleted   Now  suppose something goes wrong  the second email sends two delete requests  The counter being equal to zero means there are no more links to the file  but the magic number  which equals 222  signals a problem  the file can t be deleted until the data is consistent   Let s develop the situation a little more  Suppose the first email also gets deleted  In this case  the magic number   123  still signals inconsistency   As a safety measure  once the counter becomes zero but the magic number doesn t  in our case  the magic number is 222  and the counter is zero   the file is assigned the do not delete flag  This way  even if   after a series of deletes and uploads   both the magic number and the counter somehow become zero  we ll still know that this file is problematic and must not be deleted   Now back to FileDB  Every entity has a set of flags  Whether you plan to use them or not  you re going to need them  if  say  a file needs to be marked as undeletable    We have all the file attributes  except for where it s physically located  This place is identified by a server  IP  and a disk  There should be two such servers and two such disks   But one disk stores lots of files  in our case  about 1 000 000   which means these records will be identified by the same disk pair in FileDB  so it would be wasteful to store this information in FileDB  Let s put it into a separate table  PairDB  linked to FileDB via a disk pair ID   I guess it goes without saying that  apart from a disk pair ID  we ll also need a flags field  I ll jump ahead a bit and mention that this field signals whether the disks are locked  say  one disk crashed and the other s being copied  so no new data can be written to either of them    Also  we need to know how much free space each disk has   hence the corresponding fields   To make sure everything works fast  both FileDB and PairDB must be RAM resident  We were using Tarantool 1 5  but now the latest version should be used  FileDB has five fields  20  4  4  4 and 4 bytes long   which sums up to 36 bytes  Also  each record has a 16 byte header plus one byte length pointer per field  which brings the total record size to 57 bytes   Tarantool allows specifying the minimum allocation size  so memory associated overheads can be kept close to zero  We ll be allocating the exact amount of memory needed for one record  We have 12 000 000 000 files    57   12   10      1024     637 GB  But that s not all  we ll also need an index on the sha1 field  which adds 12 more bytes to the total record size    12   12   10      1024     179 GB  All in all  800 GB of RAM is needed  But let s not forget about replication  which doubles the amount of RAM required   If we buy machines with 256 GB of RAM on board  we ll need eight of them   We can assess the size of PairDB  The average file size is 1 MB and disk capacity is 1 TB  which allows storing about 1 000 000 files on a single disk  so we need some 28 000 disks  One PairDB record describes two disks  Therefore  PairDB contains 14 000 records   it s negligible compared to FileDB   File upload  Now that we got the database structure out of the way  let s turn to the API for interacting with the system  At first glance  we need the upload and deletemethods  But don t forget about deduplication  chances are  the file we re trying to upload is already in the storage  and it doesn t make sense to upload it a second time  So  the following methods are necessary   inc sha1  magic    increments the counter  If a file doesn t exist  returns an error  Let s recall we also need a magic number that helps prevent incorrect file deletions     increments the counter  If a file doesn t exist  returns an error  Let s recall we also need a magic number that helps prevent incorrect file deletions  upload sha1  magic    should be called if inc has returned an error  which means this file doesn t exist and must be uploaded     should be called if inc has returned an error  which means this file doesn t exist and must be uploaded  dec sha1  magic    called if a user deletes an email  The counter is decremented first     called if a user deletes an email  The counter is decremented first  GET  sha1   downloads a file via HTTP   Let s take a closer look at what happens during upload  For the daemon that implements this interface  we chose the IProto protocol  Daemons must scale well to any number of machines  so they don t store states  Suppose we receive a request via a socket   The command name tells us the header length  so we read the header first  Now  we need to know the length of the origin len file  It s necessary to choose a couple of servers to upload it  We just retrieve all the records  a few thousands  from PairDB and use the standard algorithm for finding a needed pair  take a segment with the length equal to the sum of free spaces on all pairs  randomly pick a point on this segment and choose whatever pair this point belongs to   However  choosing a pair this way is risky  Suppose all our disks are 90  full   and then we add a new empty disk  It s highly likely that all new files will be uploaded to this disk  To avoid this problem  we should sum not the free space of a disk pair  but the nth root of this free space   So  we ve chosen a pair  but our daemon is a streaming one  and if we start uploading a file to the storage  there s no turning back  That said  before uploading a real file  we upload a small test file first  If the test upload is successful  we read filecontent from the socket and upload it to the storage  otherwise  another pair is chosen  SHA 1 hash can be read on the fly  so it is also checked during upload   Let s now examine a file upload from a loader to a chosen disk pair  On machines that contain the disks  we set up Nginx and use the WebDAV protocol  An email arrives  FileDB doesn t have this file yet  so it needs to be uploaded to a disk pair via a loader   But nothing prevents another user from receiving the same email  suppose two recipients are specified  Remember  FileDB doesn t have this file yet  so one more loader will be uploading this very file and may pick the same disk pair for upload   Nginx will likely resolve this situation correctly  but we need to control the whole process  so we re saving the file with a complex name   The part of the name in red is where each loader puts a random number  Thus  the two PUT methods don t overlap and upload two different files  Once Nginx responds with 201  OK   the first loader performs an atomic MOVE operation  which specifies the final name of the file   When the second loader is done uploading the file and also performs MOVE  the file will get overwritten  but it s no big deal since the file is one and the same  Once the file is on the disks  a new record needs to be added to FileDB  Our version of Tarantool is divided into two spaces  We ve only been using space0 so far   However  instead of simply adding a new record to FileDB  we re using a stored procedure that either increments the file counter or adds a new record to FileDB  Why so  In the time that has passed since the loader made sure the file doesn t exist in FileDB  uploaded it and proceeded to add a new record to FileDB  someone else could have already uploaded this file and added the corresponding record  We ve considered this very case above  two recipients are specified for one email  so two loaders start uploading the file  once the second loader finishes the upload  it ll also proceed to add a new record to FileDB   In this case  the second loader just increments the file counter   Let s now take a look at the dec method  Our system has two high priority tasks  reliably write a file to disk and quickly give it back to a client from this disk  Physically deleting a file generates a certain workload and slows down these two tasks  That s why we perform deletions offline  The dec method itself decrements the counter  If the latter becomes zero  just like the magic number  it means nobody needs the file anymore  so we move the corresponding record from space0 to space1 in Tarantool   decrement  sha1  magic    counter    current_magic    magic    if  counter    0    current_magic    0    move sha1  space1         Valkyrie  Each storage has a Valkyrie daemon that monitors data integrity and consistency   and it works with space1  There s one daemon instance per disk  The daemon iterates over all the files on a disk and checks if space1 contains a record corresponding to a particular file  which would mean this file should be deleted   But after the dec method is called and a file is moved to space1  it may take Valkyrie a while to find this file  It means that in the time span between these two events the file may be re uploaded and thus moved to space0 again   That s why Valkyrie also checks if a file exists in space0  If this is the case and pair_id of the corresponding record points to a pair of disks this Valkyrie instance is running on  the record is deleted from space1   If no record is found in space0  then a file is a potential candidate for deletion  However  between a request to space0 and the physical deletion of a file there s still a certain time window in which a new record corresponding to this file may appear in space0  To deal with it  we put the file in quarantine   Instead of deleting the file  we append deleted and a timestamp to its name  It means we ll physically delete the file at timestamp   some time specified in the config file  If a crash occurs and the file is deleted by mistake  the user will come to reclaim it  We ll be able to restore it and resolve the issue without losing any data   Now  recall there are two disks  with a Valkyrie instance running on each  The two instances are not synched  Hence the question  when should the record be deleted from space1   We ll do two things  First  for the file in question let s make one of the Valkyrie instances the master  It s easily done by using the first bit of the file name  if it equals zero  disk0 is the master  otherwise  disk1 is the master   Now  let s introduce a processing delay  Recall that when a record sits in space0  it contains the magic field for checking consistency  When the record is moved to space1  this field is not used  so we ll put there a timestamp corresponding to the time this record appeared in space1  This way  the master Valkyrie instance will be processing records in space1 at once  whereas the slave will be adding some delay to the timestamp and will be processing and deleting records from space1 a little later   There s one more advantage to this approach  If a file was mistakenly put in quarantine on the master  it ll be evident from the log once we query the master  Meanwhile  the client that requested the file will fall back to the slave   and the user will receive the needed file   So  we have considered the situation where the Valkyrie daemon locates a file named sha1  and this file  being a potential candidate for deletion  has a corresponding record in space1  What other variants are possible   Suppose a file s on a disk  but FileDB doesn t have any corresponding record  If  in the case considered above  the master Valkyrie instance  for some reason  hasn t been working for some time  it would mean the slave s had enough time to put the file in quarantine and delete the corresponding record from space1  In this case  we also put the file in quarantine by using sha1 deleted timestamp   Another situation  a record exists but points to a different disk pair  This may happen during upload if two recipients are specified for one email  Recall the scheme   What happens if the second loader uploads the file to a different pair than the first one  It will increment the counter in space0  but the disk pair where the file was uploaded will contain some junk files  What we need to do is make sure these files can be read and they match sha1  If everything s fine  such files can be deleted at once   Also  Valkyrie may encounter a file that has been put in quarantine  If the quarantine is over  this file will get deleted   Now  imagine Valkyrie encounters a good file  It needs to be read from disk  checked for integrity and compared to sha1  Then Valkyrie needs to query the second disk to see if it has this same file  A simple HEAD method is enough here  the daemon running on the second disk will itself check the file integrity  If the file is corrupt on the first disk  it s immediately copied over from the second disk  If the second disk doesn t have the file  its copy is uploaded from the first disk   We re left with the last situation  which has to do with disk problems  If any problems with a disk are identified in the course of system monitoring  the problematic disk is put in service  read only  mode  while on the second disk  the UNMOVE operation is run  It effectively distributes all the files on disk two among other disk pairs   Result  Let s get back to where we started  Our email storage used to look like this   We managed to reduce the total size by 18 PB after implementing the new system   Emails now occupy 32 PB  25    indexes  75    files   Thanks to the 18 petabyte cut  we didn t have to buy new hardware for quite some time   P S  About SHA 1  As of now  there are no known examples of SHA 1 collisions  There exist collision examples for its internal compression function  SHA 1 freestart collision   though  Given 12 billion files  the probability of a hash collision is less than 10  38   But let s assume it s possible  In this case  when a file is requested by its hash value  we check if it has the correct size and CRC32 checksum that were saved in the indexes of the corresponding email during upload  That is to say  the requested file will be provided if and only if it was uploaded along with this email,"[720 805 50 1051 933 1177 1130 957 1310 136 383]"
805,training-dataset/engineering/1221.txt,engineering,Why Uber Engineering Switched from Postgres to MySQLby Evan Klitzke  Introduction  The early architecture of Uber consisted of a monolithic backend application written in Python that used Postgres for data persistence  Since that time  the architecture of Uber has changed significantly  to a model of microservices and new data platforms  Specifically  in many of the cases where we previously used Postgres  we now use Schemaless  a novel database sharding layer built on top of MySQL  In this article  we ll explore some of the drawbacks we found with Postgres and explain the decision to build Schemaless and other backend services on top of MySQL   The Architecture of Postgres  We encountered many Postgres limitations   Inefficient architecture for writes  Inefficient data replication  Issues with table corruption  Poor replica MVCC support  Difficulty upgrading to newer releases  We ll look at all of these limitations through an analysis of Postgres s representation of table and index data on disk  especially when compared to the way MySQL represents the same data with its InnoDB storage engine  Note that the analysis that we present here is primarily based on our experience with the somewhat old Postgres 9 2 release series  To our knowledge  the internal architecture that we discuss in this article has not changed significantly in newer Postgres releases  and the basic design of the on disk representation in 9 2 hasn t changed significantly since at least the Postgres 8 3 release  now nearly 10 years old    On Disk Format  A relational database must perform a few key tasks   Provide insert update delete capabilities  Provide capabilities for making schema changes  Implement a multiversion concurrency control  MVCC  mechanism so that different connections have a transactional view of the data they work with  Considering how all of these features will work together is an essential part of designing how a database represents data on disk   One of the core design aspects of Postgres is immutable row data  These immutable rows are called  tuples  in Postgres parlance  These tuples are uniquely identified by what Postgres calls a ctid  A ctid conceptually represents the on disk location  i e   physical disk offset  for a tuple  Multiple ctids can potentially describe a single row  e g   when multiple versions of the row exist for MVCC purposes  or when old versions of a row have not yet been reclaimed by the autovacuum process   A collection of organized tuples form a table  Tables themselves have indexes  which are organized as data structures  typically B trees  that map index fields to a ctid payload   Typically  these ctids are transparent to users  but knowing how they work helps you understand the on disk structure of Postgres tables  To see the current ctid for a row  you can add  ctid  to the column list in a WHERE clause   uber  local  uber   SELECT ctid    FROM my_table LIMIT 1     RECORD 1                                          ctid    0 1     other fields here     To explain the details of the layout  let s consider an example of a simple users table  For each user  we have an auto incrementing user ID primary key  the user s first and last name  and the user s birth year  We also define a compound secondary index on the user s full name  first and last name  and another secondary index on the user s birth year  The DDL to create such a table might be like this   CREATE TABLE users   id SERIAL  first TEXT  last TEXT  birth_year INTEGER  PRIMARY KEY  id     CREATE INDEX ix_users_first_last ON users  first  last   CREATE INDEX ix_users_birth_year ON users  birth_year    Note the three indexes in this definition  the primary key index plus the two secondary indexes we defined   For the examples in this article we ll start with the following data in our table  which consists of a selection of influential historical mathematicians   id first last birth_year 1 Blaise Pascal 1623 2 Gottfried Leibniz 1646 3 Emmy Noether 1882 4 Muhammad al Khw rizm  780 5 Alan Turing 1912 6 Srinivasa Ramanujan 1887 7 Ada Lovelace 1815 8 Henri Poincar  1854  As described earlier  each of these rows implicitly has a unique  opaque ctid  Therefore  we can think of the internal representation of the table like this   ctid id first last birth_year A 1 Blaise Pascal 1623 B 2 Gottfried Leibniz 1646 C 3 Emmy Noether 1882 D 4 Muhammad al Khw rizm  780 E 5 Alan Turing 1912 F 6 Srinivasa Ramanujan 1887 G 7 Ada Lovelace 1815 H 8 Henri Poincar  1854  The primary key index  which maps ids to ctids  is defined like this   id ctid 1 A 2 B 3 C 4 D 5 E 6 F 7 G 8 H  The B tree is defined on the id field  and each node in the B tree holds the ctid value  Note that in this case  the order of the fields in the B tree happens to be the same as the order in the table due to the use of an auto incrementing id  but this doesn t necessarily need to be the case   The secondary indexes look similar  the main difference is that the fields are stored in a different order  as the B tree must be organized lexicographically  The  first  last  index starts with first names toward the top of the alphabet   first last ctid Ada Lovelace G Alan Turing E Blaise Pascal A Emmy Noether C Gottfried Leibniz B Henri Poincar  H Muhammad al Khw rizm  D Srinivasa Ramanujan F  Likewise  the birth_year index is clustered in ascending order  like this   birth_year ctid 780 D 1623 A 1646 B 1815 G 1854 H 1887 F 1882 C 1912 E  As you can see  in both of these cases the ctid field in the respective secondary index is not increasing lexicographically  unlike in the case of an auto incrementing primary key   Suppose we need to update a record in this table  For instance  let s say we re updating the birth year field for another estimate of al Khw rizm  s year of birth  770 CE  As we mentioned earlier  row tuples are immutable  Therefore  to update the record  we add a new tuple to the table  This new tuple has a new opaque ctid  which we ll call I  Postgres needs to be able to distinguish the new  active tuple at I from the old tuple at D  Internally  Postgres stores within each tuple a version field and pointer to the previous tuple  if there is one   Accordingly  the new structure of the table looks like this   ctid prev id first last birth_year A null 1 Blaise Pascal 1623 B null 2 Gottfried Leibniz 1646 C null 3 Emmy Noether 1882 D null 4 Muhammad al Khw rizm  780 E null 5 Alan Turing 1912 F null 6 Srinivasa Ramanujan 1887 G null 7 Ada Lovelace 1815 H null 8 Henri Poincar  1854 I D 4 Muhammad al Khw rizm  770  As long as two versions of the al Khw rizm  row exist  the indexes must hold entries for both rows  For brevity  we omit the primary key index and show only the secondary indexes here  which look like this   first last ctid Ada Lovelace G Alan Turing E Blaise Pascal A Emmy Noether C Gottfried Leibniz B Henri Poincar  H Muhammad al Khw rizm  D Muhammad al Khw rizm  I Srinivasa Ramanujan F  birth_year ctid 770 I 780 D 1623 A 1646 B 1815 G 1854 H 1887 F 1882 C 1912 E  We ve represented the old version in red and the new row version in green  Under the hood  Postgres uses another field holding the row version to determine which tuple is most recent  This added field lets the database determine which row tuple to serve to a transaction that may not be allowed to see the latest row version   Replication  When we insert a new row into a table  Postgres needs to replicate it if streaming replication is enabled  For crash recovery purposes  the database already maintains a write ahead log  WAL  and uses it to implement two phase commit  The database must maintain this WAL even when streaming replication is not enabled because the WAL allows the atomicity and durability aspects of ACID   We can understand the WAL by considering what happens if the database crashes unexpectedly  like during a sudden power loss  The WAL represents a ledger of the changes the database plans to make to the on disk contents of tables and indexes  When the Postgres daemon first starts up  the process compares the data in this ledger with the actual data on disk  If the ledger contains data that isn t reflected on disk  the database corrects any tuple or index data to reflect the data indicated by the WAL  It then rolls back any data that appears in the WAL but is from a partially applied transaction  meaning that the transaction was never committed    Postgres implements streaming replication by sending the WAL on the master database to replicas  Each replica database effectively acts as if it s in crash recovery  constantly applying WAL updates just as it would if it were starting up after a crash  The only difference between streaming replication and actual crash recovery is that replicas in  hot standby  mode serve read queries while applying the streaming WAL  whereas a Postgres database that s actually in crash recovery mode typically refuses to serve any queries until the database instance finishes the crash recovery process   Because the WAL is actually designed for crash recovery purposes  it contains low level information about the on disk updates  The content of the WAL is at the level of the actual on disk representation of row tuples and their disk offsets  i e   the row ctids   If you pause a Postgres master and replica when the replica is fully caught up  the actual on disk content on the replica exactly matches what s on the master byte for byte  Therefore  tools like rsync can fix a corrupted replica if it gets out of date with the master   Consequences of Postgres s Design  Postgres s design resulted in inefficiencies and difficulties for our data at Uber   Write Amplification  The first problem with Postgres s design is known in other contexts as write amplification  Typically  write amplification refers to a problem with writing data to SSD disks  a small logical update  say  writing a few bytes  becomes a much larger  costlier update when translated to the physical layer  The same issue arises in Postgres  In our previous example when we made the small logical update to the birth year for al Khw rizm   we had to issue at least four physical updates   Write the new row tuple to the tablespace Update the primary key index to add a record for the new tuple Update the   first   last   index to add a record for the new tuple Update the birth_year index to add a record for the new tuple  In fact  these four updates only reflect the writes made to the main tablespace  each of these writes needs to be reflected in the WAL as well  so the total number of writes on disk is even larger   What s noteworthy here are updates 2 and 3  When we updated the birth year for al Khw rizm   we didn t actually change his primary key  nor did we change his first and last name  However  these indexes still must be updated with the creation of a new row tuple in the database for the row record  For tables with a large number of secondary indexes  these superfluous steps can cause enormous inefficiencies  For instance  if we have a table with a dozen indexes defined on it  an update to a field that is only covered by a single index must be propagated into all 12 indexes to reflect the ctid for the new row   Replication  This write amplification issue naturally translates into the replication layer as well because replication occurs at the level of on disk changes  Instead of replicating a small logical record  such as  Change the birth year for ctid D to now be 770   the database instead writes out WAL entries for all four of the writes we just described  and all four of these WAL entries propagate over the network  Thus  the write amplification problem also translates into a replication amplification problem  and the Postgres replication data stream quickly becomes extremely verbose  potentially occupying a large amount of bandwidth   In cases where Postgres replication happens purely within a single data center  the replication bandwidth may not be a problem  Modern networking equipment and switches can handle a large amount of bandwidth  and many hosting providers offer free or cheap intra data center bandwidth  However  when replication must happen between data centers  issues can quickly escalate  For instance  Uber originally used physical servers in a colocation space on the West Coast  For disaster recovery purposes  we added servers in a second East Coast colocation space  In this design we had a master Postgres instance  plus replicas  in our western data center and a set of replicas in the eastern one   Cascading replication limits the inter data center bandwidth requirements to the amount of replication required between just the master and a single replica  even if there are many replicas in the second data center  However  the verbosity of the Postgres replication protocol can still cause an overwhelming amount of data for a database that uses a lot of indexes  Purchasing very high bandwidth cross country links is expensive  and even in cases where money is not an issue it s simply not possible to get a cross country networking link with the same bandwidth as a local interconnect  This bandwidth problem also caused issues for us with WAL archival  In addition to sending all of the WAL updates from West Coast to East Coast  we archived all WALs to a file storage web service  both for extra assurance that we could restore data in the event of a disaster and so that archived WALs could bring up new replicas from database snapshots  During peak traffic early on  our bandwidth to the storage web service simply wasn t fast enough to keep up with the rate at which WALs were being written to it   Data Corruption  During a routine master database promotion to increase database capacity  we ran into a Postgres 9 2 bug  Replicas followed timeline switches incorrectly  causing some of them to misapply some WAL records  Because of this bug  some records that should have been marked as inactive by the versioning mechanism weren t actually marked inactive   The following query illustrates how this bug would affect our users table example   SELECT   FROM users WHERE id   4   This query would return two records  the original al Khw rizm  row with the 780 CE birth year  plus the new al Khw rizm  row with the 770 CE birth year  If we were to add ctid to the WHERE list  we would see different ctid values for the two returned records  as one would expect for two distinct row tuples   This problem was extremely vexing for a few reasons  To start  we couldn t easily tell how many rows this problem affected  The duplicated results returned from the database caused application logic to fail in a number of cases  We ended up adding defensive programming statements to detect the situation for tables known to have this problem  Because the bug affected all of the servers  the corrupted rows were different on different replica instances  meaning that on one replica row X might be bad and row Y would be good  but on another replica row X might be good and row Y might be bad  In fact  we were unsure about the number of replicas with corrupted data and about whether the problem had affected the master   From what we could tell  the problem only manifested on a few rows per database  but we were extremely worried that  because replication happens at the physical level  we could end up completely corrupting our database indexes  An essential aspect of B trees are that they must be periodically rebalanced  and these rebalancing operations can completely change the structure of the tree as sub trees are moved to new on disk locations  If the wrong data is moved  this can cause large parts of the tree to become completely invalid   In the end  we were able to track down the actual bug and use it to determine that the newly promoted master did not have any corrupted rows  We fixed the corruption issue on the replicas by resyncing all of them from a new snapshot of the master  a laborious process  we only had enough capacity to take a few replicas out of the load balancing pool at a time   The bug we ran into only affected certain releases of Postgres 9 2 and has been fixed for a long time now  However  we still find it worrisome that this class of bug can happen at all  A new version of Postgres could be released at any time that has a bug of this nature  and because of the way replication works  this issue has the potential to spread into all of the databases in a replication hierarchy   Replica MVCC  Postgres does not have true replica MVCC support  The fact that replicas apply WAL updates results in them having a copy of on disk data identical to the master at any given point in time  This design poses a problem for Uber   Postgres needs to maintain a copy of old row versions for MVCC  If a streaming replica has an open transaction  updates to the database are blocked if they affect rows held open by the transaction  In this situation  Postgres pauses the WAL application thread until the transaction has ended  This is problematic if the transaction takes a long amount of time  since the replica can severely lag behind the master  Therefore  Postgres applies a timeout in such situations  if a transaction blocks the WAL application for a set amount of time  Postgres kills that transaction   This design means that replicas can routinely lag seconds behind master  and therefore it is easy to write code that results in killed transactions  This problem might not be apparent to application developers writing code that obscures where transactions start and end  For instance  say a developer has some code that has to email a receipt to a user  Depending on how it s written  the code may implicitly have a database transaction that s held open until after the email finishes sending  While it s always bad form to let your code hold open database transactions while performing unrelated blocking I O  the reality is that most engineers are not database experts and may not always understand this problem  especially when using an ORM that obscures low level details like open transactions   Postgres Upgrades  Because replication records work at the physical level  it s not possible to replicate data between different general availability releases of Postgres  A master database running Postgres 9 3 cannot replicate to a replica running Postgres 9 2  nor can a master running 9 2 replicate to a replica running Postgres 9 3   We followed these steps to upgrade from one Postgres GA release to another   Shut down the master database   Run a command called pg_upgrade on the master  which updates the master data in place  This can easily take many hours for a large database  and no traffic can be served from the master while this process takes place   Start the master again   Create a new snapshot of the master  This step completely copies all data from the master  so it also takes many hours for a large database   Wipe each replica and restore the new snapshot from the master to the replica   Bring each replica back into the replication hierarchy  Wait for the replica to fully catch up to all updates applied by the master while the replica was being restored   We started out with Postgres 9 1 and successfully completed the upgrade process to move to Postgres 9 2  However  the process took so many hours that we couldn t afford to do the process again  By the time Postgres 9 3 came out  Uber s growth increased our dataset substantially  so the upgrade would have been even lengthier  For this reason  our legacy Postgres instances run Postgres 9 2 to this day  even though the current Postgres GA release is 9 5   If you are running Postgres 9 4 or later  you could use something like pglogical  which implements a logical replication layer for Postgres  Using pglogical  you can replicate data among different Postgres releases  meaning that it s possible to do an upgrade such as 9 4 to 9 5 without incurring significant downtime  This capability is still problematic because it s not integrated into the Postgres mainline tree  and pglogical is still not an option for people running on older Postgres releases   The Architecture of MySQL  In addition to explaining some of Postgres s limitations  we also explain why MySQL is an important tool for newer Uber Engineering storage projects  such as Schemaless  In many cases  we found MySQL more favorable for our uses  To understand the differences  we examine MySQL s architecture and how it contrasts with that of Postgres  We specifically analyze how MySQL works with the InnoDB storage engine  Not only do we use InnoDB at Uber  it s perhaps the most popular MySQL storage engine   InnoDB On Disk Representation  Like Postgres  InnoDB supports advanced features like MVCC and mutable data  An exhaustive discussion of InnoDB s on disk format is outside the scope of this article  instead  we ll focus on its core differences from Postgres   The most important architectural difference is that while Postgres directly maps index records to on disk locations  InnoDB maintains a secondary structure  Instead of holding a pointer to the on disk row location  like the ctid does in Postgres   InnoDB secondary index records hold a pointer to the primary key value  Thus  a secondary index in MySQL associates index keys with primary keys   first last id  primary key  Ada Lovelace 7 Alan Turing 5 Blaise Pascal 1 Emmy Noether 3 Gottfried Leibniz 2 Henri Poincar  8 Muhammad al Khw rizm  4 Srinivasa Ramanujan 6  In order to perform an index lookup on the  first  last  index  we actually need to do two lookups  The first lookup searches the table and finds the primary key for a record  Once the primary key is found  a second lookup searches the primary key index to find the on disk location for the row   This design means that InnoDB is at a slight disadvantage to Postgres when doing a secondary key lookup  since two indexes must be searched with InnoDB compared to just one for Postgres  However  because the data is normalized  row updates only need to update index records that are actually changed by the row update  Additionally  InnoDB typically does row updates in place  If old transactions need to reference a row for the purposes of MVCC MySQL copies the old row into a special area called the rollback segment   Let s follow what happens when we update al Khw rizm  s birth year  If there is space  the birth year field in the row with id 4 is updated in place  in fact  this update always happens in place  as the birth year is an integer that occupies a fixed amount of space   The birth year index is also updated in place to reflect the new date  The old row data is copied to the rollback segment  The primary key index does not need to be updated  nor does the  first  last  name index  If we have a large number of indexes on this table  we still only have to update the indexes that actually index over the birth_year field  So say we have indexes over fields like signup_date  last_login_time  etc  We don t need to update these indexes  whereas Postgres would have to   This design also makes vacuuming and compaction more efficient  All of the rows that are eligible to be vacuumed are available directly in the rollback segment  By comparison  the Postgres autovacuum process has to do full table scans to identify deleted rows   Replication  MySQL supports multiple different replication modes   Statement based replication replicates logical SQL statements  e g   it would literally replicate literal statements such as  UPDATE users SET birth_year 770 WHERE id   4    Row based replication replicates altered row records  Mixed replication mixes these two modes  There are various tradeoffs to these modes  Statement based replication is usually the most compact but can require replicas to apply expensive statements to update small amounts of data  On the other hand  row based replication  akin to the Postgres WAL replication  is more verbose but results in more predictable and efficient updates on the replicas   In MySQL  only the primary index has a pointer to the on disk offsets of rows  This has an important consequence when it comes to replication  The MySQL replication stream only needs to contain information about logical updates to rows  The replication updates are of the variety  Change the timestamp for row X from T_1 to T_2   Replicas automatically infer any index changes that need to be made as the result of these statements   By contrast  the Postgres replication stream contains physical changes  such as  At disk offset 8 382 491  write bytes XYZ   With Postgres  every physical change made to the disk needs to be included in the WAL stream  Small logical changes  such as updating a timestamp  necessitate many on disk changes  Postgres must insert the new tuple and update all indexes to point to that tuple  Thus  many changes will be put into the WAL stream  This design difference means that the MySQL replication binary log is significantly more compact than the PostgreSQL WAL stream   How each replication stream works also has an important consequence on how MVCC works with replicas  Since the MySQL replication stream has logical updates  replicas can have true MVCC semantics  therefore read queries on replicas won t block the replication stream  By contrast  the Postgres WAL stream contains physical on disk changes  so Postgres replicas cannot apply replication updates that conflict with read queries  so they can t implement MVCC   MySQL s replication architecture means that if bugs do cause table corruption  the problem is unlikely to cause a catastrophic failure  Replication happens at the logical layer  so an operation like rebalancing a B tree can never cause an index to become corrupted  A typical MySQL replication issue is the case of a statement being skipped  or  less frequently  applied twice   This may cause data to be missing or invalid  but it won t cause a database outage   Finally  MySQL s replication architecture makes it trivial to replicate between different MySQL releases  MySQL only increments its version if the replication format changes  which is unusual between various MySQL releases  MySQL s logical replication format also means that on disk changes in the storage engine layer do not affect the replication format  The typical way to do a MySQL upgrade is to apply the update to one replica at a time  and once you update all replicas  you promote one of them to become the new master  This can be done with almost zero downtime  and it simplifies keeping MySQL up to date   Other MySQL Design Advantages  So far  we ve focused on the on disk architecture for Postgres and MySQL  Some other important aspects of MySQL s architecture cause it to perform significantly better than Postgres  as well   The Buffer Pool  First  caching works differently in the two databases  Postgres allocates some memory for internal caches  but these caches are typically small compared to the total amount of memory on a machine  To increase performance  Postgres allows the kernel to automatically cache recently accessed disk data via the page cache  For instance  our largest Postgres replicas have 768 GB of memory available  but only about 25 GB of that memory is actually RSS memory faulted in by Postgres processes  This leaves more than 700 GB of memory free to the Linux page cache   The problem with this design is that accessing data via the page cache is actually somewhat expensive compared to accessing RSS memory  To look up data from disk  the Postgres process issues lseek 2  and read 2  system calls to locate the data  Each of these system calls incurs a context switch  which is more expensive than accessing data from main memory  In fact  Postgres isn t even fully optimized in this regard  Postgres doesn t make use of the pread 2  system call  which coalesces seek   read operations into a single system call   By comparison  the InnoDB storage engine implements its own LRU in something it calls the InnoDB buffer pool  This is logically similar to the Linux page cache but implemented in userspace  While significantly more complicated than Postgres s design  the InnoDB buffer pool design has some huge upsides   It makes it possible to implement a custom LRU design  For instance  it s possible to detect pathological access patterns that would blow out the LRU and prevent them from doing too much damage  It results in fewer context switches  Data accessed via the InnoDB buffer pool doesn t require any user kernel context switches  The worst case behavior is the occurrence of a TLB miss   which is relatively cheap and can be minimized by using huge pages    Connection Handling  MySQL implements concurrent connections by spawning a thread per connection  This is relatively low overhead  each thread has some memory overhead for stack space  plus some memory allocated on the heap for connection specific buffers  It s not uncommon to scale MySQL to 10 000 or so concurrent connections  and in fact we are close to this connection count on some of our MySQL instances today   Postgres  however  use a process per connection design  This is significantly more expensive than a thread per connection design for a number of reasons  Forking a new process occupies more memory than spawning a new thread  Additionally  IPC is much more expensive between processes than between threads  Postgres 9 2 uses System V IPC primitives for IPC instead of lightweight futexes when using threads  Futexes are faster than System V IPC because in the common case where the futex is uncontended  there s no need to make a context switch   Beside the memory and IPC overhead associated with Postgres s design  Postgres seems to simply have poor support for handling large connection counts  even when there is sufficient memory available  We ve had significant problems scaling Postgres past a few hundred active connections  While the documentation is not very specific about why  it does strongly recommend employing an out of process connection pooling mechanism to scale to large connection counts with Postgres  Accordingly  using pgbouncer to do connection pooling with Postgres has been generally successful for us  However  we have had occasional application bugs in our backend services that caused them to open more active connections  usually  idle in transaction  connections  than the services ought to be using  and these bugs have caused extended downtimes for us   Conclusion  Postgres served us well in the early days of Uber  but we ran into significant problems scaling Postgres with our growth  Today  we have some legacy Postgres instances  but the bulk of our databases are either built on top of MySQL  typically using our Schemaless layer  or  in some specialized cases  NoSQL databases like Cassandra  We are generally quite happy with MySQL  and we may have more blog articles in the future explaining some of its more advanced uses at Uber   Evan Klitzke is a staff software engineer within Uber Engineering s core infrastructure group  He is also a database enthusiast and joined Uber as an engineering early bird in September 2012,"[805 720 463 1177 933 1051 50 1310 1130 136 1060]"
933,training-dataset/engineering/748.txt,engineering,Real time Deduping At Scale   Tapjoy EngineeringAt Tapjoy  analytics is core to our platform  On an average day  we re processing over 2 million messages per minute through our analytics pipeline  These messages are generated by various user events in our platform and eventually aggregated for a close to realtime view of the system   For our technology stack  we use Kafka to store the messages  Spark to consume and aggregate the data  and Postgres to store the aggregates  In order to deliver the data in a timely and accurate manner  we aimed to build a system that was scalable  highly available  performant  and cost effective   In this post we look at how we handled the at least once semantics of our Kafka pipeline through real time deduping in order to ensure the integrity   accuracy of the data   The problem  Messaging systems tend to provide one of two types of delivery semantics for consumers  at least once and at most once  At most once semantics means messages may get lost  at least once semantics means messages may be duplicated  For critical systems  data loss is unacceptable  For example  if we re reporting ad conversions that our partners will use to make spend decisions then our data must be accurate   At Tapjoy  every messaging system we use provides at least once delivery guarantees  SQS  RabbitMQ  Kafka  etc  However  we want to process messages exactly once in order to avoid over reporting analytics  This poses the question  if the same message gets written to Kafka twice  how do you ensure that message is only processed once by our client application code   This is where dedupe comes into play   The requirements  Given the scale of the pipeline and the downstream dependence on accurate data  any dedupe solution must be   Scalable   This is more or less a given    we need to be able to scale this solution as we continue to track more analytics data or during periods of high throughput when our product goes live on a large partner   Highly Available   Most of our infrastructure is designed to be highly available so we can sustain failures and keep the platform running  Given the real time constraints of this system  we want to be able to quickly fail over to another cluster   set of servers in case  for example  an availability zone goes awry in AWS   Persistent   We need the dedupe datastore to persist even when other services in the stack  like Spark  restart  If we lost the data on every restart  we d be increasing the potential for dupes every time we deploy   Fast   2 million messages   minute is a lot to dedupe and we don t want to run into a situation where our Spark ETL can t keep up   Cost effective   We need to keep a keen eye on cost especially as the solution scales up  Sure  we could decide to add 15 servers in order to have high performance but economically it s not acceptable   Efficient storage   We want to guard against the possibility that we would receive the same message over and over again if a message producer was broken  A goal we set for ourselves was the ability to dedupe over a period of 12 to 24 hours  This means that if a message comes in at 08 00 AM  we should be able to detect any duplicate messages that come in by 08 00 PM  To store data over a broad time range  but keep storage costs down  we need a way to evict old data in the dedupe store  We ll be looking for a technology that supports Least Recently Used  or LRU  eviction   Approach  At a high level  the approach we re looking at isn t much different than a standard dedupe algorithm  For every message   Grab the message s transaction id  the id that identifies the event that occurred  Perform a  set if not already set  atomic command on the transaction id in our data store If the key was successfully set  then process the message  otherwise  it s a duplicate so ignore  This is the approach we use in a lot of places and it s the general strategy we use here   In Spark  ETL jobs are run in multiple stages  At any point  a stage may fail and need to be retried or a job may get killed and the application needs to get restarted  The impact of this is that if a job dies after we ve marked the transaction ids as having been seen  then we need to make sure the job is able to process that same message when it s re run   To address this issue  our dedupe process is tweaked to the following   Grab the message s transaction id  the id that identifies the event that occurred  Attempt a  set if not already set  command where the key is the transaction id and the value is the Kafka partition   offset of the message  that s the important part  If the key was successfully set  then process the message  If the key already existed and the value matches the current Kafka partition   offset  process it as a retry due to a previous failure  otherwise  it s a duplicate so ignore   The Kafka partition   offset is what we refer to as the  owner id  in our system    it s basically which message in Kafka owns the transaction id that s being processed  This allows ETL jobs to get retried multiple times if failures are encountered   Technology choice  While we ve previously used Memcached for detecting duplicate messages in other systems like SQS  we instead landed on Redis   Elasticache for this system  The reasoning was   Scalability  Similar to Memcached  Redis can be scaled by sharding your keys across multiple servers  Since both are restrained by available memory on a single box  the only way to scale out to support more keys is by adding more servers  Availability  Elasticache  and Redis  support high availability through replication and automatic failover  Amazon s Elasticache exposes a DNS endpoint and will automatically fail over to the backup if the primary Redis server fails  This is a bit cleaner than Memcached where we must write to multiple servers from the client in order to achieve high availability  This adds a bit more complexity than what we would prefer on the client  Persistence  Since Redis can store the data both in memory and on disk  process restarts avoid data loss  On the other hand  Memcached loses all of the data on restart  We would face the same limitation if we store the data in process in Spark  Fast   Cheap   Efficient storage   This is largely dependent on how the client uses the technology regardless of whether it s Redis  Memcached  or something else  However  the Redis API provides more flexibility around how commands are executed  which leads to much better performance  Details of Redis optimizations are reviewed later in this post   Other solutions  such as a memory mapped hashmap in the Spark ETL process or use of our Postgresql cluster  were considered  However  each fell short in different areas ranging from difficulty in scaling   providing high availability to ensuring efficient performance and data storage   All things considered  Redis proved to be the best fit for our needs   Optimizing storage efficiency  As mentioned earlier  we wanted a solution that allowed us to detect duplicate messages that were 12 24 hours apart  With 2 million messages   minute  memory quickly becomes a valuable resource  As a result  it was important for us to store data in the most effective manner in order to make best use of the available memory in Redis   Naive implementation  In the case of Tapjoy s analytics pipeline  message transaction ids are UUIDs and are mapped to the Kafka partition   offset when it was first seen  e g  partition 3  offset 81418110694    In a naive implementation  the UUID keys are 36 bytes  32 characters with 4 dashes  and the values that represent the Kafka owner id are a comma delimited string like  1 64942845052   13 bytes   With an approximate 64 byte overhead per key in Redis  this would leave the amount of memory required for 24 hours of data to be    overhead   key_bytes   value_bytes    keys_per_minute   60   24  64   36   13    2 000 000   60   24   300GB  That s going to cost a lot to have that memory  so let s see if we can do better     Key   Value efficiency  To get our first improvement in memory efficiency  consider the best way to represent the keys   values being stored   Under the hood  UUIDs are actually made from 16 bytes of information  If we can take those 16 bytes and convert them to their binary representation  this would result in more than a 50  reduction in memory usage of just the key data  This would mean that instead of representing a UUID as  ce059644 18a0 4f27 bc2b c2a2d4d4e7bf   we could represent it as   xbf  xd4 x91V IG x9f5 x9a xf9 x16K x9b xc8    This translation can be done like so   val uuid   java util UUID fromString  ce059644 18a0 4f27 bc2b c2a2d4d4e7bf   val hi   uuid getMostSignificantBits val lo   uuid getLeastSignificantBits ByteBuffer allocate 16  putLong hi  putLong lo  array       Array  50  5   106  68  24   96  79  39   68  43   62   94   44   44   25   65   Using this as a reference  we can use a similar algorithm to represent the owner id  We know the owner id is a combination of the partition id and the offset id  As a result  the question becomes  what s the smallest number of bytes we need to represent both of those values   Partitions are represented in Kafka as integers  4 bytes   but our maximum partition number is only 6  If we assume that we ll never go above 32 767 partitions for a single Kafka topic  then we can represent the partition in 2 bytes  2 15   1    Offsets are represented in Kafka as longs  8 bytes   but the only requirement that we have is that we don t see the same offset value within a partition over a 24 hour period  The reason we have this requirement is that we only intend on storing up to 24 hours worth of message ids in Redis  Therefore  if we end up effectively reusing offset values  it s okay as long as it happens outside of that 24 hour window   Given the above  if we target 6 bytes to represent the Kafka offset  this means we ll rollover every  2 47   1  messages  281 474 976 710 656 messages   This is way beyond what we normally see in a 24 hour period  we see about 2B messages    This means our owner id effectively gets calculated like so   ByteBuffer allocate 2  putShort partition toShort  array    ByteBuffer allocate 8  putLong offset  array slice 2  8   The end result is that instead of requiring 13 bytes to represent Partition 1   Offset 64942845052  it only requires 8 bytes   Given all of this  the new calculation is    per_key_overhead   key_bytes   value_bytes    keys_per_minute   1440  64   16   8    2 000 000   60   24   230GB  Great  we ve saved ourselves 70GB    but the amount of memory being stored is still pretty high  Let s see if we can do any better     Reduce number of keys  At this point  the only thing we could potentially do to reduce the amount of memory required is reduce the number of keys that we re actually storing  At first blush  this seems impossible    if we need to store 2B unique message ids in Redis  then how can you reduce the number of keys  This is where Redis s Hash operations come into play  e g  HSET   HGET   HSETNX   etc     Our original inspiration for using Redis s Hash data type to reduce the total number of 1st level Redis keys being stored came from a blog post by the Instagram Engineering team  previously posted here   Under the hood  Redis will try to use a ziplist data structure to store the keys values for Hashes  This structure is significantly more memory efficient than using normal Redis keys if you don t have different TTL requirements for each key   To utilize Redis Hash keys  we effectively need to create buckets of message ids  In our case  we can bucket messages using a timestamp within the message  We know through our own testing  and Instagram s own results  that we see diminishing returns with buckets containing more than 1 000 keys  We also don t want to exceed the hash max zipmap entries value since the data would no longer be stored as a ziplist  In evaluating memory usage  we find that storing between 100 and 1 000 keys per bucket is ideal  This gives us room to grow without having too much of an impact on memory consumption   If we target 100 keys per bucket  the next piece is to figure out how many buckets we re going to need  That calculation is   buckets   keys_per_minute   keys_per_bucket buckets_per_min   2 000 000   100   20 000 buckets_per_sec   20 000   60   333  We can now run our new calculation for how much memory will be consumed     per_key_overhead   key_bytes   value_bytes    keys_per_minute    1440     per_bucket_overhead   bucket_key_bytes    buckets_per_minute   1440   1   16   8    2 000 000    60   24     64   14    20 000   60   24    72 000 000 000   2 246 400 000    69GB  In the above formula   The per_bucket_overhead is the same as the per_key_overhead used in previously calculations  is the same as the used in previously calculations The per_key_overhead is now the number of additional bytes Redis s Hash data type requires to store the key  is now the number of additional bytes Redis s Hash data type requires to store the key The bucket key is assumed to be 14 bytes  combination of message timestamp   3 digit bucket slice for sharding purposes   With this change  we get our biggest savings    we re now at 30  of previous usage   Optimizing performance  Alright  now that we ve got our technology and we know how we re storing the data  the next piece is to figure out how we do it fast   Recall we re processing 2M messages per minute in our Spark cluster  Relative to everything else that happens in our ETL every minute  we have to make sure the dedupe process doesn t add too much time to our jobs  If deduping was too expensive  our ETL could start falling behind   For our system  we wanted to target a total time of 1s to be spent generating requests for Redis  storing the data  and removing the dupes every minute   Naive implementation  Despite the fact that we re using this bucketed approach above  we still need to write 2M keys to Redis  The naive implementation here is to send 2 million HSETNX and HGET commands to Redis  see HSETNX   For example   message_ids each do  message_id  owner_id  client HSETNX message_id  owner_id    claim owner client HGET message_id    determine winner end  As you might imagine  this is pretty slow  This requires 4M separate commands to be processed by Redis and 4M responses to be processed by the client  Even if you were to shard the buckets across multiple Redis servers and parallelize commands  the performance is unacceptable   For 2M messages sharded across 3 Redis instances  it takes about 26s  Way beyond what we re trying to target   Pipelined implementation  Redis offers pipelining that allows commands to continue to get processed by Redis even while the client is reading the results  As documented by Redis  this can have a pretty good impact on your performance  For example   client pipeline do  pipeline  message_ids each do  message_id  owner_id  pipeline HSETNX message_id  owner_id    claim owner pipeline HGET message_id    determine winner end end  If we take the same approach as above  but with the commands sent via a pipeline instead  it takes about 3s to process that same 2M messages  Pretty good     about an 8x performance improvement  though still above our target   Lua scripting  Redis supports Lua scripting that allows us to run logic as close to the data as possible  within the Redis process itself   and control what actually gets returned to the client  The reasons Lua scripting might offer us a benefit here are   By being close to the data  we can reduce both the number of round trips with the client and the number of reads in Redis  By calling HSETNX multiple times on the same bucket  Redis is forced to read through and parse the contents of the Hash key multiple times  If we can contain that to a single command  we can reduce processing time  By controlling the results  we can reduce the amount of data sent back to the client by only returning message ids that were detected as dupes  We re currently transmitting all message ids   owner ids back to the client  This puts a lot of strain on the client to process  In the common case  we don t really get any dupes    so there s really no need to have all of that data come back to the client   The dedupe lua script we landed on ended up looking like so   HMGET bucket  message_ids  message_ids each do if existing owner matches or not set not dupe else dupe end end HMSET bucket  new_message_ids_and_values  return dupes  By switching to a Lua script  we bring the time required down to 1 4s    about as close as we re going to get to our target   Scaling out  Update  Expanded on this to explain our need for sharding  So far we ve built a solution that addresses our needs today    great  At this point  the question is  how do we architect this solution to allow us to scale to 10x the traffic we re handling today  Instead of 2 million messages   minute  how could we meet our memory and performance requirements while processing 20 million messages   minute  This is where sharding comes into play   Scaling memory  First  let s talk about memory requirements  If we assume that 10x the amount of traffic is roughly equivalent to 10x memory usage over a 24 hour period  we re suddenly talking about storing 700GB worth of data in Redis  Since we use AWS  the largest possible instance available in Elasticache only has 237GB  This means we need a way to be able to store message ids across multiple Redis instances    and always have the same message ids stored on the same Redis instance   At the same time  we don t necessarily want to scale up 237GB at a time  Instead  we want to add capacity as we need it    which means using smaller servers  This helps us keep costs down as much as we can  In a sharding solution  by simply adding new Redis clusters to increase memory capacity  we ve effectively created limitless capacity to store messages   Scaling performance  As we scale  there are going to be some concerns around performance  If we scale out to 10x traffic  we expect the processing time to dedupe all of those messages to be 10x as well  14 seconds instead of 1 4 seconds    To meet our real time needs  this is unacceptable   The naive solution is to simply parallelize requests to our Redis cluster  However  there s a big gotcha here  Redis is single threaded  This means  at any time  Redis is only able to process a single request at a time  To truly parallelize requests to Redis and gain the performance benefits of doing so  we need to run requests across multiple Redis clusters   Sharding solution  So what does a sharding solution like this actually look like  Let s say  for example  we re going to use 3 Redis servers and 300 bucket keys per second  Based on the contents of a message  we re going to have to choose a Redis instance   bucket shard to store the message id  To choose a shard  you typically use a hashing function  For example   If the number of shards changes  then the hashing function is going to start storing messages in different shards than we were before   In order to account for this scenario  we effectively have the concept of a  previous  Redis configuration and a  current  Redis configuration   Previous  configurations apply to all messages with a timestamp prior to some maximum time  Any messages with transaction times after that maximum time will start using the  current   new  configuration  This allows us to effectively transition from one cluster to another or from one hashing algorithm to another without losing any of the data we ve stored from previous hours   Final thoughts  Deduping at scale is a hard problem and there are certainly other effective solutions employed elsewhere in the industry  In fact  even the folks at Kafka are working on a standard solution  Typically you hope that either your actions are idempotent or accuracy isn t so critical that a complex system like this is necessary  In our case  this was important to ensuring the integrity of our analytics pipeline   Hopefully  this post serves as a helpful example of tackling a thorny engineering problem   Oh no  I have a huge pipeline of data flying around and I can t keep track of whether I ve seen this before    in a deliberate way,"[933 1353 1177 136 805 720 463 1051 50 1310 954]"
954,training-dataset/engineering/680.txt,engineering,Swapping  memory limits  and cgroupsThis week there was a bug at work  and I learned something new about memory   swap   cgroups   Understanding how memory usage works has been an ongoing project for a while   back in December  I wrote How much memory is my process using  which explains how memory works on Linux   So I felt surprised and worried yesterday when something was happening with memory on Linux that I didn t understand  Here s the situation and why I was confused   We had some machines running builds  They were swapping  They had say 30GB of RAM in total  15GB was being used by some processes and 15GB by the filesystem cache  I was really confused about why these machines were swapping  because   I know about memory  If 15GB of memory is being used by the filesystem cache  the OS can always free that memory  There s no reason to swap   I then learned about the  swappiness  setting  and that if  swappiness  is high  then the OS is more likely to swap  even if it doesn t absolutely need to  We tried setting sysctl vm swappiness 1   which lets you tell the operating system  no  really  please don t swap  just take memory away from the filesystem cache instead   The machine continued to swap  I was confused   After a while  we turned off swap and things got worse  Some processes started being OOM killed   the OOM killer on Linux will kill processes if you run out of memory  But why  The boxes had free memory  didn t they  In my head I had an axiom  if a computer has free memory  there is no reason the processes on it should get OOM killed   Obviously there was something I did not understand   I finally looked at the output of dmesg  which is how you see messages the Linux kernel prints about what it s up to  to understand why the processes were being OOM killed  And then there was this magic word  cgroups   Everything became clear pretty quickly   the processes that were being killed were in a cgroup  which we talked about back in this namespaces   groups post   cgroups can have memory limits   which are like  you are only allowed to use 15GB of memory otherwise your processes will get killed by the OOM killer     which are like  you are only allowed to use 15GB of memory otherwise your processes will get killed by the OOM killer  and this memory limit was the reason the processes were being killed even though there was free memory   swap   cgroup memory limits   a little surprising  My model of memory limits on cgroups was always  if you use more than X memory  you will get killed right away   It turns out that that assumptions was wrong  If you use more than X memory  you can still use swap   And apparently some kernels also support setting separate swap limits  So you could set your memory limit to X and your swap limit to 0  which would give you more predictable behavior  Swapping is weird and confusing   Anyway  we found out through all this that the processes in question had recently started using much more memory for very understandable reasons  and rolled back that change  and everything made sense again   And more importantly than everything making sense  the build system was happy again   does swap even make sense   It s not completely clear to me under what circumstances having swap on a computer at all even makes sense  It seems like swap has some role on desktop computers   I am not sure though if any of the servers we operate benefit by having swap enabled  This seems like it would be a good thing to understand   Like I hear the advice  no  just always turn off swap  it will be better overall  a lot and maybe that is the right thing  I think the reason that swap is considered bad in production systems is that it has weird unpredictable effects and predictability is good   Somewhat relately  this swap insanity article looks really interesting   understanding memory models is cool  I learned  the vm swappiness exists and you can use it to make a machine more or less likely to swap  that when Linux OOM kills a process in a cgroup   container    it actually prints a bunch of very useful stuff about the memory usage of everything else in the cgroup at the time  I should remember to look at dmesg earlier on   It s really important to me to understand what s happening on the computers that I work with   when something happens like  this computer is swapping and I don t know WHY  it bothers me a lot   Now if I ever see a process mysteriously swapping hopefully I will remember about memory limits,"[954 1177 50 1310 957 1353 1051 1130 933 463 383]"
957,training-dataset/engineering/671.txt,engineering,Things to learn about LinuxI asked on Twitter today what Linux things they would like to know more about  I thought the replies were really cool so here s a list  many of them could be discussed on any Unixy OS  some of them are Linux specific   tcp ip   networking stuff  what is a port socket   seccomp  systemd  IPC  interprocess communication  pipes   permissions  setuid  sticky bits  how does chown work  how the shell uses fork   exec  how can I make my computer a router   process groups  session leaders  shell job control  memory allocation  how do heaps work  what does malloc do   ttys  how do terminals work  process scheduling  drivers  what s the difference between Linux and Unix  the kernel  modern X servers  how does X11 work   Linux s zero copy API  sendfile  splice  tee   what is dmesg even doing  how kernel modules work  embedded stuff  realtime  GPIO  etc  btrfs  QEMU KVM  shell redirection  HAL  chroot  filesystems   inodes  what is RSS  how do I know how much memory my process is using  iptables  what is a network interface exactly   what is syslog and how does it work   how are logs usually organized   virtual memory  BPF  bootloader  initrd  kernel parameters  the ip command  command what are all the files that are not file files   dev  stdin   proc   sys   dbus  sed and awk  namespaces  cgroups  docker  SELinux  AppArmor  debuggers  what s the difference between threads and processes   if unix is text based  how do desktop environments like GNOME fit in   how does the  man  system work   kpatch  kgraph  kexec  more about the stack  Are C vars really stack slots  How tf do setjmp and longjmp work   package management  mounts and vfs  this is great for so many reasons   I need to draw 11 more drawings about Linux this month and these are such great ideas there are many things I don t know on this list and it s a cool reminder of how much interesting stuff there still is to learn  A few of these I barely even know what they are  dbus  SELinux  or only have a pretty sketchy notion  seccomp  how X11 works  many more  it s also a cool reminder of how far I ve come   I at least know where to start with most of the things on this list  even if I definitely could not explain a lot of them in detail without looking some stuff up   Also I sometimes want to remind people that you too could write interesting blog posts   drawings on the internet   for instance  what is dmesg even doing  is an interesting topic  and totally possible to learn about   I just read dmesg on Wikipedia and now I know more,"[957 1177 954 50 1310 1051 1353 720 1130 933 805]"
1051,training-dataset/engineering/1306.txt,engineering,Pyflame  Uber Engineering s Ptracing Profiler for Pythonby Evan Klitzke  At Uber  we make an effort to write efficient backend services to keep our compute costs low  This becomes increasingly important as our business grows  seemingly small inefficiencies are greatly magnified at Uber s scale  We ve found flame graphs to be an effective tool for understanding the CPU and memory characteristics of our services  and we ve used them to great effect with our Go and JavaScript services  In order to get high quality flame graphs for Python services  we wrote a high performance profiler called Pyflame  implemented in C    In this article  we explore design considerations and some unique implementation characteristics that make Pyflame a better alternative for profiling Python code   Deterministic Profilers  Python offers several built in deterministic profilers via the profile and cProfile modules  The deterministic profilers in Python  profile and cProfile  work by using the sys settrace   facility to install a trace function that s run at various points of interest  such as the start and end of each function and at the beginning of each logical line of code  This mechanism yields high resolution profiling information  but it has a number of shortcomings   High Overhead  The first drawback is its extremely high overhead  we commonly see it slowing down programs by 2x  Worse  we found this overhead to cause inaccurate profiling numbers in many cases  The cProfile module has difficulty accurately reporting timing statistics for methods that run very quickly because the profiler overhead itself is significant in those cases  Many engineers don t use profiling information because they can t trust its accuracy   Lack of Full Call Stack Information  The second problem with the built in deterministic profilers is that they don t record full call stack information  The built in profiling modules only record information going up one stack level  which limits the usefulness of these modules  For example  when one decorator is applied to a large number of functions  the decorator frequently shows up in the callees and callers sections of the profiling output  with the true call information obscured due to the flattened call stack information  This clutter makes it difficult to understand true callee and caller information   Lack of Services Written for Profiling  Finally  the built in deterministic profilers require that the code be explicitly instrumented for profiling  A common problem for us is that many services weren t written with profiling in mind  Under high load  we may encounter serious performance problems with the service and want to collect profiling information quickly  Since the code isn t already instrumented for profiling  there s no way to immediately start collecting profiling information  If the load is severe enough  we may need an engineer to write code to enable a deterministic profiler  typically by adding an RPC method to turn it on and another to dump profiling data   This code then needs to be reviewed  tested  and deployed  The whole cycle might take several hours  which is not fast enough for us   Sampling Profilers  There are also a number of third party sampling profilers for Python  These sampling profilers typically work by installing a POSIX interval timer  which periodically interrupts the process and runs a signal handler to record stack information  Sampling profilers sample the profiled process rather than deterministically collecting profiling information  This technique is effective because the sampling resolution can be dialed up or down  When the sampling resolution is high  the profiling data is more accurate but performance suffers  For instance  the sampling resolution can be set high to get detailed profiles with a correspondingly high amount of overhead  or it can be set low to get less detailed profiles with less overhead   A few limitations come with sampling profilers  First  they typically come with high overhead because they re implemented in Python  Python itself is not fast  especially compared to C or C    In fact  the cProfile deterministic profiler is implemented in C for this reason  With these sampling profilers  getting acceptable performance often means setting the timer frequency to something that is relatively coarse grained   The other limitation is that the code needs to be explicitly instrumented for profiling  just as with deterministic profilers  Therefore  existing sampling profilers lead to the same problem as before  under high load  we want to profile some code  only to realize we have to rewrite it first   Pyflame to the Rescue  With Pyflame  we wanted to maintain all of the possible profiling benefits   Collect the full Python stack  all the way to its root  Emit data in a format that could be used to generate a flame graph  Have low overhead  Work with processes not explicitly instrumented for profiling  More importantly  we aimed to avoid all existing limitations  It might sound impossible to ask for all of the features without making any sacrifices  But it s not as impossible as it sounds   Using ptrace for Python Profiling  Most Unix systems implement a special process trace system call called ptrace 2   ptrace is not part of the POSIX specification  but Unix implementations like BSD  OS X  and Linux all provide a ptrace implementation that allows a process to read and write to arbitrary virtual memory addresses  read and write CPU registers  deliver signals  etc  If you ve ever used a debugger like GDB  then you ve used software that s implemented using ptrace   It s possible to use ptrace to implement a Python profiler  The idea is to periodically ptrace attach to the process  use the memory peeking routines to get the Python stack trace  and then detach from the process  Specifically with Linux ptrace  a profiler can be written using the request types PTRACE_ATTACH  PTRACE_PEEKDATA  and PTRACE_DETACH  In theory  this is pretty straightforward  In practice  it s complicated by the fact that recovering the stack trace using only the PTRACE_PEEKDATA request is very low level and unintuitive   First  we ll briefly cover how the PTRACE_PEEKDATA request works on Linux  This request type reads data at a virtual memory address in the traced process  The signature of the ptrace system call on Linux looks like this   long ptrace enum __ptrace_request request  pid_t pid  void  addr  void  data    When using PTRACE_PEEKDATA  the following function arguments are supplied   Parameter Value request PTRACE_PEEKDATA pid The traced process ID addr The memory address to read data Unused   NULL by convention   The value ptrace 2  returns is the long at that memory address  On Linux with GCC  the long type is defined to be the same as the native architecture word size  so on a 32 bit system the return value is a signed 32 bit integer  and on a 64 bit system the return value is a signed 64 bit integer   There is one additional complication here  On error  ptrace 2  returns the value  1 and sets errno appropriately  However  the data at the address we re reading could actually contain the value  1  Therefore  a return value of  1 is ambiguous  was there an error  or did that memory address really contain  1  To resolve this ambiguity when reading data  we must first clear errno and then make the ptrace request  Then  if the return value is  1  we check to see if errno was set during the ptrace call  Curiously  the ambiguity in the interpretation of the return value is an artifact of the GNU libc wrapper  The underlying system call on Linux uses the return value to signal an error  and it stores the peeked data into the data field  which must be supplied in this case   Extracting the Thread State  Internally  Python is structured with one or more independent interpreters  and each sub interpreter tracks one or more threads  Due to the global interpreter lock  only one thread actually runs at any given time  The currently executing thread information is held in a global variable named _PyThreadState_Current  which is normally not exported by the Python C API  From this variable  Pyflame can find the current frame object  From the current frame  the entire stack trace can be unwound  Therefore  once Pyflame locates the memory location of _PyThreadState_Current  it can recover the rest of the stack information by using PTRACE_PEEKDATA  as described above  Pyflame follows the thread state pointer to a frame object  and each frame object has a back pointer to another frame  The final frame has a back pointer to NULL  Each frame object holds fields which can be used to recover the filename  line number  and function name for the frame   The most difficult part of this is actually locating the address of _PyThreadState_Current  Depending on how the Python interpreter was compiled  there are two possibilities   In the default build mode  _PyThreadState_Current is a regular symbol with a well known address in the text area that does not change  While the address doesn t change  the actual value for the address depends on what compiler is used  what compilation flags are used  etc   When Python is compiled with  enable shared   the _PyThreadState_Current symbol is not built into Python itself but in a dynamic library   In this case  address space layout randomization  ASLR  means that the virtual memory address is different every time the interpreter runs   In either case on Linux  the symbol can be located by parsing the ELF information from the interpreter  or from libpython in a dynamic build   Linux systems include a header file called elf h that has the necessary definitions to parse an ELF file  Pyflame memory maps the file and then uses these ELF struct definitions to parse out the relevant ELF structures  If the special ELF  dynamic section indicates that the build links against libpython  then Pyflame proceeds to parse that file  Next  it locates the _PyThreadState_Current symbol in the  dynsym ELF section  either from the Python executable itself or from libpython  depending on the build mode   For dynamic Python builds  the address of _PyThreadState_Current has to be augmented with the ASLR offset  This is done by reading  proc PID maps to get the virtual memory mapping offsets for the process  The offset from this file is added to the value read from libpython to get the true virtual memory address for the symbol   Interpreting Frame Data  In the source code for the Python interpreter  you see regular C syntax for dereferencing pointers and accessing struct fields      frame has type void  void  f_code    struct _frame  frame  f_code  void  co_filename    PyCodeObject  f_code  co_filename   Instead  Pyflame has to use ptrace to read from the Python process s virtual memory space and manually implement pointer dereferencing  The following is a representative snippet of code from Pyflame that emulates the code in the previous code listing   const long f_code   PtracePeek  pid  frame   offsetof    _frame  f_code    const long co_filename   PtracePeek pid  f_code   offsetof PyCodeObject  co_filename     Here  a helper method called PtracePeek   implements the call to ptrace with the PTRACE_PEEKDATA parameter and handles error checking  Pointers are represented as unsigned longs  and the offsetof macro is used to compute struct offsets  The ptrace code in Pyflame is more verbose than regular C code  but the logical structure of the two code listings is exactly the same   The code to actually extract filenames and line numbers is interesting  Python 2 stores filenames using a type called PyStringObject  which simply stores the string data inline  at a fixed offset from the head of the struct   Python 3 has much more complicated string handling due to the internal unification of the string type and unicode types  For strings that contain only ASCII data  the raw string data can be found inline in the struct in much the same way  Pyflame currently only supports all ASCII filenames on Python 3   Implementing the line number decoding for Pyflame was one of the more challenging parts of developing Pyflame  Python stores the line number data in an interesting data structure called the  line number table   in a field in the code object called f_lnotab  There s a file called lnotab_notes txt in the Python source code that explains the exact data structure  First  know that the Python interpreter works by translating regular Python code to a lower level bytecode representation  Typically  one line of Python code expands to many bytecode instructions  Bytecode instructions therefore typically advance much more quickly than lines of code  Instead of storing and updating a line number field in each frame  the Python interpreter uses a compressed data structure that associates bytecode offsets to line number offsets  The bytecode to line number data structure is computed once for each code object  The line number can be computed implicitly for any bytecode instruction   Profiling Dockerized Services Containers  At Uber  we run most of our services in Linux containers using Docker  One of the interesting challenges of building Pyflame was making it work with Linux containers  Typically  processes on the host cannot interact with containerized processes  However  in most cases the root user can ptrace containerized processes  and this is how we run Pyflame in production at Uber   Docker containers use mount namespaces to isolate filesystem resources between the host and the container  Pyflame has to access files inside the container to access the correct ELF file and compute symbol offsets  Pyflame enters the container s mount namespace using the setns 2  system call  First  Pyflame compares  proc self ns fs to  proc PID ns fs  If they differ  Pyflame enters the process s mount namespace by calling open 2  on  proc PID ns fs and then calling setns 2  on the resulting file descriptor  By retaining an open file descriptor to the original  proc self ns fs  Pyflame can subsequently return to its original namespace  i e   escape the container    Like What You re Reading  Try Pyflame Yourself   We ve found Pyflame to be an extremely useful tool for profiling Python code at Uber and finding inefficient code paths to optimize  We re releasing Pyflame today as free software  under the Apache 2 0 license  Please try it out and let us know if you find any bugs  And  as always  we love getting pull requests  so please send them if you have improvements   Evan Klitzke is a staff software engineer within Uber Engineering s core infrastructure group  He joined Uber just over 4 years ago in September 2012   Photo Credits for Header   Flame  by Liz West  licensed under CC BY 2 0  Image cropped for header dimensions   Like what you re reading  Sign up for our newsletter for updates from the Uber Engineering blog,"[1051 1310 50 1177 463 720 933 805 957 954 1060]"
1060,training-dataset/engineering/1502.txt,engineering,Rewriting Uber Engineering s Android Rider App with Deep Scope Hierarchiesby Brian Attwell  When we rewrote the Uber iOS and Android rider apps in 2016  we subdivided the app into a deep hierarchy of dependency injection scopes  This allows more features to be written without knowledge of one another and reduces the amount of stale state in the application  thereby increasing engineering velocity and facilitating growth   While iOS frameworks have ViewControllers that always supported composite patterns  AOSP  Android Open Source Project  frameworks have not traditionally supported deeply nested controllers or scopes well  As a result  writing Android applications with deep scope hierarchies is difficult and uncommon  Uber s Rider app is an example of how this difficulty can be worth overcoming to solve structural challenges   The Uber rider app s UX contains states that share common objects such as the map  for example  the home screen view  the product selection view  and the airport terminal view  shown in Figure 1   The existence of shared objects between different screens means the application cannot be composed of a distinct set of Activities  e g  HomeActivity  AirportActivity  and ProductSelectionActivity  unless shared objects are stored as singletons in global scope  To address this  a pattern is needed to control how objects are shared between screens and subscreens   In short  we needed an effective scoping pattern to support the new Android Uber rider app   So  how did we do this  In this article  we discuss   The shallow scope pattern we used in the old rider app and its problems  The deep scope pattern we used in the rewritten rider app and its improvements  Different architectural frameworks and how they support deep scope hierarchies   The Old Uber Rider App  A Two Level Scope Hierarchy  By 2016  it became apparent that we had outgrown the existing Uber rider app  as it could no longer keep up with the scale and speed we needed to maintain and grow operations   Most of the old rider app is contained inside a single activity  called LoggedInActivity  that behaves like a controller with a single scope  There are multiple distinct screens within LoggedInActivity that perform drastically different behaviors  Therefore  LoggedInActivity is composed of one additional layer of sub controller classes that handles different UI and business logic  Some of these sub controllers are shown below in Figure 3   All of the sub controllers such as CommuteController  PricingController  and AirportController live in the same LoggedInScope and all dependency injection objects can be shared between all sub controllers  See the dagger snippet below    LoggedInScope   Component modules   LoggedInActivityModule class  dependencies   AppComponent class   public interface LoggedInComponent    void inject LoggedInActivity activity       Consider controllers such as the AirportController  refer to the middle screen in Figure 1   this AirportController exists in memory for the entire duration of LoggedInActivity  This condition has several downsides   Coupling   Other controllers  such as CommuteController  can read and write from AirportController s utility objects since they share a scope  This inevitably leads to coupling between unrelated controllers and features  Stale state   All utility objects used by AirportController exist in memory after AirportController has finished displaying itself on the screen  This forces engineers to write error prone reset logic for AirportController s utility objects when AirportController is hidden and then later shown again on the screen  State combinatorics   Since objects and controllers remain in memory for the duration of the entire LoggedInScope  classes need to know how they should behave during every LoggedIn substate  This requires engineers to write larger classes with more error prone state condition logic   Hard to update and test   When adding a new substate  engineers need to consider how dozens of controllers  such as AirportController  and utility classes should behave in this new state  The Uber rider app has a shockingly large number of features as a consequence of operating in hundreds of different cities with varying constraints and optimizations  so a two level scope hierarchy quickly becomes unmanageable   Are Three Level Scope Hierarchies Better   With two layers of scopes  the old rider app is a drastic example of what happens when your scope hierarchy is too shallow  Unfortunately  creating an additional layer of scopes by giving each controller its own scope fails to solve most of the problems outlined above   Objects often need to be shared between two or three controllers  With only three layers of scopes  these shared objects need to be stored in the LoggedInScope  Over time  the third scope layer becomes  thin  as many objects are refactored into the second layer of scopes   Clearly  adding a third layer of scopes is an improvement  But this still causes a  fat  second scope layer with lots of the same problems   New Rider App  Deep Scope Hierarchy  Given that two and three level scope hierarchies have major problems  we did not limit ourselves to a set number of scope layers when we were developing the new app  Instead  we created new intermediary scope layers wherever useful  For example  the PreRequest scope is used to store objects that need to be shared by all PreRequest screen states such as Home  ProductSelection  and RefinementSteps   This pattern results in a deep hierarchy of scopes  see Figure 4   providing two high level benefits   No data or views are shared between siblings in the scope tree  Objects that need to be shared are stored in intermediate nodes  so leaf scopes are well encapsulated  Since no internal data is shared between scopes like Airport Door Selection  Location Refinement  and Product Selection  none of the Airport data needs to be in memory after the Airport Door Selection flow is complete  As a result  the new app s controller hierarchy can map 1 1 to its scope hierarchy    The problems in the old rider app caused by two level scope hierarchy disappear when we subdivide the application into a set of small scopes with short lifespans  per Figure 4  Consider how this new scope and hierarchy affects the Airport feature  below   Less coupling   The Airport logic cannot access any memory from sibling or cousin scopes  As a result  development of features can be done independently inside Home  Product Selection  Airport  and Location Refinement  Less stale state   The Airport Selection scope is only in memory when its logic is executing  Therefore  there is no need to write error prone reset logic when hiding the airport UI  Less state combinatorics   Most objects no longer live for the entire duration of the LoggedIn scope  For example  the Product Selection logic doesn t need to make any decisions regarding its behavior when inside the Airport Selection state because none of the Product Selection logic exists in memory during airport door selection  Easier to update and test   When adding a new substate to the application  engineers do not need to test its impact on as many existing features because of the lack of state combinatorics   Common Architectural Frameworks  There are many different ways to create deep scope hierarchies  so we had to assess all our options before settling on one  We discuss the architectures considered before we decided to rewrite the rider app using RIBs  otherwise known as Riblets   our internal architectural framework  below   MVC and VIPER  The codebase engineering inherited from the old rider app followed the MVC  Model View Controller  pattern  Common textbook patterns like MVC or VIPER are general enough that they can support deeply nested scope hierarchies  but the controller hierarchies are typically dictated by view nesting  This is inconvenient for deep scope hierarchies since many scopes do not create any views     So we didn t go with MVC or VIPER   Flow Apps  Flow was primarily designed for the purpose of supporting multiple levels of nested scopes  Since you can create viewless scopes that contain nothing except shared objects  e g   a LoggedInScope   Flow was a strong option  But other factors  for example  its lack of a corresponding iOS framework  prevented us from using it     So we didn t  go with the Flow    Conductor Apps  Frameworks like Conductor don t explicitly support scoping or nested scoping  You can add scopes to every controller if you re willing to overcome   No enforcement of DI patterns   This is important if you re going to use lots of layers of scoping   Redundant views   Conductor forces every controller to create a view  leading to redundant views when using deeply nested scope hierarchies   Given these constraints  we resisted choosing Conductor   Scoop Apps  Other applications also contain shared view objects and business data between their screens  For example  Scoop was based on an early version of Flow to formalize a controller pattern that can share views like maps without creating global state   The Scoop framework strongly emphasizes scopes  With Scoop  scopes are correlated with the navigation stack  going deeper in the navigation stack nests a scope below the current scope  For example  when transitioning from HomeController to ConfirmationController  objects can be shared between them by giving ConfirmationController access to HomeController s scope   Scoop s design provides convenient navigation and animation patterns at the cost of encouraging greater coupling from controller to controller and from controller to activity  patterns we were determined to avoid   So we didn t Scoop up this option   What s for Dinner  RIBs  Since none of the pre existing options we considered met Uber s requirements  we created our own architectural framework for our new rider app  RIBs  which we detailed shortly after its debut  Unlike Scoop  the navigation stack and scopes are decoupled  and the only shared objects between Home and Confirmation exist inside an intermediate PreRequest scope  With RIBs  using a nested scope pattern for our rider app is easy because of two design decisions   The scope boilerplate is generated for RIBs  We created an internal IntelliJ plugin that generates the RIB and Dagger 2 component  subcomponent boilerplate whenever an engineer creates a new RIB  As a result  nested scoping is a low friction norm  Scopes don t need to be coupled to views   With a deeply nested scope controller hierarchy  this functionality is useful  Many leaf scopes will want to mutate views from intermediate scopes instead of creating their own views  and many intermediate scopes will contain only business logic instead of views   Deep Scopes FTW   Deep scope hierarchies enable applications like our rider app  with its feature dense screens and shared objects between subscreens  to increase separation of concerns  reduce possibilities for stale data  and increase developer velocity   Once an app contains a deep scope hierarchy with highly decoupled controllers  it becomes easier to add powerful technologies such as static analysis to detect memory leaks  plugin based programming  and various performance optimizations   If this kind of work excites you  come be part of this story and improve the Uber rider experience for everyone through iOS and Android engineering  There s plenty more to do with RIBs and beyond  and we will be writing more on the development architecture of Uber s apps for the remainder of 2017   Brian Attwell is a software engineer on Uber s Mobile Platform team,"[1060 1177 1310 1353 805 1051 463 933 383 50 1130]"
1130,training-dataset/product/265.txt,product,User Memory Design  How To Design For Experiences That LastStreams And Snapshots  What we have learned from studies like Kahneman s is that the way we remember experiences is not related to the sum of goodness or badness that we experienced  Instead  memory relies on a few key moments and mostly ignores the rest  Experience is a stream  Memory is a collection of snapshots   Psychologists describe experience with phrases like a  constant stream of  self talk  or a  stream of transient states that vary from moment to moment   But when we remember something  we don t just  play back the tape  and re experience that stream  Our brains make an imprint of the key moments  gradually discard the noise of moment to moment experience  and heavily weight the peak and end moments   Two Selves  As Kahneman puts it  each of us has two selves  an experiencing self and a remembering self  Those selves take in the world in different and sometimes conflicting ways   On one hand  our experiencing self asks the question   How do I feel right now   and experiences the world from moment to moment  feeling things like pleasure  boredom  frustration and fear  On the other hand  the remembering self asks the question   How did I feel overall   It interprets experiences after they occur  neglects their duration and focuses on key moments like the peak and end   Ultimately  the remembering self is the boss  because it determines what we learn from our experiences  It constructs the story that we tell ourselves about our experiences that determines our future decisions  In other words  your customer s remembering self decides whether they liked your thing  whether they re going to use the thing again and what they re going to tell people about it  The experiencing self was just a passive onlooker to those decisions   User Memory Design  If the memory of an experience can differ so much from the experience itself  and the remembering self is the ultimate decider  should we throw out the whole focus on userexperience design  Should we be thinking of ourselves as user experience designers or should we be user memory designers   My answer is yes  We should be both   Let s be real  I work on the web  a fast medium where people can close the browser tab whenever they feel like it  There s obviously no value to a great ending if people never make it there  As designers  we absolutely need to focus on removing moment to moment frustrations  and that s why things like usability testing are always going to be important   because we should design for an experience that people can and will complete   But we also need to consider how that experience translates into positive memories that will make people choose to use our thing again and to tell other people about it  What we need to do is design with both selves in mind  the experiencing self and the remembering self   Don t Screw Up the Ending  Think about where your design s endings are and the ways they might be screwing things up for users  We can think about the ending as the most recent time that a user completed a session and then stepped away from the experience for some length of time  If you re doing it right  people will have lots of endings  because they ll keep coming back  As that happens  past endings become just another moment in the overall user experience  a concept that s been called scalability of experience   By the way  this is not to say that beginnings aren t also crucially important in shaping users  perceptions of your design  Plenty of studies have shown that first impressions of websites have a powerful anchoring effect on the subsequent experience  What Kahneman s research should make clear to us is that beginnings aren t the only thing that s important  Endings matter  too  And  in my experience  endings are more likely to be overlooked in the design process   Here are some examples of good endings in digital experiences   At the end of a shopping experience  the user is given the option to check out as a guest  rather than sign up for an account   Websites like GitHub and Gmail protect against accidental deletion  the ultimate unhappy ending  by making the user confirm the deletion or by allowing for undoing   And here are some examples of endings that aren t so good   After finishing an article  the user sees a handful of clickbait links to trashy sponsored content   As the user tries to leave a website  an exit modal appears and makes one last desperate attempt to capture the user s email   After going through the onboarding process with a new app  the user receives a terse plain text welcome email that looks like it was written as an afterthought   Finally  if someone has decided to unsubscribe from your marketing emails  don t be a monster and make it as difficult as possible to unsubscribe  There s an entire Tumblr website called Spot the Unsubscribe  dedicated to emails that make it ridiculously difficult to even find an unsubscribe link   Slow Down  When Appropriate   When people recall experiences  they factor in key moments  but they tend to ignore the length of the experience  This means that in some situations  you may have more freedom to control the pace of the user experience than you think you do  When it s appropriate  look for opportunities to purposefully slow things down to create a better experience   Yes  the web is a fast medium and users  attention is often hanging by a thread  but we shouldn t take the don t make me think philosophy too far  Instead of focusing on moving users through an experience as fast and thoughtlessly as possible  we can add what Andrew Grimes calls meta moments   tiny moments of reflection that prompt us to think consciously about what we re experiencing   For example  Slack uses pulsing tooltips during the onboarding flow  which add time and friction but ultimately help transform brand new users into more informed  more engaged users with a lot more potential energy to start using Slack   Psychologist Dan Ariely tells a story on his blog about meeting a locksmith and talking to him about his job,"[1130 1353 1177 383 954 805 1310 463 720 933 50]"
1177,training-dataset/engineering/642.txt,engineering,How much memory is my process using I often want to know how much memory my processes are using  When I run top   I see this   For years I ve been reading explanations of the VIRT  RSS  and SHARED columns and for years I ve been like  ugh  why is this so complicated  why can t I just know how much memory my process is using  this is silly   Today I read the super helpful article htop explained  and together with learning a little bit more about how virtual memory works on Linux  I think I understand what s going on   The short version is  the way memory works on Linux is just fundamentally more complicated than  process Y is using memory X  so asking  how much memory is my process using  can only ever have approximate answers   For all this to make sense  first we need to understand how virtual memory works and we re going to learn what a  page table  is   virtual memory   the page table  You have RAM in your computer  i have touched mine with my hands    That RAM has addresses  and data lives at addresses in RAM   When you access memory in your program  like  0x39242345    those addresses are not physical RAM addresses  They re  virtual  memory addresses  that are specific to your process  I wrote a comic about how this works   Here s a text version of that comic   every process has its own memory space  so 0x39242000 might point to  dog  in one process and  cat  in another process   might point to  dog  in one process and  cat  in another process  The virtual memory addresses like 0x39242000 map to physical RAM addresses  and diferent processes have different mappings  map to physical RAM addresses  and diferent processes have different mappings The mapping of virtual    physical addresses is different for each process  The mapping is generally done one 4 kilobyte block at a time  The virtual    physical addresses map is called the page table  I d heard about the page table before  but one thing that was really confusing to me until yesterday was   how do these lookups of virtual addresses happen  exactly  Wouldn t doing a lookup in a table every single time you access memory be really slow   The answer is that your CPU itself does the lookups in the page table  It knows which part of physical RAM the mapping for your process lives in  and then every time it executes an instruction that accesses memory  it looks up which real physical memory it needs for that instruction in RAM   When you switch processes  the kernel updates the address of the page table so that the CPU knows where to look   side note  page tables   caches  You might be thinking  wow  does that mean that every time I do a memory access I actually have to do two memory accesses   And you would be right  It is kind of expensive to do that   Your CPU helps make this faster by caching page table lookups in the translation lookaside buffer  TLB   This is a super fast cache and if your virtual memory address is in the TLB then you just need to do 1 memory access to access memory instead of 2   I think if you access your memory in a predictable way  like access a bunch of memory all at once  your CPU can do smart optimizations like  huh  I m going to need to look up that address in the page table next  I will preload it into the TLB    This is because modern CPUs are not actually linear things  but do all kinds of operations at the same time   At this point we re deeper in hardware optimizations than we need to be for the purposes for this blog post  and besides I ve already probably said something wrong about how hardware works  so let s go back to discussing how much memory our processes are using      shared memory  Now that we understand how memory accesses work on Linux  we can talk about the first complicated thing  shared memory  Here is a comic about copy on write   So  this is weird  This means that 2 processes can be using exactly the same physical memory  And worse  that shared memory could be scattered in totally random places through the process  so you could have 4MB of non shared memory  then 8MB of shared memory  then more unshared memory  etc    There s no clean way to logically split up shared and non shared memory  Basically any 4 kilobyte page could be shared or not shared  This is great for performance  less copying   and reducing memory usage  but makes it hard   So I could be running 16 Google Chrome processes  and you could have  some memory that all of them share  C libraries   some memory that only a few of them share  some memory that is shared for a while  and then slowly becomes unshared over time  You could imagine counting every single page of memory  saying  okay  that page is shared by 6 processes  I will divide it by 6 and attribute that amount to my process   This seems like a resaonable way to count memory to me  It turns out that you can actually do this somewhow via  proc  PID smaps and the PSS column but I have not investigated this yet and it is not how top reports memory use   swap and mmap  The next  and  I think  last  reason counting memory is complicated is that not all virtual memory actually maps to physical memory   You might have heard of swap    In your page table  instead of a virtual memory address being mapped to a physical address  you can map it to  EMERGENCY ASK THE OPERATING SYSTEM FOR HELP    basically null or something   When the CPU comes across this  it ll be like  what the hell is this  operating system  please fix it    the technical term is a  page fault    At this point the operating system could in principle do whatever it wants  but usually the reason this is happening is that that virtual memory address is actually some data on disk  So the operating system will go read the data from disk  put it into physical memory  and then the CPU will go merrily on its way and keep running your code   There are two normal reasons you might have memory addresses that actually map to the disk   swap   your operating system actually ran out of physical memory so it put a bunch of data from RAM on disk    your operating system actually ran out of physical memory so it put a bunch of data from RAM on disk a program asked your operating system to do it with mmap  Basically if you want to read a 1GB file  you can say  hey  please map this file into memory  and just load it lazily when i access that memory   This can have better performance than reading the file normally  and it s a very common pattern   how much memory is my program using   Okay  we know know about a bajillion things about how memory works on Linux now  Let s go back to what we saw in top    VIRT  means  this is how much virtual address space this program has   This could be mmaped files  and all kinds of stuff that does not actually live in physical memory at all  This number is not really that useful if you want to know how much RAM you re using   RSS  resident set size  is much closer to your normal notion of  how much memory I am using    it s the amount of actual physical RAM that is used by that process in some way  For example  in the table above compiz is using 1 6GB of virtual memory but only 270MB of actual real RAM   SHR is the amount of physical RAM for that process that is shared with other processes  I think you have no way of knowing how many other programs that RAM is shared with  or if it s likely to continue being shared in the future  like if it s copy on write shared memory   We can see that half of Google Chrome s memory is shared with other processes  probably other Chrome processes     MEM is the percentage of physical RAM that the process is using  so it s RSS divided by total physical memory   i feel way better  I ve been confused about this for years so it feels nice to actually understand what these numbers mean and why there are so many of them and why it s so confusing  It turns out that virtual memory systems are kinda complicated  but that if I understand the basics it helps to clear up a lot of confusion  Awesome,"[1177 50 954 1310 957 1051 1353 933 805 463 720]"
1199,training-dataset/engineering/881.txt,engineering,Fast Multi GPU collectives with NCCLToday many servers contain 8 or more GPUs  In principle then  scaling an application from one to many GPUs should provide a tremendous performance boost  But in practice  this benefit can be difficult to obtain  There are two common culprits behind poor multi GPU scaling  The first is that enough parallelism has not been exposed to efficiently saturate the processors  The second reason for poor scaling is that processors exchange too much data and spend more time communicating than computing  To avoid such communication bottlenecks  it is important to make the most of the available inter GPU bandwidth  and this is what NCCL is all about   NCCL  pronounced  Nickel   is a library of multi GPU collective communication primitives that are topology aware and can be easily integrated into your application  Initially developed as an open source research project  NCCL is designed to be light weight  depending only on the usual C   and CUDA libraries  NCCL can be deployed in single process or multi process applications  handling required inter process communication transparently  Finally  the API will be very familiar to anyone with experience using MPI s collectives   Collective Communication  Collective communication routines are common patterns of data transfer among many processors  If you have experience with MPI  then you are probably already familiar with several collective operations  For example  all reduce starts with independent arrays of N values on each of K processors and ends with identical arrays S of N values  where   for each processor k  See Figure 1   Another common collective is all gather  where each of K processors begins with an independent array of N values  and collects data from all other processors to form a result of dimension N K  as Figure 2 shows   Broadcast is a third example  Here an N element buffer on one processor is copied to all other processors  as Figure 3 shows   All of the above collectives can be performed either  out of place   with separate input and output buffers  or  in place   with overlapping input and output   There are numerous approaches to implementing collectives efficiently  However  it is critical that our implementation takes the topology of interconnects between processors into account  For example  consider a broadcast of data from GPU0 to all other GPUs in the PCIe tree topology pictured below   A two step tree algorithm is a common choice in this situation  In the first step the data is sent from the GPU0 to a second GPU  and in the second step both of these send data to the remaining processors  However  we have a choice  We can either send data from GPU0 to GPU1 in the first step and then GPU0 to GPU2 and GPU1 to GPU3 in the second  or we can perform the initial copy form GPU0 to GPU2 and then GPU0 to GPU1 and GPU2 to GPU3 in the second step  Examining the topology  it is clear that the second option is preferred  since sending data simultaneously from GPU0 to GPU2 and GPU1 to GPU3 would cause contention on the upper PCIe links  halving the effective bandwidth for this step  In general  achieving good performance for collectives requires careful attention to the interconnect topology   To optimize Broadcast bandwidth  an even better approach is to treat the PCIe topology above as a ring   The broadcast is then performed by relaying small chunks of the input around the ring from GPU0 to GPU3  Interestingly  ring algorithms provide near optimal bandwidth for nearly all of the standard collective operations  even when applied to  tree like  PCIe topologies  But note that selecting the correct ring order remains important   GPU Collectives with NCCL  In order to provide maximum bandwidth  NCCL implements ring style collectives  NCCL implicitly indexes the GPUs into the optimal ring order under the hood  This provides great performance for your application while freeing you from having to worry about specific hardware configurations   Many collectives requires a buffer for intermediate results  In order to minimize memory overhead to a few MB on each GPU  NCCL splits large collectives into many small chunks  It would be highly inefficient to launch separate kernels and cudaMemcpy calls for every step and chunk of a collective algorithm  Instead  each collective is implemented by a monolithic CUDA kernel  NCCL makes extensive use of GPUDirect Peer to Peer direct access to push data between processors  Where peer to peer direct access is not available  e g   when traversing a QPI interconnect   the pushed data is staged through a buffer in pinned system memory  Similarly  synchronization is performed by polling on volatile variables in device  or pinned system  memory   Internally  NCCL implements each collective in terms of three primitives  Copy  Reduce  and ReduceAndCopy  Each of these is optimized to efficiently transfer fine grained slices of data  4 16KB  between GPUs  The kernels have also been specifically optimized to achieve maximum bandwidth at low occupancy  As a result  NCCL can saturate a PCIe 3 0 x16 interconnect using a single block of CUDA threads  This leaves the bulk of the GPU free to execute compute tasks concurrently with the communication   NCCL currently supports the all gather  all reduce  broadcast  reduce  and reduce scatter collectives  Any number of GPUs can be used  as long as they reside in a single node   Using NCCL  The open source release of NCCL is available on Github  The library should build on any common Linux distribution and is compatible with CUDA 7 0 and later  CUDA supports direct access only for GPUs of the same model sharing a common PCIe root hub  GPUs not fitting these criteria are still supported by NCCL  though performance will be reduced since transfers are staged through pinned system memory   The NCCL API closely follows MPI  Before performing collective operations  a communicator object must be created for each GPU  The communicators identify the set of GPUs that will communicate and maps the communication paths between them  We call the set of associated communicators a clique  There are two ways to initialize communicators in NCCL  The most general method is to call ncclCommInitRank   once for each GPU   ncclResult_t ncclCommInitRank ncclComm_t  comm  int nGPUs  ncclUniqueId cliqueId  int rank    This function assumes that the GPU belonging to the specified rank has already been selected using cudaSetDevice     nGPUs is the number of GPUs in the clique  cliqueId allows the ranks of the clique to find each other  The same cliqueId must be used by all ranks  To achieve this  call ncclGetUniqueId   in one rank and broadcast the resulting uniqueId to the other ranks of the clique using the communication framework of your choice  for example  MPI_Bcast     The last argument to ncclCommInitRank   specifies the index of the current GPU within the clique  It must be unique for each rank in the clique and in the range  0  nGPUs    This index is used  for example  to identify the source GPU for a broadcast operation  or to order the contributions to an all gather   Upon successful initialization  ncclCommInitRank   returns ncclSuccess and  comm is set to the new NCCL communicator object   Internally  ncclInitRank   performs a synchronization between all communicators in the clique  As a consequence  it must be called from a different host thread for each GPU  or from separate processes  e g   MPI ranks   Specifically  calling ncclInitRank   in a single threaded loop over GPUs will result in deadlock   The second way to initialize the communicators is to use ncclCommInitAll     This is essentially a convenience routine that spares you the effort of spawning extra host threads to initialize NCCL in an otherwise single threaded application   ncclResult_t ncclCommInitAll ncclComm_t  comms  int nGPUs  int  devList    The comms argument now refers to an array of ncclComm_t objects  one for each of the nGPUs ranks in the clique  devList specifies which CUDA device gets associated with each rank  With the communicator object initialized  you can call the collectives through their host functions  such as ncclAllReduce      ncclResult_t ncclAllReduce void  sendoff  void  recvbuff  int count  ncclDataType_t type  ncclRedOp_t op  ncclComm_t comm  cudaStream_t stream    Documentation for each collective is provided in nccl h   Most arguments have analogues in the MPI collectives API  The notable exception is the stream argument  Similar to many CUDA  Async  routines  NCCL collectives schedule the operation in a stream but may return before the collective is complete  By queuing collectives and other compute kernels in separate streams  the GPU is able to overlap collective communication with more compute intensive workloads  To maximize overlap  schedule NCCL collectives on high priority streams to allow them to slip in among compute intensive grids   ncclAllReduce   must be called once for each communicator in the clique  each call providing its own send and receive buffers  etc  NCCL collectives assume that the appropriate CUDA context is already current  Because NCCL is asynchronous  a simple loop can be used to initiate a collective from a single threaded application   for  int gpu 0  gpu nGPUs    gpu    cudaSetDevice devList gpu    ncclAllReduce          Performance  The performance that NCCL collectives achieve depends on the exact topology of the computer  In the best case  all GPUs share peer access  This is most common for workstations equipped with a few GPUs  Larger servers usually sport dual CPUs  with some GPUs segregated on different IO hubs  Figure 6 shows NCCL bandwidth for various collectives measured on an NVIDIA Digits DevBox equipped with 4 GeForce GTX Titan X GPUs arranged as in Figure 4 above   The red bar at 10 4 GB s represents the bandwidth achieved by a large cudaMemcpy between two of the GPUs  specifically GPU 0 and 2 in Figure 4   NCCL sustains a high percentage of this peak bandwidth while performing communication among all four GPUs   Future Directions   Final Remarks  The goal of NCCL is to deliver topology aware collectives that can improve the scalability of your multi GPU applications  By using NCCL you can get great performance without having to think about low level hardware details   Simple code examples for both single process and MPI applications are distributed with NCCL  As a research project  we welcome your feedback as we continue to evolve the project  Please try it out and let us know what you think  For an update on the latest developments  come see my NCCL talk at GTC    parallel,"[1199 136 933 463 805 1051 50 1177 1310 1060 1353]"
1310,training-dataset/engineering/1009.txt,engineering,Dismissing Python Garbage Collection at Instagram   Instagram EngineeringDismissing Python Garbage Collection at Instagram  By dismissing the Python garbage collection  GC  mechanism  which reclaims memory by collecting and freeing unused data  Instagram can run 10  more efficiently  Yes  you heard it right  By disabling GC  we can reduce the memory footprint and improve the CPU LLC cache hit ratio  If you re interested in knowing why  buckle up   How We Run Our Web Server  Instagram s web server runs on Django in a multi process mode with a master process that forks itself to create dozens of worker processes that take incoming user requests  For the application server  we use uWSGI with pre fork mode to leverage memory sharing between master and worker processes     In order to prevent the Django server from running into OOM  the uWSGI master process provides a mechanism to restart the worker processes when its RSS memory exceeds the predefined limits   Understanding Memory  We started by looking into why worker RSS memory grows so fast right after it is spawned by the master process  One observation is that even though the RSS memory starts with 250MB  its shared memory drops very quickly   from 250MB to about 140MB within a few seconds  shared memory size can be read from  proc PID smaps    The numbers here are uninteresting because they change all the time  but the scale of shared memory dropping is very interesting   about 1 3 of the total memory  Next we wanted to understand why this shared memory becomes private memory per process at the beginning of the worker spawning   Our theory  Copy on Read  Linux kernel has a mechanism called Copy on Write  CoW  that serves as an optimization for forked processes  A child process starts by sharing every memory page with its parent  A page copied to the child s memory space only when the page is written to  for more details refer to the wiki https   en wikipedia org wiki Copy on write      But in Python land  because of reference counting  things get interesting  Every time we read a Python object  the interpreter will increase its refcount  which is essentially a write to its underlying data structure  This causes CoW  So with Python  we re doing Copy on Read  CoR     define PyObject_HEAD    _PyObject_HEAD_EXTRA    Py_ssize_t ob_refcnt     struct _typeobject  ob_type        typedef struct _object    PyObject_HEAD    PyObject   So the question is  are we copy on writing immutable objects such as the code objects  Given PyCodeObject is indeed a  sub class  of PyObject   apparently yes  Our first thought was to disable the reference counting on PyCodeObject    Attempt 1  Disable reference count on code objects  At Instagram  we do the simple thing first  Given that this was an experiment  we made some small but hacky changes to CPython interpreter  verified the reference count number didn t change on code object  and then shipped that CPython to one of our production servers     The result was disappointing because there was no change on shared memory  When we tried to figure out why  we realized we couldn t find any reliable metrics to prove our hack worked  nor could we prove the connection between the shared memory and the copy of code objects  Apparently  something was missing here  Lesson learned  prove your theory before going for it   Profiling page faults  After some googling on Copy on Write  we learned Copy on Write is associated with page faults in the system  Each CoW triggers a page fault in the process  Perf tools that come with Linux allow recording hardware software system events  including page faults  and can even provide stack trace when possible     So we went to a prod server  restarted the server  waited for it to fork  got a worker process PID  and then ran the following command   perf record  e page faults  g  p  PID   Then  we got an idea about when page faults happen in the process with stack trace   The results were different than our expectations  Rather than copying the code object  the top suspect is collect   which belongs to gcmodule c   and is called when a garbage collection is triggered  After reading how GC works in CPython  we have the following theory     CPython s GC is triggered deterministically based on the threshold  The default threshold is very low  so it kicks in at a very early stage  It maintains linked lists of generations of objects  and during GC  the linked lists are shuffled  Because the linked list structure lives with the object itself  just like ob_refcount    shuffling these objects in the linked lists will cause the pages to be CoWed  which is an unfortunate side effect      GC information is stored BEFORE the object structure      typedef union _gc_head    struct    union _gc_head  gc_next   union _gc_head  gc_prev   Py_ssize_t gc_refs     gc   long double dummy     force worst case alignment       PyGC_Head   Attempt 2  Let s try disabling GC  Well  since GC is backstabbing us  let s disable it     We added a gc disable   call to our bootstrapping script  We restarted the server  but again  no luck  If we look at perf again  we ll see gc collect is still called  and the memory is still copied  With some debugging with GDB  we found that apparently one of the third party libraries we used  msgpack  calls gc enable   to bring it back  so gc disable   at bootstrapping was washed     Patching msgpack is the last thing we would do because it leaves the door for other libraries to do the same thing in the future without us noticing  First  we need to prove disabling GC actually helps  The answer again lives in gcmodule c  As an alternative to gc disable   we did gc set_threshold 0    and this time  no libraries brought it back     With that  we successfully raised the shared memory of each worker process from 140MB to 225MB  and the total memory usage on the host dropped by 8GB per machine  This saved 25  RAM for the whole Django fleet  With such big head room  we re capable of running a lot more processes or running with a much higher RSS memory threshold  In effect  this improves the throughput of Django tier by more than 10    Attempt 3  Completely shutdown GC takes churns  After we experimented with a bunch of settings  we decided to try it on a larger scale  a cluster  The feedback was pretty quick  and our continuous deployment broke because restarting our web server became much slower with GC disabled  Usually restarting takes less than 10 seconds  but with GC disabled  it sometimes took more than 60 seconds   2016 05 02_21 46 05 57499 WSGI app 0  mountpoint     ready in 115 seconds on interpreter 0x92f480 pid  4024654  default app   It was very painful to re produce this bug because it s not deterministic  After a lot of experiments  a real re pro shows in atop  When this happened  the free memory on that host dropped to nearly zero and jumped back  forcing out all of the cached memory  Then came the moment where all the code data needed to be read from disk  DSK 100    and everything was slow     This rung a bell that Python would do a final GC before the interpreter shut down  which would cause a huge jump in memory usage in a very short period of time  Again  I wanted to prove it first  then figure out how to deal with it properly  So  I commented out the call to Py_Finalize in uWSGI s python plugin  and the problem disappeared     But apparently we couldn t just disable Py_Finalize as it was  We had a bunch of important cleanups using atexit hooks that relied on it  What we ended up doing is adding a runtime flag to CPython that would disable GC completely     Finally  we got to roll it out to a larger scale  We tried our entire fleet after this  but the continuous deployment broke again  However  this time it only broke on machines with old CPU models  Sandybridge   and was even harder to re pro  Lesson learned  always test the old clients models because they re often the easiest ones to break     Because our continuous deployment is a fairly fast procedure  to really catch what happened  I added a separate atop to our rollout command  We re able to catch a moment where cache memory goes really low  and all of uWSGI processes trigger a lot of MINFLT  minor page faults    Again  by perf profiling  we saw Py_Finalize again  Upon shutdown  other than the final GC  Python did a bunch of cleanup operations  like destroying type objects and unloading modules  Again  this hurt shared memory   Attempt 4  Final step for shutting down GC  No cleanup  Why do we need to clean up at all  The process is going to die  and we re going to get another replacement for it  What we really care about is our atexit hooks that do cleanup for our apps  As to Python s cleanup  we don t have to do it  This is what we ended up with in our bootstrapping script     gc disable   doesn t work  because some random 3rd party library will    enable it back implicitly   gc set_threshold 0     Suicide immediately after other atexit functions finishes     CPython will do a bunch of cleanups in Py_Finalize which    will again cause Copy on Write  including a final GC  atexit register os _exit  0   This is based on that fact atexit functions run in the reverse order of registry  The atexit function finishes the other cleanups  then calls os _exit 0   to exit the current process in the last step     With this two line change  we finally finished rolling it out to our entire fleet  After carefully adjusting the memory threshold  we got a 10  global capacity win   Looking back  In reviewing this performance win  we had two questions     First  without garbage collection  wasn t the Python memory going to blow up  as all memory allocation wouldn t be freed ever   Remember  there is no real stack in Python memory because all objects are allocated on heap      Fortunately  this was not true  The primary mechanism in Python to free objects is still reference count  When an object is de referenced  calling Py_DECREF    Python runtime always checks if its reference count drops to zero  In such cases  the deallocator of the objects will be called  The main purpose of garbage collection is to break the reference cycles where reference count does not work    define Py_DECREF op     do      if  _Py_DEC_REFTOTAL _Py_REF_DEBUG_COMMA        PyObject   op    ob_refcnt    0     _Py_CHECK_REFCNT op     else    _Py_Dealloc  PyObject    op         while  0   Breaking down the gains  Second question  where did the gain come from     The gain of disabling GC was two fold   We freed up about 8GB RAM for each server we used to create more worker processes for memory bound server generation  or lower the worker respawn rate for CPU bound server generation   CPU throughput also improved as CPU instructions per cycle  IPC  increased by about 10      perf stat  a  e cache misses cache references    sleep 10  Performance counter stats for  system wide    268 195 790 cache misses   12 240   of all cache refs  100 00    2 191 115 722 cache references  10 019172636 seconds time elapsed  With GC disabled  there was a 2 3  of cache miss rate drop  which was the main reason behind the 10  IPC improvement  CPU cache miss is expensive because it stalls CPU pipeline  Small improvements on the CPU cache hit rate can usually improve IPC significantly  With less CoW  more CPU cache lines with different virtual addresses  in different worker processes  point to the same physical memory address  leading to better cache hit rate     As we can see  not every component worked as expected  and sometimes  the results can be very surprising  So keep digging and sniffing around  and you ll be amazed how things really work   Chenyang Wu is a software engineer and Min Ni is an engineering manager at Instagram,"[1310 1177 1051 50 954 463 1060 383 957 1353 933]"
1312,training-dataset/engineering/324.txt,engineering,Learn Enough Git to Be DangerousGet access to all tutorials and screencasts Learn Enough Society members get access to all the published tutorials  all screencasts  and immediate access to new tutorials  including the upcoming Rails 5 edition of the Rails Tutorial,"[1312 1177 957 1199 136 1060 463 1130 805 1353 1051]"
1353,training-dataset/engineering/1127.txt,engineering,Reducing Slack s memory footprint   Several People Are CodingReducing Slack s memory footprint  by Johnny Rodgers  Charlie Hess  Raissa Largman  Jamie Scheinblum and Chris Sullivan  Our desktop app is the most widely used and most capable Slack client that we offer  For many of our customers  it is one of just a few apps they keep open on their computer throughout the work day  It allows them to communicate and work with all the teams they belong to  reading and writing messages  composing posts  uploading files  taking calls  and responding to notifications   However  these capabilities come at a cost  the desktop client can use a lot of memory  This memory footprint increases as the user signs into more teams  as each team runs in its own webview  More memory usage means worse performance  degrading our customer s experience of Slack and their other applications   Work is underway to fix the underlying factors affecting client memory consumption  but in the meantime we ve built a tiny new Slack client to help address this issue  It is loaded silently in place of the teams that you haven t looked at in a while  This background client does just a few things   updates your unread indicator and notification badge in the team sidebar  displays desktop notifications  that you can click on and reply to as normal   consumes as little memory as possible  To achieve this we had to teach our servers to be smarter  and to design a thin client that did just enough work to keep your work day flowing   The problem  At the time of writing  about 36  of active Slack users are signed into more than one team in the desktop app  17  are signed into 3 or more teams  and 5  are signed into 5 or more teams  Each of these teams is running the full JavaScript web application and maintaining and updating the DOM tree as the state of the team changes  This application can consume between  130MB  p10  and  960MB  p99  of memory depending on team activity and UI state   Distribution of memory usage at p10  p50  p90  and p99 for a single team  Collected using a 10  sample of all teams   Our data tells us that most people who are signed into multiple teams have 1 or 2 that they actively pay attention to during their work day while they check the others less frequently  Regardless of usage  each of these teams eats up a full quota of memory   A tiny client  This got us thinking  what is the lightest weight client we could build that could do everything you needed it to do while it was in the background  maintain your presence  display notifications  and keep your unreads and badges up to date   We explored several options  including   unload the DOM for backgrounded teams package a client out of a subset of our JavaScript concerned with just model and notification logic build a stripped down client specifically for this purpose  Our exploration indicated that the first two options either didn t make the impact on memory usage that we were targeting  or increased the complexity of implementation for our primary client unacceptably  So we decided on the third option and built a thin client that maintains minimal state and only presents data as transmitted by the server   The aspect of the project concerned with reclaiming resources from backgrounded views isn t original  Chrome takes a similar approach to tab discarding in order to recover memory from backgrounded tabs  and several popular browser extensions similarly suspend memory hungry tabs that haven t been viewed recently   Redrawing the map  In order for our new client to be as thin and lightweight as we needed it to be  we had to teach the server to do a lot of work that was previously the responsibility of the client   For example  the server now needs to know   How to calculate a user s notification count  Which messages qualify for desktop notifications  and how to format those notifications for consumption by the desktop client   Whether the user had any unreads across all of their channels and direct messages  And it needs to do all this with full awareness of the impact of various team and user preferences and states on the above  Slack is known for its granular and powerful notification preferences   which is great for customers but meant we needed to migrate a good deal of client side logic to the server,"[1353 1177 933 954 1130 383 1310 50 957 1060 463]"
