X1,file_name,category,text,recommendations
52,training-dataset/engineering/698.txt,engineering,Serverless ArchitecturesLike many trends in software there s no one clear view of what  Serverless  is  and that isn t helped by it really coming to mean two different but overlapping areas   Fast forward to today  mid 2016  and one sees examples such as the recent Serverless Conference   plus the various Serverless vendors are embracing the term from product descriptions to job descriptions  Serverless as a term  for better or for worse  is here to stay   We start to see the term used more frequently in 2015  after AWS Lambda s launch in 2014 and even more so after Amazon s API Gateway launched in July 2015  Here s an example where Ant Stanley writes about Serverless following the API Gateway announcement  In October 2015 there was a talk at Amazon s re Invent conference titled   The Serverless Company using AWS Lambda    referring to PlayOn  Sports   Towards the end of 2015 the  Javascript Amazon Web Services  JAWS   open source project renamed themselves to the Serverless Framework   continuing the trend   First usages of the term seem to have appeared around 2012  including this article by Ken Fromm   Badri Janakiraman says that he also heard usage of the term around this time in regard to continuous integration and source control systems being hosted as a service  rather than on a company s own servers  However this usage was about development infrastructure rather than incorporation into products   The term  Serverless  is confusing since with such applications there are both server hardware and server processes running somewhere  but the difference to normal approaches is that the organization building and supporting a  Serverless  application is not looking after the hardware or the processes   they are outsourcing this to a vendor   Mostly I m going to talk about the second of these areas because it is the one that is newer  has significant differences to how we typically think about technical architecture  and has been driving a lot of the hype around Serverless   However these concepts are related and  in fact  converging  A good example is Auth0   they started initially with BaaS  Authentication as a Service   but with Auth0 Webtask they are entering the FaaS space   Furthermore in many cases when developing a  BaaS shaped  application  especially when developing a  rich  web based app as opposed to a mobile app  you ll likely still need some amount of custom server side functionality  FaaS functions may be a good solution for this  especially if they are integrated to some extent with the BaaS services you re using  Examples of such functionality include data validation  protecting against imposter clients  and compute intensive processing  e g  image or video manipulation    The FaaS environment may also process several clicks in parallel by instantiating multiple copies of the function code   depending on how we d written the original process this may be a new concept we need to consider   There s a much smaller difference to the architecture here compared to our first example  We ve replaced a long lived consumer application with a FaaS function that runs within the event driven context the vendor provides us  Note that the vendor supplies both the Message Broker and the FaaS environment   the two systems are closely tied to each other   Traditionally  the architecture may look like this  The  Ad Server  synchronously responds to the user   we don t care about that interaction for the sake of this example   but it also posts a message to a channel that can be asynchronously processed by a  click processor  application that updates a database  e g  to decrement the advertiser s budget   A different example is a backend data processing service  Say you re writing a user centric application that needs to quickly respond to UI requests  but secondarily you want to capture all the different types of activity that are occurring  Let s think about an online ad system   when a user clicks on an advertisement you want to very quickly redirect them to the target of the ad  but at the same time you need to collect the fact that the click has happened so that you can charge the advertiser   This example is not hypothetical   my former team at Intent Media recently went through this exact redesign    Since the original server was implemented in Java  and AWS Lambda  our FaaS vendor of choice in this instance  supports functions implemented in Java  we can port the search code from the Pet Store server to the Pet Store Search function without a complete re write   This is a massively simplified view  but even with this there are a number of significant changes that have happened here  Please note this is not a recommendation of an architectural migration  I m merely using this as a tool to expose some Serverless concepts   With this architecture the client can be relatively unintelligent  with much of the logic in the system   authentication  page navigation  searching  transactions   implemented by the server application   Traditionally the architecture will look something like this  and let s say it s implemented in Java on the server side  with a HTML   Javascript component as the client   Another example is Apex   a project to  Build  deploy  and manage AWS Lambda functions with ease   One particularly interesting aspect of Apex is that it allows you to develop Lambda functions in languages other than those directly supported by Amazon  e g  Go   Apart from runtime implementation though there are already open source tools and frameworks to help with definition  deployment and runtime assistance  For instance the Serverless Framework makes working with API Gateway   Lambda significantly easier than using the first principles provided by AWS  It s Javascript heavy but if you re writing JS API Gateway apps it s definitely worth a look   One of the main benefits of Serverless FaaS applications is transparent production runtime provisioning  and so open source is not currently as relevant in this world as it is for  say  Docker and containers  In future we may see a popular FaaS   API Gateway platform implementation that will run  on premise  or on a developer workstation  IBM s OpenWhisk is an example of such an implementation and it will be interesting to see whether this  or an alternative implementation  picks up adoption   Debugging and monitoring are tricky in general in Serverless apps   we ll get into this further in subsequent installments of this article   The comment above about API Gateway tooling being immature actually applies  on the whole  to Serverless FaaS in general  There are exceptions however   one example is Auth0 Webtask which places significant priority on Developer UX in its tooling  Tomasz Janczuk gave a very good demonstration of this at the recent Serverless Conference   At present tooling for API gateways is achingly immature and so while defining applications with API gateways is possible it s most definitely not for the faint hearted   One use case for API Gateway   FaaS is for creating http fronted microservices in a Serverless way with all the scaling  management and other benefits that come from FaaS functions   Beyond purely routing requests API Gateways may also perform authentication  input validation  response code mapping  etc  Your spidey sense may be buzzing about whether this is actually such a good idea  if so hold that thought   we ll consider this further later   One aspect of FaaS that we brushed upon earlier is an  API Gateway   An API Gateway is an HTTP server where routes   endpoints are defined in configuration and each route is associated with a FaaS function  When an API Gateway receives a request it finds the routing configuration matching the request and then calls the relevant FaaS function  Typically the API Gateway will allow mapping from http request parameters to inputs arguments for the FaaS function  The API Gateway transforms the result of the FaaS function call to an http response  and returns this to the original caller   Whether or not you think your app may have problems like this you should test with production like load to see what performance you see  If your use case doesn t work now you may want to try again in a few months time since this is a major area of development by FaaS vendors   Are these issues a concern  It depends on the style and traffic shape of your application  My former team has an asynchronous message processing Lambda app implemented in Java which processes hundreds of millions of messages   day  and they have no concerns with startup latencey  That said if you were writing a low latency trading application you probably wouldn t want to use FaaS systems at this time  no matter the language you were using for implementation   The former of these may be avoided in certain situations by the ugly hack of pinging your function every 5 minutes to keep it alive   If your Lambda function is implemented on the JVM you may occasionally see long response times  e g    10 seconds  while the JVM is spun up  However this only notably happens with either of the following scenarios   If your function is implemented in Javascript or Python and isn t huge  i e  less than a thousand lines of code  then the overhead of running it in should never be more than 10   100 ms  Bigger functions may occasionally see longer times   At present how long it takes your FaaS function to respond to a request depends on a large number of factors  and may be anywhere from 10ms to 2 minutes  That sounds bad  but let s get a little more specific  using AWS Lambda as an example   This means that certain classes of long lived task are not suited to FaaS functions without re architecture  e g  you may need to create several different coordinated FaaS functions where in a traditional environment you may have one long duration task performing both coordination and execution   FaaS functions are typically limited in how long each invocation is allowed to run  At present AWS Lambda functions are not allowed to run for longer than 5 minutes and if they do they will be terminated   Given this restriction what are alternatives  Typically it means that FaaS functions are either naturally stateless   i e  they provide pure functional transformations of their input   or that they make use of a database  a cross application cache  e g  Redis   or network file store  e g  S3  to store state across requests or for further input to handle a request   FaaS functions have significant restrictions when it comes to local  machine   instance bound  state  In short you should assume that for any given invocation of a function none of the in process or host state that you create will be available to any subsequent invocation  This includes state in RAM and state you may write to local disk  In other words from a deployment unit point of view FaaS functions are stateless    Let s return to our click processor  Say that we were having a good day and customers were clicking on 10 times as many ads as usual  Would our click processing application be able to handle this  For example did we code to be able to handle multiple messages at a time  Even if we did would one running instance of the application be enough to process the load  If we are able to run multiple processes is auto scaling automatic or do we need to reconfigure that manually  With FaaS you need to write the function ahead of time to assume parallelism  but from that point on the FaaS provider automatically handles all scaling needs   Let s consider our click processing example again   the only code that needs to change when moving to FaaS is the  main method   startup  code  in that it is deleted  and likely the specific code that is the top level message handler  the  message listener interface  implementation   but this might only be a change in method signature  All of the rest of the code  e g  the code that writes to the database  is no different in a FaaS world   If we go back to our click processing example from earlier what FaaS does is replace the click processing server  possibly a physical machine  but definitely a specific application  with something that doesn t need a provisioned server  nor an application that is running all the time   AWS Lambda lets you run code without provisioning or managing servers   1      With Lambda  you can run code for virtually any type of application or backend service  2    all with zero administration  Just upload your code and Lambda takes care of everything required to run  3  and scale  4  your code with high availability  You can set up your code to automatically trigger from other AWS services  5  or call it directly from any web or mobile app  6     We ve mentioned the FaaS idea a lot already but it s time to dig into what it really means  To do this let s look at the opening description for Amazon s Lambda product  I ve added some tokens to it  which I then expand upon   What isn t Serverless   So far in this article I ve defined  Serverless  to mean the union of a couple of other ideas    Backend as a Service  and  Functions as a Service   I ve also dug into the capabilities of the second of these   Before we start looking at the very important area of benefits and drawbacks I d like to spend one more moment on definition  or at least defining what Serverless isn t  I ve seen some people  including me in the recent past  get confused about these things and I think it s worth discussing them for clarity s sake   Comparison with PaaS Given that Serverless FaaS functions are very similar to 12 Factor applications  are they in fact just another form of  Platform as a Service   PaaS  like Heroku  For a brief answer I refer to Adrian Cockcroft If your PaaS can efficiently start instances in 20ms that run for half a second  then call it serverless     Adrian Cockcroft In other words most PaaS applications are not geared towards bringing entire applications up and down for every request  whereas FaaS platforms do exactly this  OK  but so what  if I m being a good 12 Factor App developer there s still no difference to how I code  That s true  but there is a big difference to how you operate your app  Since we re all good DevOps savvy engineers we re thinking about operations as much as we are about development  right  The key operational difference between FaaS and PaaS is scaling  With most PaaS s you still need to think about scale  e g  with Heroku how many Dynos you want to run  With a FaaS application this is completely transparent  Even if you setup your PaaS application to auto scale you won t be doing this to the level of individual requests  unless you have a very specifically shaped traffic profile   and so a FaaS application is much more efficient when it comes to costs  Given this benenfit  why would you still use a PaaS  There are several reasons but tooling  and maturity of API gateways  are probably the biggest  Furthermore 12 Factor Apps implemented in a PaaS may use an in app readonly cache for optimization  which isn t an option for FaaS functions   Comparison with containers One of the reasons for Serverless FaaS is to avoid having to manage computational processes at the operating system level or lower  Platforms as a Service  like Heroku  are another  and I ve described above how PaaS s are different to Serverless FaaS  Another popular abstraction of processes are containers  with Docker being the most visible example of such a technology  We also see increasing popularity of container hosting systems  such as Mesos and Kubernetes  which abstract individual applications from OS level deployment  And even further there are cloud hosting container platforms like Amazon ECS and Google Container Engine which  like Serverless FaaS  let teams avoid having to manage their own server systems at all  So given all the momentum around containers is it still worth considering Serverless FaaS  Principally the argument I made for PaaS still holds with containers   for Serverless FaaS scaling is automatically managed  transparent  and fine grained  Container platforms do not yet offer such a solution  Furthermore I d argue that container technology while having seen massive popularity in the last couple of years is still not mature  That s not to say that Serverless FaaS is mature either  but picking which rough edges you d like is still the order of the day  I ll admit  however  that both of these arguments may start to wear thin over time  While true no management auto scaling in container platforms isn t at the level of Serverless FaaS yet  we see areas like Kubernetes  Horizontal Pod Autoscaling  as tending towards it  I can imagine some very smart traffic pattern analysis being introduced to such features  as well as more load implying metrics  Furthermore the rapid evolution of Kubernetes may give a wonderfully simple  stable  platform before too long  If we see the gap of management and scaling between Serverless FaaS and hosted containers narrow the choice between them may just come down to style  and type of application  For example it may be that FaaS is seen as a better choice for event driven style with few event types per application component  and containers are seen as a better choice for synchronous request driven components with many entry points  I expect in 5 years time that many applications and teams will use both architectural approaches  and it will be fascinating to see patterns of such use emerge    NoOps Serverless doesn t mean  No Ops   It might mean  No internal Sys Admin  depending on how far down the serverless rabbit hole you go  There are 2 important things to consider here  Firstly  Ops  means a lot more than server administration  It also means at least monitoring  deployment  security  networking and often also means some amount of production debugging and system scaling  These problems all still exist with Serverless apps and you re still going to need a strategy to deal with them  In some ways Ops is harder in a Serverless world because a lot of this is so new  Second even the Sys Admin is still happening   you re just outsourcing it with Serverless  That s not necessarily a bad thing   we outsource a lot  But depending on what precisely you re trying to do this might be a good or a bad thing  and either way at some point the abstraction will likely leak and you ll need to know that human sys admins somewhere are supporting your application  Charity Majors gave a great talk on this subject at the recent Serverless Conference and I recommend checking it out once it s available online  Until then you can read her write up here and here,"[79 440 1120 52 1397 290 1119 1066 354 993 1123]"
79,training-dataset/engineering/1283.txt,engineering,Serverless ArchitecturesLike many trends in software there s no one clear view of what  Serverless  is  and that isn t helped by it really coming to mean two different but overlapping areas   Fast forward to today  mid 2016  and one sees examples such as the recent Serverless Conference   plus the various Serverless vendors are embracing the term from product descriptions to job descriptions  Serverless as a term  for better or for worse  is here to stay   We start to see the term used more frequently in 2015  after AWS Lambda s launch in 2014 and even more so after Amazon s API Gateway launched in July 2015  Here s an example where Ant Stanley writes about Serverless following the API Gateway announcement  In October 2015 there was a talk at Amazon s re Invent conference titled   The Serverless Company using AWS Lambda    referring to PlayOn  Sports   Towards the end of 2015 the  Javascript Amazon Web Services  JAWS   open source project renamed themselves to the Serverless Framework   continuing the trend   First usages of the term seem to have appeared around 2012  including this article by Ken Fromm   Badri Janakiraman says that he also heard usage of the term around this time in regard to continuous integration and source control systems being hosted as a service  rather than on a company s own servers  However this usage was about development infrastructure rather than incorporation into products   The term  Serverless  is confusing since with such applications there are both server hardware and server processes running somewhere  but the difference to normal approaches is that the organization building and supporting a  Serverless  application is not looking after the hardware or the processes   they are outsourcing this to a vendor   Mostly I m going to talk about the second of these areas because it is the one that is newer  has significant differences to how we typically think about technical architecture  and has been driving a lot of the hype around Serverless   However these concepts are related and  in fact  converging  A good example is Auth0   they started initially with BaaS  Authentication as a Service   but with Auth0 Webtask they are entering the FaaS space   Furthermore in many cases when developing a  BaaS shaped  application  especially when developing a  rich  web based app as opposed to a mobile app  you ll likely still need some amount of custom server side functionality  FaaS functions may be a good solution for this  especially if they are integrated to some extent with the BaaS services you re using  Examples of such functionality include data validation  protecting against imposter clients  and compute intensive processing  e g  image or video manipulation    The FaaS environment may also process several clicks in parallel by instantiating multiple copies of the function code   depending on how we d written the original process this may be a new concept we need to consider   There s a much smaller difference to the architecture here compared to our first example  We ve replaced a long lived consumer application with a FaaS function that runs within the event driven context the vendor provides us  Note that the vendor supplies both the Message Broker and the FaaS environment   the two systems are closely tied to each other   Traditionally  the architecture may look like this  The  Ad Server  synchronously responds to the user   we don t care about that interaction for the sake of this example   but it also posts a message to a channel that can be asynchronously processed by a  click processor  application that updates a database  e g  to decrement the advertiser s budget   A different example is a backend data processing service  Say you re writing a user centric application that needs to quickly respond to UI requests  but secondarily you want to capture all the different types of activity that are occurring  Let s think about an online ad system   when a user clicks on an advertisement you want to very quickly redirect them to the target of the ad  but at the same time you need to collect the fact that the click has happened so that you can charge the advertiser   This example is not hypothetical   my former team at Intent Media recently went through this exact redesign    Since the original server was implemented in Java  and AWS Lambda  our FaaS vendor of choice in this instance  supports functions implemented in Java  we can port the search code from the Pet Store server to the Pet Store Search function without a complete re write   This is a massively simplified view  but even with this there are a number of significant changes that have happened here  Please note this is not a recommendation of an architectural migration  I m merely using this as a tool to expose some Serverless concepts   With this architecture the client can be relatively unintelligent  with much of the logic in the system   authentication  page navigation  searching  transactions   implemented by the server application   Traditionally the architecture will look something like this  and let s say it s implemented in Java on the server side  with a HTML   Javascript component as the client   Another example is Apex   a project to  Build  deploy  and manage AWS Lambda functions with ease   One particularly interesting aspect of Apex is that it allows you to develop Lambda functions in languages other than those directly supported by Amazon  e g  Go   Apart from runtime implementation though there are already open source tools and frameworks to help with definition  deployment and runtime assistance  For instance the Serverless Framework makes working with API Gateway   Lambda significantly easier than using the first principles provided by AWS  It s Javascript heavy but if you re writing JS API Gateway apps it s definitely worth a look   One of the main benefits of Serverless FaaS applications is transparent production runtime provisioning  and so open source is not currently as relevant in this world as it is for  say  Docker and containers  In future we may see a popular FaaS   API Gateway platform implementation that will run  on premise  or on a developer workstation  IBM s OpenWhisk is an example of such an implementation and it will be interesting to see whether this  or an alternative implementation  picks up adoption   Debugging and monitoring are tricky in general in Serverless apps   we ll get into this further in subsequent installments of this article   The comment above about API Gateway tooling being immature actually applies  on the whole  to Serverless FaaS in general  There are exceptions however   one example is Auth0 Webtask which places significant priority on Developer UX in its tooling  Tomasz Janczuk gave a very good demonstration of this at the recent Serverless Conference   At present tooling for API gateways is achingly immature and so while defining applications with API gateways is possible it s most definitely not for the faint hearted   One use case for API Gateway   FaaS is for creating http fronted microservices in a Serverless way with all the scaling  management and other benefits that come from FaaS functions   Beyond purely routing requests API Gateways may also perform authentication  input validation  response code mapping  etc  Your spidey sense may be buzzing about whether this is actually such a good idea  if so hold that thought   we ll consider this further later   One aspect of FaaS that we brushed upon earlier is an  API Gateway   An API Gateway is an HTTP server where routes   endpoints are defined in configuration and each route is associated with a FaaS function  When an API Gateway receives a request it finds the routing configuration matching the request and then calls the relevant FaaS function  Typically the API Gateway will allow mapping from http request parameters to inputs arguments for the FaaS function  The API Gateway transforms the result of the FaaS function call to an http response  and returns this to the original caller   Whether or not you think your app may have problems like this you should test with production like load to see what performance you see  If your use case doesn t work now you may want to try again in a few months time since this is a major area of development by FaaS vendors   Are these issues a concern  It depends on the style and traffic shape of your application  My former team has an asynchronous message processing Lambda app implemented in Java which processes hundreds of millions of messages   day  and they have no concerns with startup latencey  That said if you were writing a low latency trading application you probably wouldn t want to use FaaS systems at this time  no matter the language you were using for implementation   The former of these may be avoided in certain situations by the ugly hack of pinging your function every 5 minutes to keep it alive   If your Lambda function is implemented on the JVM you may occasionally see long response times  e g    10 seconds  while the JVM is spun up  However this only notably happens with either of the following scenarios   If your function is implemented in Javascript or Python and isn t huge  i e  less than a thousand lines of code  then the overhead of running it in should never be more than 10   100 ms  Bigger functions may occasionally see longer times   At present how long it takes your FaaS function to respond to a request depends on a large number of factors  and may be anywhere from 10ms to 2 minutes  That sounds bad  but let s get a little more specific  using AWS Lambda as an example   This means that certain classes of long lived task are not suited to FaaS functions without re architecture  e g  you may need to create several different coordinated FaaS functions where in a traditional environment you may have one long duration task performing both coordination and execution   FaaS functions are typically limited in how long each invocation is allowed to run  At present AWS Lambda functions are not allowed to run for longer than 5 minutes and if they do they will be terminated   Given this restriction what are alternatives  Typically it means that FaaS functions are either naturally stateless   i e  they provide pure functional transformations of their input   or that they make use of a database  a cross application cache  e g  Redis   or network file store  e g  S3  to store state across requests or for further input to handle a request   FaaS functions have significant restrictions when it comes to local  machine   instance bound  state  In short you should assume that for any given invocation of a function none of the in process or host state that you create will be available to any subsequent invocation  This includes state in RAM and state you may write to local disk  In other words from a deployment unit point of view FaaS functions are stateless    Let s return to our click processor  Say that we were having a good day and customers were clicking on 10 times as many ads as usual  Would our click processing application be able to handle this  For example did we code to be able to handle multiple messages at a time  Even if we did would one running instance of the application be enough to process the load  If we are able to run multiple processes is auto scaling automatic or do we need to reconfigure that manually  With FaaS you need to write the function ahead of time to assume parallelism  but from that point on the FaaS provider automatically handles all scaling needs   Let s consider our click processing example again   the only code that needs to change when moving to FaaS is the  main method   startup  code  in that it is deleted  and likely the specific code that is the top level message handler  the  message listener interface  implementation   but this might only be a change in method signature  All of the rest of the code  e g  the code that writes to the database  is no different in a FaaS world   If we go back to our click processing example from earlier what FaaS does is replace the click processing server  possibly a physical machine  but definitely a specific application  with something that doesn t need a provisioned server  nor an application that is running all the time   AWS Lambda lets you run code without provisioning or managing servers   1      With Lambda  you can run code for virtually any type of application or backend service  2    all with zero administration  Just upload your code and Lambda takes care of everything required to run  3  and scale  4  your code with high availability  You can set up your code to automatically trigger from other AWS services  5  or call it directly from any web or mobile app  6     We ve mentioned the FaaS idea a lot already but it s time to dig into what it really means  To do this let s look at the opening description for Amazon s Lambda product  I ve added some tokens to it  which I then expand upon   What isn t Serverless   So far in this article I ve defined  Serverless  to mean the union of a couple of other ideas    Backend as a Service  and  Functions as a Service   I ve also dug into the capabilities of the second of these   Before we start looking at the very important area of benefits and drawbacks I d like to spend one more moment on definition  or at least defining what Serverless isn t  I ve seen some people  including me in the recent past  get confused about these things and I think it s worth discussing them for clarity s sake   Comparison with PaaS Given that Serverless FaaS functions are very similar to 12 Factor applications  are they in fact just another form of  Platform as a Service   PaaS  like Heroku  For a brief answer I refer to Adrian Cockcroft If your PaaS can efficiently start instances in 20ms that run for half a second  then call it serverless     Adrian Cockcroft In other words most PaaS applications are not geared towards bringing entire applications up and down for every request  whereas FaaS platforms do exactly this  OK  but so what  if I m being a good 12 Factor App developer there s still no difference to how I code  That s true  but there is a big difference to how you operate your app  Since we re all good DevOps savvy engineers we re thinking about operations as much as we are about development  right  The key operational difference between FaaS and PaaS is scaling  With most PaaS s you still need to think about scale  e g  with Heroku how many Dynos you want to run  With a FaaS application this is completely transparent  Even if you setup your PaaS application to auto scale you won t be doing this to the level of individual requests  unless you have a very specifically shaped traffic profile   and so a FaaS application is much more efficient when it comes to costs  Given this benenfit  why would you still use a PaaS  There are several reasons but tooling  and maturity of API gateways  are probably the biggest  Furthermore 12 Factor Apps implemented in a PaaS may use an in app readonly cache for optimization  which isn t an option for FaaS functions   Comparison with containers One of the reasons for Serverless FaaS is to avoid having to manage computational processes at the operating system level or lower  Platforms as a Service  like Heroku  are another  and I ve described above how PaaS s are different to Serverless FaaS  Another popular abstraction of processes are containers  with Docker being the most visible example of such a technology  We also see increasing popularity of container hosting systems  such as Mesos and Kubernetes  which abstract individual applications from OS level deployment  And even further there are cloud hosting container platforms like Amazon ECS and Google Container Engine which  like Serverless FaaS  let teams avoid having to manage their own server systems at all  So given all the momentum around containers is it still worth considering Serverless FaaS  Principally the argument I made for PaaS still holds with containers   for Serverless FaaS scaling is automatically managed  transparent  and fine grained  Container platforms do not yet offer such a solution  Furthermore I d argue that container technology while having seen massive popularity in the last couple of years is still not mature  That s not to say that Serverless FaaS is mature either  but picking which rough edges you d like is still the order of the day  I ll admit  however  that both of these arguments may start to wear thin over time  While true no management auto scaling in container platforms isn t at the level of Serverless FaaS yet  we see areas like Kubernetes  Horizontal Pod Autoscaling  as tending towards it  I can imagine some very smart traffic pattern analysis being introduced to such features  as well as more load implying metrics  Furthermore the rapid evolution of Kubernetes may give a wonderfully simple  stable  platform before too long  If we see the gap of management and scaling between Serverless FaaS and hosted containers narrow the choice between them may just come down to style  and type of application  For example it may be that FaaS is seen as a better choice for event driven style with few event types per application component  and containers are seen as a better choice for synchronous request driven components with many entry points  I expect in 5 years time that many applications and teams will use both architectural approaches  and it will be fascinating to see patterns of such use emerge    NoOps Serverless doesn t mean  No Ops   It might mean  No internal Sys Admin  depending on how far down the serverless rabbit hole you go  There are 2 important things to consider here  Firstly  Ops  means a lot more than server administration  It also means at least monitoring  deployment  security  networking and often also means some amount of production debugging and system scaling  These problems all still exist with Serverless apps and you re still going to need a strategy to deal with them  In some ways Ops is harder in a Serverless world because a lot of this is so new  Second even the Sys Admin is still happening   you re just outsourcing it with Serverless  That s not necessarily a bad thing   we outsource a lot  But depending on what precisely you re trying to do this might be a good or a bad thing  and either way at some point the abstraction will likely leak and you ll need to know that human sys admins somewhere are supporting your application  Charity Majors gave a great talk on this subject at the recent Serverless Conference and I recommend checking it out once it s available online  Until then you can read her write up here and here,"[79 440 1120 52 1397 290 1119 1066 354 993 1123]"
258,training-dataset/product/1275.txt,product,Not Found   Medium,"[258 1119 993 1066 1123 436 354 440 1120 52 290]"
290,training-dataset/engineering/1488.txt,engineering,Serverless ArchitecturesLike many trends in software there s no one clear view of what  Serverless  is  and that isn t helped by it really coming to mean two different but overlapping areas   Fast forward to today  mid 2016  and one sees examples such as the recent Serverless Conference   plus the various Serverless vendors are embracing the term from product descriptions to job descriptions  Serverless as a term  for better or for worse  is here to stay   We start to see the term used more frequently in 2015  after AWS Lambda s launch in 2014 and even more so after Amazon s API Gateway launched in July 2015  Here s an example where Ant Stanley writes about Serverless following the API Gateway announcement  In October 2015 there was a talk at Amazon s re Invent conference titled   The Serverless Company using AWS Lambda    referring to PlayOn  Sports   Towards the end of 2015 the  Javascript Amazon Web Services  JAWS   open source project renamed themselves to the Serverless Framework   continuing the trend   First usages of the term seem to have appeared around 2012  including this article by Ken Fromm   Badri Janakiraman says that he also heard usage of the term around this time in regard to continuous integration and source control systems being hosted as a service  rather than on a company s own servers  However this usage was about development infrastructure rather than incorporation into products   The term  Serverless  is confusing since with such applications there are both server hardware and server processes running somewhere  but the difference to normal approaches is that the organization building and supporting a  Serverless  application is not looking after the hardware or the processes   they are outsourcing this to a vendor   Mostly I m going to talk about the second of these areas because it is the one that is newer  has significant differences to how we typically think about technical architecture  and has been driving a lot of the hype around Serverless   However these concepts are related and  in fact  converging  A good example is Auth0   they started initially with BaaS  Authentication as a Service   but with Auth0 Webtask they are entering the FaaS space   Furthermore in many cases when developing a  BaaS shaped  application  especially when developing a  rich  web based app as opposed to a mobile app  you ll likely still need some amount of custom server side functionality  FaaS functions may be a good solution for this  especially if they are integrated to some extent with the BaaS services you re using  Examples of such functionality include data validation  protecting against imposter clients  and compute intensive processing  e g  image or video manipulation    The FaaS environment may also process several clicks in parallel by instantiating multiple copies of the function code   depending on how we d written the original process this may be a new concept we need to consider   There s a much smaller difference to the architecture here compared to our first example  We ve replaced a long lived consumer application with a FaaS function that runs within the event driven context the vendor provides us  Note that the vendor supplies both the Message Broker and the FaaS environment   the two systems are closely tied to each other   Traditionally  the architecture may look like this  The  Ad Server  synchronously responds to the user   we don t care about that interaction for the sake of this example   but it also posts a message to a channel that can be asynchronously processed by a  click processor  application that updates a database  e g  to decrement the advertiser s budget   A different example is a backend data processing service  Say you re writing a user centric application that needs to quickly respond to UI requests  but secondarily you want to capture all the different types of activity that are occurring  Let s think about an online ad system   when a user clicks on an advertisement you want to very quickly redirect them to the target of the ad  but at the same time you need to collect the fact that the click has happened so that you can charge the advertiser   This example is not hypothetical   my former team at Intent Media recently went through this exact redesign    Since the original server was implemented in Java  and AWS Lambda  our FaaS vendor of choice in this instance  supports functions implemented in Java  we can port the search code from the Pet Store server to the Pet Store Search function without a complete re write   This is a massively simplified view  but even with this there are a number of significant changes that have happened here  Please note this is not a recommendation of an architectural migration  I m merely using this as a tool to expose some Serverless concepts   With this architecture the client can be relatively unintelligent  with much of the logic in the system   authentication  page navigation  searching  transactions   implemented by the server application   Traditionally the architecture will look something like this  and let s say it s implemented in Java on the server side  with a HTML   Javascript component as the client   Another example is Apex   a project to  Build  deploy  and manage AWS Lambda functions with ease   One particularly interesting aspect of Apex is that it allows you to develop Lambda functions in languages other than those directly supported by Amazon  e g  Go   Apart from runtime implementation though there are already open source tools and frameworks to help with definition  deployment and runtime assistance  For instance the Serverless Framework makes working with API Gateway   Lambda significantly easier than using the first principles provided by AWS  It s Javascript heavy but if you re writing JS API Gateway apps it s definitely worth a look   One of the main benefits of Serverless FaaS applications is transparent production runtime provisioning  and so open source is not currently as relevant in this world as it is for  say  Docker and containers  In future we may see a popular FaaS   API Gateway platform implementation that will run  on premise  or on a developer workstation  IBM s OpenWhisk is an example of such an implementation and it will be interesting to see whether this  or an alternative implementation  picks up adoption   Debugging and monitoring are tricky in general in Serverless apps   we ll get into this further in subsequent installments of this article   The comment above about API Gateway tooling being immature actually applies  on the whole  to Serverless FaaS in general  There are exceptions however   one example is Auth0 Webtask which places significant priority on Developer UX in its tooling  Tomasz Janczuk gave a very good demonstration of this at the recent Serverless Conference   At present tooling for API gateways is achingly immature and so while defining applications with API gateways is possible it s most definitely not for the faint hearted   One use case for API Gateway   FaaS is for creating http fronted microservices in a Serverless way with all the scaling  management and other benefits that come from FaaS functions   Beyond purely routing requests API Gateways may also perform authentication  input validation  response code mapping  etc  Your spidey sense may be buzzing about whether this is actually such a good idea  if so hold that thought   we ll consider this further later   One aspect of FaaS that we brushed upon earlier is an  API Gateway   An API Gateway is an HTTP server where routes   endpoints are defined in configuration and each route is associated with a FaaS function  When an API Gateway receives a request it finds the routing configuration matching the request and then calls the relevant FaaS function  Typically the API Gateway will allow mapping from http request parameters to inputs arguments for the FaaS function  The API Gateway transforms the result of the FaaS function call to an http response  and returns this to the original caller   Whether or not you think your app may have problems like this you should test with production like load to see what performance you see  If your use case doesn t work now you may want to try again in a few months time since this is a major area of development by FaaS vendors   Are these issues a concern  It depends on the style and traffic shape of your application  My former team has an asynchronous message processing Lambda app implemented in Java which processes hundreds of millions of messages   day  and they have no concerns with startup latencey  That said if you were writing a low latency trading application you probably wouldn t want to use FaaS systems at this time  no matter the language you were using for implementation   The former of these may be avoided in certain situations by the ugly hack of pinging your function every 5 minutes to keep it alive   If your Lambda function is implemented on the JVM you may occasionally see long response times  e g    10 seconds  while the JVM is spun up  However this only notably happens with either of the following scenarios   If your function is implemented in Javascript or Python and isn t huge  i e  less than a thousand lines of code  then the overhead of running it in should never be more than 10   100 ms  Bigger functions may occasionally see longer times   At present how long it takes your FaaS function to respond to a request depends on a large number of factors  and may be anywhere from 10ms to 2 minutes  That sounds bad  but let s get a little more specific  using AWS Lambda as an example   This means that certain classes of long lived task are not suited to FaaS functions without re architecture  e g  you may need to create several different coordinated FaaS functions where in a traditional environment you may have one long duration task performing both coordination and execution   FaaS functions are typically limited in how long each invocation is allowed to run  At present AWS Lambda functions are not allowed to run for longer than 5 minutes and if they do they will be terminated   Given this restriction what are alternatives  Typically it means that FaaS functions are either naturally stateless   i e  they provide pure functional transformations of their input   or that they make use of a database  a cross application cache  e g  Redis   or network file store  e g  S3  to store state across requests or for further input to handle a request   FaaS functions have significant restrictions when it comes to local  machine   instance bound  state  In short you should assume that for any given invocation of a function none of the in process or host state that you create will be available to any subsequent invocation  This includes state in RAM and state you may write to local disk  In other words from a deployment unit point of view FaaS functions are stateless    Let s return to our click processor  Say that we were having a good day and customers were clicking on 10 times as many ads as usual  Would our click processing application be able to handle this  For example did we code to be able to handle multiple messages at a time  Even if we did would one running instance of the application be enough to process the load  If we are able to run multiple processes is auto scaling automatic or do we need to reconfigure that manually  With FaaS you need to write the function ahead of time to assume parallelism  but from that point on the FaaS provider automatically handles all scaling needs   Let s consider our click processing example again   the only code that needs to change when moving to FaaS is the  main method   startup  code  in that it is deleted  and likely the specific code that is the top level message handler  the  message listener interface  implementation   but this might only be a change in method signature  All of the rest of the code  e g  the code that writes to the database  is no different in a FaaS world   If we go back to our click processing example from earlier what FaaS does is replace the click processing server  possibly a physical machine  but definitely a specific application  with something that doesn t need a provisioned server  nor an application that is running all the time   AWS Lambda lets you run code without provisioning or managing servers   1      With Lambda  you can run code for virtually any type of application or backend service  2    all with zero administration  Just upload your code and Lambda takes care of everything required to run  3  and scale  4  your code with high availability  You can set up your code to automatically trigger from other AWS services  5  or call it directly from any web or mobile app  6     We ve mentioned the FaaS idea a lot already but it s time to dig into what it really means  To do this let s look at the opening description for Amazon s Lambda product  I ve added some tokens to it  which I then expand upon   What isn t Serverless   So far in this article I ve defined  Serverless  to mean the union of a couple of other ideas    Backend as a Service  and  Functions as a Service   I ve also dug into the capabilities of the second of these   Before we start looking at the very important area of benefits and drawbacks I d like to spend one more moment on definition  or at least defining what Serverless isn t  I ve seen some people  including me in the recent past  get confused about these things and I think it s worth discussing them for clarity s sake   Comparison with PaaS Given that Serverless FaaS functions are very similar to 12 Factor applications  are they in fact just another form of  Platform as a Service   PaaS  like Heroku  For a brief answer I refer to Adrian Cockcroft If your PaaS can efficiently start instances in 20ms that run for half a second  then call it serverless     Adrian Cockcroft In other words most PaaS applications are not geared towards bringing entire applications up and down for every request  whereas FaaS platforms do exactly this  OK  but so what  if I m being a good 12 Factor App developer there s still no difference to how I code  That s true  but there is a big difference to how you operate your app  Since we re all good DevOps savvy engineers we re thinking about operations as much as we are about development  right  The key operational difference between FaaS and PaaS is scaling  With most PaaS s you still need to think about scale  e g  with Heroku how many Dynos you want to run  With a FaaS application this is completely transparent  Even if you setup your PaaS application to auto scale you won t be doing this to the level of individual requests  unless you have a very specifically shaped traffic profile   and so a FaaS application is much more efficient when it comes to costs  Given this benenfit  why would you still use a PaaS  There are several reasons but tooling  and maturity of API gateways  are probably the biggest  Furthermore 12 Factor Apps implemented in a PaaS may use an in app readonly cache for optimization  which isn t an option for FaaS functions   Comparison with containers One of the reasons for Serverless FaaS is to avoid having to manage computational processes at the operating system level or lower  Platforms as a Service  like Heroku  are another  and I ve described above how PaaS s are different to Serverless FaaS  Another popular abstraction of processes are containers  with Docker being the most visible example of such a technology  We also see increasing popularity of container hosting systems  such as Mesos and Kubernetes  which abstract individual applications from OS level deployment  And even further there are cloud hosting container platforms like Amazon ECS and Google Container Engine which  like Serverless FaaS  let teams avoid having to manage their own server systems at all  So given all the momentum around containers is it still worth considering Serverless FaaS  Principally the argument I made for PaaS still holds with containers   for Serverless FaaS scaling is automatically managed  transparent  and fine grained  Container platforms do not yet offer such a solution  Furthermore I d argue that container technology while having seen massive popularity in the last couple of years is still not mature  That s not to say that Serverless FaaS is mature either  but picking which rough edges you d like is still the order of the day  I ll admit  however  that both of these arguments may start to wear thin over time  While true no management auto scaling in container platforms isn t at the level of Serverless FaaS yet  we see areas like Kubernetes  Horizontal Pod Autoscaling  as tending towards it  I can imagine some very smart traffic pattern analysis being introduced to such features  as well as more load implying metrics  Furthermore the rapid evolution of Kubernetes may give a wonderfully simple  stable  platform before too long  If we see the gap of management and scaling between Serverless FaaS and hosted containers narrow the choice between them may just come down to style  and type of application  For example it may be that FaaS is seen as a better choice for event driven style with few event types per application component  and containers are seen as a better choice for synchronous request driven components with many entry points  I expect in 5 years time that many applications and teams will use both architectural approaches  and it will be fascinating to see patterns of such use emerge    NoOps Serverless doesn t mean  No Ops   It might mean  No internal Sys Admin  depending on how far down the serverless rabbit hole you go  There are 2 important things to consider here  Firstly  Ops  means a lot more than server administration  It also means at least monitoring  deployment  security  networking and often also means some amount of production debugging and system scaling  These problems all still exist with Serverless apps and you re still going to need a strategy to deal with them  In some ways Ops is harder in a Serverless world because a lot of this is so new  Second even the Sys Admin is still happening   you re just outsourcing it with Serverless  That s not necessarily a bad thing   we outsource a lot  But depending on what precisely you re trying to do this might be a good or a bad thing  and either way at some point the abstraction will likely leak and you ll need to know that human sys admins somewhere are supporting your application  Charity Majors gave a great talk on this subject at the recent Serverless Conference and I recommend checking it out once it s available online  Until then you can read her write up here and here,"[79 440 1120 52 1397 290 1119 1066 354 993 1123]"
354,training-dataset/engineering/1417.txt,engineering,Part 2 of Thinking Serverless   Platform Level IssuesMonday  February 6  2017 at 8 56AM  This is a guest repost by Ken Fromm  a 3x tech co founder   Vivid Studios  Loomia  and Iron io  Here s Part 1   Job processing at scale at high concurrency across a distributed infrastructure is a complicated feat  There are many components involvement   servers and controllers to process and monitor jobs  controllers to autoscale and manage servers  controllers to distribute jobs across the set of servers  queues to buffer jobs  and whole host of other components to ensure jobs complete and or are retried  and other critical tasks that help maintain high service levels  This section peels back the layers a bit to provide insight into important aspects within the workings of a serverless platform   Throughput  Throughput has always been the coin of the realm in computer processing   how quickly can events  requests  and workloads be processed  In the context of a serverless architecture  I ll break throughput down further when discussing both latency and concurrency  At the base level  however  a serverless architecture does provide a more beneficial architecture than legacy applications and large web apps when it comes to throughput because it provide for far better resource utilization   In a post by Travis Reeder on What is Serverless Computing and Why is it Important he addresses this topic   Cost and optimal use of resources is a huge reason to do serverless  If you are a big company with a bunch of apps APIs microservices  you are currently running those things 24 7 and they are using resources 100  of the time  no matter if they are in use or not  With a FaaS infrastructure  instead of running apps 24 7  you can execute functions for any number of apps on demand and share all the same resources  Theoretically  you could reduce waste  idle time  to almost nothing while still providing fast response time  For a FaaS provider  this cost savings is passed up to the end user  the developer  For an enterprise  this can reduce capex and opex big time   Another way of looking at it is that by moving to more discrete tasks that can run in universal platform with self contained dependencies  tasks can run anytime anywhere across a serverless architecture  This is in contrast to a set of stand alone monolithic applications whereby operations teams have to spend significant cycles arbitrating which applications to scale  when  and how   A serverless architecture can also increase throughput of application and feature development but much has been said in this regard as it relates to microservices and functions as a service    A Graph of Tasks and Projects  The graph below shows a set of tasks over time for a single account on the a serverless platform  The overarching yellow line indicates all tasks for an account and the other lines represent projects within the account  The project lines should be viewed as a microservice or a specific set of application functions  A few years ago  the total set would have been built as a traditional web application and hosted as a long running application  As you can see  however  each service or set of functions has a different workload characteristic  Managing the aggregated set at an application level is far more complex than managing at the task level within a serverless platform  not to mention the resource savings by scaling commodity task servers as opposed to much more complex application servers   All Tasks  Application View  vs Specific Tasks  Serverless View   Latency  The main issues arbitrating throughput in a serverless world have to do with latency and concurrency  Latency refers to the time it takes to start processing a task  Concurrency means the number of independent tasks that can be running at any one time   There are other factors involved such as task processing times but the primary concern of developers will often lie in how fast jobs can start and or how many events can be processed at any given time  In other words  performance optimization of individual tasks is a separate subject and engineering concern    The latency requirements in job processing are highly variable  Transactional events may need immediate processing whereas others can be just fine with relaxed latency requirements   on the order of seconds or minutes  While still others can run on a spectrum from minutes or hours  Much like zero to sixty time and horsepower in the automobile world  it s often the case that the highest performance is not only  not  needed but that optimizing for it becomes a waste of resources and is counterproductive  When thinking about tasks and services  it s important to define the latency needs up front as that will impact how you build the tasks and the pipeline and the requirements you expect out of your serverless platform   One way of looking at latency is in terms of various types of processing  A rough breakdown might separate processing into three categories   real time processing  background processing  and batch processing   Real time Processing   1 sec   Real time processing can mean a lot of things to a lot of people   expectations can range from instantaneous to human real time  It combines both latency to start the task and the task duration  Task start latency has generally settled on 20ms as far as a community standard  Task processing time  however  is a more fungible metric although a general guideline should probably human response expectations for a response which is typically is less than 1 second from start to finish  task latency included    This is fast in terms of distributed cloud processing at scale especially when you factor in network latencies  container overhead  and task workload fluctuations  i e  how many tasks may be in a queue   This type of processing requires making a key architectural choices within both a serverless platform and a serverless application  Servers need to be optimized for a particular set of tasks  containers pre loaded  and task memory and processing limits strictly enforced  Applications need to be constructed with these constraints and critical load monitoring to maintain SLA compliance   Note  One definition holds that only real time processing fits the definition of serverless  i e  task start latency  20ms  but others and I take a more expansive view of the definition defining it as an abstraction layer above the infrastructure where the emphasis is on processing of discrete self contained stateless tasks   Real time Processing   Jobs Processed with Low Latency and Fast Response Rates  Background Processing  seconds and minutes   Background processing is a general term to describe event driven workloads that are processed outside of the main event or response loop in a time sensitive and scalable manner  While the processing requirements may not be on the order of millisecond latency  tasks of this nature often need to be completed in seconds and minutes   Updates to social networks  generation of pdfs  ETL processing  processing of streaming data inputs  processing of images  transcription of audio or video  and other media processing needs   all of these use cases describe where background processing is needed and where a serverless approach fits well  These are the more common use cases that I saw used for serverless computing at the beginning as it modeled the way that job and worker queues worked   Background Processing   Jobs Processed Outside the Request Response Loop  Batch Processing  minutes and hours   Batch processing is not much different than what it s always been   the processing of a large number of important  relatively singular but not necessary time critical tasks  The difference now is that batch processing does not need to wait for particular time windows in the day  or night  or rely on a set of big and beefy servers specifically conditioned for the workloads  Batch processing can take place at any time across thousands of cores in multiple regions  The type of patterns seen with batch processing in the cloud vs legacy systems may be the same  but the method of processing and the self contained nature of the processing units is a dramatic departure from prior renditions   Batch Processing   Jobs Processed En Masse or on Regular Schedules  A Graph on Task Latency  The graph below represents the task latency or wait time on a serverless platform for a particular snap shot of time  Note that the vertical count axis uses a logarithmic scale  The number indicate that the majority of tasks have very little wait time   The outliers could be delayed tasks or certain batch jobs that are deprioritized to run with less restrictive SLAs    Snapshot of Task Latencies  logarithmic scale   Concurrency  With respect to concurrency  this refers to the number of similar tasks that can be executing at any one time  The thresholds can either be dictated by the serverless platform as in the case of a plan or service level threshold  Alternatively  it can be an application based restriction put in place either because latency thresholds are not stringent  i e  tasks can sit in queue for a bit prior to process  or because of resource restrictions and failures can occur downstream with high concurrency  In the case of a database  having too many tasks running at a point in time could exceed the connection limit   Concurrency  Container technology allows for the processing multiple tasks within a single server  Five  ten  twenty  fifty  or more tasks can run on a single server and so concurrency levels or thresholds can be exceedingly large  providing the underlying infrastructure can support an almost limitless capacity  Many serverless platforms are able to autoscale to meet varying concurrency needs  large in the day  smaller at night  or bursting at specific isolated time periods  This limits over provisioning while still meeting concurrency SLAs   Note that we re using the term  concurrency  to refer to independently executing processes much in keeping with Rob Pike s explanation    w hen people hear the word concurrency they often think of parallelism  a related but quite distinct concept  In programming  concurrency is the composition of independently executing processes  while parallelism is the simultaneous execution of  possibly related  computations  Concurrency is about dealing with lots of things at once  Parallelism is about doing lots of things at once    A Graph on Task Flow  The graph below represents the task flow per minute for a specific duration and region for a particular serverless platform  The top line indicates the number of tasks started  the green line indicates tasks finished  and the red lines indicate errors and timeouts   More on errors below   The tasks include all tasks but if a particular processing flow allows for high currency  a serverless approach allows for an almost limitless concurrency and throughput capacity   Snapshot of Task Flow Per Minute  seven day period  single region   Memory Limits  Working at a job or task level does free you up from a number of infrastructure considerations but you never are quite free from resource constraints  Memory and processing time limits still play big factors in a serverless platform  Because tasks are likely running in containers  they are bounded by RAM restrictions  and also I O  port  and other restrictions as may be enforced at the container or platform level    At present  the restriction in most platforms is relatively strict and so developers need to be conscious of the memory requirements of their tasks and stay within the limits of the platform  You may have to restrict the amount of data a task can process by reducing the size of the data slices that tasks are assigned  i e  more tasks greater concurrency   It also might mean being conscious of how a task uses RAM  making sure to use the right data structures to avoid unnecessary allocations  Just as you would profile portions of an application to ensure optimal performance and memory handling  you ll want to do the same within a serverless environment  although it this case it is on a task by task and service by service basis   Note that most serverless platforms will offer local and temporary read write data storage for use by tasks  to be erased when the task ends   Effective use of this type of storage can reduce the need to allocate large blocks of RAM  Effective use of key value data stores is also a big part of serverless programming which we ll go detail in a separate post   In the future  serverless systems are likely to be able to profile tasks and make the appropriate routing to address variable memory needs  although we re not quite there yet  Some serverless platforms  however  do offer different memory configurations so as to accommodate memory intensive tasks  Types of tasks of this nature can include image  audio and video processing  large file processing  and scientific data analysis  In this case  once the workloads are identified either via config or header data or by profiling  it will be a matter of the platform routing the tasks to higher memory processing clusters or alternatively adjusting the container process limits on the fly   Processing Time Limits  How long tasks take to process can have a significant impact on the throughput of a system  Even with autoscaling  a consistently high number of long running tasks will eventually consume and block other tasks from running within their desired timeframes  It is for this reason that serverless platforms will often have strict limits on how long a task can process  In the case of AWS Lambda  the maximum processing time is 300 seconds  In the case of IronWorker  the maximum is 3600 seconds although this can be increased for dedicated  private  and hybrid installations   Some systems may be able to offer less strict processing time limits  One approach is to isolate long running tasks  route them to dedicated clusters  and provide for sufficient resources by autoscaling or provisioning to high maximum thresholds   Processing time limits can have a significant impact on how you might architect a serverless workflow  Much like memory limits  processing time limits can influence how much data a task might be able to process  Developers may need to reduce the size of input data and possibly increase task concurrency  Tasks may be need to be broken into more more discrete functions  It may also cause developers to have to think about external resources or operations that might block   which in turn would cause tasks to timeout   If services and functions are architected correctly  memory and processing limits should not be as restrictive as they might appear  The resource limits in most serverless platforms are generally sufficient for most processing needs  with increased limits available certain platforms   Keeping the issues mentioned in this article in mind will help  Once you get to an  aha  moment with serverless processing   typically it s uploading code and watching it run at high concurrency   your thinking will likely easily adapt to how to structure your functions and workflows to fit this shift in processing   A Graph on Task Duration  The graph below shows the distribution of task durations within the Iron io platform for a particular period of time  As with the graph above  that the vertical count axis makes use a logarithmic scale which means that short running tasks represent the vast majority of the total task volume  Each task is a stateless and ephemeral unit of work running independently from other tasks   Snapshot of Task Durations  logarithmic scale   Synchronous vs Asynchronous  I touched on synchronous vs asynchronous processing in part 1 of this series  Synchronous processing is where a connection is maintained with the calling process while the serverless task is executing  Once processing finished  a response is sent back to the calling process  Asynchronous processing is where the calling function sends a processing request but does not block while the processing is running   Many of the new serverless platforms allow for synchronous processing  The advantage of synchronous processing is that a result may be obtained directly from the processing platform whereas with asynchronous processing  obtaining results has to be done as an independent effort   Synchronous processing is most appropriate for real time needs and for lighter weight functions i e  processing of singular events that you want processed right away  Asynchronous processing is better for longer  more involved processing jobs  audio transcription or processing of a set of events as a small batch processing job  as well as where the application component function initiating the process does not need to block and wait for a result   Note that running in a synchronous processing mode can introduce additional exception or failure issues  Because the calling process blocks  it s paramount that the serverless platform is able to handle the scale of tasks that are sent to it  If not  then the calling applications can potentially block far beyond the expected wait durations  Additionally  the error recovery mechanisms  i e  task retries  and metrics collection may not be as high as in an asynchronous mode  The reason being that processing tolerances for asynchronous processing are much stricter and so compromises may be made   As a result  developers may need to build in exception handling to address blocks as well as put in logging and job processing analysis to make sure that tasks execute as they should,"[354 1066 1123 993 440 1120 52 290 1397 79 1119]"
436,training-dataset/engineering/1180.txt,engineering,Business Case for ServerlessYou can t pick a technical direction without considering the business implications  Mat Ellis  Founder CEO of Cloudability  in a recent CloudCast episode  makes the business case for Serverless  The argument goes something like   Enterprises know they can t run services cheaper than Amazon  Even if the cost is 2x the extra agility of the cloud is often worth the multiple   So enterprises are moving to the cloud   Moving to the cloud is a move to services  How do you build services now  Using Serverless   With services businesses use a familiar cost per unit billing model  they can think of paying for services as a cost per database query  cost per terabyte of data  and so on   Since employees are no longer managing boxes and infrastructure they can now focus entirely on business goals   There s now an opportunity to change business models  Serverless will make new businesses economically viable because they can do things they could never do before based on price and capabilities   Serverless makes it faster to iterate and deploy new code which makes it faster to find a proper product market fit   Smaller teams with smaller budgets with smaller revenues can do things now that only big companies could do before  Serverless attempts to industrialise developer impact,"[436 1119 993 440 1120 52 79 290 1397 1066 1123]"
440,training-dataset/engineering/1489.txt,engineering,Serverless ArchitecturesLike many trends in software there s no one clear view of what  Serverless  is  and that isn t helped by it really coming to mean two different but overlapping areas   Fast forward to today  mid 2016  and one sees examples such as the recent Serverless Conference   plus the various Serverless vendors are embracing the term from product descriptions to job descriptions  Serverless as a term  for better or for worse  is here to stay   We start to see the term used more frequently in 2015  after AWS Lambda s launch in 2014 and even more so after Amazon s API Gateway launched in July 2015  Here s an example where Ant Stanley writes about Serverless following the API Gateway announcement  In October 2015 there was a talk at Amazon s re Invent conference titled   The Serverless Company using AWS Lambda    referring to PlayOn  Sports   Towards the end of 2015 the  Javascript Amazon Web Services  JAWS   open source project renamed themselves to the Serverless Framework   continuing the trend   First usages of the term seem to have appeared around 2012  including this article by Ken Fromm   Badri Janakiraman says that he also heard usage of the term around this time in regard to continuous integration and source control systems being hosted as a service  rather than on a company s own servers  However this usage was about development infrastructure rather than incorporation into products   The term  Serverless  is confusing since with such applications there are both server hardware and server processes running somewhere  but the difference to normal approaches is that the organization building and supporting a  Serverless  application is not looking after the hardware or the processes   they are outsourcing this to a vendor   Mostly I m going to talk about the second of these areas because it is the one that is newer  has significant differences to how we typically think about technical architecture  and has been driving a lot of the hype around Serverless   However these concepts are related and  in fact  converging  A good example is Auth0   they started initially with BaaS  Authentication as a Service   but with Auth0 Webtask they are entering the FaaS space   Furthermore in many cases when developing a  BaaS shaped  application  especially when developing a  rich  web based app as opposed to a mobile app  you ll likely still need some amount of custom server side functionality  FaaS functions may be a good solution for this  especially if they are integrated to some extent with the BaaS services you re using  Examples of such functionality include data validation  protecting against imposter clients  and compute intensive processing  e g  image or video manipulation    The FaaS environment may also process several clicks in parallel by instantiating multiple copies of the function code   depending on how we d written the original process this may be a new concept we need to consider   There s a much smaller difference to the architecture here compared to our first example  We ve replaced a long lived consumer application with a FaaS function that runs within the event driven context the vendor provides us  Note that the vendor supplies both the Message Broker and the FaaS environment   the two systems are closely tied to each other   Traditionally  the architecture may look like this  The  Ad Server  synchronously responds to the user   we don t care about that interaction for the sake of this example   but it also posts a message to a channel that can be asynchronously processed by a  click processor  application that updates a database  e g  to decrement the advertiser s budget   A different example is a backend data processing service  Say you re writing a user centric application that needs to quickly respond to UI requests  but secondarily you want to capture all the different types of activity that are occurring  Let s think about an online ad system   when a user clicks on an advertisement you want to very quickly redirect them to the target of the ad  but at the same time you need to collect the fact that the click has happened so that you can charge the advertiser   This example is not hypothetical   my former team at Intent Media recently went through this exact redesign    Since the original server was implemented in Java  and AWS Lambda  our FaaS vendor of choice in this instance  supports functions implemented in Java  we can port the search code from the Pet Store server to the Pet Store Search function without a complete re write   This is a massively simplified view  but even with this there are a number of significant changes that have happened here  Please note this is not a recommendation of an architectural migration  I m merely using this as a tool to expose some Serverless concepts   With this architecture the client can be relatively unintelligent  with much of the logic in the system   authentication  page navigation  searching  transactions   implemented by the server application   Traditionally the architecture will look something like this  and let s say it s implemented in Java on the server side  with a HTML   Javascript component as the client   Another example is Apex   a project to  Build  deploy  and manage AWS Lambda functions with ease   One particularly interesting aspect of Apex is that it allows you to develop Lambda functions in languages other than those directly supported by Amazon  e g  Go   Apart from runtime implementation though there are already open source tools and frameworks to help with definition  deployment and runtime assistance  For instance the Serverless Framework makes working with API Gateway   Lambda significantly easier than using the first principles provided by AWS  It s Javascript heavy but if you re writing JS API Gateway apps it s definitely worth a look   One of the main benefits of Serverless FaaS applications is transparent production runtime provisioning  and so open source is not currently as relevant in this world as it is for  say  Docker and containers  In future we may see a popular FaaS   API Gateway platform implementation that will run  on premise  or on a developer workstation  IBM s OpenWhisk is an example of such an implementation and it will be interesting to see whether this  or an alternative implementation  picks up adoption   Debugging and monitoring are tricky in general in Serverless apps   we ll get into this further in subsequent installments of this article   The comment above about API Gateway tooling being immature actually applies  on the whole  to Serverless FaaS in general  There are exceptions however   one example is Auth0 Webtask which places significant priority on Developer UX in its tooling  Tomasz Janczuk gave a very good demonstration of this at the recent Serverless Conference   At present tooling for API gateways is achingly immature and so while defining applications with API gateways is possible it s most definitely not for the faint hearted   One use case for API Gateway   FaaS is for creating http fronted microservices in a Serverless way with all the scaling  management and other benefits that come from FaaS functions   Beyond purely routing requests API Gateways may also perform authentication  input validation  response code mapping  etc  Your spidey sense may be buzzing about whether this is actually such a good idea  if so hold that thought   we ll consider this further later   One aspect of FaaS that we brushed upon earlier is an  API Gateway   An API Gateway is an HTTP server where routes   endpoints are defined in configuration and each route is associated with a FaaS function  When an API Gateway receives a request it finds the routing configuration matching the request and then calls the relevant FaaS function  Typically the API Gateway will allow mapping from http request parameters to inputs arguments for the FaaS function  The API Gateway transforms the result of the FaaS function call to an http response  and returns this to the original caller   Whether or not you think your app may have problems like this you should test with production like load to see what performance you see  If your use case doesn t work now you may want to try again in a few months time since this is a major area of development by FaaS vendors   Are these issues a concern  It depends on the style and traffic shape of your application  My former team has an asynchronous message processing Lambda app implemented in Java which processes hundreds of millions of messages   day  and they have no concerns with startup latencey  That said if you were writing a low latency trading application you probably wouldn t want to use FaaS systems at this time  no matter the language you were using for implementation   The former of these may be avoided in certain situations by the ugly hack of pinging your function every 5 minutes to keep it alive   If your Lambda function is implemented on the JVM you may occasionally see long response times  e g    10 seconds  while the JVM is spun up  However this only notably happens with either of the following scenarios   If your function is implemented in Javascript or Python and isn t huge  i e  less than a thousand lines of code  then the overhead of running it in should never be more than 10   100 ms  Bigger functions may occasionally see longer times   At present how long it takes your FaaS function to respond to a request depends on a large number of factors  and may be anywhere from 10ms to 2 minutes  That sounds bad  but let s get a little more specific  using AWS Lambda as an example   This means that certain classes of long lived task are not suited to FaaS functions without re architecture  e g  you may need to create several different coordinated FaaS functions where in a traditional environment you may have one long duration task performing both coordination and execution   FaaS functions are typically limited in how long each invocation is allowed to run  At present AWS Lambda functions are not allowed to run for longer than 5 minutes and if they do they will be terminated   Given this restriction what are alternatives  Typically it means that FaaS functions are either naturally stateless   i e  they provide pure functional transformations of their input   or that they make use of a database  a cross application cache  e g  Redis   or network file store  e g  S3  to store state across requests or for further input to handle a request   FaaS functions have significant restrictions when it comes to local  machine   instance bound  state  In short you should assume that for any given invocation of a function none of the in process or host state that you create will be available to any subsequent invocation  This includes state in RAM and state you may write to local disk  In other words from a deployment unit point of view FaaS functions are stateless    Let s return to our click processor  Say that we were having a good day and customers were clicking on 10 times as many ads as usual  Would our click processing application be able to handle this  For example did we code to be able to handle multiple messages at a time  Even if we did would one running instance of the application be enough to process the load  If we are able to run multiple processes is auto scaling automatic or do we need to reconfigure that manually  With FaaS you need to write the function ahead of time to assume parallelism  but from that point on the FaaS provider automatically handles all scaling needs   Let s consider our click processing example again   the only code that needs to change when moving to FaaS is the  main method   startup  code  in that it is deleted  and likely the specific code that is the top level message handler  the  message listener interface  implementation   but this might only be a change in method signature  All of the rest of the code  e g  the code that writes to the database  is no different in a FaaS world   If we go back to our click processing example from earlier what FaaS does is replace the click processing server  possibly a physical machine  but definitely a specific application  with something that doesn t need a provisioned server  nor an application that is running all the time   AWS Lambda lets you run code without provisioning or managing servers   1      With Lambda  you can run code for virtually any type of application or backend service  2    all with zero administration  Just upload your code and Lambda takes care of everything required to run  3  and scale  4  your code with high availability  You can set up your code to automatically trigger from other AWS services  5  or call it directly from any web or mobile app  6     We ve mentioned the FaaS idea a lot already but it s time to dig into what it really means  To do this let s look at the opening description for Amazon s Lambda product  I ve added some tokens to it  which I then expand upon   What isn t Serverless   So far in this article I ve defined  Serverless  to mean the union of a couple of other ideas    Backend as a Service  and  Functions as a Service   I ve also dug into the capabilities of the second of these   Before we start looking at the very important area of benefits and drawbacks I d like to spend one more moment on definition  or at least defining what Serverless isn t  I ve seen some people  including me in the recent past  get confused about these things and I think it s worth discussing them for clarity s sake   Comparison with PaaS Given that Serverless FaaS functions are very similar to 12 Factor applications  are they in fact just another form of  Platform as a Service   PaaS  like Heroku  For a brief answer I refer to Adrian Cockcroft If your PaaS can efficiently start instances in 20ms that run for half a second  then call it serverless     Adrian Cockcroft In other words most PaaS applications are not geared towards bringing entire applications up and down for every request  whereas FaaS platforms do exactly this  OK  but so what  if I m being a good 12 Factor App developer there s still no difference to how I code  That s true  but there is a big difference to how you operate your app  Since we re all good DevOps savvy engineers we re thinking about operations as much as we are about development  right  The key operational difference between FaaS and PaaS is scaling  With most PaaS s you still need to think about scale  e g  with Heroku how many Dynos you want to run  With a FaaS application this is completely transparent  Even if you setup your PaaS application to auto scale you won t be doing this to the level of individual requests  unless you have a very specifically shaped traffic profile   and so a FaaS application is much more efficient when it comes to costs  Given this benenfit  why would you still use a PaaS  There are several reasons but tooling  and maturity of API gateways  are probably the biggest  Furthermore 12 Factor Apps implemented in a PaaS may use an in app readonly cache for optimization  which isn t an option for FaaS functions   Comparison with containers One of the reasons for Serverless FaaS is to avoid having to manage computational processes at the operating system level or lower  Platforms as a Service  like Heroku  are another  and I ve described above how PaaS s are different to Serverless FaaS  Another popular abstraction of processes are containers  with Docker being the most visible example of such a technology  We also see increasing popularity of container hosting systems  such as Mesos and Kubernetes  which abstract individual applications from OS level deployment  And even further there are cloud hosting container platforms like Amazon ECS and Google Container Engine which  like Serverless FaaS  let teams avoid having to manage their own server systems at all  So given all the momentum around containers is it still worth considering Serverless FaaS  Principally the argument I made for PaaS still holds with containers   for Serverless FaaS scaling is automatically managed  transparent  and fine grained  Container platforms do not yet offer such a solution  Furthermore I d argue that container technology while having seen massive popularity in the last couple of years is still not mature  That s not to say that Serverless FaaS is mature either  but picking which rough edges you d like is still the order of the day  I ll admit  however  that both of these arguments may start to wear thin over time  While true no management auto scaling in container platforms isn t at the level of Serverless FaaS yet  we see areas like Kubernetes  Horizontal Pod Autoscaling  as tending towards it  I can imagine some very smart traffic pattern analysis being introduced to such features  as well as more load implying metrics  Furthermore the rapid evolution of Kubernetes may give a wonderfully simple  stable  platform before too long  If we see the gap of management and scaling between Serverless FaaS and hosted containers narrow the choice between them may just come down to style  and type of application  For example it may be that FaaS is seen as a better choice for event driven style with few event types per application component  and containers are seen as a better choice for synchronous request driven components with many entry points  I expect in 5 years time that many applications and teams will use both architectural approaches  and it will be fascinating to see patterns of such use emerge    NoOps Serverless doesn t mean  No Ops   It might mean  No internal Sys Admin  depending on how far down the serverless rabbit hole you go  There are 2 important things to consider here  Firstly  Ops  means a lot more than server administration  It also means at least monitoring  deployment  security  networking and often also means some amount of production debugging and system scaling  These problems all still exist with Serverless apps and you re still going to need a strategy to deal with them  In some ways Ops is harder in a Serverless world because a lot of this is so new  Second even the Sys Admin is still happening   you re just outsourcing it with Serverless  That s not necessarily a bad thing   we outsource a lot  But depending on what precisely you re trying to do this might be a good or a bad thing  and either way at some point the abstraction will likely leak and you ll need to know that human sys admins somewhere are supporting your application  Charity Majors gave a great talk on this subject at the recent Serverless Conference and I recommend checking it out once it s available online  Until then you can read her write up here and here,"[79 440 1120 52 1397 290 1119 1066 354 993 1123]"
993,training-dataset/engineering/369.txt,engineering,Part 4 of Thinking Serverless   Addressing Security IssuesMonday  March 6  2017 at 8 56AM  This is a guest repost by Ken Fromm  a 3x tech co founder   Vivid Studios  Loomia  and Iron io  Here s Part 1 and 2 and 3   This post is the last of a four part series of that will dive into developing applications in a serverless way  These insights are derived from several years working with hundreds of developers while they built and operated serverless applications and functions   The platform was the serverless platform from Iron io but these lessons can also apply to AWS Lambda  Google Cloud Functions  Azure Functions  and IBM s OpenWhisk project   Arriving at a good definition of cloud IT security is difficult especially in the context of highly scalable distributed systems like those found in serverless platforms  The purpose of this post is to not to provide an exhaustive set of principles but instead highlight areas that developers  architects  and security officers might wish to consider when evaluating or setting up serverless platforms   Serverless Processing   Similar But Different  High scale task processing is certainly not a new concept in IT as it has parallels that date back to the days of job processing on mainframes  The abstraction layer provided by serverless process   in combination with large scale cloud infrastructure and advanced container technologies   does  however  bring about capabilities that are markedly different than even just a few years ago   By plugging into an serverless computing platforms  developers do not need to provision resources based on current or anticipated loads or put great effort into planning for new projects  Working and thinking at the task level means that developers are not paying for resources they are not using  Also  regardless of the number of projects in production or in development  developers using serverless processing do not have to worry about managing resources or provisioning systems   While serving as Iron io s security officer  I answered a number of security questionnaires from customers  One common theme is that they were all in need of a serious update to bring them forward into this new world  Very few had any accommodation for cloud computing much less serverless processing   Most questionnaires still viewed servers as persistent entities needing constant care and feeding  They presumed physical resources as opposed to virtualization  autoscaling  shared resources  and separation of concerns  Their questions lack differentiation between data centers and development and operation centers  A few still asked for the ability to physically inspect data centers which is  by and large  not really an option these days  And very few addressed APIs  logging  data persistence  or data retention   The format of the sections below follows the order found in many of these security questionnaires as well as several cloud security policies  The order has been flipped a bit to start with areas where developers can have an impact  Later sections will address platform and system issues which teams will want to be aware of but are largely in the domain of serverless platforms and infrastructure providers   Security Topics  Data Security  Data security is one area where developers can take steps to augment existing security measures serverless platforms might have in place  A first concern should be to make sure data payloads are secure not just in transit but also at rest  Because there can be many independent workloads and processes in place and because processing can take place across a large number of cores   and potentially regions and even clouds   it is important to treat each task payload as a separate entity with separate exposure and threat vectors   Yes  SSL should be used when transmitting data  virtual private networks can and should be deployed  and data storage components can offer encryption capabilities but even with these measures  task payloads can and will be replicated and persisted in a number of places  Payloads may be placed on message queues  for example  or included as part of audit trails   And even after a payload is processed  it may persist in the system for a longer period of time as a deleted archived record but not fully erased  How long a processed workload might persist in a system depends on the platform s data retention and backup policies   Data Encryption  The potential exposure of task payloads means that developers should make sure that task payloads are encrypted separate from any network encryption  At run time  the task can decrypt the payload as an initial step in the processing   Some serverless platforms may encrypt task payloads as a normal course of operation but you will want to verify with the platform in question  Even with platform level encryption  you may want to encrypt task payloads on your own as an added measure as any system decryption keys will be out of your hands and therefore at risk for compromise by internal or external parties   Encrypting Task Payloads  A couple considerations to point out here   Providing a key for tasks to use to decrypt data does take conscious thought  Given tasks are largely stateless and ephemeral  the only form of data available for tasks to use comes from task payloads  data calls  or config data   the access to which is provided by the serverless platform  Given storing the key in the task payload is not an option and data calls are expensive and impractical for this use case  the only real way to provide a decrypt key is via the config data  Storing the key as part of the platform config data does mean the key is available and partially exposed within your system  Some platforms do allow for both global and local config data which means in the case of the latter  access would only be for the particular class of task or function   There have been some recent changes in Docker container security which could open up avenues for either injecting keys into containers or for sharing between containers  This can be helpful especially in situations where task containers are not broken down after execution but instead persist and process a continual stream of payloads   Note that serverless platforms will be likely be the ones to make use of these capabilities as as opposed to end users    Encrypting payloads on the client side will make introspection of tasks more difficult  A number of the serverless platforms offer dashboards that allow introspection of individual tasks including display of the task payloads  Encrypting payloads can make this capability less useful and debugging errors in tasks harder   Data Partitioning  Another way to ensure safe payload handling when processing jobs in the cloud is to take advantage of data partitioning strategies  Data partitioning refers to separating elements of a data object and only providing a task with the elements needed for that task to process  In the email example described in Part 1 of this series  data partitioning means sending only the object or objects needed by a task and not the entire email message  In this way  task payloads cannot expose what they do not have   Partitioning Data  Data Anonymization  Similarly  data anonymization is a way of sanitizing information by either removing or obscuring certain elements of the data so as to protect personally identifiable information   The Privacy Technology Focus Group defines it as  technology that converts clear text data into a non human readable and irreversible form  including preimage resistant hashes  e g   one way hashes  and encryption techniques in which the decryption key has been discarded   1   Data anonymization is often used as a way to transfer and share data between entities such as two different companies or agencies  In the context of medical data  it can refer to data from a patient not being able to identify the patient  Within a task processing environment  keys can be used in replacement of the sensitive data and then the results de anonymized by cross referencing the keys with other data to re identify and re associate the processed data   Anonymizing Sensitive IDs in Task Payloads  A Note on Adding in Security Measures  Security often seems to be an afterthought   addressed after getting things running and into production  It is better though to put basic measures in place sooner in the process rather than later   Adding in security measures as part of a secondary release can easily cross into several components or layers of the architecture which only magnifies the time and cost to do it later  It is also not uncommon for outwardly facing features to take priority over internal fixes   For example  encrypting  partitioning  or anonymizing data after the fact introduces a ton of additional complexity and brings with it the need to perform a full set of comprehensive tests  This work wouldn t be needed if data protection was added in at the start   Task Security  Container Isolation Security  Because tasks can be running in shared public servers  it is critical that serverless platforms maintain strict isolation between tasks  Fortunately  container technologies provide such a mechanism  Serverless platforms that make proper use of containers can make it so tasks run within their own isolated environments and are unable to interact with other tasks or areas of the system  Each task container is able to isolate processes  memory  and the file system  Additionally  host based firewalls can add additional restrictions to prevent applications from establishing local and inbound network connections   Note though that despite strict isolation there still is the potential for disruption via a noisy neighbor issue  Despite running as separate entities  tasks running on a server can collide for resources thereby creating blocks and or limitations  For example  while the stated memory limit for a task may be a set size  the combination of limits may exceed the total resource limit for the server   that is if the all the tasks max out their memory and the system limit is less than the sum of the tasks  This situation could result in a task failure which might be hard to debug as it might show memory error but pertain to a server limit as opposed to a task limit   Containers Isolate System Resources for Tasks  Concurrency Limits  Monitoring  and Alerts  Another area that developers can take measures to create a more secure processing environment is to apply reasonable limits on the task parameters wherever possible  The primary limit to address is concurrency but other limits such as memory or task duration can also apply  In the case where demand for a service or function is highly scalable  concurrency limits may not be practical  In cases where demand is within a narrower range  however  projects can benefit by using concurrency as a governor against unintended actions   Note that even under throttled conditions  a large amount of tasks can still be processed  Applying practical concurrency limits  however  can allow for alerts and monitoring measures to kick in to catch unintended or malicious activity   If alerting capabilities exist within a serverless platform then putting in alerts for queue sizes  concurrency  or wait times can make a lot of sense  For platforms that lack alerting capabilities  consider making use of real time logging services to set up relevant triggers and notifications  Highly concurrent processing does bring with it the risk of out of control processing scenarios  Putting in restrictions and monitoring to address them makes both good business and engineering sense   Code Security  A related item with task security is code security  Ideally  serverless platforms will encrypt the code that is stored in a system and decode it at run time  If the code is stored as a container and the container is encrypted by the platform  then the issue should largely be taken care of  As with difficulties inspecting encrypted payloads  inspecting encrypted code via a dashboard may pose interface challenges for a serverless platform   Additionally  it makes sense to add in code and image scanning as part of the process of uploading tasks into a serverless platform  Even though tasks and functions tend to be smaller and easier to manually inspect  third party code packages can contain malicious code  Creating scripts or putting in place an automated system to scan and verify code is good way to gain additional peace of mind   System Security  System Verification and Updates  With serverless architectures  many system level security measures take place prior to run time  Because tasks in a serverless platform are not meant to run on an infinite duration  servers are viewed as largely disposable  In other words  instead of running a virus scan or applying runtime patches to production servers  servers can be simply be removed from the distribution list that job controllers use to route tasks  After all tasks have completed processing on these servers  the servers are terminated and other servers launched and placed within the rotation as replacements   As a result  serverless platforms will often include image container verification and virus scanning as part of the development and deployment stages of their ops cycle and will recycle them on a regular schedule  This is different from past behaviors where performing checks  tests  and updates on production systems is more standard  One advantage is that by limiting both access and changes to production systems  ops teams can reduce the attack vectors that are possible on production servers   Since base Docker images and other container images can have vulnerabilities  it is important for the serverless platforms themselves to know about these and address them  Docker recommends the use of animage scanner  Likewise CoreOS has a similar project   Lifecycle for a Task Processing Server  System Throttling  Access Controls  and Workflow Management  The concept of limiting concurrency and other task resources is mentioned in a previous section  When it comes to managing large scale distributed workflows  though  this step is only a basic on  It is somewhat brittle as teams have to be aware of the limits and increase them when loads increase   To properly manage serverless workflows  a higher level approach is needed that acts as an overlay over all calls and invocations in a system   not just calls for task processing  In other words  processing a single task is one element in what could be a series of task invoccations  message queue calls  cache hits  database accesses  and external http requests   Gaining insight to these calls and managing them means addressing it via a layer above the processing platform  As we are in somewhat early in this serverless cycle  no clear approach has surfaced on how to provide this global control layer  Given the interest and growth rate in serverless computing  however  I expect there to be solid movement in this area in the near and mid term   Network Security  API Gateways  API gateways provide a uniformed and constrained method of service access with strict authentication   OAuth or JWT  for example  API gateways also typically offer sophisticated monitoring and alerting capabilities  They are critical components in reducing security threats and enabling improved monitoring of system activity   A recommendation here is to make use of private API endpoints if they are available  This move can help mitigate the effects of a DDOS attacks on publicly available API endpoints   Dereferenced URls and or direct IP addresses can address DDOS attacks on nameservers although this is a more advanced topic to address with a platform s security experts   Proper Use of APIs are Critical within a Distributed Processing Workflow  Virtual Private Networks  Virtual private networks are another measure to increase network security  VPNs can be set up and provisioned much more easily than physical networks and firewalls  Most cloud infrastructures will provide these capabilities which  in turn  means serverless platforms can inherit them and offer them as additional features to their customers   VPNs in the context of serverless processing are worth any initial upfront effort to employ  Using a VPN is another measure that is easier to do at the start then to try and add it in later   Penetration Testing  In terms of penetration testing  the use of APIs as well as highly limited access to internal components makes this issue an easier one to address on one level   fewer access targets in combination with more limited access methods   On the other hand  the distributed topology and componentry involved in a serverless architecture as well as the large number of server makes it important that these servers and components have strict conformity regarding internal access methods and are not available in any public form   Fortunately  most network security measures  aside from making use of private API endpoints and VPNs where available  will be the domain of the serverless platform and or the underlying infrastructure provider   Physical Security  Most security questionnaires will often address physical security as a one size fits all proposition  It goes without saying  however  that production data centers should be considered differently than development offices or business operation centers  Gaining unauthorized access into a business office is a serious matter but comprising a data center running production workloads is on a completely different scale   Data centers running production and development workloads  for example  are not likely to provide inspection capabilities except in the most severe circumstances  Development and operations centers may not store data onsite  Questions and concerns should therefore be tailored so as to separate workload processing and data management of production loads  i e  your workloads and your data  from development efforts and business operations   As part of this understanding  you will want to understand the full topology of the platform   where code and data components are stored and processed  how logging is handled  and any downstream providers that may be involved  Fortunately  most serverless platforms will use reputable cloud infrastructures which will  by and large  have sophisticated physical security measures in place thereby allowing them to inherit these measures   Datacenters  Development Offices  and Business Ops Centers  Summary  Making use of a serverless approaches is not only a tactical tool for resource optimization but also a strategic necessity that can be directly applied to increase organizational speed and agility  Going serverless lets developers shift their focus from the server level to the task level  It lets them focus on the features and capabilities of their applications and systems instead of managing the complexity of the backend infrastructure   Despite the title of this series  there are often less things to think about when moving to a serverless architecture  Writing code  uploading it  and seeing it run out the gate at high concurrency never gets old  This agility translates into faster cycles for creating things and releasing them into production  Things do get more complicated when you bring in task issues  workflows  data issues  and security but some simple foresight  steady guidelines  and solid architectural principles will make these seem as second nature   Back in 2012  I made a call back that the future of computing will be serverless  Here in 2017  that view has become even more clear,"[993 1123 1066 354 1119 440 1120 52 290 79 1397]"
1066,training-dataset/engineering/855.txt,engineering,Part 1 of Thinking Serverless   How New Approaches Address Modern Data Processing NeedsMonday  January 30  2017 at 8 56AM  This is a guest repost by Ken Fromm  a 3x tech co founder   Vivid Studios  Loomia  and Iron io   First I should mention that of course there are servers involved  I m just using the term that popularly describes an approach and a set of technologies that abstracts job processing and scheduling from having to manage servers  In a post written for ReadWrite back in 2012 on the future of software and applications  I described  serverless  as the following   The phrase  serverless  doesn t mean servers are no longer involved  It simply means that developers no longer have to think that much about them  Computing resources get used as services without having to manage around physical capacities or limits  Service providers increasingly take on the responsibility of managing servers  data stores and other infrastructure resources Going serverless lets developers shift their focus from the server level to the task level  Serverless solutions let developers focus on what their application or system needs to do by taking away the complexity of the backend infrastructure   At the time of that post  the term  serverless  was not all that well received  as evidenced by the comments on Hacker News  With the introduction of a number of serverless platforms and a significant groundswell on the wisdom of using microservices and event driven architectures  that backlash has fortunately subsided   A Sample Use Case  Since it is useful to have an example in mind as I discuss issues and concerns in developing a serverless app  I will use the example of a serverless pipeline for processing email and detecting spam  It is event driven in that when an email comes in  it will spawn a series of jobs or functions intended to operate specifically on that email   In this pipeline  you may have tasks that perform parsing of text  images  links  mail attributes  and other items or embedded objects in the email  Each item or element might have different processing requirements which in turn would entail one or more separate tasks as well as even its own processing pipeline or sequence  An image link  for example  might be analyzed across several different processing vectors to determine the content and veracity of the image  Depending on the message scoring and results   spam or not   various courses of actions will then be taken  which would likely  in turn  involve other serverless functions   Thinking at the Task Level  The unit of scale within a serverless environment is the task or job  It is an instantiation and execution of a finite amount of processing around a particular workload  Task processing has existed since the beginning of programming and so to some  it may seem as if not a lot is new  But given the highly distributed nature and abstracted manner in which workloads are processed  it is useful to have a broad understanding across many levels of the process   Synchronous vs Asynchronous  While the nature of processing of a task   whether it s synchronous or asynchronous is more of a platform issue  it is an important element to consider at the task level  Traditional worker and job processing systems have largely been asynchronous  meaning that calling process does not maintain a persistent connection to the entity or component performing the task processing  Jobs will be queued up and  as such  they may not run instantly  The only specific connection between calling function and processor is queuing the task up for running   Note that certain platforms may allow for introspection of tasks to obtain status but via API calls and not direct persistent connections    Many of the new serverless platforms allow for synchronous processing whereby the connection is maintained and the client waits while the function is processing  The advantage of synchronous processing is that a result may be obtained directly from the processing platform whereas with asynchronous processing  obtaining results has to be done as an independent effort  I ll go into more detail in the platform section although a general rule is that synchronous processing is appropriate for lightweight functions  similar to an API call to obtain weather information  whereas asynchronous processing is better for longer  more involved processing jobs  audio transcription or processing of a set of events as a small batch processing job  as well as where the application component function initiating the process is not the one to process the results   Stateless  One of the core tenets in developing microservices and or serverless functions  regardless of the processing approach  is that each service or function should be considered stateless  Stateless refers to each task being a separate and distinct processing request containing enough information on its own to fulfill this request  Services and functions should not store any unique software configuration or state within them  Any configuration data should come from outside the function  commonly as part of the task payload or via a config capability within the platform  The function should be used solely for its computational resources  persisting only for the processing of a singular workload   Additionally  there should be a distinct beginning state and end state  and the service or function should process each payload in the same manner  Borrowing from the principles of clean code  bad code   and bad microservices and serverless functions   tries to do too much  Clean code   and likewise clean microservices and functions   should be focused and largely conform to the Single Responsibility Principle  SRP   A good way at thinking about serverless functions is that each function should have one and only one dimension or vector of change  In other words  if there are multiple ways that a function might be extended  image analysis that would check for multiple characteristics  for example   then it s likely that there should be two or more distinct functions for each vector   In the use case we re using  each email is a separate event  and so each would have a separate sequence of tasks  Each task would bear a payload that provides the data for each task or function to process   Ephemeral  Serverless functions are also ephemeral   meaning that they persist for a limited period of time  The basis of a serverless application is largely around event handling and the task processing that takes place in service of these events  The advent of powerful container technology makes it so that tasks can be processed in a distributed environment with the decisions on where to run made at runtime   In other words  the task processing essentially becomes container processing with the containers set up and removed on a task by task basis   For greater insights into containers and how they fit within a serverless platform  you can read several past articles I co wrote on the topic  They go into great detail on why containers and highly scalable task processing go hand in hand  You can find the posts here and here   By way of example  each task in the email processing example only persists to perform a particular action for a specific email  After it completes  the task and the container should terminate   There may be situations where persistent or long running processes are needed   as in the case of application servers or API servers  but these fall outside of a serverless paradigm  In most cases where you feel you may need long running tasks  it is more than likely that there are ways to avoid this overhead  For example  serverless platforms  message queues  or other components might be able to accommodate any routing needs  Likewise  scheduled tasks might be able to provide regular status checks or processing cycles  An example here is consolidating and processing streaming data from a various number of IoT devices  If data is collected in one or more queues or databases  scheduled jobs can run on periodic  and frequent  bases  look at data in a queue or in a database and initiate one or more sub tasks to perform the consolidation and processing of each data slice   Note that in synchronous and or real time serverless processing scenarios  the container may not terminate after each task  largely for performance reasons  The containers may persist from task to task but their state and storage will be erased and reset such that each task or event processing cycle is isolated and ephemeral   Idempotent  Idempotence is a critical attribute to build into microservices and serverless functions  At the base level  it is the ability to be able to run the same task and get the same result  It is also the ability to make it so that multiple identical requests have the same effect as a single request  It s this second definition that is critical to design for when tasks are operating a highly concurrent and asynchronous manner  In any job processing environment  tasks may not complete for any number of reasons   server crashes  resource limits  third party service timeouts  task timeouts  and more   In other cases  a task may complete but a duplicate processing request for the same payload might have been invoked  An example of this is a queue registering a time out of a message because the task might still be processing a request  and therefore didn t delete or unreserve the message in time   As a result  the queue might trigger another processing request for that message payload   If a task just goes ahead and processes the payload   puts it on a queue or writes it to a database  it may have bad effects  especially in a transactional situation  For example  two orders may go out  It is paramount then to make sure that only one request gets processed for the same payload  It is for this reason that developers working in a serverless platform  as in most other processing worlds  put effort into performing checks prior to processing and or prior to writing or outputting the results   Another way of thinking about this  in the words of one developer friend  is  Think about it as if a server crashed while the task is in mid processing and the task was retried  Or if it just gets queued or scheduled twice  What do you need to do to make sure you re not overwriting data  adding a duplicate transaction  or generally mucking with things because it ran again   There  simple  Check and validate at the appropriate points in the processing cycle to make sure the work hasn t already been performed   Polyglot  Polyglot programming refers to programming in multiple languages  In the case of serverless programming  it refers to the ability to write and execute tasks in multiple languages  Although each function likely to be just one language  the right serverless platform should be able to handle a multitude of languages  Which means that it should also provide a significant level of code independence such that developers can work transparently and not have to concern themselves with OS and server level dependencies   The advantage of course  is the ability to use the right tool for the right job  Or alternatively  to use the right team for the right job  Too often in development  the adage rings true that if all you have is a hammer  you see everything as a nail  With a single language  an approach to a problem can be constrained  Developers may struggle to adapt code packages in that language to fit their need  when in fact are other libraries in other languages can better serve the purpose   For calculating Bayesian statistics or doing machine learning  you may want to make use of packages that are written in C  C    Python  or Java  Likewise  a development team that you work with may be proficient in a particular web crawling package that is written for a particular language  Nokogiri written in Ruby for example   Being able to make use of this knowledge and experience can eliminate development cycles as well as reduce the risk of a late or failed project   Note that a number of the newer serverless platforms support only a handful of languages at present but I expect that to change quickly over the course of this year   IronWorker  for example  is able to handle most popular languages and executable code    Compatible  Serverless tasks need designed with two orders of compatibility in mind  The first is compatibility among tasks and the second is across versions  It goes without saying that when you split things up   as you do with microservices and serverless functions   you need strong contracts and specifications between components  Solid API formats can solve a part of the issue via common auth and transfer protocols but with custom functions and services  developers are still tasked with defining input and output schemas that make sense and are easy to understand   In addition to clean interfaces  developers need to address versioning needs  For example  suppose that function X is running within a platform and it invokes function Y  If function Y has been updated without function X knowing about it and function Y is not designed correctly  it could fail when processing or it could produce an incorrect result  or it cause downstream failure in a task that is expecting the original result    As with code packages  changes and updates within a serverless task can ripple through applications  In the case of code packages  these conflicts might be caught early via packaging and compilation tools  With microservices and serverless programming  however  it might only be through running services  preferably in test or staging  that issues become known   Which means that with all this statelessness and task and service independence  teams need to be careful about not only designing well constructed interfaces but also addressing backwards compatibility and rolling out updates in a mindful manner  One help can be to use semantic versioning in describing versioning serverless tasks  It s not a panacea but with the propagation of this versioning convention  it does provide some grounding for other developers who may be using tasks you create   This need for compatibility and consistency also means that testing of tasks used in production should be ongoing  Fortunately with serverless processing  that capability is largely baked in  Because serverless tasks are  by nature  executable  this capability provides a nearly self contained framework for continuous testing  Setting up regular tests with various inputs is then the only other element that needs to be added to the mix  If a serverless platform provides for schedules jobs  then boom  that need is solved   To Be Continued   But this is not all the issues when working with serverless platforms just some as they relate to the task level  Over the course of the next two weeks  I ll be posting other sections on serverless processing  Here s the breakdown  Stay tuned,"[1066 354 1123 993 440 1120 52 290 1397 79 1119]"
1119,training-dataset/engineering/1290.txt,engineering,Serverless Architecture  Five Design PatternsAmazon Web Services is seeing five predominant usage patterns for its Lambda serverless service  according to a presentation at the APIdays Australia conference last week in Sydney  given by AWS Solution Architect Cassandra Bonner   Summing up the current maturity of serverless technologies  consultant Ken Fromm recently wrote on A Cloud Guru s Medium blog that serverless architecture allows developers to focus on managing what their applications and systems need to do  rather than the physical capabilities  limits and complexities of their backend infrastructure  That makes a job or task the unit of scale in a serverless environment   That means  in the main  tasks tend to be asynchronous  stateless and ephemeral  or as Fromm puts it   The task processing essentially becomes container processing with the containers set up and removed on a task by task basis    At APIdays  Bonner says the AWS team sees serverless as comprising four main principles   Simple but usable primitives  i e  small  useable building blocks    Scales with usage  servers are autoscaled on the user s behalf    Pay only usage  customers only pay for the time using services    Built in availability and fault tolerance  i e  NoOps    Earlier at APIdays  Peter Stanski  a Senior Manager in Solutions Architecture at Amazon Web Services in Australia  had said that the traditional adoption path of AWS in the Australian and New Zealand market  as for much of the world  was changing among medium businesses and enterprises   For the past year or two  whereas startups had usually been  all in  from the beginning  established businesses with legacy systems often traveled a path from developing and testing proof of concepts  building new applications  adding analytics and then starting to migrate some of their legacy applications  finally focusing on migrating mission critical apps to AWS   This year  Stanski said  businesses of all sizes tended to be  all in  from the very beginning  They would embark on migration activities of existing legacy stacks while at the same time building new applications directly from their cloud hosting services   Bonner described a similar trend in the uptake of serverless  While new applications might be being built leveraging a serverless architecture  for the most part  businesses were employing hybrid approaches that saw new features built in a serverless environment and then weaving in and out of existing application hosting architecture environments    This is how we are seeing a lot of customers starting off   says Bonner  pointing to one common example  a business might have an existing workflow of managing image data and were now adding an image recognition feature that was being run in a serverless environment and then the catalogued findings being piped back into the existing legacy workflow  which might be situated in a user s cloud hosted environment   The five serverless patterns for use cases that Bonner defined were   Event driven data processing  Web applications  Mobile and Internet of Things applications  Application ecosystems  Event workflows   Use case  1  Event driven Data Processing  One of the most common applications for serverless environments is to trigger actions after an event occurs  Bonner gave the example of an image object being added to an Amazon S3 bucket  That may trigger a lambda function  for example  to run a compression task so that the image is re archived into S3 in a standard size format   This style of use case also fits well with the hybrid trend  where serverless is being leveraged to perform specific functions within a wider hosted environment    2  Serverless Web Applications  In a serverless web app  there may be a combination of running processes that determine contextual and personal elements of the user to serve content and functionality that meets the user s needs  In this use case  for example  static content might be stored in S3 to display when the application is opened in the browser   At the same time  processing is initiated through the application s API gateway to run Lambda functions that determine the application user s context  The static content is then enhanced with more dynamic content that is generated through the lambda functions and stored as dynamic data in DynamoDB    3  Mobile and Internet of Things Applications  Similar to the web apps use case  mobile and IoT applications built in a serverless environment are looking to decide on what content to offer the user based on their context  Serverless authentication elements are used to ensure the user   whether that be a human or a machine   is authorized appropriately to access information or functionality  Lambdas then carry out functionality and interact with data in a DynamoDB to meet the user s needs    4  Serverless Application Ecosystems  In an app ecosystem  applications or workflows are created in a serverless environment and draw on a combination of AWS functionalities and products alongside third party provider APIs   Bonner gave an example of someone telling an Amazon Echo that they were giving a presentation  that voice data then triggering a lambda function to pass that message on to a remote team via the Slack API  Polling within the serverless environment then identifies when the team has responded and messages the Echo with the feedback    5  Event Workflow  The recent release of AWS Step Functions is now adding greater sophistication to serverless workflow possibilities  Decision trees can be created in Step Functions that then align with Lambdas and AWS products to carry out workflow branched actions  although it was confusing that Amazon Web Services were highlighting Step Functions in Australia given it is not available there as yet    For example  using a pub sub messaging model  it is possible to visualize and have all functions and distributed components drawn through a state machine  Users can map the customer onboarding process  entry into CRM  shopping cart ordering and order fulfillment through Step Functions and lambdas and AWS tools are automatically added to ensure the workflow can be completed   So while Fromm talks about serverless being predominantly asynchronous  stateless and ephemeral  once getting into more complex workflows and applications  some capability to persist state and make synchronous calls comes into play   Step Functions provides state machines so that Lambda functions can have some degree of transient state available for business processes where serverless applications may need to marry synchronous and asynchronous call chains   But power users of serverless like Ben Kehoe from iRobots believes the current pricing structure of Step Functions is too prohibitive to use it in production in serverless environments  In addition  the default throttling limits suggest that the intended use does not tend to cover scaled but transient event scheduling    I m hopeful that State as a Service is a paradigm that can be made to work for low level  transient state needed by FaaS in serverless architectures  whether it s through Step Functions or another service   wrote Kehoe late last year after Step Functions was first announced   Authentication Choices  Within the serverless environment  it is also possible to choose a user authentication and identity management workflow best suited to the serverless design pattern and use case being implemented  Authentication methods may change depending on whether a number of users from the one organization need access with varying permission roles  whether users are predominantly customers  or whether partners have some access   A video of Jim Tran and Justin Pirtle presenting at AWS Re Invent last November describes in more detail some of the authentication choices available in the serverless environment   The Maturing Serverless Tooling and Ecosystem  Serverless is still very much in progress to become a major force in how businesses deliver via cloud   As serverless matures and is increasingly taken up for both new projects and as hybrid  it is likely that the business adoption of cloud will be repeated  albeit at a smaller rate in serverless  as businesses begin to choose to migrate existing applications to serverless architecture  While mission critical apps for businesses and enterprises at scale may not be on the cards just yet  with some security capabilities and overall ecosystem tooling still needing to mature  the use cases of serverless suggest that beyond startups  it is increasing probable that serverless will become an  all in  decision in the same way that cloud is today,"[1119 79 440 1120 52 290 1397 1066 993 1123 354]"
1120,training-dataset/engineering/103.txt,engineering,Serverless ArchitecturesLike many trends in software there s no one clear view of what  Serverless  is  and that isn t helped by it really coming to mean two different but overlapping areas   Fast forward to today  mid 2016  and one sees examples such as the recent Serverless Conference   plus the various Serverless vendors are embracing the term from product descriptions to job descriptions  Serverless as a term  for better or for worse  is here to stay   We start to see the term used more frequently in 2015  after AWS Lambda s launch in 2014 and even more so after Amazon s API Gateway launched in July 2015  Here s an example where Ant Stanley writes about Serverless following the API Gateway announcement  In October 2015 there was a talk at Amazon s re Invent conference titled   The Serverless Company using AWS Lambda    referring to PlayOn  Sports   Towards the end of 2015 the  Javascript Amazon Web Services  JAWS   open source project renamed themselves to the Serverless Framework   continuing the trend   First usages of the term seem to have appeared around 2012  including this article by Ken Fromm   Badri Janakiraman says that he also heard usage of the term around this time in regard to continuous integration and source control systems being hosted as a service  rather than on a company s own servers  However this usage was about development infrastructure rather than incorporation into products   The term  Serverless  is confusing since with such applications there are both server hardware and server processes running somewhere  but the difference to normal approaches is that the organization building and supporting a  Serverless  application is not looking after the hardware or the processes   they are outsourcing this to a vendor   Mostly I m going to talk about the second of these areas because it is the one that is newer  has significant differences to how we typically think about technical architecture  and has been driving a lot of the hype around Serverless   However these concepts are related and  in fact  converging  A good example is Auth0   they started initially with BaaS  Authentication as a Service   but with Auth0 Webtask they are entering the FaaS space   Furthermore in many cases when developing a  BaaS shaped  application  especially when developing a  rich  web based app as opposed to a mobile app  you ll likely still need some amount of custom server side functionality  FaaS functions may be a good solution for this  especially if they are integrated to some extent with the BaaS services you re using  Examples of such functionality include data validation  protecting against imposter clients  and compute intensive processing  e g  image or video manipulation    The FaaS environment may also process several clicks in parallel by instantiating multiple copies of the function code   depending on how we d written the original process this may be a new concept we need to consider   There s a much smaller difference to the architecture here compared to our first example  We ve replaced a long lived consumer application with a FaaS function that runs within the event driven context the vendor provides us  Note that the vendor supplies both the Message Broker and the FaaS environment   the two systems are closely tied to each other   Traditionally  the architecture may look like this  The  Ad Server  synchronously responds to the user   we don t care about that interaction for the sake of this example   but it also posts a message to a channel that can be asynchronously processed by a  click processor  application that updates a database  e g  to decrement the advertiser s budget   A different example is a backend data processing service  Say you re writing a user centric application that needs to quickly respond to UI requests  but secondarily you want to capture all the different types of activity that are occurring  Let s think about an online ad system   when a user clicks on an advertisement you want to very quickly redirect them to the target of the ad  but at the same time you need to collect the fact that the click has happened so that you can charge the advertiser   This example is not hypothetical   my former team at Intent Media recently went through this exact redesign    Since the original server was implemented in Java  and AWS Lambda  our FaaS vendor of choice in this instance  supports functions implemented in Java  we can port the search code from the Pet Store server to the Pet Store Search function without a complete re write   This is a massively simplified view  but even with this there are a number of significant changes that have happened here  Please note this is not a recommendation of an architectural migration  I m merely using this as a tool to expose some Serverless concepts   With this architecture the client can be relatively unintelligent  with much of the logic in the system   authentication  page navigation  searching  transactions   implemented by the server application   Traditionally the architecture will look something like this  and let s say it s implemented in Java on the server side  with a HTML   Javascript component as the client   Another example is Apex   a project to  Build  deploy  and manage AWS Lambda functions with ease   One particularly interesting aspect of Apex is that it allows you to develop Lambda functions in languages other than those directly supported by Amazon  e g  Go   Apart from runtime implementation though there are already open source tools and frameworks to help with definition  deployment and runtime assistance  For instance the Serverless Framework makes working with API Gateway   Lambda significantly easier than using the first principles provided by AWS  It s Javascript heavy but if you re writing JS API Gateway apps it s definitely worth a look   One of the main benefits of Serverless FaaS applications is transparent production runtime provisioning  and so open source is not currently as relevant in this world as it is for  say  Docker and containers  In future we may see a popular FaaS   API Gateway platform implementation that will run  on premise  or on a developer workstation  IBM s OpenWhisk is an example of such an implementation and it will be interesting to see whether this  or an alternative implementation  picks up adoption   Debugging and monitoring are tricky in general in Serverless apps   we ll get into this further in subsequent installments of this article   The comment above about API Gateway tooling being immature actually applies  on the whole  to Serverless FaaS in general  There are exceptions however   one example is Auth0 Webtask which places significant priority on Developer UX in its tooling  Tomasz Janczuk gave a very good demonstration of this at the recent Serverless Conference   At present tooling for API gateways is achingly immature and so while defining applications with API gateways is possible it s most definitely not for the faint hearted   One use case for API Gateway   FaaS is for creating http fronted microservices in a Serverless way with all the scaling  management and other benefits that come from FaaS functions   Beyond purely routing requests API Gateways may also perform authentication  input validation  response code mapping  etc  Your spidey sense may be buzzing about whether this is actually such a good idea  if so hold that thought   we ll consider this further later   One aspect of FaaS that we brushed upon earlier is an  API Gateway   An API Gateway is an HTTP server where routes   endpoints are defined in configuration and each route is associated with a FaaS function  When an API Gateway receives a request it finds the routing configuration matching the request and then calls the relevant FaaS function  Typically the API Gateway will allow mapping from http request parameters to inputs arguments for the FaaS function  The API Gateway transforms the result of the FaaS function call to an http response  and returns this to the original caller   Whether or not you think your app may have problems like this you should test with production like load to see what performance you see  If your use case doesn t work now you may want to try again in a few months time since this is a major area of development by FaaS vendors   Are these issues a concern  It depends on the style and traffic shape of your application  My former team has an asynchronous message processing Lambda app implemented in Java which processes hundreds of millions of messages   day  and they have no concerns with startup latencey  That said if you were writing a low latency trading application you probably wouldn t want to use FaaS systems at this time  no matter the language you were using for implementation   The former of these may be avoided in certain situations by the ugly hack of pinging your function every 5 minutes to keep it alive   If your Lambda function is implemented on the JVM you may occasionally see long response times  e g    10 seconds  while the JVM is spun up  However this only notably happens with either of the following scenarios   If your function is implemented in Javascript or Python and isn t huge  i e  less than a thousand lines of code  then the overhead of running it in should never be more than 10   100 ms  Bigger functions may occasionally see longer times   At present how long it takes your FaaS function to respond to a request depends on a large number of factors  and may be anywhere from 10ms to 2 minutes  That sounds bad  but let s get a little more specific  using AWS Lambda as an example   This means that certain classes of long lived task are not suited to FaaS functions without re architecture  e g  you may need to create several different coordinated FaaS functions where in a traditional environment you may have one long duration task performing both coordination and execution   FaaS functions are typically limited in how long each invocation is allowed to run  At present AWS Lambda functions are not allowed to run for longer than 5 minutes and if they do they will be terminated   Given this restriction what are alternatives  Typically it means that FaaS functions are either naturally stateless   i e  they provide pure functional transformations of their input   or that they make use of a database  a cross application cache  e g  Redis   or network file store  e g  S3  to store state across requests or for further input to handle a request   FaaS functions have significant restrictions when it comes to local  machine   instance bound  state  In short you should assume that for any given invocation of a function none of the in process or host state that you create will be available to any subsequent invocation  This includes state in RAM and state you may write to local disk  In other words from a deployment unit point of view FaaS functions are stateless    Let s return to our click processor  Say that we were having a good day and customers were clicking on 10 times as many ads as usual  Would our click processing application be able to handle this  For example did we code to be able to handle multiple messages at a time  Even if we did would one running instance of the application be enough to process the load  If we are able to run multiple processes is auto scaling automatic or do we need to reconfigure that manually  With FaaS you need to write the function ahead of time to assume parallelism  but from that point on the FaaS provider automatically handles all scaling needs   Let s consider our click processing example again   the only code that needs to change when moving to FaaS is the  main method   startup  code  in that it is deleted  and likely the specific code that is the top level message handler  the  message listener interface  implementation   but this might only be a change in method signature  All of the rest of the code  e g  the code that writes to the database  is no different in a FaaS world   If we go back to our click processing example from earlier what FaaS does is replace the click processing server  possibly a physical machine  but definitely a specific application  with something that doesn t need a provisioned server  nor an application that is running all the time   AWS Lambda lets you run code without provisioning or managing servers   1      With Lambda  you can run code for virtually any type of application or backend service  2    all with zero administration  Just upload your code and Lambda takes care of everything required to run  3  and scale  4  your code with high availability  You can set up your code to automatically trigger from other AWS services  5  or call it directly from any web or mobile app  6     We ve mentioned the FaaS idea a lot already but it s time to dig into what it really means  To do this let s look at the opening description for Amazon s Lambda product  I ve added some tokens to it  which I then expand upon   What isn t Serverless   So far in this article I ve defined  Serverless  to mean the union of a couple of other ideas    Backend as a Service  and  Functions as a Service   I ve also dug into the capabilities of the second of these   Before we start looking at the very important area of benefits and drawbacks I d like to spend one more moment on definition  or at least defining what Serverless isn t  I ve seen some people  including me in the recent past  get confused about these things and I think it s worth discussing them for clarity s sake   Comparison with PaaS Given that Serverless FaaS functions are very similar to 12 Factor applications  are they in fact just another form of  Platform as a Service   PaaS  like Heroku  For a brief answer I refer to Adrian Cockcroft If your PaaS can efficiently start instances in 20ms that run for half a second  then call it serverless     Adrian Cockcroft In other words most PaaS applications are not geared towards bringing entire applications up and down for every request  whereas FaaS platforms do exactly this  OK  but so what  if I m being a good 12 Factor App developer there s still no difference to how I code  That s true  but there is a big difference to how you operate your app  Since we re all good DevOps savvy engineers we re thinking about operations as much as we are about development  right  The key operational difference between FaaS and PaaS is scaling  With most PaaS s you still need to think about scale  e g  with Heroku how many Dynos you want to run  With a FaaS application this is completely transparent  Even if you setup your PaaS application to auto scale you won t be doing this to the level of individual requests  unless you have a very specifically shaped traffic profile   and so a FaaS application is much more efficient when it comes to costs  Given this benenfit  why would you still use a PaaS  There are several reasons but tooling  and maturity of API gateways  are probably the biggest  Furthermore 12 Factor Apps implemented in a PaaS may use an in app readonly cache for optimization  which isn t an option for FaaS functions   Comparison with containers One of the reasons for Serverless FaaS is to avoid having to manage computational processes at the operating system level or lower  Platforms as a Service  like Heroku  are another  and I ve described above how PaaS s are different to Serverless FaaS  Another popular abstraction of processes are containers  with Docker being the most visible example of such a technology  We also see increasing popularity of container hosting systems  such as Mesos and Kubernetes  which abstract individual applications from OS level deployment  And even further there are cloud hosting container platforms like Amazon ECS and Google Container Engine which  like Serverless FaaS  let teams avoid having to manage their own server systems at all  So given all the momentum around containers is it still worth considering Serverless FaaS  Principally the argument I made for PaaS still holds with containers   for Serverless FaaS scaling is automatically managed  transparent  and fine grained  Container platforms do not yet offer such a solution  Furthermore I d argue that container technology while having seen massive popularity in the last couple of years is still not mature  That s not to say that Serverless FaaS is mature either  but picking which rough edges you d like is still the order of the day  I ll admit  however  that both of these arguments may start to wear thin over time  While true no management auto scaling in container platforms isn t at the level of Serverless FaaS yet  we see areas like Kubernetes  Horizontal Pod Autoscaling  as tending towards it  I can imagine some very smart traffic pattern analysis being introduced to such features  as well as more load implying metrics  Furthermore the rapid evolution of Kubernetes may give a wonderfully simple  stable  platform before too long  If we see the gap of management and scaling between Serverless FaaS and hosted containers narrow the choice between them may just come down to style  and type of application  For example it may be that FaaS is seen as a better choice for event driven style with few event types per application component  and containers are seen as a better choice for synchronous request driven components with many entry points  I expect in 5 years time that many applications and teams will use both architectural approaches  and it will be fascinating to see patterns of such use emerge    NoOps Serverless doesn t mean  No Ops   It might mean  No internal Sys Admin  depending on how far down the serverless rabbit hole you go  There are 2 important things to consider here  Firstly  Ops  means a lot more than server administration  It also means at least monitoring  deployment  security  networking and often also means some amount of production debugging and system scaling  These problems all still exist with Serverless apps and you re still going to need a strategy to deal with them  In some ways Ops is harder in a Serverless world because a lot of this is so new  Second even the Sys Admin is still happening   you re just outsourcing it with Serverless  That s not necessarily a bad thing   we outsource a lot  But depending on what precisely you re trying to do this might be a good or a bad thing  and either way at some point the abstraction will likely leak and you ll need to know that human sys admins somewhere are supporting your application  Charity Majors gave a great talk on this subject at the recent Serverless Conference and I recommend checking it out once it s available online  Until then you can read her write up here and here,"[79 440 1120 52 1397 290 1119 1066 354 993 1123]"
1123,training-dataset/engineering/631.txt,engineering,Part 3 of Thinking Serverless   Dealing with Data and Workflow IssuesMonday  February 13  2017 at 8 56AM  This is a guest repost by Ken Fromm  a 3x tech co founder   Vivid Studios  Loomia  and Iron io  Here s Part 1 and 2   This post is the third of a four part series of that will dive into developing applications in a serverless way  These insights are derived from several years working with hundreds of developers while they built and operated serverless applications and functions   The platform was the serverless platform from Iron io but these lessons can also apply to AWS Lambda  Google Cloud Functions  Azure Functions  and IBM s OpenWhisk project   Serverless Processing   Data Diagram  Thinking Serverless  The Data  If you are more data and security minded than the average developer  serverless processing will cause you to think more about data handling than you might with a legacy and or web application  The reason being is that with a more monolithic application  data concerns are typically isolated to a limited set of tiers   for example  there is often an application tier  a caching layer  and a database tier  In a traditional legacy web architecture  all these tiers are relatively self contained  clustered in one or more zones and or regions   In an serverless world  processing tends to be far more deconstructed   and data handling and storage far more distributed and segmented   Physical and virtual resource boundaries can be much larger  more disparate  and often not even readily apparent  The reason is because serverless platforms can spread workloads across a much wider range of infrastructure  Additional components  also distributed in their own right  can be used in conjunction with this type of processing   In the email use case referenced in Part 1 of this series  processing an email as part of a legacy application via a serialized workflow would typically result in the processing and the data handling taking place within fixed number of servers for the entire sequence of processing  With a serverless approach  though  each task might run in a different server across a number of clusters  or even in a different zone or data center  And if a serverless platform uses multiple infrastructure providers  data payloads could even cross multiple clouds   Processing an image  for example  might be performed in different clusters specifically set up to handle the higher memory requirements needed for these types of tasks  Different portions of an email message might be distributed across a wider topology either as payloads for tasks or within key value stores or databases that the tasks access  Consequently  developers have to think about not just different type of data stores but also different locations and even different types and states of data   input data  output data  temporary data  config data  and more   Below is a general breakdown of the types of data encountered and where they may be stored and or transmitted within in a serverless processing environment   Task Payloads  Keeping in mind that tasks should be stateless  everything that a task needs for processing should therefore be provided at runtime  Typically  a task payload serves as the primary mechanism for providing the workload that the serverless task should use  Much like functional processing  data is passed to tasks  tasks operate on the data  tasks store or return the data  and then the tasks exit or terminate   In certain situations  tasks may not have payloads per se as they may pull in data from a queue  database  key value store  or other data source  In this case  tasks may use config data to provide the location of the data and then as the first order of business  pull the data in and process it   Data Payload for a Task  As with functional processing  developers will want to pay attention to what gets passed to tasks  Tasks should only receive data that it will operate on meaning that developers will want to send only the portion of a data record that is pertinent for that task as opposed to the full data record   Although platforms may allow you to pass large amounts of data within a task payload  it is usually not wise to do so  The reasons are because of both efficiency and security  Less data means less transmission and less storage  hence greater efficiency  Fewer copies of the data and fewer locations translates into greater security  To reduce this overhead  it is better to pass IDs to data instead of the data itself  especially when the data is large and or unstructured   Message Queues and Databases  In some cases  serverless tasks may get the workloads they are processing from a message queue or a database  This is commonly seen in certain event driven loops as well as when scheduled tasks come into play within a particular workflow   In the first example  events might be placed on a queue  and after they reach a certain threshold  tasks might get triggered to run  Alternatively  scheduled tasks might run at regular intervals  Each task might take one or more messages from the queue  perform its operations  and then exit   Message Queues and Databases  In the case of a scheduled task  it may get data from a queue or a database  An example here might be consolidating and post processing of sensor data  Data from any of number of devices might be placed in a queue or stored in a database at a regular frequency  Tasks could be set up to run on a particular schedule which would take this data from the queue or database and consolidate it and or perform other analytical operations for storage or presentation   Temporary File Storage   internal task processing  Most serverless platforms provide a certain amount of non persistent file storage that is available for a task to read from and write to  In AWS Lambda  for example  this temporary storage is provided via the  tmp directory  In IronWorker  it is accessed via the current working directory      This file storage is accessible only by the task and cannot be written to or read from by other tasks  It will also be erased when a task completes   As with any temp file storage  it can be used as a way to process large amounts of data by reading and writing slices of the data  store temporary results to be used by other functions within the task  or otherwise perform other memory intensive operations   Temporary Storage   task to task data sharing  Another form of temporary data storage that is useful in serverless processing is storage that can be between separate tasks in a processing workflow  Certain events might involve a sequence of tasks which in turn might require some element of data sharing between the tasks   In the case of email processing  one type of task might parse the body of an email message  separating it into text  links  and other objects  Other tasks might take these data objects and process them  Text might go through natural language processing for contextual analysis  Links might get analyzed for page ranking or spam detection   Temporary Data Storage   Useful for Task to Task Data Sharing  In order to reduce the amount of data replication as well as provide an easy mechanism for data sharing  key value datastores are well suited for storing these shared data objects  The key to the value may be placed on a queue or within the task payload  The receiving task then the key to get the data and perform its operation   More depth on key value datastores is provided below but one of the nice things about this type of datastore is that you can set expiration times for the data elements  This means that you do not have to consciously delete entries as you might have to do with a database   Third Party Data  Another form of data is third party data  Serverless tasks may make HTTPS or other network calls or  as mentioned above  reach out to a queue or database to pull in data  The general rules and guidelines around data access calls will also apply within a task processing environment  These include encryption  authorization protocols  timeouts  and exception handling   One difference from long running applications  however  is that within an application  you will likely have credentials for access to databases and third party sites provided as part of the global memory of the application  These are likely read into memory from a config file at startup time  Within a serverless processing environment  however  tasks are largely stateless and do not have any sense of application awareness  The question then becomes how do you get URIs and or access credentials for these tasks to use   Developers get around this in a couple ways  In some cases  a serverless platform might provide a global config capability  as in the case of environment variables within IronWorker   In this case  developers are able to load config information into the system independently from any any task  thereby allowing any task access to this information at run time  In other cases  the credentials could be passed in with the data payload  This is less advisable because it means many copies of the credentials may exist   What you want to avoid is hard coding credentials into a task  It is a basic rule to strictly separate code and config data  see Factor III in Twelve Factor App   With serverless processing  however  given how simple it is to change a task and upload it into a platform  it can be pretty tempting to embed data within code   By this I mean it is not much different than changing a config file in a long running app  In this case  you change a single file and upload it  Here  you have the added step of having to restart the app  In a serverless platform  you don t need to restart  because any code changes within a task will trigger use of the updated task  As such  handling config data will likely take some thought  given it will largely be platform dependent  as well as some discipline given the ease of combining code with data   Data Results  Handling data results in a serverless platform differs depending on whether tasks are synchronous or asynchronous  In the case of synchronous processing  data could be returned to the calling task  In the case of asynchronous processing  data results will need to be stored  temporarily or permanently  in order for another process or entity to make use of it   Data Results  Data results might be placed on a queue  in a key value datastore  in an SQL or NoSQL database  written to block storage  or even pushed out via a webhook   A couple points to consider   a transaction completes successfully and the resulting data gets written to some form of persistent storage that actions are idempotent meaning that transactions do not get duplicated or data gets overwritten with different results  In the first case  it is critical that writes get verified and committed before a task ends  In the second case  it is a matter of checking whether another performed an operation on the same data  In both cases  you ll need to insert logic as well as exception handling to address these processing and storage concerns   One point to mention has to do with writing to databases  Some architectures may place limits on access to databases  requiring processes to be behind particular firewalls  come from certain IP whitelists  or follow other restrictive measures   If the cloud processing takes place outside of these requirements  then there will be difficulty in updating a database  In this type of situation  one answer might be to have a process behind the firewall pull in results from a queue or datastore and store the data  This setup would uphold the access restrictions while still allowing for flexible and scalable processing via a serverless platform   Another point about databases is that they may have hard connection limits and so developers will need to take concurrency levels into account so as not to exceed these limits  Concurrency is mentioned as a platform level issue in Part 2 of this series   The Value of Key Value Stores  One of the key insights when starting to use a serverless platform is the value that a good key value datastore provides  As mentioned above  there are a number of data sharing use cases whether it s accessing global config data or sharing data between tasks   It is somewhat ironic in that while serverless tasks should be stateless  a fair amount of  state  needs to be preserved as part of a processing pipeline or workflow   The reason key value datastores are highlighted as opposed to SQL databases or document stores is that while these others can be used  key value stores have a number of advantages to them   Key value datastores are fast  They are designed to do a limited set of actions really well   read and write key value pairs   Key value pairs map well to the needs of task processing   here s the key  and here s the data  Whether it is storing a data set that needs to be processed or storing data between tasks  passing keys instead of large blocks of data provides more flexibility and can also improve performance   Less copying and transferring of data within the system    Key value datastores typically scale better  They are usually designed and structured to scale horizontally and so there are less concerns with resource limits  locks  and bottlenecks   I don t have a recommendation to make as to what key value datastore to use   i e  internally hosted  cloud based  managed service  or otherwise   but do recommend that high availability and scalability factor high in the requirements  Any third party dependency that can serve as a block on processing needs to be addressed with a fail safe design and scalable manner in mind   Idempotency  Key value stores can also be used to ensure idempotency  By using the appropriate key  it can be possible to a  determine if you need to process data or b  determine if you need to write data to a data store   For example  in the case of a web crawling application  a big part of the process it to grab all the links that are available within a site  By using a key value store to contain the links of pages to process and doing so in a way that uses the links as keys  you can avoid the need to store duplicate links  Likewise  by checking to see if a key for a certain data result already exists  you can also avoid processing   Data Store Types  attribution   Thinking Serverless  The Workflow  With serverless processing  some tasks might run independent of others  perform their work  and terminate without additional processing  Other workflows might entail a sequence of tasks   The email use case mentioned in this series is an example of this  One task might parse an email message  others might process data objects within the email  and still others might perform actions based on the results of the data analysis   This is why orchestration and workflow issues need to be addressed as part of building and deploying serverless applications  Here is a rundown of the issues developers might come across   Synchronous and Asynchronous  Serverless workloads can processed either be synchronously or asynchronously  With synchronous processing  connections are maintained throughout the invocation of the serverless task  Which means the calling task will block until the serverless task is processed and then return a response   Because of this aspect  synchronous tasks should be fast processing and operate in an environment that imposes strict time limits  Use cases for a synchronous processing can be transactional in nature or other situations where real time response is critical   Note that synchronous may or may not provide strict First In First Out processing  FIFO  and so if this is critical developers might want to address this via the use of a message queues with FIFO capabilities in combination with asynchronous processing   Asynchronous tasks  on the other hand  do not maintain a connection and therefore can have more relaxed SLAs regarding latency as well as increased limits on processing time  In all but the most time sensitive situations  asynchronous processing should address most application needs   Synchronous processing is less forgiving  requiring greater attention be paid to task performance and load capacities  Asynchronous processing provides a lot more flexibility and fails more gracefully  tasks may be delayed when run asynchronously as opposed to creating blocks that might ripple through other parts of an application creating resource or processing overloads    Synchronous Processing  Asynchronous Processing  Sequencing and Processing Pipelines  Serverless processing lends itself to working on problems in a deconstructed and loosely coupled way  By focusing on building single purpose functions and tasks  developers are able to build a system that can scale easily and be modified and extended in much more flexible manner than if the processing was contained within a larger more monolithic application framework   When constructing sequences and pipelines  developers can make use of message queues as a way to buffer workloads between tasks  This traditional approach offers advantages of allowing introspection as to the size and throughput of queues as as to better understand the flow of events through the system  Some serverless platforms might provide the ability to invoke or trigger tasks directly from another task  Under the hood of the serverless platforms  message queues may be used to buffer the work so in the end  it may be an arbitrary choice   For complex applications  message queues do make sense as they provide an added layer of visibility and inspection  For simpler applications and workflows  external queues may introduce unnecessary complexity   For serverless tasks invoking serverless tasks  developers need to be extremely conscious about recursion and take measures to put limit on the number of levels of recursion  For example  an application that analyzes links might hit some form of a recursive pattern  Recursive tasks are a more common occurrence than you might think   In the early development of IronWorker  developers were quickly forced to put measures in to prevent recursive calls beyond a certain threshold  Rather than rely on any platform limits  however  serverless apps that run the risk of recursion should make their own efforts to prevent things from getting out of hand   Email Message Analysis Processing Pipeline  Task Invocation   Triggers  Webhooks  and Push vs Pull  How tasks get created  when they run  and how much they process is entirely dependent on the event  the workflow  and the serverless platform you re using  With an internal project that  a task might be invoked directly by making a direct HTTP call to a serverless platform  With external applications  it might be via a webhook   which essentially provides a shell around an API call to the platform   In other cases  events may need to be processed on a regular schedule or when a certain number of events happen  In these cases  tasks could be triggered to run either by a queue that can push messages the serverless platform  Each message could contain data for a single event or for multiple events  In other situations  a scheduled task could be used to pick up one or more messages from a queue or one or more records from a database   Fortunately for developers  more and more use cases and patterns are being published around serverless computing and so developers will have a growing supply of models that they can work from  Serverless platforms are also likely to offer an increasing amount of hooks and internal capabilities around sequencing and workflow issues   The graph below depicts a segment of users and their task concurrencies over a 24 hour period within a serverless platform  Many of the spikes occur on the hourly mark and illustrate the processing that is taking place as part of a regular schedule   Task Currencies by User   Spikes Are Result of Scheduled Jobs  Retries  It goes without saying that in computing processing  things break  Whether it is legacy system  a large web application  or serverless architectures  internal or external system failures will occur  Developers  therefore  need to assume that tasks will fail and plan accordingly   Most serverless platforms will provide as a feature the ability to automatically retry failed tasks   Provided that developers have written their tasks to be idempotent  any retry will be largely unnoticed   much like retries on disk writes go unnoticed   Sample Task Error Rate  It is important  however  to monitor failed tasks   either through the platform or via log files  High error rates may indicate a system conflict when a task uses too many resources such as RAM  storage space  CPU  Or the high error rates could signify an external issue such as exceeding database connections or errors with access calls   The ability to develop microservices and executable functions and move them quickly into production and run them at high concurrency means that many errors may not surface in testing  Analyzing error rates and resulting log files will help identify and address resources issues or external conflicts   Exception Handling  Even when running in systems designed to be highly available and with the ability to retry tasks  tasks are still at risk for failing   whether it s running into memory limits  third party read access issues  network outages  versioning conflicts  or even just bad code   Some remedies can be enabled at the platform level  either via APIs that allow access to failed task lists or dead letter queues that might put failed workloads on a queue   It may still be left as an exercise for developers to put in the logic to address failed tasks  One of the great things about serverless processing  however  is that it is relatively easy to write new features and put them into production quickly   This benefit makes it so that you can build in exception handling capabilities from the start or as an additional hardening phase  In doing so  you can make it independent from the primary processing sequences to serve as either a check and balance or as a clean up or oversight process   One method to address failed task might be to retry them   under the assumption that they failed because of a third party outage or block   Another might be to go through a series of tests to isolate the issue and highlight it for automated and or manual post processing   A third approach might be to automatically parse through log files and post the resulting data to a chat room or other alert forum   Logging and Audit Trails  On the outside looking in  making sense of log files within a serverless processing environment can be more difficult than in a legacy application  The reason being the work units might be much smaller and far more distributed  A greater number of components might also be involved   tasks  server configurations  zones  regions  clouds  storage facilities  data access methods  and more  All this involves more data  more noise  and more difficulty in following what was processed  when  and where   With a solid logging strategy  added processing complexity does not necessarily have to translate into added logging complexity  especially with today s logging technologies   Whether you are using a logging service or hosting on your own something like ELK   Elasticsearch  Logstash  Kibana   the first order of business is to get access to logs from any task or sequence of tasks  The second component is in when and what to log  The third involves how to search and retrieve the data and the fourth is how to create automated queries  triggers  and other alerting mechanisms   Here are some recommendations when it comes to logging  LogFmt   Use a standard well formulated logging format such as logfmt  Logfmt is a way to use key value pairs that make logs human readable as well as machine processable   Log all events   You should log all events but use different log levels and labels to inform or direct action  For example  one form of labels include warn error info debug as a way to indicate of the severity or actionable nature of the event   Some serverless platforms will allow logging to STDOUT STDERR which in turn will redirect to various services   IronWorker  for example  provides this capability which lets developers make use of a number of real time logging services    Capture versions as well as timestamps   down to the nanosecond w timezone  Inputs are important although see the note below regarding log files as a source of data breach   Capture process IDs and server instances as well as any other location system data  Although task based processing is designed to be transparent across servers and infrastructures  this may not always be the case  In rare instances where you need to track down system level conflicts  having system level data will provide the only way to do so   Logging levels can help with recursive function calls but since tasks operate independently  you ll need to maintain the level via an external variable   although it will need to be unique to the thread    One key issue to note is how to preserve logs in the event of a task failure or a logging service outage  In the case of the former  when a task fails  local file storage will not be accessible after the termination of the task  And so even if a task was logged locally to disk   as you might do with a persistent application   log data will likely not be accessible after the fact   With real time logging to external services  this risk is reduced as data will be persisted in the third party service  Note that STDOUT and STDERR might need to be periodically flushed in order to make sure logging data gets transmitted to the third party service   At the same time  external logging can introduce its own costs and or points of failure  If you need to guarantee that logs get written and persisted outside of a running task  you may be in introducing certain performance hits  If the logging service is delayed or is unavailable  it can cause a block on the task   Even with asynchronous logging  a service outage with the logging service can mean the absence of logs  If logging is paramount  they you may need to address alternative strategies to address task failures and or service outages   One concern about logs is that because they can have a long duration of persistence  e g  7 days  30 days  or indefinitely   they can be a significant source for data breach  Developers need to give hard thought as to what they log and how where logs are stored   Developers will want to take the same precautions when logging data with respect to data segmenting  data anonymizing  and possibly even data encryption as they do in the original processing of the workloads and the transmission of the data payloads   For more information on logging  an easily digestible set of anti patterns can be found here on StackExchange,"[1123 993 1066 354 1119 440 1120 52 290 79 1397]"
1397,training-dataset/engineering/829.txt,engineering,Serverless ArchitecturesLike many trends in software there s no one clear view of what  Serverless  is  and that isn t helped by it really coming to mean two different but overlapping areas   Fast forward to today  mid 2016  and one sees examples such as the recent Serverless Conference   plus the various Serverless vendors are embracing the term from product descriptions to job descriptions  Serverless as a term  for better or for worse  is here to stay   We start to see the term used more frequently in 2015  after AWS Lambda s launch in 2014 and even more so after Amazon s API Gateway launched in July 2015  Here s an example where Ant Stanley writes about Serverless following the API Gateway announcement  In October 2015 there was a talk at Amazon s re Invent conference titled   The Serverless Company using AWS Lambda    referring to PlayOn  Sports   Towards the end of 2015 the  Javascript Amazon Web Services  JAWS   open source project renamed themselves to the Serverless Framework   continuing the trend   First usages of the term seem to have appeared around 2012  including this article by Ken Fromm   Badri Janakiraman says that he also heard usage of the term around this time in regard to continuous integration and source control systems being hosted as a service  rather than on a company s own servers  However this usage was about development infrastructure rather than incorporation into products   The term  Serverless  is confusing since with such applications there are both server hardware and server processes running somewhere  but the difference to normal approaches is that the organization building and supporting a  Serverless  application is not looking after the hardware or the processes   they are outsourcing this to a vendor   Mostly I m going to talk about the second of these areas because it is the one that is newer  has significant differences to how we typically think about technical architecture  and has been driving a lot of the hype around Serverless   However these concepts are related and  in fact  converging  A good example is Auth0   they started initially with BaaS  Authentication as a Service   but with Auth0 Webtask they are entering the FaaS space   Furthermore in many cases when developing a  BaaS shaped  application  especially when developing a  rich  web based app as opposed to a mobile app  you ll likely still need some amount of custom server side functionality  FaaS functions may be a good solution for this  especially if they are integrated to some extent with the BaaS services you re using  Examples of such functionality include data validation  protecting against imposter clients  and compute intensive processing  e g  image or video manipulation    The FaaS environment may also process several clicks in parallel by instantiating multiple copies of the function code   depending on how we d written the original process this may be a new concept we need to consider   There s a much smaller difference to the architecture here compared to our first example  We ve replaced a long lived consumer application with a FaaS function that runs within the event driven context the vendor provides us  Note that the vendor supplies both the Message Broker and the FaaS environment   the two systems are closely tied to each other   Traditionally  the architecture may look like this  The  Ad Server  synchronously responds to the user   we don t care about that interaction for the sake of this example   but it also posts a message to a channel that can be asynchronously processed by a  click processor  application that updates a database  e g  to decrement the advertiser s budget   A different example is a backend data processing service  Say you re writing a user centric application that needs to quickly respond to UI requests  but secondarily you want to capture all the different types of activity that are occurring  Let s think about an online ad system   when a user clicks on an advertisement you want to very quickly redirect them to the target of the ad  but at the same time you need to collect the fact that the click has happened so that you can charge the advertiser   This example is not hypothetical   my former team at Intent Media recently went through this exact redesign    Since the original server was implemented in Java  and AWS Lambda  our FaaS vendor of choice in this instance  supports functions implemented in Java  we can port the search code from the Pet Store server to the Pet Store Search function without a complete re write   This is a massively simplified view  but even with this there are a number of significant changes that have happened here  Please note this is not a recommendation of an architectural migration  I m merely using this as a tool to expose some Serverless concepts   With this architecture the client can be relatively unintelligent  with much of the logic in the system   authentication  page navigation  searching  transactions   implemented by the server application   Traditionally the architecture will look something like this  and let s say it s implemented in Java on the server side  with a HTML   Javascript component as the client   Another example is Apex   a project to  Build  deploy  and manage AWS Lambda functions with ease   One particularly interesting aspect of Apex is that it allows you to develop Lambda functions in languages other than those directly supported by Amazon  e g  Go   Apart from runtime implementation though there are already open source tools and frameworks to help with definition  deployment and runtime assistance  For instance the Serverless Framework makes working with API Gateway   Lambda significantly easier than using the first principles provided by AWS  It s Javascript heavy but if you re writing JS API Gateway apps it s definitely worth a look   One of the main benefits of Serverless FaaS applications is transparent production runtime provisioning  and so open source is not currently as relevant in this world as it is for  say  Docker and containers  In future we may see a popular FaaS   API Gateway platform implementation that will run  on premise  or on a developer workstation  IBM s OpenWhisk is an example of such an implementation and it will be interesting to see whether this  or an alternative implementation  picks up adoption   Debugging and monitoring are tricky in general in Serverless apps   we ll get into this further in subsequent installments of this article   The comment above about API Gateway tooling being immature actually applies  on the whole  to Serverless FaaS in general  There are exceptions however   one example is Auth0 Webtask which places significant priority on Developer UX in its tooling  Tomasz Janczuk gave a very good demonstration of this at the recent Serverless Conference   At present tooling for API gateways is achingly immature and so while defining applications with API gateways is possible it s most definitely not for the faint hearted   One use case for API Gateway   FaaS is for creating http fronted microservices in a Serverless way with all the scaling  management and other benefits that come from FaaS functions   Beyond purely routing requests API Gateways may also perform authentication  input validation  response code mapping  etc  Your spidey sense may be buzzing about whether this is actually such a good idea  if so hold that thought   we ll consider this further later   One aspect of FaaS that we brushed upon earlier is an  API Gateway   An API Gateway is an HTTP server where routes   endpoints are defined in configuration and each route is associated with a FaaS function  When an API Gateway receives a request it finds the routing configuration matching the request and then calls the relevant FaaS function  Typically the API Gateway will allow mapping from http request parameters to inputs arguments for the FaaS function  The API Gateway transforms the result of the FaaS function call to an http response  and returns this to the original caller   Whether or not you think your app may have problems like this you should test with production like load to see what performance you see  If your use case doesn t work now you may want to try again in a few months time since this is a major area of development by FaaS vendors   Are these issues a concern  It depends on the style and traffic shape of your application  My former team has an asynchronous message processing Lambda app implemented in Java which processes hundreds of millions of messages   day  and they have no concerns with startup latencey  That said if you were writing a low latency trading application you probably wouldn t want to use FaaS systems at this time  no matter the language you were using for implementation   The former of these may be avoided in certain situations by the ugly hack of pinging your function every 5 minutes to keep it alive   If your Lambda function is implemented on the JVM you may occasionally see long response times  e g    10 seconds  while the JVM is spun up  However this only notably happens with either of the following scenarios   If your function is implemented in Javascript or Python and isn t huge  i e  less than a thousand lines of code  then the overhead of running it in should never be more than 10   100 ms  Bigger functions may occasionally see longer times   At present how long it takes your FaaS function to respond to a request depends on a large number of factors  and may be anywhere from 10ms to 2 minutes  That sounds bad  but let s get a little more specific  using AWS Lambda as an example   This means that certain classes of long lived task are not suited to FaaS functions without re architecture  e g  you may need to create several different coordinated FaaS functions where in a traditional environment you may have one long duration task performing both coordination and execution   FaaS functions are typically limited in how long each invocation is allowed to run  At present AWS Lambda functions are not allowed to run for longer than 5 minutes and if they do they will be terminated   Given this restriction what are alternatives  Typically it means that FaaS functions are either naturally stateless   i e  they provide pure functional transformations of their input   or that they make use of a database  a cross application cache  e g  Redis   or network file store  e g  S3  to store state across requests or for further input to handle a request   FaaS functions have significant restrictions when it comes to local  machine   instance bound  state  In short you should assume that for any given invocation of a function none of the in process or host state that you create will be available to any subsequent invocation  This includes state in RAM and state you may write to local disk  In other words from a deployment unit point of view FaaS functions are stateless    Let s return to our click processor  Say that we were having a good day and customers were clicking on 10 times as many ads as usual  Would our click processing application be able to handle this  For example did we code to be able to handle multiple messages at a time  Even if we did would one running instance of the application be enough to process the load  If we are able to run multiple processes is auto scaling automatic or do we need to reconfigure that manually  With FaaS you need to write the function ahead of time to assume parallelism  but from that point on the FaaS provider automatically handles all scaling needs   Let s consider our click processing example again   the only code that needs to change when moving to FaaS is the  main method   startup  code  in that it is deleted  and likely the specific code that is the top level message handler  the  message listener interface  implementation   but this might only be a change in method signature  All of the rest of the code  e g  the code that writes to the database  is no different in a FaaS world   If we go back to our click processing example from earlier what FaaS does is replace the click processing server  possibly a physical machine  but definitely a specific application  with something that doesn t need a provisioned server  nor an application that is running all the time   AWS Lambda lets you run code without provisioning or managing servers   1      With Lambda  you can run code for virtually any type of application or backend service  2    all with zero administration  Just upload your code and Lambda takes care of everything required to run  3  and scale  4  your code with high availability  You can set up your code to automatically trigger from other AWS services  5  or call it directly from any web or mobile app  6     We ve mentioned the FaaS idea a lot already but it s time to dig into what it really means  To do this let s look at the opening description for Amazon s Lambda product  I ve added some tokens to it  which I then expand upon   What isn t Serverless   So far in this article I ve defined  Serverless  to mean the union of a couple of other ideas    Backend as a Service  and  Functions as a Service   I ve also dug into the capabilities of the second of these   Before we start looking at the very important area of benefits and drawbacks I d like to spend one more moment on definition  or at least defining what Serverless isn t  I ve seen some people  including me in the recent past  get confused about these things and I think it s worth discussing them for clarity s sake   Comparison with PaaS Given that Serverless FaaS functions are very similar to 12 Factor applications  are they in fact just another form of  Platform as a Service   PaaS  like Heroku  For a brief answer I refer to Adrian Cockcroft If your PaaS can efficiently start instances in 20ms that run for half a second  then call it serverless     Adrian Cockcroft In other words most PaaS applications are not geared towards bringing entire applications up and down for every request  whereas FaaS platforms do exactly this  OK  but so what  if I m being a good 12 Factor App developer there s still no difference to how I code  That s true  but there is a big difference to how you operate your app  Since we re all good DevOps savvy engineers we re thinking about operations as much as we are about development  right  The key operational difference between FaaS and PaaS is scaling  With most PaaS s you still need to think about scale  e g  with Heroku how many Dynos you want to run  With a FaaS application this is completely transparent  Even if you setup your PaaS application to auto scale you won t be doing this to the level of individual requests  unless you have a very specifically shaped traffic profile   and so a FaaS application is much more efficient when it comes to costs  Given this benenfit  why would you still use a PaaS  There are several reasons but tooling  and maturity of API gateways  are probably the biggest  Furthermore 12 Factor Apps implemented in a PaaS may use an in app readonly cache for optimization  which isn t an option for FaaS functions   Comparison with containers One of the reasons for Serverless FaaS is to avoid having to manage computational processes at the operating system level or lower  Platforms as a Service  like Heroku  are another  and I ve described above how PaaS s are different to Serverless FaaS  Another popular abstraction of processes are containers  with Docker being the most visible example of such a technology  We also see increasing popularity of container hosting systems  such as Mesos and Kubernetes  which abstract individual applications from OS level deployment  And even further there are cloud hosting container platforms like Amazon ECS and Google Container Engine which  like Serverless FaaS  let teams avoid having to manage their own server systems at all  So given all the momentum around containers is it still worth considering Serverless FaaS  Principally the argument I made for PaaS still holds with containers   for Serverless FaaS scaling is automatically managed  transparent  and fine grained  Container platforms do not yet offer such a solution  Furthermore I d argue that container technology while having seen massive popularity in the last couple of years is still not mature  That s not to say that Serverless FaaS is mature either  but picking which rough edges you d like is still the order of the day  I ll admit  however  that both of these arguments may start to wear thin over time  While true no management auto scaling in container platforms isn t at the level of Serverless FaaS yet  we see areas like Kubernetes  Horizontal Pod Autoscaling  as tending towards it  I can imagine some very smart traffic pattern analysis being introduced to such features  as well as more load implying metrics  Furthermore the rapid evolution of Kubernetes may give a wonderfully simple  stable  platform before too long  If we see the gap of management and scaling between Serverless FaaS and hosted containers narrow the choice between them may just come down to style  and type of application  For example it may be that FaaS is seen as a better choice for event driven style with few event types per application component  and containers are seen as a better choice for synchronous request driven components with many entry points  I expect in 5 years time that many applications and teams will use both architectural approaches  and it will be fascinating to see patterns of such use emerge    NoOps Serverless doesn t mean  No Ops   It might mean  No internal Sys Admin  depending on how far down the serverless rabbit hole you go  There are 2 important things to consider here  Firstly  Ops  means a lot more than server administration  It also means at least monitoring  deployment  security  networking and often also means some amount of production debugging and system scaling  These problems all still exist with Serverless apps and you re still going to need a strategy to deal with them  In some ways Ops is harder in a Serverless world because a lot of this is so new  Second even the Sys Admin is still happening   you re just outsourcing it with Serverless  That s not necessarily a bad thing   we outsource a lot  But depending on what precisely you re trying to do this might be a good or a bad thing  and either way at some point the abstraction will likely leak and you ll need to know that human sys admins somewhere are supporting your application  Charity Majors gave a great talk on this subject at the recent Serverless Conference and I recommend checking it out once it s available online  Until then you can read her write up here and here,"[79 440 1120 52 1397 290 1119 1066 354 993 1123]"
