X1,file_name,category,text,recommendations
44,training-dataset/business/53.txt,business,How Top Investors Separate A I  Hype From RealityArtificial intelligence has captured public imagination  dominated media coverage  and driven furious volumes of investment and acquisition activity  In the midst of this hype cycle  spotting the difference between phony wannabes and true investments can be a challenge   We interviewed seasoned VCs from top firms like CRV  IA Ventures  Two Sigma  and more to find how these successful investors evaluate artificial intelligence startups  If you re a founder thinking of starting an artificial intelligence company  be sure to have solid answers for all of these key questions   Is artificial intelligence core to your company s value proposition    Many companies who can t raise money try to shoehorn themselves as AI companies   warns Varun Jain of Qualcomm Ventures  Jain has seen pitches ranging from AI powered wifi routers to AI powered juicers and everything in between   In many of these cases  AI is an add on feature and not core to the company s value proposition   Traditional wifi routers can use artificial intelligence methods to detect anomalies in network data and label those errors  but this feature does not materially change the value add   explains Jain   By contrast  Qualcomm invested in Clarifai and Cruise Automation  acquired by GM   Cruise provides intelligence to power self driving cars while Clarifai leverages advanced deep learning and computer vision techniques to identify objects within images and video with high accuracy   How credible is your technical team and pedigree   According to Max Gazor of CRV   companies with significant and novel AI technology will have pedigrees from strong research labs in academia or come from reputable industry groups like Google Brain or Facebook s AI teams    CRV s commitment to technical pedigree is reflected in the extraordinary experience of the founders they invest in  Rod Brooks of Rethink Robotics was the founding director of MIT s artificial intelligence lab and as well as a founder of successful robotics company iRobot  NASDAQ  IRBT   Cynthia Breazeal of Jibo previously founded the Personal Robotics Group at the MIT Media Lab and is a world renowned expert on social robotics  Oren Jacob of Pullstring was previously CTO of Pixar and worked alongside Steve Jobs since the early days of the company   David Cheng of DCM Ventures adds   At this point in the industry s lifecycle  there are a limited number of AI experts available who have the requisite experience from large companies or top universities to build truly innovative solutions  This scarcity allows us to be highly skeptical if a team purports to use AI in their product without a team that matches    Do you solve real business problems that customers will pay for    One rule I ve found is  the more the CEO talks about AI  and not about their customer s problems  the less interested I get   quips Michael Dolbec of GE Ventures   We fund valuable outcomes  not science projects    Every single investor we spoke to agreed    If I had to pick one  domain expertise trumps machine learning expertise   added Brad Gillespie of IA Ventures  IA Ventures invested in Vectra Networks  a cybersecurity company headed by experienced domain experts who focused heavily on solving top customer problems and maximizing usability for security analysts   A competitor to Vectra emphasized their sophisticated machine intelligence to customers  but the buyer feedback was   These guys are smart but they don t understand my business  Their product has lots of bells   whistles  but I can t understand what it does    Solving business problems effectively requires a team to think beyond narrow technical approaches  but also to focus on and own a specific business domain and function  Colin Beirne of Two Sigma Ventures points out that  using an ensemble of different techniques is required to solve most hard problems today  but targeting A I  on a narrow domain space reduces the complexity of what it has to learn to understand    Do you have a relevant  proprietary  and scalable data source   Jain of Qualcomm Ventures always asks potential investments   How do you source data  Are you relying on big companies to give you data or do you have an independent manner of sourcing your own   Both methods can be viable  but independence is strongly preferred   Self driving cars are traditionally tested in suburbs  parking lots  and contained environments that do not reflect the reality of driving  Qualcomm s portfolio company Cruise Automation captured critically missing data by operating test vehicles  monitored by human drivers  in urban environments  Similarly  their other AI investment Clarifai started with a popular consumer app that enabled them to capture unique crowdsourced data before scaling further to work with business specific data   In addition to data sources being unique and defensible  they must also be relevant to the challenge being solved  According to Dharmesh Thakker of Battery Ventures   next generation artificial intelligence depends on the complexity of the data you are mining  Unstructured images  video  and audio data is far more difficult to mine than text   Thakker also considers whether a company works with fast moving data or static data  Algorithms for fast moving data  such as the real time images processed by a self driving car  are often much more complex   Finally  a team must demonstrate that they are continually improving their performance based on their unique data  Qualcomm s Jain checks in periodically to observe whether a team can  showcase the ability to quickly process training data and optimize efficiently so that systems are more robust    Have you built unique technology or do you mostly leverage open source frameworks    The extent of which companies are leveraging open source frameworks versus development of their own proprietary technology tends to be a giveaway   observes Suresh Madhavan of Verizon Ventures   Leveraging open source will let you analyze some superficial relationships  but it s unlikely to be at the level that is required to solve hard business problems    Cheng of DCM Ventures agrees  The investment team at DCM relies on a robust network of industry advisors and technical experts that  help vet technology stacks  data architecture  and identify whether or not the team is properly approaching data collection  storing  parsing  or annotation  They also help sniff out the phonies    Do you have a sticky product and robust revenue   Sumant Mandal is a Partner at March Capital and also co founder of The Hive  an early stage incubator focused on AI driven startups   If you do not bring at least a 5 10x improvement in efficiency  then it s very hard for a new company to break in and to bring value to investors   emphasizes Mandal  who advises that startups think in terms of their customers  revenue  For example  if you want to apply AI to recruiting processes  Mandal suggests you ask yourself  If I provide a 5x improvement in efficiency  will this result in a 100x improvement in revenue from the people my customers hire    Additionally  he cautions that value improvement must be delivered to customers  in a way they can consume  such as in the form of a dashboard or actionable insights   While cybersecurity is ripe for disruption by AI due to volume of data and dearth of human talent  Mandal warns that  security analysts do not need more alerts    Even if you have a desirable product  getting customers to commit to a single pilot program does not make for a viable business  Kartik Gada of Woodside Capital looks for diversified revenue and diversified customers   Is your revenue robust and recurring  Do your customers want the same or more of your solution    Do you have a diverse team that includes business experts and sales   marketing professionals   Last  but not least  investors look for diverse teams that can address all the challenges of starting and scaling an artificial intelligence business  Kiersten Stead of Monsanto Growth Ventures explains that  companies who have been successful hired diversely  to include domain experts  business leaders  and salespeople  not just engineering teams   In contrast  Stead observes that homogenous startup teams  especially when comprised solely of AI researchers with no industry specific experience  tend to fail more often  This is particularly important for outsiders seeking to address the niche applications that Monsanto invests in  such as agricultural operations and genetic breeding    Technical AI teams can t relate to sales people very well  and vice versa   she emphasizes   We look either for an experienced AI founder with an established career who can kick it with a wide range of people  or a stacked team   Sales and marketing are often overlooked in tech heavy AI startups  yet sorely needed for success    The biggest mistake AI startups make is to skimp on marketing   warns Gada of Woodside Capital   Most customers don t know they need their product,"[44 837 706 646 1284 102 1078 771 449 1369 335]"
102,training-dataset/business/22.txt,business,Machine intelligence  part 1This is going to be a two part post one on why machine intelligence is something we should be afraid of  and one on what we should do about it  If you re already afraid of machine intelligence  you can skip this one and read the second post tomorrow I was planning to only write part 2  but when I asked a few people to read drafts it became clear I needed part 1         WHY YOU SHOULD FEAR MACHINE INTELLIGENCE  Development of superhuman machine intelligence  SMI   1  is probably the greatest threat to the continued existence of humanity  There are other threats that I think are more certain to happen  for example  an engineered virus with a long incubation period and a high mortality rate  but are unlikely to destroy every human in the universe in the way that SMI could  Also  most of these other big threats are already widely feared   It is extremely hard to put a timeframe on when this will happen  more on this later   and it certainly feels to most people working in the field that it s still many  many years away  But it s also extremely hard to believe that it isn t very likely that it will happen at some point   SMI does not have to be the inherently evil sci fi version to kill us all  A more probable scenario is that it simply doesn t care about us much either way  but in an effort to accomplish some other goal  most goals  if you think about them long enough  could make use of resources currently being used by humans  wipes us out  Certain goals  like self preservation  could clearly benefit from no humans  We wash our hands not because we actively wish ill towards the bacteria and viruses on them  but because we don t want them to get in the way of our plans    Incidentally  Nick Bostrom s excellent book  Superintelligence  is the best thing I ve seen on this topic  It is well worth a read    Most machine intelligence development involves a  fitness function  something the program tries to optimize  At some point  someone will probably try to give a program the fitness function of  survive and reproduce   Even if not  it will likely be a useful subgoal of many other fitness functions  It worked well for biological life  Unfortunately for us  one thing I learned when I was a student in the Stanford AI lab is that programs often achieve their fitness function in unpredicted ways   Evolution will continue forward  and if humans are no longer the most fit species  we may go away  In some sense  this is the system working as designed  But as a human programmed to survive and reproduce  I feel we should fight it   How can we survive the development of SMI  It may not be possible  One of my top 4 favorite explanations for the Fermi paradox is that biological intelligence always eventually creates machine intelligence  which wipes out biological life and then for some reason decides to makes itself undetectable   It s very hard to know how close we are to machine intelligence surpassing human intelligence  Progression of machine intelligence is a double exponential function  human written programs and computing power are getting better at an exponential rate  and self learning self improving software will improve itself at an exponential rate  Development progress may look relatively slow and then all of a sudden go vertical things could get out of control very quickly  it also may be more gradual and we may barely perceive it happening    As mentioned earlier  it is probably still somewhat far away  especially in its ability to build killer robots with no help at all from humans  But recursive self improvement is a powerful force  and so it s difficult to have strong opinions about machine intelligence being ten or one hundred years away   We also have a bad habit of changing the definition of machine intelligence when a program gets really good to claim that the problem wasn t really that hard in the first place  chess  Jeopardy  self driving cars  etc    This makes it seems like we aren t making any progress towards it  Admittedly  narrow machine intelligence is very different than general purpose machine intelligence  but I still think this is a potential blindspot   It s hard to look at the rate or improvement in the last 40 years and think that 40 years for now we re not going to be somewhere crazy  40 years ago we had Pong  Today we have virtual reality so advanced that it s difficult to be sure if it s virtual or real  and computers that can beat humans in most games   Though  to be fair  in the last 40 years we have made little progress on the parts of machine intelligence that seem really hard learning  creativity  etc  Basic search with a lot of compute power has just worked better than expected   One additional reason that progress towards SMI is difficult to quantify is that emergent behavior is always a challenge for intuition  The above common criticism of current machine intelligence that no one has produced anything close to human creativity  and that this is somehow inextricably linked with any sort of real intelligence causes a lot of smart people to think that SMI must be very far away   But it s very possible that creativity and what we think of us as human intelligence are just an emergent property of a small number of algorithms operating with a lot of compute power  In fact  many respected neocortex researchers believe there is effectively one algorithm for all intelligence  I distinctly remember my undergrad advisor saying the reason he was excited about machine intelligence again was that brain research made it seem possible there was only one algorithm computer scientists had to figure out    Because we don t understand how human intelligence works in any meaningful way  it s difficult to make strong statements about how close or far away from emulating it we really are  We could be completely off track  or we could be one algorithm away   Human brains don t look all that different from chimp brains  and yet somehow produce wildly different capabilities  We decry current machine intelligence as cheap tricks  but perhaps our own intelligence is just the emergent combination of a bunch of cheap tricks   Many people seem to believe that SMI would be very dangerous if it were developed  but think that it s either never going to happen or definitely very far off  This is sloppy  dangerous thinking            1  I prefer calling it  machine intelligence  and not  artificial intelligence  because artificial seems to imply it s not real or not very good  When it gets developed  there will be nothing artificial about it,"[102 1369 453 837 44 335 186 1284 771 1040 646]"
119,training-dataset/engineering/1206.txt,engineering,It is Not Only Deep Learning That Requires Rethinking GeneralizationIt is Not Only Deep Learning That Requires Rethinking Generalization  TL DR  it is not only DNNs that can fit random labels under a setting where correct labels can be fitted with small generalization errors   The recent paper  Understanding deep learning requires rethinking generalization  by Chiyuan  et al  is attracting high interest as it is an unintuitive and surprising fact that standard DNN models can even  over fit random labels as well  I think the paper deserves the highest point at the ICLR 17 review as it brought us a great insight  However  as only DNNs  and some kinds of linear models  are discussed in the paper  it is natural to have the following question  how about other machine learning algorithms   So  I conducted brief experiments that are similar to the original paper with our favorite tree based machine learning algorithms  random forest  RF   extra trees  ET   and gradient boosting decision trees  GBDT    Experiment 1  RF  ET  and GBDT  The following figure illustrates the result of our first experiment  where I trained and tested the three methods for MNIST dataset using correct and random labels   We used 60 000 examples for training and the remaining 10 000 examples for testing  For the random label setting  we applied random permutations to the target labels of training examples  Solid and dashed lines correspond to training and testing accuracy  I used scikit learn for random forest and extra trees  and XGBoost for GBDT  Default values are used for all parameters  The code used for the experiments is available here   From the result for correct labels  the upper plot   we observe that all three methods can learn original MNIST with small generalization error  i e   small difference between training and testing accuracy   On the other hand  we see that the training accuracy of RF and ET gets quite high with moderate numbers of trees  the bottom plot   which indicates that they can fit random labels  too  under the same configurations  This is the same phenomenon as those reported in the original paper for DNNs   Experiment 2  varying a parameter of GBDT  In the above result  GBDT seemed an exception  i e   it did not fit random labels  However  we are going to see that it is not the case  The below figure depicts the result where I conducted the same experiments varying the max_depth parameter of GDBT   We see that  for relatively large max_depth values  GBDT fits random labels  while it has small generalization errors for correct labels   On the other hand  I could also prevent RF by using different parameters from fitting random labels under the restriction that it can learn correct labels well   Summary and Discussions  The key takeaway of this article is as follows  It is not only DNNs that can fit random labels under a setting where correct labels can be fitted with small generalization errors  We observed the same phenomena with random forest  extra trees  and gradient boosting trees   Possible points of discussions are as below   My experiments only show that there exist configurations where they can fit random labels under the restriction that they can learn correct labels well  However  it is not clear that they are common configurations   It might be more insightful if we could have the numbers of total tree nodes as x axis  instead of the numbers of trees   as they would be more relevant to the  capacity  of models   Also  it might be interesting to conduct quantitative investigations on the relationship between generalization errors and ability to fit random labels   The code used for the experiments is available here,"[119 995 1262 493 892 869 1035 1284 186 449 620]"
186,training-dataset/engineering/1111.txt,engineering,Machines That LearnREVOLUTION  There is a revolution happening right now in computing  Computers are becoming capable of many tasks that were previously considered only achievable by humans  As an example  back around 2011  if you asked an expert if a computer could tell the difference between a picture of a cat and a dog  they would probably tell you that it s a hard problem  They are both furry creatures of varying colors that can have pictures taken from so many angles and in so many ways  How could a computer possibly figure this out  Today  it s safe to say that this problem has been solved  And a whole lot of other challenging problems have been solved along with it   The driving force behind these advancements is a field called machine learning  Machine learning is when a computer learns by example instead of by strict rules that have been programmed  Specifically  there are algorithms called neural networks  deep neural networks  or deep learning that have been making huge advancements in the field  Neural networks borrow some ideas from biology in an effort to mimic the way a human brain works  Deep neural networks and deep learning build on the basic neural network algorithms in a way that lets them learn higher level concepts   Let s look at one example called the ImageNet challenge  ImageNet is a collection of images that are all tagged with a word describing what is in the image  Every year there is a challenge where teams compete to have their computer programs recognize these images  In 2011  the error rate of the best program was about 26   The way they score this is that out of many images  the computer has to guess what those images are from 1000 categories   various things like different dog breeds  plants and buildings  The computer has 5 guesses per image and if it can t guess correctly  it is considered to have failed that image  In 2012  a deep learning approach was used for the first time to win the challenge  Since then  the error rate has been almost cut in half every year  At the time of this writing  the error rate is 3 08   This looks even more impressive when you look at the human score for this challenge  One person tried to do this challenge himself  so there would be a reference for human performance  He got 5 1  error  So it s safe to say that computers are pretty good at image recognition now  and this is something they have been historically bad at   HOW IT WORKS  Before getting to other examples and applications of neural nets  I d like to explain a little about how the image recognition works   Machine learning differs from other ways of programming a computer because it learns from examples  Usually  when you program a computer  you give it exact rules to follow  As an example  if you want to make software to recognize an image of a tree  you could write a program that says   If the image is green on top and brown on the bottom  then it s a tree   That fails pretty quickly though with different kinds of trees and different lighting  and of course  in the fall when leaves turn red  You can solve that by writing more rules about what makes a tree a tree  but you quickly realize you re fighting a losing battle  The machine learning approach is to show a computer program images of thousands or millions of trees  and have it learn the patterns in the images automatically based on patterns it finds   More recently  many of the techniques that have been gaining traction are called  deep learning   Deep learning is machine learning  but instead of looking for simple patterns  it is able to look for patterns of patterns  or patterns of patterns of patterns  and so on  By doing that  the deep learning system can start to understand higher level concepts  In the case of image recognition  it will start by recognizing simple patterns like edges  From there  it will look for patterns of patterns   things that you can make from the simple edges  like corners or circles  From there  it can start recognizing higher level concepts  like if it sees a car  maybe it can start to put together the headlights or wheels from those edges and circles  And then finally  it can put all the patterns that make up car pieces into a whole car  Each one of these stages of finding patterns is called a  layer  of the neural network and it s the fact that these systems use many layers that is the reason this is called  deep   The  learning  part of  deep learning  is because all of these patterns that the computer looks for are learned from examples  not from manually designed rules   SIGNIFICANCE  Image recognition is just one example of how new machine learning techniques are changing what computers can do  Machine learning is generally good at problems where computers need to understand and or predict real world data that is not exact  Other examples are things like speech recognition and understanding natural language  There are limitless applications of this technology and every industry on the planet will be affected by it if it hasn t been already  In some applications  it will be easy to tell that machine learning is being used  Voice recognition on your phone primarily uses machine learning  For other applications  it will be less obvious that machine learning is involved  Better image recognition can help in the process of understanding medical images  There are companies working on medical diagnosis using the same image recognition technologies that are winning the ImageNet challenge   My work uses deep learning to recognize images of text and translate them between different languages in real time on a phone It shows that we can now take these neural networks and run them on phones  which are much less powerful than your typical desktop or cloud computers  Even though machine learning is the underlying technology  the user of the software doesn t necessarily know that  To them  it is an app that they can use to break down language barriers   Vocabulary to know   Convolutional Neural Network  CNN   This type of neural network is very good at image recognition  The ImageNet challenge winners tend to use variations of this algorithm   Long short term memory  LSTM   This type of neural network is good at understanding or predicting sequences of data  Things like speech or natural language will often be handled by LSTMs   Deep learning   deep neural networks  Usually when people talk about  deep  learning  they are talking about CNNs or LSTMs  These networks recognize patterns and then feed those patterns into another stage of the neural network that then recognizes patterns of patterns  This process can be repeated many times to learn higher level concepts   FUTURE   In the past  80s   90s   neural nets were hyped up and then didn t live up to the hype  This time things are different  Even if progress in the field were to stop this instant  the progress we have seen so far would still be quite significant and game changing for many industries  But it s not stopping  Every month exciting new research is released that pushes the boundaries and has people rethinking what was considered cutting edge last month   The initial spark for this recent progress came primarily from computers getting faster and from access to more data  Now we are also seeing so much research effort directed at this problem  that there are many significant algorithmic improvements happening  So let s look at each of these 3 things   performance  data  and algorithms   Computer performance improvements overall have slowed a bit in recent years  but there are companies making hardware specifically designed for neural networks  So performance will continue to improve for neural networks  allowing for more capable machine learning systems and allowing complex applications to run on lower power processors like those found in phones  More and more things are going online around the world and with that comes more data  Data quantity  quality  and diversity will continue to improve  This data can then be used to train machine learning systems  More attention is being paid to the field of machine learning  and with that  more research and investment is happening in companies in the space  There will continue to be algorithmic progress   Progress in machine learning is not slowing down  There are applications of this technology that will deeply affect every industry  This will be a revolution as big as  or bigger than  personal computers  the internet  or mobile phones  Machine learning is the next underlying technology,"[186 1284 449 453 837 869 230 646 818 1262 620]"
230,training-dataset/engineering/285.txt,engineering,Getting Started with Deep LearningGetting Started with Deep Learning A review of available tools   February 15th  2017  At SVDS  our R D team has been investigating different deep learning technologies  from recognizing images of trains to speech recognition  We needed to build a pipeline for ingesting data  creating a model  and evaluating the model performance  However  when we researched what technologies were available  we could not find a concise summary document to reference for starting a new deep learning project   One way to give back to the open source community that provides us with tools is to help others evaluate and choose those tools in a way that takes advantage of our experience  We offer the chart below  along with explanations of the various criteria upon which we based our decisions   These rankings are a combination of our subjective experiences with image and speech recognition applications for these technologies  as well as publicly available benchmarking studies  Please note that this is not an exhaustive list of available deep learning toolkits  more of which can be found here  In the coming months  our team is excited to checkout DeepLearning4j  Paddle  Chainer  Apache Signa  and Dynet  We explain our scoring of the reviewed tools below   Languages  When getting started with deep learning  it is best to use a framework that supports a language you are familiar with  For instance  Caffe  C    and Torch  Lua  have Python bindings for its codebase  but we would recommend that you are proficient with C   or Lua respectively if you would like to use those technologies  In comparison  TensorFlow and MXNet have great multi language support that make it possible to utilize the technology even if you are not proficient with C     Note  We have not had an opportunity to test out the new Python wrapper for Torch  PyTorch  released by Facebook AI Research  FAIR  in January 2017  This framework was built for Python programmers to leverage Torch s dynamic construction of neural networks   Tutorials and Training Materials  Deep learning technologies vary dramatically in the quality and quantity of tutorials and getting started materials  Theano  TensorFlow  Torch  and MXNet have well documented tutorials that are easy to understand and implement  While Microsoft s CNTK and Intel s Nervana Neon are powerful tools  we struggled to find beginner level materials  Additionally  we ve found that the engagement of the GitHub community is a strong indicator of not only a tool s future development  but also a measure of how likely fast an issue or bug can be solved through searching StackOverflow or the repo s Git Issues  It is important to note that TensorFlow is the 800 pound Gorilla in the room in regards to quantity of tutorials  training materials  and community of developers and users   CNN Modeling Capability  Convolutional neural networks  CNNs  are used for image recognition  recommendation engines  and natural language processing  A CNN is composed of a set of distinct layers that transform the initial data volume into output scores of predefined class scores  For more information  check out Eugenio Culurciello s overview of Neural Network architectures   CNN s can also be used for regression analysis  such as models that output of steering angles in autonomous vehicles  We consider a technology s CNN modeling capability to include several features  These features include the opportunity space to define models  the availability of prebuilt layers  and the tools and functions available to connect these layers  We ve seen that Theano  Caffe  and MXNet all have great CNN modeling capabilities  That said  TensorFlow s easy ability to build upon it s InceptionV3 model and Torch s great CNN resources including easy to use temporal convolution set these two technologies apart for CNN modeling capability   RNN Modeling Capability  Recurrent neural networks  RNNs  are used for speech recognition  time series prediction  image captioning  and other tasks that require processing sequential information  As prebuilt RNN models are not as numerous as CNNs  it is therefore important if you have a RNN deep learning project that you consider what RNN models have been previously implemented and open sourced for a specific technology  For instance  Caffe has minimal RNN resources  while Microsoft s CNTK and Torch have ample RNN tutorials and prebuilt models  While vanilla TensorFlow has some RNN materials  TFLearn and Keras include many more RNN examples that utilize TensorFlow   Architecture  In order to create and train new models in a particular framework  it is critical to have an easy to use and modular front end  TensorFlow  Torch  and MXNet have a straightforward  modular architecture that makes development straightforward  In comparison  frameworks such as Caffe require significant amount of work to create a new layer  We ve found that TensorFlow in particular is easy to debug and monitor during and after training  as the TensorBoard web GUI application is included   Speed  Torch and Nervana have the best documented performance for open source convolutional neural network benchmarking tests  TensorFlow performance was comparable for most tests  while Caffe and Theano lagged behind  Microsoft s CNTK claims to have some of the fastest RNN training time  Another study comparing Theano  Torch  and TensorFlow directly for RNN showed that Theano performs the best of the three   Multiple GPU Support  Most deep learning applications require an outstanding number of floating point operations  FLOPs   For example  Baidu s DeepSpeech recognition models take 10s of ExaFLOPs to train  That is  10e18 calculations  As leading Graphics Processing Units  GPUs  such as NVIDIA s Pascal TitanX can execute 11e9 FLOPs a second  it would take over a week to train a new model on a sufficiently large dataset  In order to decrease the time it takes to build a model  multiple GPUs over multiple machines are needed  Luckily  most of the technologies outlined above offer this support  In particular  MXNet is reported to have one the most optimized multi GPU engine   Keras Compatible  Keras is a high level library for doing fast deep learning prototyping  We ve found that it is a great tool for getting data scientists comfortable with deep learning  Keras currently supports two back ends  TensorFlow and Theano  and will be gaining official support in TensorFlow in the future  Keras is also a good choice for a high level library when considering that its author recently expressed that Keras will continue to exist as a front end that can be used with multiple back ends   If you are interested in getting started with deep learning  I would recommend evaluating your own team s skills and your project needs first  For instance  for an image recognition application with a Python centric team we would recommend TensorFlow given its ample documentation  decent performance  and great prototyping tools  For scaling up an RNN to production with a Lua competent client team  we would recommend Torch for its superior speed and RNN modeling capabilities   In the future we will discuss some of our challenges in scaling up our models  These challenges include optimizing GPU usage over multiple machines and adapting open source libraries like CMU Sphinx and Kaldi for our deep learning pipeline   sign up for our newsletter to stay in touch,"[230 1219 186 449 869 1284 1262 837 706 646 1040]"
335,training-dataset/engineering/467.txt,engineering,AI  Deep Learning  and Machine Learningwatch time  45 minutes   One person  in a literal garage  building a self driving car   That happened in 2015  Now to put that fact in context  compare this to 2004  when DARPA sponsored the very first driverless car Grand Challenge  Of the 20 entries they received then  the winning entry went 7 2 miles  in 2007  in the Urban Challenge  the winning entries went 60 miles under city like constraints   Things are clearly progressing rapidly when it comes to machine intelligence  But how did we get here  after not one but multiple  A I  winters   What s the breakthrough  And why is Silicon Valley buzzing about artificial intelligence again   From types of machine intelligence to a tour of algorithms  a16z Deal and Research team head Frank Chen walks us through the basics  and beyond  of AI and deep learning in this slide presentation   image credits,"[335 102 186 1284 44 837 646 453 1040 672 869]"
404,training-dataset/engineering/1224.txt,engineering,Unsupervised Feature Learning and Deep Learning TutorialWelcome to the Deep Learning Tutorial   Description  This tutorial will teach you the main ideas of Unsupervised Feature Learning and Deep Learning  By working through it  you will also get to implement several feature learning deep learning algorithms  get to see them work for yourself  and learn how to apply adapt these ideas to new problems     This tutorial assumes a basic knowledge of machine learning  specifically  familiarity with the ideas of supervised learning  logistic regression  gradient descent   If you are not familiar with these ideas  we suggest you go to this Machine Learning course and complete sections II  III  IV  up to Logistic Regression  first     Material contributed by  Andrew Ng  Jiquan Ngiam  Chuan Yu Foo  Yifan Mai  Caroline Suen  Adam Coates  Andrew Maas  Awni Hannun  Brody Huval  Tao Wang  Sameep Tandon  See this page for Getting Starter Code,"[404 1284 449 186 869 1040 230 1078 646 453 1262]"
445,training-dataset/engineering/733.txt,engineering,Building The LinkedIn Knowledge GraphAuthors  Qi He  Bee Chung Chen  Deepak Agarwal  A shorter version of this post first appeared on Pulse  our main publishing platform at LinkedIn  In this version  we ll dive deeper into the technical details behind the construction of our knowledge graph   At LinkedIn  we use machine learning technology widely to optimize our products  for instance  ranking search results  advertisements  and updates in the news feed  or recommending people  jobs  articles  and learning opportunities to members  An important component of this technology stack is a knowledge graph that provides input signals to machine learning models and data insight pipelines to power LinkedIn products  This post gives an overview of how we build this knowledge graph   LinkedIn s knowledge graph  LinkedIn s knowledge graph is a large knowledge base built upon  entities  on LinkedIn  such as members  jobs  titles  skills  companies  geographical locations  schools  etc  These entities and the relationships among them form the ontology of the professional world and are used by LinkedIn to enhance its recommender systems  search  monetization and consumer products  and business and consumer analytics   Creating a large knowledge base is a big challenge  Websites like Wikipedia and Freebase primarily rely on direct contributions from human volunteers  Other related work  such as Google s Knowledge Vault and Microsoft s Satori  focuses on automatically extracting facts from the internet for constructing knowledge bases  Different from these efforts  we derive LinkedIn s knowledge graph primarily from a large amount of user generated content from members  recruiters  advertisers  and company administrators  and supplement it with data extracted from the internet  which is noisy and can have duplicates  The knowledge graph needs to scale as new members register  new jobs are posted  new companies  skills  and titles appear in member profiles and job descriptions  etc   To solve the challenges we face when building the LinkedIn knowledge graph  we apply machine learning techniques  which is essentially a process of data standardization on user generated content and external data sources  in which machine learning is applied to entity taxonomy construction  entity relationship inference  data representation for downstream data consumers  insight extraction from graph  and interactive data acquisition from users to validate our inference and collect training data  LinkedIn s knowledge graph is a dynamic graph  New entities are added to the graph and new relationships are formed continuously  Existing relationships can also change  For example  the mapping from a member to her current title changes when she has a new job  We need to update the LinkedIn knowledge graph in real time upon member profile changes and when new entities emerge   Construction of entity taxonomy  For LinkedIn  an entity taxonomy consists of the identity of an entity  e g   its identifier  definition  canonical name  and synonyms in different languages  etc   and the attributes of an entity  Entities are created in two ways   Organic entities are generated by users  where informational attributes are produced and maintained by users  Examples include members  premium jobs  companies created by their administrators  etc   Auto created entities are generated by LinkedIn  Since the member coverage of an entity  number of members who have this entity  is key to the value that data can drive across both monetization and consumer products  we focus on creating new entities for which we can map members to  By mining member profiles for entity candidates and utilizing external data sources and human validations to enrich candidate attributes  we created tens of thousands of skills  titles  geographical locations  companies  certificates  etc   to which we can map members   To date  there are 450M members  190M historical job listings  9M companies  200  countries  where 60  have granular geolocational data   35K skills in 19 languages  28K schools  1 5K fields of study  600  degrees  24K titles in 19 languages  and 500  certificates  among other entities   Entities represent the nodes in the LinkedIn knowledge graph  We need to clean up user generated organic entities  which can have meaningless names  invalid or incomplete attributes  stale content  or no member mapped to them  We inductively generate rules to identify inaccurate or problematic organic entities  For auto created entities  the generation process includes   Generate candidates  Each entity has a canonical name which is an English phrase in most cases  Entity candidates are common phrases in member profiles and job descriptions based on intuitive rules   Disambiguate entities  A phrase can have different meanings in different contexts  By representing each phrase as a vector of top co occurred phrases in member profiles and job descriptions  we developed a soft clustering algorithm to group phrases  An ambiguous phrase can appear in multiple clusters and represent different entities   De duplicate entities  Multiple phrases can represent the same entity if they are synonyms of each other  By representing each phrase as a word vector  e g   produced by a word2vec model trained on member profiles and job descriptions   we run a clustering algorithm combined with manual validations from taxonomists to de duplicate entities  Similar techniques are also used to cluster entities if the taxonomy has a hierarchical structure   Translate entities into other languages  Given the power law nature of the member coverage of entities  linguistic experts at LinkedIn manually translate the top entities with high member coverages into international languages to achieve high precision  and PSCFG based machine translation models are applied to automatically translate long tail entities to achieve high recall   The below figure visualizes an example title entity  Software Engineer  in the title taxonomy  The title taxonomy has a hierarchical structure  similar titles such as  Programmer  and  Web Developer  are clustered into the same supertitle of  Software Developer   and similar supertitles are clustered into the same function of  Engineering,"[445 903 1284 1412 449 706 771 646 186 1040 837]"
449,training-dataset/engineering/992.txt,engineering,How to use Deep Learning when you have Limited DataNanoNets   How to use Deep Learning when you have Limited Data  Disclaimer  I m building nanonets ai to help build ML with less data  I think AI is akin to building a rocket ship  You need a huge engine and a lot of fuel  If you have a large engine and a tiny amount of fuel  you won t make it to orbit  If you have a tiny engine and a ton of fuel  you can t even lift off  To build a rocket you need a huge engine and a lot of fuel   The analogy to deep learning is that the rocket engine is the deep learning models and the fuel is the huge amounts of data we can feed to these algorithms    Andrew Ng  There has been a recent surge in popularity of Deep Learning  achieving state of the art performance in various tasks like Language Translation  playing Strategy Games and Self Driving Cars requiring millions of data points  One common barrier for using deep learning to solve problems is the amount of data needed to train a model  The requirement of large data arises because of the large number of parameters in the model that machines have to learn   A few examples of number of parameters in these recent models are   Details of Deep Learning Models  Neural Networks aka Deep Learning are layered structures which can be stacked together  think LEGO   Deep Learning is nothing but Large Neural networks  they can be thought of as a flow chart where data comes in from one side and inference knowledge comes out the other  You can also break the neural network  pull it apart and take the inference out from wherever you please  You might get nothing meaningful but you can do it nonetheless eg Google DeepDream  Size Model    Size Data    Complexity Problem   There is an interesting almost linear relationship in the amount of data required and the size of the model  Basic reasoning is that your model should be large enough to capture relations in your data  eg textures and shapes in images  grammar in text and phonemes in speech  along with specifics of your problem  eg number of categories   Early layers of the model capture high level relations between the different parts of the input  like edges and patterns   Later layers capture information that helps make the final decision  usually information that can help discriminate between the desired outputs  Therefore if the complexity of the problem is high  like Image Classification  the number of parameters and the amount of data required is also very large   What AlexNet sees at every step  Transfer Learning to the Rescue   When working on a problem specific to your domain  often the amount of data needed to build models of this size is impossible to find  However models trained on one task capture relations in the data type and can easily be reused for different problems in the same domain  This technique is referred to as Transfer Learning   Qiang Yang  Sinno Jialin Pan   A Survey on Transfer Learning   IEEE Transactions on Knowledge   Data Engineering  vol  22  no    pp  1345 1359  October 2010  doi 10 1109 TKDE 2009 191  Transfer Learning is like the best kept secret that nobody is trying to keep  Everybody in the industry knows about it but nobody outside does   Referring to Awesome   Most Cited Deep Learning Papers for the top papers in Deep Learning  More than 50  of the papers use some form of Transfer Learning or Pretraining  Transfer Learning becomes more and more applicable for people with limited resources  data and compute  unfortunately the idea has not been socialised nearly enough as it should  The people who need it the most don t know about it yet   If Deep Learning is the holy grail and data is the gate keeper  transfer learning is the key   With transfer learning  we can take a pretrained model  which was trained on a large readily available dataset  trained on a completely different task  with the same input but different output   Then try to find layers which output reusable features  We use the output of that layer as input features to train a much smaller network that requires a smaller number of parameters  This smaller network only needs to learn the relations for your specific problem having already learnt about patterns in the data from the pretrained model  This way a model trained to detect Cats can be reused to Reproduce the work of Van Gogh  Another major advantage of using transfer learning is how well the model generalizes  Larger models tend to overfit  ie modeling the data more than the underlying phenomenon  the data and don t work as well when you test it out on unseen data  Since transfer learning allows the model to see different types of data its learning underlying rules of the world better   Think of overfitting as memorizing as opposed to learning    James Faghmous  Data Reduction because of Transfer Learning  Let s say you want to end the debate of blue and black vs  white and gold dress  You start collecting images of verified blue black dresses and white gold dresses  To build an accurate model on your own like the one mentioned above  with 140M parameters     to train this model you will need to find 1 2M images which is an impossible task  So you give transfer learning a shot   Calculating the number of parameters needed to train for this problem using transfer learning   No of parameters    Size inputs    1     Size outputs    1      2048 1   1 1   4098 parameters  We see a reduction in number of parameters from 1 4 10  to 4 10  which is 5 orders of magnitude  So we should be fine collecting less than hundred images of dresses  Phew   If your impatient and can t wait to find out the actual color of the dress  scroll down to the bottom and see how to build the model for dresses yourself,"[449 1284 186 706 1262 230 869 404 1078 837 44]"
453,training-dataset/product/1102.txt,product,Is Machine Learning Overhyped Is Machine Learning Overhyped   For the nine years I ve been a venture capitalist  there s always been a buzzword of the year  Solomo  social local mobile   Mobile first  Realtime  Big data  2016 was the year of machine learning  Is ML just another wave to crash and dissipate on the trough of disillusionment   I don t think so  In this rare case  I think hype is masking quite a bit of true technical innovation  During last quarter of 2016  machine learning research has made huge strides   Computers now understand human speech as well as other humans  Computers can talk in a way that s close to indistiguishable from true human speech  Computers can translate from one language to another  never having read the second language  Computers can generate new encryption schemes without human input  Computers can write captions to describe a given image   These innovations aren t limited to the lab  Tesla s self driving car reduces crash rates by 40  and a brick laying robot builds walls 3 times as quickly as a human   While some may groan that every pitch deck is littered with the words machine learning or artificial intelligence  I think each deck ought to be  Because over the next five to ten years  nearly every company will use machine learning in some form   Advertising optimization  Antifraud software  Intrusion prevention  Stock trading These first uses of ML in the 2000s reveal the characteristics of problems that benefitted from machine learning  frequently repeated processes whose decision making could be measured  quantified and back tested  And processes in which humans could determine which factors in the decision are important   The algorithmic advances in 2016 enumerated above broaden the range of applications for ML  With the right volumes and types data and processing power  computers can develop enough of an understanding to predict  optimize  segment or detect anomalies in many new domains like speech  like language generation  like image recognition  like natural language understanding  like image and music creation  And they can do it with far less human guidance than before   While the we may not yet be able to build many of the things we dream of in software  we re getting much closer much faster  Startups are going to revolutionize existing software categories with ML  and they will create new categories of software with ML   Published 2017 01 22 in,"[453 186 646 1284 102 1369 837 1040 449 771 516]"
493,training-dataset/engineering/6.txt,engineering,terryum awesome deep learning papers  The most cited deep learning papersAwesome   Most Cited Deep Learning Papers  A curated list of the most cited deep learning papers  since 2012   We believe that there exist classic deep learning papers which are worth reading regardless of their application domain  Rather than providing overwhelming amount of papers  We would like to provide a curated list of the awesome deep learning papers which are considered as must reads in certain research domains   Before this list  there exist other awesome deep learning lists  for example  Deep Vision and Awesome Recurrent Neural Networks  Also  after this list comes out  another awesome list for deep learning beginners  called Deep Learning Papers Reading Roadmap  has been created and loved by many deep learning researchers   Although the Roadmap List includes lots of important deep learning papers  it feels overwhelming for me to read them all  As I mentioned in the introduction  I believe that seminal works can give us lessons regardless of their application domain  Thus  I would like to introduce top 100 deep learning papers here as a good starting point of overviewing deep learning researches   To get the news for newly released papers everyday  follow my twitter or facebook page   Awesome list criteria  A list of top 100 deep learning papers published from 2012 to 2016 is suggested  If a paper is added to the list  another paper  usually from  More Papers from 2016  section  should be removed to keep top 100 papers   Thus  removing papers is also important contributions as well as adding papers  Papers that are important  but failed to be included in the list  will be listed in More than Top 100 section  Please refer to New Papers and Old Papers sections for the papers published in recent 6 months or before 2012    Citation criteria     6 months   New Papers  by discussion     New Papers  by discussion  2016    60 citations or  More Papers from 2016      60 citations or  More Papers from 2016  2015    200 citations     200 citations 2014    400 citations     400 citations 2013    600 citations     600 citations 2012    800 citations     800 citations  2012   Old Papers  by discussion   Please note that we prefer seminal deep learning papers that can be applied to various researches rather than application papers  For that reason  some papers that meet the criteria may not be accepted while others can be  It depends on the impact of the paper  applicability to other researches scarcity of the research domain  and so on   We need your contributions   If you have any suggestions  missing papers  new papers  key researchers or typos   please feel free to edit and pull a request   Please read the contributing guide for further instructions  though just letting me know the title of papers can also be a big contribution to us     Update  You can download all top 100 papers with this and collect all authors  names with this  Also  bib file for all top 100 papers are available  Thanks  doodhwala  Sven and grepinsight   Can anyone contribute the code for obtaining the statistics of the authors of Top 100 papers    More than Top 100   Understanding   Generalization   Transfer  Distilling the knowledge in a neural network  2015   G  Hinton et al   pdf    2015   G  Hinton et al   pdf  Deep neural networks are easily fooled  High confidence predictions for unrecognizable images  2015   A  Nguyen et al   pdf    2015   A  Nguyen et al   pdf  How transferable are features in deep neural networks   2014   J  Yosinski et al   pdf    2014   J  Yosinski et al   pdf  CNN features off the Shelf  An astounding baseline for recognition  2014   A  Razavian et al   pdf    2014   A  Razavian et al   pdf  Learning and transferring mid Level image representations using convolutional neural networks  2014   M  Oquab et al   pdf    2014   M  Oquab et al   pdf  Visualizing and understanding convolutional networks  2014   M  Zeiler and R  Fergus  pdf    2014   M  Zeiler and R  Fergus  pdf  Decaf  A deep convolutional activation feature for generic visual recognition  2014   J  Donahue et al   pdf   Optimization   Training Techniques  Batch normalization  Accelerating deep network training by reducing internal covariate shift  2015   S  Loffe and C  Szegedy  pdf    2015   S  Loffe and C  Szegedy  pdf  Delving deep into rectifiers  Surpassing human level performance on imagenet classification  2015   K  He et al   pdf    2015   K  He et al   pdf  Dropout  A simple way to prevent neural networks from overfitting  2014   N  Srivastava et al   pdf    2014   N  Srivastava et al   pdf  Adam  A method for stochastic optimization  2014   D  Kingma and J  Ba  pdf    2014   D  Kingma and J  Ba  pdf  Improving neural networks by preventing co adaptation of feature detectors  2012   G  Hinton et al   pdf    2012   G  Hinton et al   pdf  Random search for hyper parameter optimization  2012  J  Bergstra and Y  Bengio  pdf   Unsupervised   Generative Models  Pixel recurrent neural networks  2016   A  Oord et al   pdf    2016   A  Oord et al   pdf  Improved techniques for training GANs  2016   T  Salimans et al   pdf    2016   T  Salimans et al   pdf  Unsupervised representation learning with deep convolutional generative adversarial networks  2015   A  Radford et al   pdf    2015   A  Radford et al   pdf  DRAW  A recurrent neural network for image generation  2015   K  Gregor et al   pdf    2015   K  Gregor et al   pdf  Generative adversarial nets  2014   I  Goodfellow et al   pdf    2014   I  Goodfellow et al   pdf  Auto encoding variational Bayes  2013   D  Kingma and M  Welling  pdf    2013   D  Kingma and M  Welling  pdf  Building high level features using large scale unsupervised learning  2013   Q  Le et al   pdf   Convolutional Neural Network Models  Rethinking the inception architecture for computer vision  2016   C  Szegedy et al   pdf    2016   C  Szegedy et al   pdf  Inception v4  inception resnet and the impact of residual connections on learning  2016   C  Szegedy et al   pdf    2016   C  Szegedy et al   pdf  Identity Mappings in Deep Residual Networks  2016   K  He et al   pdf    2016   K  He et al   pdf  Deep residual learning for image recognition  2016   K  He et al   pdf    2016   K  He et al   pdf  Going deeper with convolutions  2015   C  Szegedy et al   pdf    2015   C  Szegedy et al   pdf  Very deep convolutional networks for large scale image recognition  2014   K  Simonyan and A  Zisserman  pdf    2014   K  Simonyan and A  Zisserman  pdf  Spatial pyramid pooling in deep convolutional networks for visual recognition  2014   K  He et al   pdf    2014   K  He et al   pdf  Return of the devil in the details  delving deep into convolutional nets  2014   K  Chatfield et al   pdf    2014   K  Chatfield et al   pdf  OverFeat  Integrated recognition  localization and detection using convolutional networks  2013   P  Sermanet et al   pdf    2013   P  Sermanet et al   pdf  Maxout networks  2013   I  Goodfellow et al   pdf    2013   I  Goodfellow et al   pdf  Network in network  2013   M  Lin et al   pdf    2013   M  Lin et al   pdf  ImageNet classification with deep convolutional neural networks  2012   A  Krizhevsky et al   pdf   Image  Segmentation   Object Detection  You only look once  Unified  real time object detection  2016   J  Redmon et al   pdf    2016   J  Redmon et al   pdf  Region based convolutional networks for accurate object detection and segmentation  2016   R  Girshick et al   pdf    2016   R  Girshick et al   pdf  Fully convolutional networks for semantic segmentation  2015   J  Long et al   pdf    2015   J  Long et al   pdf  Faster R CNN  Towards Real Time Object Detection with Region Proposal Networks  2015   S  Ren et al   pdf    2015   S  Ren et al   pdf  Fast R CNN  2015   R  Girshick  pdf    2015   R  Girshick  pdf  Rich feature hierarchies for accurate object detection and semantic segmentation  2014   R  Girshick et al   pdf    2014   R  Girshick et al   pdf  Semantic image segmentation with deep convolutional nets and fully connected CRFs   L  Chen et al   pdf     L  Chen et al   pdf  Learning hierarchical features for scene labeling  2013   C  Farabet et al   pdf   Image   Video   Etc  Image Super Resolution Using Deep Convolutional Networks  2016   C  Dong et al   pdf    2016   C  Dong et al   pdf  A neural algorithm of artistic style  2015   L  Gatys et al   pdf    2015   L  Gatys et al   pdf  Deep visual semantic alignments for generating image descriptions  2015   A  Karpathy and L  Fei Fei  pdf    2015   A  Karpathy and L  Fei Fei  pdf  Show  attend and tell  Neural image caption generation with visual attention  2015   K  Xu et al   pdf    2015   K  Xu et al   pdf  Show and tell  A neural image caption generator  2015    O  Vinyals et al   pdf     O  Vinyals et al   pdf  Long term recurrent convolutional networks for visual recognition and description  2015   J  Donahue et al   pdf    2015   J  Donahue et al   pdf  VQA  Visual question answering  2015   S  Antol et al   pdf    2015   S  Antol et al   pdf  DeepFace  Closing the gap to human level performance in face verification  2014   Y  Taigman et al   pdf     2014   Y  Taigman et al   pdf   Large scale video classification with convolutional neural networks  2014   A  Karpathy et al   pdf    2014   A  Karpathy et al   pdf  DeepPose  Human pose estimation via deep neural networks  2014   A  Toshev and C  Szegedy  pdf    2014   A  Toshev and C  Szegedy  pdf  Two stream convolutional networks for action recognition in videos  2014   K  Simonyan et al   pdf    2014   K  Simonyan et al   pdf  3D convolutional neural networks for human action recognition  2013   S  Ji et al   pdf   Recurrent Neural Network Models  Conditional random fields as recurrent neural networks  2015   S  Zheng and S  Jayasumana   pdf    2015   S  Zheng and S  Jayasumana   pdf  Memory networks  2014   J  Weston et al   pdf    2014   J  Weston et al   pdf  Neural turing machines  2014   A  Graves et al   pdf    2014   A  Graves et al   pdf  Generating sequences with recurrent neural networks  2013   A  Graves   pdf   Natural Language Processing  Neural Architectures for Named Entity Recognition  2016   G  Lample et al   pdf    2016   G  Lample et al   pdf  Exploring the limits of language modeling  2016   R  Jozefowicz et al   pdf    2016   R  Jozefowicz et al   pdf  Teaching machines to read and comprehend  2015   K  Hermann et al   pdf    2015   K  Hermann et al   pdf  Effective approaches to attention based neural machine translation  2015   M  Luong et al   pdf    2015   M  Luong et al   pdf  Neural machine translation by jointly learning to align and translate  2014   D  Bahdanau et al   pdf    2014   D  Bahdanau et al   pdf  Sequence to sequence learning with neural networks  2014   I  Sutskever et al   pdf    2014   I  Sutskever et al   pdf  Learning phrase representations using RNN encoder decoder for statistical machine translation  2014   K  Cho et al   pdf    2014   K  Cho et al   pdf  A convolutional neural network for modeling sentences  2014   N  Kalchbrenner et al   pdf    2014   N  Kalchbrenner et al   pdf  Convolutional neural networks for sentence classification  2014   Y  Kim  pdf    2014   Y  Kim  pdf  Glove  Global vectors for word representation  2014   J  Pennington et al   pdf    2014   J  Pennington et al   pdf  Distributed representations of sentences and documents  2014   Q  Le and T  Mikolov  pdf    2014   Q  Le and T  Mikolov  pdf  Distributed representations of words and phrases and their compositionality  2013   T  Mikolov et al   pdf    2013   T  Mikolov et al   pdf  Efficient estimation of word representations in vector space  2013   T  Mikolov et al   pdf    2013   T  Mikolov et al   pdf  Recursive deep models for semantic compositionality over a sentiment treebank  2013   R  Socher et al   pdf   Speech   Other Domain  End to end attention based large vocabulary speech recognition  2016   D  Bahdanau et al   pdf    2016   D  Bahdanau et al   pdf  Deep speech 2  End to end speech recognition in English and Mandarin  2015   D  Amodei et al   pdf    2015   D  Amodei et al   pdf  Speech recognition with deep recurrent neural networks  2013   A  Graves  pdf    2013   A  Graves  pdf  Deep neural networks for acoustic modeling in speech recognition  The shared views of four research groups  2012   G  Hinton et al   pdf    2012   G  Hinton et al   pdf  Context dependent pre trained deep neural networks for large vocabulary speech recognition  2012  G  Dahl et al   pdf    2012  G  Dahl et al   pdf  Acoustic modeling using deep belief networks  2012   A  Mohamed et al   pdf   Reinforcement Learning   Robotics  End to end training of deep visuomotor policies  2016   S  Levine et al   pdf    2016   S  Levine et al   pdf  Learning Hand Eye Coordination for Robotic Grasping with Deep Learning and Large Scale Data Collection  2016   S  Levine et al   pdf    2016   S  Levine et al   pdf  Asynchronous methods for deep reinforcement learning  2016   V  Mnih et al   pdf    2016   V  Mnih et al   pdf  Deep Reinforcement Learning with Double Q Learning  2016   H  Hasselt et al   pdf    2016   H  Hasselt et al   pdf  Mastering the game of Go with deep neural networks and tree search  2016   D  Silver et al   pdf    2016   D  Silver et al   pdf  Continuous control with deep reinforcement learning  2015   T  Lillicrap et al   pdf    2015   T  Lillicrap et al   pdf  Human level control through deep reinforcement learning  2015   V  Mnih et al   pdf    2015   V  Mnih et al   pdf  Deep learning for detecting robotic grasps  2015   I  Lenz et al   pdf    2015   I  Lenz et al   pdf  Playing atari with deep reinforcement learning  2013   V  Mnih et al   pdf    More Papers from 2016  Layer Normalization  2016   J  Ba et al   pdf    2016   J  Ba et al   pdf  Learning to learn by gradient descent by gradient descent  2016   M  Andrychowicz et al   pdf    2016   M  Andrychowicz et al   pdf  Domain adversarial training of neural networks  2016   Y  Ganin et al   pdf    2016   Y  Ganin et al   pdf  WaveNet  A Generative Model for Raw Audio  2016   A  Oord et al   pdf   web    2016   A  Oord et al   pdf   web  Colorful image colorization  2016   R  Zhang et al   pdf    2016   R  Zhang et al   pdf  Generative visual manipulation on the natural image manifold  2016   J  Zhu et al   pdf    2016   J  Zhu et al   pdf  Texture networks  Feed forward synthesis of textures and stylized images  2016   D Ulyanov et al   pdf    2016   D Ulyanov et al   pdf  SSD  Single shot multibox detector  2016   W  Liu et al   pdf    2016   W  Liu et al   pdf  SqueezeNet  AlexNet level accuracy with 50x fewer parameters and  1MB model size  2016   F  Iandola et al   pdf    2016   F  Iandola et al   pdf  Eie  Efficient inference engine on compressed deep neural network  2016   S  Han et al   pdf    2016   S  Han et al   pdf  Binarized neural networks  Training deep neural networks with weights and activations constrained to  1 or 1  2016   M  Courbariaux et al   pdf    2016   M  Courbariaux et al   pdf  Dynamic memory networks for visual and textual question answering  2016   C  Xiong et al   pdf    2016   C  Xiong et al   pdf  Stacked attention networks for image question answering  2016   Z  Yang et al   pdf    2016   Z  Yang et al   pdf  Hybrid computing using a neural network with dynamic external memory  2016   A  Graves et al   pdf    2016   A  Graves et al   pdf  Google s neural machine translation system  Bridging the gap between human and machine translation  2016   Y  Wu et al   pdf   New papers  Newly published papers    6 months  which are worth reading  Mask R CNN  2017   K  He   pdf   Learning to discover cross domain relations with generative adversarial networks  2017   T  Kim et al   pdf   Deep voice  Real time neural text to speech  2017   S  Arik et al    pdf   PixelNet  Representation of the pixels  by the pixels  and for the pixels  2017   A  Bansal et al   pdf   Batch renormalization  Towards reducing minibatch dependence in batch normalized models  2017   S  Ioffe   pdf   Wasserstein GAN  2017   M  Arjovsky et al   pdf   Understanding deep learning requires rethinking generalization  2017   C  Zhang et al   pdf   Least squares generative adversarial networks  2016   X  Mao et al   pdf   Old Papers  Classic papers published before 2012  An analysis of single layer networks in unsupervised feature learning  2011   A  Coates et al   pdf   Deep sparse rectifier neural networks  2011   X  Glorot et al   pdf   Natural language processing  almost  from scratch  2011   R  Collobert et al   pdf   Recurrent neural network based language model  2010   T  Mikolov et al   pdf   Stacked denoising autoencoders  Learning useful representations in a deep network with a local denoising criterion  2010   P  Vincent et al   pdf   Learning mid level features for recognition  2010   Y  Boureau  pdf   A practical guide to training restricted boltzmann machines  2010   G  Hinton  pdf   Understanding the difficulty of training deep feedforward neural networks  2010   X  Glorot and Y  Bengio  pdf   Why does unsupervised pre training help deep learning  2010   D  Erhan et al   pdf   Learning deep architectures for AI  2009   Y  Bengio   pdf   Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations  2009   H  Lee et al   pdf   Greedy layer wise training of deep networks  2007   Y  Bengio et al   pdf   Reducing the dimensionality of data with neural networks  G  Hinton and R  Salakhutdinov   pdf   A fast learning algorithm for deep belief nets  2006   G  Hinton et al   pdf   Gradient based learning applied to document recognition  1998   Y  LeCun et al   pdf   Long short term memory  1997   S  Hochreiter and J  Schmidhuber   pdf   HW   SW   Dataset  OpenAI gym  2016   G  Brockman et al   pdf   TensorFlow  Large scale machine learning on heterogeneous distributed systems  2016   M  Abadi et al   pdf   Theano  A Python framework for fast computation of mathematical expressions  R  Al Rfou et al   Torch7  A matlab like environment for machine learning  R  Collobert et al   pdf   MatConvNet  Convolutional neural networks for matlab  2015   A  Vedaldi and K  Lenc  pdf   Imagenet large scale visual recognition challenge  2015   O  Russakovsky et al   pdf   Caffe  Convolutional architecture for fast feature embedding  2014   Y  Jia et al   pdf   Book   Survey   Review  On the Origin of Deep Learning  2017   H  Wang and Bhiksha Raj   pdf   Deep Reinforcement Learning  An Overview  2017   Y  Li   pdf   Neural Machine Translation and Sequence to sequence Models 2017   A Tutorial  G  Neubig   pdf   Neural Network and Deep Learning  Book  Jan 2017   Michael Nielsen   html   Deep learning  Book  2016   Goodfellow et al   html   LSTM  A search space odyssey  2016   K  Greff et al   pdf   Tutorial on Variational Autoencoders  2016   C  Doersch   pdf   Deep learning  2015   Y  LeCun  Y  Bengio and G  Hinton  pdf   Deep learning in neural networks  An overview  2015   J  Schmidhuber  pdf   Representation learning  A review and new perspectives  2013   Y  Bengio et al   pdf   Video Lectures   Tutorials   Blogs   Lectures   CS231n  Convolutional Neural Networks for Visual Recognition  Stanford University  web   CS224d  Deep Learning for Natural Language Processing  Stanford University  web   Oxford Deep NLP 2017  Deep Learning for Natural Language Processing  University of Oxford  web    Tutorials   NIPS 2016 Tutorials  Long Beach  web   ICML 2016 Tutorials  New York City  web   ICLR 2016 Videos  San Juan  web   Deep Learning Summer School 2016  Montreal  web   Bay Area Deep Learning School 2016  Stanford  web    Blogs   OpenAI  web   Distill  web   Andrej Karpathy Blog  web   Colah s Blog  Web   WildML  Web   FastML  web   TheMorningPaper  web   Appendix  More than Top 100   2016   A character level decoder without explicit segmentation for neural machine translation  2016   J  Chung et al   pdf   Dermatologist level classification of skin cancer with deep neural networks  2017   A  Esteva et al   html   Weakly supervised object localization with multi fold multiple instance learning  2017   R  Gokberk et al   pdf   Brain tumor segmentation with deep neural networks  2017   M  Havaei et al   pdf   Professor Forcing  A New Algorithm for Training Recurrent Networks  2016   A  Lamb et al   pdf   Adversarially learned inference  2016   V  Dumoulin et al   web  pdf   Understanding convolutional neural networks  2016   J  Koushik  pdf   Taking the human out of the loop  A review of bayesian optimization  2016   B  Shahriari et al   pdf   Adaptive computation time for recurrent neural networks  2016   A  Graves  pdf   Densely connected convolutional networks  2016   G  Huang et al   pdf   Continuous deep q learning with model based acceleration  2016   S  Gu et al   pdf   A thorough examination of the cnn daily mail reading comprehension task  2016   D  Chen et al   pdf   Achieving open vocabulary neural machine translation with hybrid word character models  M  Luong and C  Manning   pdf   Very Deep Convolutional Networks for Natural Language Processing  2016   A  Conneau et al   pdf   Bag of tricks for efficient text classification  2016   A  Joulin et al   pdf   Efficient piecewise training of deep structured models for semantic segmentation  2016   G  Lin et al   pdf   Learning to compose neural networks for question answering  2016   J  Andreas et al   pdf   Perceptual losses for real time style transfer and super resolution  2016   J  Johnson et al   pdf   Reading text in the wild with convolutional neural networks  2016   M  Jaderberg et al   pdf   What makes for effective detection proposals   2016   J  Hosang et al   pdf   Inside outside net  Detecting objects in context with skip pooling and recurrent neural networks  2016   S  Bell et al   pdf    Instance aware semantic segmentation via multi task network cascades  2016   J  Dai et al   pdf   Conditional image generation with pixelcnn decoders  2016   A  van den Oord et al   pdf   Deep networks with stochastic depth  2016   G  Huang et al    pdf   Generative Short Term Stochastic Gibbs Networks 2016   I  Lenz et al   pdf    2015   Spatial transformer network  2015   M  Jaderberg et al    pdf   Ask your neurons  A neural based approach to answering questions about images  2015   M  Malinowski et al   pdf   Exploring models and data for image question answering  2015   M  Ren et al   pdf   Are you talking to a machine  dataset and methods for multilingual image question  2015   H  Gao et al   pdf   Mind s eye  A recurrent visual representation for image caption generation  2015   X  Chen and C  Zitnick   pdf   From captions to visual concepts and back  2015   H  Fang et al   pdf    Towards AI complete question answering  A set of prerequisite toy tasks  2015   J  Weston et al   pdf   Ask me anything  Dynamic memory networks for natural language processing  2015   A  Kumar et al   pdf   Unsupervised learning of video representations using LSTMs  2015   N  Srivastava et al   pdf   Deep compression  Compressing deep neural networks with pruning  trained quantization and huffman coding  2015   S  Han et al   pdf   Improved semantic representations from tree structured long short term memory networks  2015   K  Tai et al   pdf   Character aware neural language models  2015   Y  Kim et al   pdf   Grammar as a foreign language  2015   O  Vinyals et al   pdf   Trust Region Policy Optimization  2015   J  Schulman et al   pdf   Beyond short snippents  Deep networks for video classification  2015   pdf   Learning Deconvolution Network for Semantic Segmentation  2015   H  Noh et al   pdf   Learning spatiotemporal features with 3d convolutional networks  2015   D  Tran et al   pdf   Understanding neural networks through deep visualization  2015   J  Yosinski et al   pdf   An Empirical Exploration of Recurrent Network Architectures  2015   R  Jozefowicz et al   pdf   Training very deep networks  2015   R  Srivastava et al   pdf   Deep generative image models using a  laplacian pyramid of adversarial networks  2015   E Denton et al   pdf   Gated Feedback Recurrent Neural Networks  2015   J  Chung et al   pdf   Fast and accurate deep network learning by exponential linear units  ELUS   2015   D  Clevert et al   pdf   Pointer networks  2015   O  Vinyals et al   pdf   Visualizing and Understanding Recurrent Networks  2015   A  Karpathy et al   pdf   Attention based models for speech recognition  2015   J  Chorowski et al   pdf   End to end memory networks  2015   S  Sukbaatar et al   pdf   Describing videos by exploiting temporal structure  2015   L  Yao et al   pdf   A neural conversational model  2015   O  Vinyals and Q  Le   pdf   Improving distributional similarity with lessons learned from word embeddings  O  Levy et al    pdf    https   www transacl org ojs index php tacl article download 570 124   Transition Based Dependency Parsing with Stack Long Short Term Memory  2015   C  Dyer et al   pdf   Improved Transition Based Parsing by Modeling Characters instead of Words with LSTMs  2015   M  Ballesteros et al   pdf   Finding function in form  Compositional character models for open vocabulary word representation  2015   W  Ling et al   pdf     2014   Learning a Deep Convolutional Network for Image Super Resolution  2014  C  Dong et al   pdf   Recurrent models of visual attention  2014   V  Mnih et al   pdf   Empirical evaluation of gated recurrent neural networks on sequence modeling  2014   J  Chung et al   pdf   Addressing the rare word problem in neural machine translation  2014   M  Luong et al   pdf   On the properties of neural machine translation  Encoder decoder approaches  2014   K  Cho et  al   Recurrent neural network regularization  2014   W  Zaremba et al   pdf   Intriguing properties of neural networks  2014   C  Szegedy et al   pdf   Towards end to end speech recognition with recurrent neural networks  2014   A  Graves and N  Jaitly   pdf   Scalable object detection using deep neural networks  2014   D  Erhan et al   pdf   On the importance of initialization and momentum in deep learning  2013   I  Sutskever et al   pdf   Regularization of neural networks using dropconnect  2013   L  Wan et al   pdf   Learning Hierarchical Features for Scene Labeling  2013   C  Farabet et al   pdf   Linguistic Regularities in Continuous Space Word Representations  2013   T  Mikolov et al   pdf   Large scale distributed deep networks  2012   J  Dean et al   pdf   A Fast and Accurate Dependency Parser using Neural Networks  Chen and Manning   pdf   Thank you for all your contributions  Please make sure to read the contributing guide before you make a pull request   To the extent possible under law  Terry T  Um has waived all copyright and related or neighboring rights to this work,"[493 892 186 119 818 449 869 1284 230 453 1262]"
516,training-dataset/business/1420.txt,business,How to Improve Your Decision Making by Learning from ComputersHow to Improve Your Decision Making by Learning from Computers  We ve taught computers to do many things  We ve researched how to teach them to identify cats  spot fraudulent charges  even categorize cucumbers  But what can we apply in our daily lives that computers have taught us  That is the premise of the book called Algorithms to Live By  Which of the advances in computer science can be applied to laundry  choosing an executive assistant  picking the best strategic plan and optimizing your schedule   One of the hardest challenges in hiring someone to fill a role is knowing when to stop looking  when to declare that the current best candidate is very likely the most perfect fit  After all  we never have full information  We never know who else might apply for the role  or perfect references   There s a simple rule that a computer scientist proved mathematically  You should choose the next best candidate after completing 37  of the search  It sounds crazy  almost too simplistic  but the math bears that out  assuming certain conditions   There s also a chapter on how to choose the best among a group of options  In the world of computer science  it s called the multi arm bandit problem  A one armed bandit is a nickname for slot machine  and this branch of research has to do with how to pick among a group of slot machines  Do you pick the one that you ve played on for a while  or a new one  and when do you switch   In a management team meeting  the same dynamic is a play when debating continuing with the current strategy or migrating to a new one  When is the right time to switch  When do you have enough information  And how do you pick among a group of options  In short  it s all about figuring out which option has the greatest upside   But this book isn t entirely about algorithms  There is a chapter on when to think less    What would happen if we started from the premise that we can t measure what matters and go from there  Then instead of measurement we have to use something very scary  it s called judgment    Algorithms to Live By has changed the way I think about investing in startups and advising them  The book provides a framework for thinking through difficult decisions  the kinds of questions we ve asked computers to solve for decades  There s a lot we can learn from all of their hard work   Published 2017 02 15 in,"[516 186 453 1284 1369 102 672 706 837 1078 547]"
547,training-dataset/product/819.txt,product,Hack your productivity with the 5 hour rule from  Mr  Robot For starters  let s not call it the 5 hour rule  How about  deliberate learning  instead   Doesn t that already sound better  Maybe it doesn t  but let s just go with it    What is a Mr  Robot  and what can it teach me about deliberate learning   you ask   Mr  Robot is a Golden Globe winning TV show about a young  anti social computer programmer named Elliot Alderson  Before you dismiss this tidbit of information  just know that Elliot is a little bit like Batman   Elliot works as a cybersecurity engineer by day  and after meeting  spoiler alert  or so we re made to believe  a band of tech misfits  he decides to become a vigilante hacker by night   What does all this have to do with deliberate learning when Elliot s an actual hacker   Deliberate learning  or productivity hacking as it s also been dubbed  means dedicating at least 5 hours of your week to retaining new information   Not accessing new information  Retaining new information   This is usually accomplished in 3 ways  reading  reflecting  and experimenting   Here s exactly how Elliot deliberately learns  retains new information  at least once a week although in TV land  a day and a week seem to all mesh into one  so who s to say  You ve  albeit cautiously  trusted me so far   1  Read  Whether it s reading people s faces  emotions  or between the lines  you have a lot to gain by dedicating at least 5 hours of your week to reading   Some people  like Nike founder Phil Knight  follow this rule by investing in libraries that house mountains of books that they find interesting  And once a week  they pay these rooms a visit to deliberately learn something new   Elliot learns new information about people he finds strange or interesting by hacking  but the one redeeming quality about this action is that he doesn t do it with malintent  He does it as a way to relate to people better to understand why people behave the way they do   2  Reflect  In Mr  Robot s second season premiere  Elliot journals every thought he has  as he is having it  This gives the viewer some incredible insight into how his mind works  We see the dichotomy between Elliot s juvenile  almost teenage thoughts about books like the Bible  to Mr  Robot s more sinister ideas and his need for self preservation   In many instances  self reflection gives you essential information about yourself that you simply would not have access to unless you sat down in silence   3  Experiment  Elliot informs us that he has a  perfectly constructed loop  in which he has an almost mind numbing and I guess that s the purpose routine where he does what he perceives  normal  people do daily   With a twist  He s living with his strict mother in an analog world   He believes this loop will help him get his impulses in control  He writes  I am in control  repeatedly in his journal  as if it were some kind of affirmation   He s also made a new friend  A guy called Leon  who Elliot likes because Leon himself has just started watching Seinfeld and can spend hours discussing its inconsistencies   Anyway  the takeaway from this  Disrupt your routine   Get lost  I mean that literally  As in  take a different route to work  or maybe something different for a week  Experiment with new ideas and new concepts  and never stay the same   There s probably a lot more that we can learn about the 5 hour rule from someone like Elliot  but as the series is still unfolding  for now these lessons are enough to get you started   This post was originally published on Medium  To learn more about Cody and his adventures  visit his blog   Related posts,"[547 186 516 837 1284 453 771 404 646 102 449]"
551,training-dataset/engineering/143.txt,engineering,Managing Machines at SpotifyIntroduction  When you log into Spotify  browse through your Discover Weekly playlist  and play a track  you re interacting with some of our fleet of around 12 000 servers   Spotify has historically opted to run our core infrastructure on our own private fleet of physical servers  aka machines  rather than leveraging a public cloud such as Amazon Web Services  AWS   Our fleet consists of a minimal set of hardware configurations and is housed in four datacenters around the world  While we heavily utilise Helios for container based continuous integration and deployment  CI CD  each machine typically has a single role   i e  most machines run a single instance of a microservice   At Spotify we believe in every team having  operational responsibility   The people who build a microservice deploy the microservice  and manage the machines it s deployed to  As you might imagine allowing hundreds of engineers to reliably manage thousands of machines is a complicated proposition   In this post we will recount the history of Spotify s machine management infrastructure  We ll provide some detail on the technical implementations  and how those implementations affected the productivity and happiness of our engineers   2012 and Earlier   Prehistory  When Spotify was a smaller company it was feasible to have a traditional  centralised operations team  The team was constantly busy fighting fires and handling every operations task under the sun  Nonetheless this was a clever operations team   a team who didn t want to manage a rapidly growing fleet of machines by hand   One of the first tools built to manage machines at Spotify was the venerable ServerDb  ServerDb tracks details like a machine s hardware specification  its location  hostname  network interfaces  and a unique hardware name  Each machine also has a  ServerDb state   for example  in use    broken   or  installing   It was originally a simple SQL database and a set of scripts  Machines were installed using the Fully Automated Installer  FAI  and often managed using moob  which provides serial console access and power controls  Our heavily used DNS zone data was hand curated and required a manual push to take effect  All machines were  and still are  managed by Puppet once their base operating system  OS  was installed   Various parts of the stack were replaced over the years  FAI was replaced with Cobbler and debian installer  Later  debian installer was replaced with our own Duck  While many steps of the provisioning process were automated  these steps could be error prone and required human supervision  This could make provisioning 20 new machines an unpredictable and time consuming task  Engineers requested new capacity by creating an issue in a JIRA project  and it could take weeks or even months for requests to be fulfilled   Late 2013   The IO Tribe  Spotify has always liked Agile teams  or squads  in Spotify parlance   In late 2013 the operations team became part of the newly formed Infrastructure and Operations  IO  organisation  New squads were formed around specific problem spaces within operations  Our squad took ownership of provisioning and managing Spotify s machines   From the beginning we envisioned a completely self service machine management service  but the squad was swamped with busy work  We were inundated with requests for new machines  and constantly playing catchup on machine ingestion   the process of recording newly racked machines in ServerDb  We decided to start small and cobble together some minimal viable products  MVPs  to automate the major pain points in order to claim back time to work on bigger things  We began the effort on three fronts   DNS Pushes  DNS pushes were one of our earliest wins  Initially the operations team had to hand edit zone files  commit them to revision control  then run a script on our DNS master to compile and deploy the new zone data  We incrementally automated this process over time  First we built a tool to automatically generate most of our zone data from ServerDb  We then added integration tests and peer review that gave us high confidence in the quality of our changes  Soon thereafter we bit the bullet and automated the push  Cron jobs were created to automatically trigger the above process  Finally we tied up the loose ends  such as automatic creation of zones for new ServerDb subdomains   Machine Ingestion  By the time our squad took ownership ServerDb had become a RESTful web service backed by PostgreSQL  It recorded hundreds of machines by parsing CSV files hand collated by our datacenter team  These files contained data such as rack locations and MAC addresses that were easy for a technician to misread  We also assigned each server a static unique identifier in the form of a woman s name   a shrinking namespace with thousands of servers   Our goal was to completely automate ingestion such that no human intervention was required after a machine was racked  We started by switching our network boot infrastructure from Cobbler to iPXE  which can make boot decisions based on a machine s ServerDb state   In use  machines boot into their production OS  Machines in state  installing  and machines unknown to ServerDb network boot into a tiny Duck generated Linux environment we call the pxeimage  where they run a series of scripts to install or ingest them  respectively   In order to perform ingestions without human interaction we abandoned our woman s name based machine naming scheme in favour of using the machines  unique and programmatically discoverable serial numbers  Unknown machines run a reconnaissance script that determines their serial number  hardware type  network interfaces  etc and automatically registers them with ServerDb   Provisioning Requests  At some point in prehistory a kind soul had written  provgun    the Provisioning Gun  Provgun was a script that read a JIRA  provisioning request  issue and shelled out to the commands necessary to fulfill that request  A provisioning request specifies a location  role  hardware specification  and number of machines  For example  install 10 High IO machines in London with the fancydb role   Provgun would find available machines in ServerDb  allocate them hostnames  and request they be installed via ServerDb   ServerDb used Celery tasks to shell out to ipmitool  instructing target machines to network boot into the pxeimage  After installing and rebooting into the new OS Puppet would apply further configuration based on the machine s role   One of the first questions we asked was  can we put provgun in a cron loop   like our DNS pushes  Unfortunately provgun was very optimistic  and did not know whether the commands it run had actually worked  We feared that naively looping over all open provisioning requests and firing off installs would increase rather than reduce busy work   Our solution was provcannon   the Provisioning Cannon  provcannon was a Python reimplementation of provgun that monitored each installation to ensure success  This monitoring allowed us to retry failed installs with exponential backoff  and to select replacements for consistently uninstallable machines  We configured provcannon to iterate over all outstanding provisioning requests twice daily   Impact  By focusing our efforts on automating DNS changes  machine ingestion  and provisioning requests we reduced the turnaround time for getting capacity to Spotify engineers from weeks to hours  DNS updates went from something that made most of us very nervous to a process we hardly thought about anymore  Equally important was that our squad was freed from boring and error prone work to focus on further improving the turnaround time and experience for our fellow engineers   Mid 2014   Breathing Room  A few months after deploying our initial stopgaps things were calmer in the squad  Longtime Spotify engineers were pleased with the faster turnaround times for new machines  but newcomers used to AWS and similar platforms were less happy waiting a few hours  Parts of our infrastructure were unreliable  and we d only automated installations  Power cycling malfunctioning machines and  recycling  superfluous ones back to the available pool still required us to read a JIRA issue and run a script  We decided it was time to execute on our grand goal   a self service web portal and API for Spotify engineers to manage machines on demand   Neep  Starting at the bottom of the stack we first built a service   Neep   to broker jobs like installing  recycling  and power cycling machines  Neep runs in each datacenter on special machines patched into the out of band management network  Due to operational pains with Celery we built Neep as a light REST API around RQ   a simple Redis based job queue  We chose Pyramid as our blessed web framework for pragmatic reasons  we d inherited ServerDb as a Pyramid service and wanted a minimal set of technologies      status    finished    result   null   params         target    hardwarename C0MPUT3    requester    negz    action    install    ended_at    2015 07 31 17 45 53    created_at    2015 07 31 17 36 31    id    13fd7feb 69d7 4a25 821d 9520518a31d6    user    negz     An example Neep job   Much of provcannon s logic was reusable in Neep jobs that manage installation and recycling of machines  These jobs are effectively the same from Neep s perspective  Neep simply sets the machine s ServerDb state to  installing  or  recycling  to request the pxeimage either install a new OS or sanitize an old one and triggers a network boot  In order to remove the complication of shelling out we replaced calls to ipmitool with OpenStack s pyghmi IPMI library   Sid  Initially we put Neep through its paces by adapting provcannon to trigger Neep jobs  Once we d worked out a few bugs in our new stack it was time to tie it all together  and Sid was born   Sid is the primary interface for engineers to request and manage machines at Spotify today  It s another Pyramid REST service that ties together machine inventory data from ServerDb  role ownership data from Spotify s internal microservice database  and machine management jobs in Neep to allow squads at Spotify to manage their services  capacity  It expands on the parts of provcannon s logic that find and allocate the most appropriate machines to fulfill a provisioning request  Sid s charming Lingon UI was a breeze to build on top of its API  and has supplanted JIRA as the provisioning request interface   Impact  Sid and Neep have not been the only improvements in the modern machine management stack at Spotify  DNS zone data generation has been rebuilt  We apply regularly auto generated OS snapshots at install time rather than performing time consuming Puppet runs  The culmination of these improvements is that requests to provision or recycle capacity are fulfilled in minutes  not hours  Other squads have built tooling around Sid s API to automatically manage their machine fleet  As the face of the provisioning stack at Spotify Sid is one of the most praised services in IO  Sid has fulfilled 3 500 provisioning requests to date  It has issued 28 000 Neep jobs to install  recycle  or power cycle machines with a 94  success rate  Not bad considering the inherent unreliability of individual machines in a datacenter   Mid 2015   The State of the Art  In 2015 Spotify decided to start migrating away from our own physical machines in favour of Google s Cloud Platform  GCP   This paradigm shift challenged us to envision a way for many teams of engineers to manage a lot of cloud compute capacity without stepping on each others  toes   Spotify Pool Manager  In an effort to avoid the  not invented here  trap our squad assessed Google s capacity management offerings  We wanted a tool that could enforce Spotify s opinions and patterns in order to provide engineers with an easy and obvious path to get compute capacity  We felt the Developer Console was powerful  but too flexible  It would be difficult to guide engineers towards our preferred settings  Deployment Manager was also powerful  and could enforce our opinions  but in a trial our engineers found it difficult to use   After providing feedback to Google we began building Spotify Pool Manager  SPM is a relatively light layer that frames Google s powerful Compute Engine APIs in Spotify terms and provides sensible defaults  Engineers simply specify how many instances of what role they want and where  SPM ensures that number of instances will exist  using Google s instance groups behind the scenes  Pools can be grown or shrunk at will   SPM is a stateless Pyramid service  relying primarily on Sid and the Google Cloud Platform to do the heavy lifting  While Sid has a standalone web interface we re tightly integrating SPM with Spotify s internal microservice management dashboard   Physical Pools  Spotify won t be rid of physical machines in the immediate future  so we ve built pool support into Sid s backend  While Sid s provisioning request model has served us well  it can encourage too much attachment to individual machines  Having engineers manage their physical machine capacity similarly to Google s instance groups will introduce them to paradigms like automatic replacement of failed machines and randomised hostnames as we transition to GCP  Sid s pool support allows SPM to manage both Google Compute instances and physical machines  with the same user experience regardless of backend   Summary  In the last few years Spotify s machine management infrastructure has been a great example of the merits of iterative development  As a small squad of Site Reliability and Backend Engineers we ve been able to significantly improve the productivity of our colleagues by building just enough automation to free ourselves to iterate  We ve reduced the turnaround time for new machines from weeks to minutes  and drastically reduced the amount of support interactions required to manage machines at Spotify,"[551 1412 771 1284 837 186 230 44 1040 516 646]"
620,training-dataset/engineering/1263.txt,engineering,Deep Learning in a NutshellThis post is Part 4 of the Deep Learning in a Nutshell series  in which I ll dive into reinforcement learning  a type of machine learning in which agents take actions in an environment aimed at maximizing their cumulative reward   Deep Learning in a Nutshell posts offer a high level overview of essential concepts in deep learning  The posts aim to provide an understanding of each concept rather than its mathematical and theoretical details  While mathematical terminology is sometimes necessary and can further understanding  these posts use analogies and images whenever possible to provide easily digestible bits that make up an intuitive overview of the field of deep learning  Previous posts covered core concepts in deep learning  training of deep learning networks and their history  and sequence learning   Reinforcement Learning  Remember how you learned to ride a bike  More than likely an adult stood or walked behind you and encouraged you to make the first moves on your bike    and helped you get going again when you stumbled or fell  But it is very difficult to explain to a child how to ride a bike  and even a good explanation makes little sense to someone who has never ridden before  you have to get the feel for it  So how did you learn to ride a bike if it could not be clearly explained  Well  you just tried to ride the bike  and more than likely you fell  or at least stopped abruptly and had to catch yourself  You fell or stumbled repeatedly until you got some little spark of success after riding for a few meters before you fell again   During this learning process  the feedback signals that tell us how well we do are either pain   Ouch  I fell  That hurts  I will avoid doing what led to this next time    or reward   Wow  I am riding the bike  This feels great  I just need to keep doing what I am doing right now    In a reinforcement learning problem  we take the view of an agent that tries to maximize the reward that it is receiving from making decisions  Thus an agent that receives the maximum possible reward can be viewed as performing the best action for a given state  An agent here refers to an abstract entity which can be any kind of object or subject that performs actions  autonomous cars  robots  humans  customer support chat bots  go players  The state of an agent refers to the agent s position and state of being in it s abstract environment  for example  a certain position in a virtual reality world  a building  a chess board  or the position and speed on a racetrack   To simplify the problem and the solution of a reinforcement problem  the environment is often simplified so that the agent only knows about the details which are important for decision making while the rest is discarded  Just like in the bike riding example  the reinforcement algorithm has only two sources of feedback to learn from  penalty  pain of falling  and reward  thrill of riding for a few meters   If we view penalty as negative reward  then the whole learning problem concerns exploring an environment and trying to maximize the reward that our agent receives for passing from state to state until a goal state is reached  driving autonomously from A to B  winning a chess game  solving a customer problem via chat   this is reinforcement learning in a nutshell   The Value Function  Reinforcement learning is about positive and negative rewards  punishment or pain  and learning to choose the actions which yield the best cumulative reward  To find these actions  it s useful to first think about the most valuable states in our current environment  For example  on a racetrack the finish line is the most valuable  that is the state which is most rewarding  and the states which are on the racetrack are more valuable than states that are off track   Once we have determined which states are valuable we can assign  rewards  to various states  For example  negative rewards for all states where the car s position is off track  a positive reward for completing a lap  a positive reward when the car beats its current best lap time  and so on  We can imagine the states and rewards as a discrete function  For example a racetrack could be represented by a grid of 1 by 1 meter tiles  The rewards on a tile are represented by numbers  For example the goals might have reward 10  and the tiles which are off track have a reward of  2 while all other tiles have a reward of 0  If we picture this function in 3 dimensions  then we we could imagine that the best action is to stay as high as possible  maximize reward    For each of these states there are intermediate states which are not necessarily rewarding  reward 0  but are necessary milestones on the path towards a reward  For example  you need to pass the first turn on a racetrack in order to finish a lap  and you need to finish a lap before you can improve your lap time   The trained or fitted value function assigns partial rewards to the intermediate states so that  for example  completing the second turn is more valuable than the first because the second turn represents a state which is closer to the next goal  The value function helps the agent make good decisions by providing intermediate rewards that make it easier for the agent to choose which state to go to next  given the current state   This framework means that each given state s value is influenced by all the values of the states which are easily accessible from that state  Figure 1 shows an example world where each square is a state and S and G are the start and goal states  respectively  T squares are traps  and black squares are states which cannot be entered  big boulders blocking the way   The goal is to move from state S to state G  If we imagine this to be our racetrack  then over time we create a  ramp  from the start state to the goal state  so that we just have to go along the direction which is steepest in order to find the goal  This is the main goal of the value function  to provide an estimate of the value of each state  so that we can make step by step decisions which lead to a reward   The value function can also capture the subtleties of a problem  For example  the reward for maintaining a large distance from the sides of a racetrack is less negative because this state is farther from the states which are designated off track  but at the same time it is not efficient to drive in the middle of the track to achieve a good lap time  the car cannot take turns at high velocity   So safety and speed compete against each other and the value function finds the most suitable fast yet safe path on the track  The final solution can be tuned by altering the reward values  For example if a fast lap time gives a high reward  the value function will assign larger values to states which are more risky but allow faster lap times   Discounting Factor  Even though the value function handles competing rewards  it also depends on a parameter called the discounting factor that defines its exact behavior by limiting how much an agent is affected by rewards which lie in the far future  This is done by multiplying the partial reward by the discounting factor whenever a state change is made  So with a discounting factor of 0 5  the original reward will be one eighth of its value after only 3 state changes  causing the agent to pursue rewards in close proximity rather than rewards which are further away  Thus the discounting factor is a weight that controls whether the value function favors prudent or greedy actions   For example  a race car might get high reward by going fast  so a very greedy decision with discount factor close to zero might always go as fast as possible  But a more prudent agent knows that going as fast as possible is a bad idea when there is a sharp turn ahead  while slowing down at the right moment may yield a better lap time later  and thus a higher reward  To have this foresight  the agent needs to receive the reward for going faster later  so a discounting factor closer to 1 may be better   The value function s discounting factor ensures that rewards that are far away diminish proportionally to the distance or steps it takes to reach them  This factor is usually set to a value which values long term rewards highly  but not too high  for example the reward decays by 5  per state transition    The algorithm learns the specific partial reward value of each state of the value function by first initializing all states to zero or their specific reward value  and then searching all states and all their possible next states and evaluating the reward it would get in the next states  If it gets a  partial  reward in the next state  it increases the reward of the current state  This process repeats until the partial reward within each state doesn t change any more  which means that it has taken into account all possible directions to find all possible rewards from all possible states  You can see the process of this value iteration algorithm in Figure 1   This may seem inefficient at first  but a trick associated with dynamic programming makes it much more efficient  Dynamic programming solves higher order problems in terms of already solved sub problems  the computed reward for going from state B to C can be reused in computing the rewards for state chains A  B  C and D  B  C   In summary  the value function and value iteration provide a map of partial rewards which guides the agent toward rewarding states   The Policy Function  The policy function represents a strategy that  given the value function  selects the action believed to yield the highest  long term  reward  Often there is no clear winner among the possible next actions  For example  the agent might have the choice to enter one of four possible next states A B C  and D with reward values A 10  B 10  C 5 and D 5  So both A and B would be good immediate choices  but a long way down the road action A might actually have been better than B  or action C might even have been the best choice  It is worth exploring these options during training  but at the same time the immediate reward values should guide the choice  So how can we find a balance between exploiting high reward values and exploring less rewarding paths which might return high rewards in the long term   A clever way to proceed is to take choices with probability proportional to their reward  In this example the probability of taking A would be 33   10  10 10 5 5    and the probability of taking B  C  and D would be 33   16   and 16   respectively  This random choice nature of the policy function is important to learning a good policy  There might exist effective or even essential strategies that are counter intuitive but necessary for success   For example  if you train a race car to go fast on a racetrack it will try to cut the corners with the highest speed possible  However  this strategy will not be optimal if you add competitors into the mix  The agent then needs to take into account these competitors and might want to take turns slower to avoid the possibility of a competitor overtaking or worse  a collision  Another scenario might be that cornering at very high speed will wear the tires much quicker resulting in the need for a pit stop  costing valuable time   Note that the policy and value functions depend on each other  Given a certain value function  different policies may result in different choices  and given a certain policy  the agent may value actions differently  Given the policy  just win the game  for a chess game  the value function will assign high value to the moves in the game which have high probability of winning  sacrificing chess pieces in order to win safely would be valued highly   But given the policy  win with a large lead or not at all   then the policy function will just learn to select the moves which maximize the score in the particular game  never sacrifice chess pieces    These are just two examples of many  If we want to have a specific outcome we can use both the policy and value functions to guide the agent to learn good strategies to achieve that outcome  This makes reinforcement learning versatile and powerful   We train the policy function by  1  initializing it randomly for example  let each state be chosen with probability proportional to its reward and initialize the value function with the rewards  that is  set the reward of all states to zero where no direct reward is defined  for example the racetrack goal has a reward of 10  off track states have a penalty of  2  and all states on the racetrack itself have zero reward   Then  2  train the value function until convergence  see Figure 1   and  3  increase the probability of the action  moving from A to B  for a given state  state A  which most increases the reward  going from A to C might have a low or even negative reward value  like sacrificing a chess piece  but it might be in line with the policy of winning the game   Finally   4  repeat from step  1  until the policy no longer changes   The Q function  We ve seen that the policy and value functions are highly interdependent  our policy is determined mostly by what we value  and what we value determines our actions  So maybe we could combine the value and policy functions  We can  and the combination is called the Q function   The Q function takes both the current state  like the value function  and the next action  like the policy function  and returns the partial reward for the state action pair  For more complex use cases the Q function may also take more states to predict the next state  For example if the direction of movement is important one needs at least 2 states to predict the next state  since it is often not possible to infer precise direction from a single state  e g  a still image   We can also just pass input states to the Q function to get a partial reward value for each possible action  From this we can  for example  randomly choose our next action with probability proportional to the partial reward  exploration   or just take the highest valued action  exploitation    However  the main point of the Q function is actually different  Consider an autonomous car  there are so many  states  that it is impossible to create a value function for them all  it would take too long to compute all partial rewards for every possible speed and position on all the roads that exist in on Earth  Instead  the Q function  1  looks at all possible next states that lie one step ahead and  2  looks at the best possible action from the current state to that next state  So for each next state the Q function has a look ahead of one step  not all possible steps until termination  like the value function   These look aheads are represented as state action pairs  For example  state A might have 4 possible actions  so that we have the action pairs A  A  A  B  A  C  A  D  With four actions for every state and a 10 10 grid of states we can represent the entire Q function as four 10 10 matrices  or one 10x10x4 tensor  See Figure 3 for a Q function which represents the solution to a grid world problem  a 2D world where you can move to neighboring states  where the goal is located in the bottom right corner   In some cases we need to model sheer infinite states  For example  in an autonomous car the  state  is often represented as a continuous function such as a neural network  which just takes all the state variables  such as speed and position of the car  and outputs Q values for each action   Why does it help to have information about only some states  Many states are very well correlated  so the same action taken at two different  but similar  states may lead to success in both cases  For example  every turn on a racetrack will be different  but the things that you learn from each left turn when to start a turn  how speed should be adjusted  etc  are valuable for the next left turn  So over time  a car driving agent can learn to make better and better left turns  even on tracks it has never seen   Q Learning  To train the Q function we initialize all Q values of all state action pairs to zero and initialize the states with their given rewards  Because the agent does not know how to get to the rewards  an agent can only see the Q value of the next states  which will all be zero  the agent may go through many states until it finds a reward  Thus we often terminate training the Q function after a certain length  for example  100 actions  or after a certain state has been reached  one lap on a racetrack   This ensures we don t get stuck in the process of learning good actions for one state since it s possible to be stuck in a hopeless state where you can make as many iterations as you like and never receive any noticeable rewards   Learning the Q function proceeds from end  the reward  to start  the first state   Figure 4 depicts the grid world example from Figure 1 in terms of Q learning  Assume the goal is to reach the goal state G in the smallest number of steps  Initially the agent makes random moves until it  accidentally  reaches either a trap or the goal state  Because the traps are closer to the start state the agent is most likely to hit a trap first  but this pattern is broken once the agent stumbles upon the goal state  From that iteration on the states right before the goal state  one step look ahead  have a partial reward and since the reward is now closer to the start the agent is more likely to hit such a rewarding state  In this way a chain of partial rewards builds up more quickly the more often the agent reaches a partial reward state so that the chain of partial rewards from goal to start state is quickly completed  see Figure 4    Deep Q learning  An autonomous vehicle may need to consider many states  every different combination of speed and position is a different state  But most states are similar  Is it possible to combine similar states so that they have similar Q values  Here is where deep learning comes into play   We can input the current point of view of the driver an image into a convolutional neural network  CNN  trained to predict the rewards for the next possible actions  Because images for similar states are similar  many left turns look similar  they will also have similar actions  For example  a neural network will be able to generalize for many left turns on a racetrack and even make appropriate actions for left turns that were not encountered before  Just like a CNN trained on many different objects in images can learn to correctly distinguish these objects  a network trained on many similar variations of left turns will be able to make fine adjustments of speed and position for each different left turn it is presented   To use deep Q learning successfully  however  we cannot simply apply the rule to train the Q function described previously  If we apply the Q learning rule blindly  then the network will learn to do good left turns while doing left turns  but will start forgetting how to do good right turns at the same time  This is so because all actions of the neural network use the same weights  tuning the weights for left turns makes them worse for other situations  The solution is to store all the input images and the output actions as  experiences   that is  store the state  action and reward together   After running the training algorithm for a while  we choose a random selection from all the experiences gathered so far and create an average update for neural network weights which maximizes Q values  rewards  for all actions taken during those experiences  This way we can teach our neural network to do left and right turns at the same time  Since very early experiences of driving on a racetrack are not important because they come from a time where our agent was less experienced  or even a beginner we only keep track of a fixed number of past experiences and forget the rest  This process is termed experience replay   The following video shows the Deep Q learning algorithm learning to play Breakout at an expert level   Thanks to K roly Zsolnai Feh r s for permission for embedding this video  You may be interested in his 2 minute paper youtube channel  where he explains important scientific work in plain language in just two minutes    Experience replay is a method inspired by biology  The hippocampus in the human brain is a reinforcement learning center in each hemisphere of the brain  The hippocampus stores all the experiences that we make during the day but it has a limited memory capacity for experiences and once this capacity is reached learning becomes much more difficult  cramming too much before an exam   During the night this memory buffer in the hippocampus is emptied into the cortex by neural activity that spreads across the cortex  The cortex is the  hard drive  of the brain where almost all memories are stored  Memories for hand movements are stored in the  hand area   memories for hearing are stored in the  hearing area   and so on  This characteristic neural activity that spreads outward from the hippocampus is termed sleep spindle  While there is currently no strong evidence to support it  many sleep researchers believe that we dream to help the hippocampus to integrate the experiences gathered over the day with our memories in the cortex to form coherent pictures   So you see that storing memories and writing them back in a somewhat coordinated fashion is an important process not only for deep reinforcement learning but also for human learning  This biological similarity gives us a little confidence boost that our theories of the brain might be correct and also that the algorithms we design are on the right path towards intelligence   AlphaGo  AlphaGo  developed by Google DeepMind  made big news in 2016 by becoming the first computer program to beat a human professional player at the game of Go  It then went on to beat Lee Sedol  one of the top Go players in the world  four games to one  AlphaGo combines many of the elements mentioned previously in this post  namely  1  value and  2  policy neural networks which represent  1  the value function over the current configuration in a go game  and therefore predicts the relative value of one move over another  and  2  the policy function which suggests what moves should be picked in order to win the game  These networks are convolutional networks  which treat the Go board as a 19 19 input  image   one pixel for each position    Because we already have many recorded Go games  it is useful to train the policy network by using existing Go game data played by professionals  The policy network is trained on this data to predict the next move that Go champions played in their games given a certain game configuration   Once this supervised training phase is finished  reinforcement learning comes into play  Here AlphaGo plays against itself and tries to refine its strategy to pick moves  policy network  and its strategy to evaluate who is winning  value network   Even just training the policy network is a significantly better approach than the previous best known Go algorithm  called Pachi  which makes use of tree search algorithms and heuristics  However  one can still improve the the performance of the deep learning approach significantly with the help of the value network   The value network tends to generalize poorly when it is trained on whole games  because configurations are highly correlated so that the network learns to identify a game  this is game A vs  B from Beijing  1978  so I know A wins  rather than identifying good moves  To circumvent this problem  DeepMind generated a lot of data by letting AlphaGo play against itself and then taking a few positions of each game to train the value network  This is similar to experience replay  where we never look at lengthy action sequences in isolation  but rather at combinations of very different states and actions   The value network showed similar performance to Monte Carlo search trees with rollout policy  but AlphaGo used Monte Carlo search trees on top of the deep learning approach for even better performance  What are Monte Carlo search trees with rollout policy  Imagine a tree of game configurations where a move is an edge and nodes are different game configurations  For example  you are in a certain game configuration and you have 200 possible moves that is  there are 200 nodes connected to your current node and then you choose a certain move  an edge leading to a node  resulting in a new tree with 199 different nodes which can be reached after the current move  This tree  however  is never fully expanded since it grows exponentially and would take far too long to evaluate the tree completely  so in a Monte Carlo tree search we only take a route along the tree to a certain depth to make evaluation more efficient   In the rollout policy  we look at the current state  apply the policy network to select a move to the next node in the tree and repeat for all following moves for both players until the game ends and a winner is found  This provides another quick and greedy way to estimate the value of a move  This insight can then also be used to improve value network accuracy  With a more accurate value function we are also able to improve our policy function further  knowing with greater accuracy whether a move is good or bad lets us develop strategies based on good moves which were previously thought to be bad  Or in other words  AlphaGo is able to consider strategies which are unthinkable for humans to work  but they actually win games   AlphaGo also uses Monte Carlo search trees for training  The trees contain edges of Q values  visit counts  N   and a prior probability  P  which are updated at each iteration  Initially  the Q value  visit count and prior probability are zero  During an iteration  an action is selected according to the three parameters  Q N P   For example  to decide if it is a good move to play a stone on position E5 the new state of the board for this move is evaluated by a combination of  1  the policy network  which sets the initial prior probability for that move   2a  the value network which assigns a value to the move  and  2b  Monte Carlo rollout which assigns another value to the move  Steps  2a  and  2b  are weighted by a parameter and the Q values and visit counts  N  are updated with the mean evaluations on that path  increase the Q value if the move was relatively good on average  decrease otherwise   With every iteration the value for the same move is reduced slightly by the visit count  so that new moves will be explored with higher probability  This ensures a balance of exploration and exploitation  However  we also get more and more accurate estimates for moves that we already played  The tree assigns higher Q values to very strong moves over time  Through this training procedure AlphaGo learns to play better moves with every training iteration and thus learns which moves will win the game  From here the rest is just computing power and time until we create a Go bot which is better than human professionals   Conclusion  In this post we ve seen that reinforcement learning is a general framework for training agents to exhibit very complex behavior  We looked at the constituents of reinforcement learning including the value and policy functions and built on them to reach deep reinforcement learning  Deep reinforcement learning holds the promise of a very generalized learning procedure which can learn useful behavior with very little feedback  It is an exciting but also challenging area which will certainly be an important part of the artificial intelligence landscape of tomorrow  You may also be interested in the Train Your Reinforcement Learning Agents at the OpenAI Gym   If you would like to read the rest of the series  check out part 1  core concepts in deep learning  part 2  training of deep learning networks and their history  and part 3  sequence learning  I hope you enjoy this series   If you have questions  please ask in the comments below and I will answer   Acknowledgments  I want to thank Mark Harris for helping me to polish this blog post series with his thoughtful editing and helpful suggestions  I also want to thank Alison Lowndes and Stephen Jones for making this series possible    parallel,"[620 995 186 1284 1412 1262 449 102 869 1369 1035]"
646,training-dataset/product/227.txt,product,How Machine Learning Can Benefit Your SaaS StartupHow Machine Learning Can Benefit Your SaaS Startup  From the millions of Amazon Alexas to the self driving car  new products are coming to market infused with machine learning  The innovation offered by machine learning techniques are real  and they will changed the SaaS world  But how  How can startups use machine learning to their advantage   There are four broad applications of machine learning   Optimize   this morning  fastest way to travel from Sand Hill Road to South Park in San Francisco is highway 101  The job requisition for an account executive on our website uses too many clich s  To close more business  speak slower  talk about pricing later in the call  and use this case study  Identify objects   the photograph you just took with your smartphone contains a cat  Find all red plaid woolen shorts in an ecommerce store  The CT scan shows high likelihood of Parkinson s Disease  Detect anomalies   your credit card shows a  10 000 charge for a piano from a store in Nairobi  Your server cluster is operating at historically high CPU usage  Customers are responding to this morning s lead generation email at 25  greater rates than last week s campaign  Segment data   customers who come to our product through the mobile app store show 15  higher engagement   These applications alone make for tremendous advances  But  combinations of these applications lead to incredible things  Object identification   anomaly detection   robotics   self driving cars  Or brick laying robots that erect walls three times faster than humans   I ve written before about the monstrous acceleration in machine learning innovation  The monocloud vendors  Amazon  Google and Microsoft  are rapidly innovating  publishing breakthrough results and offering APIs that leverage this new research for pennies and nickels  Consequently  every startup can use these technologies for just the cost of a few ramen boxes   But just plugging those APIs  buying the  ai domain name and inserting the words machine learning artificial intelligence to your sales pitch isn t sufficient to succeed  Rather than trump machine learning  make the technology disappear into the product  Create that magical moment with the user   The best sales and fundraising pitches describe a startup s value proposition without mentioning machine learning  Instead  they focus on how the product increases revenue  decreases cost  or wins the buyer a promotion   We ve invested in more than 20 companies leveraging machine learning to create enduring and category defining companies  from Stripe to RelateIQ  Chorus to Caspida  As we look to invest in others  we ve identified five attributes of companies to invest in   Proprietary access to data   the algorithms are off the shelf and available to everyone  Creating proprietary data through product usage  perhaps as an event driven SaaS product  or through key partnerships is essential to creating sustainable competitive advantage  End to end applications  not platforms   the monoclouds are likely to win the ML as a Service business  They have more researchers  lower costs of infrastructure  and far more marketing dollars than any startup  End to end applications offering revenue increases and or cost reduction are much easier and a better path for a startup to go to market  Strong GTM enabled by ML   ML has the potential to be a technology innovation leading to a go to market advantage  By changing the way a buyer evaluates software and potentially reducing the cost of customer acquisition  ML based products can disrupt  But a technology innovation alone is not enough  Experts in the field   Yes  you can use the monocloud APIs out of the box  But those systems are tuned to be as broadly applicable as possible and generate pretty good results  To deliver an exceptional experience  a startup will need experts in speech recognition  natural language processing  or whatever their core discipline might be  Algorithmic advances   every once in a while  we may invest in a company with a fundamental algorithmic advance unlikely to be replicated elsewhere   Like the database and the graphical user interface before it  machine learning is a new enabling technology that will change the way we build and sell software broadly  And though the terms might be the zeitgeist of the day  rapidly becoming corporate clich   the impacts of the technology are just starting to be understood and leveraged   Adapted from my presentation at Saastr   Published 2017 02 13 in Trends,"[646 1284 453 44 1040 837 706 186 1078 771 869]"
672,training-dataset/engineering/710.txt,engineering,A Primer   Andreessen Horowitzwatch time  28 minutes  One of the key insights that legendary physicist and Nobel Prize laureate Richard Feynman had was that quantum mechanics  the branch of physics that deals with subatomic particles  uncertainty principle  and many other concepts beyond classic physics  is just way too complicated to simulate using traditional computers   Nature  of course  can handle these complex calculations   computers however can t do those same calculations  or would take a prohibitively long time and amount of resources to do so   But this isn t just about being able to do more with computers in a faster  or smaller  way  It s about solving problems that we couldn t solve with traditional computers  it s about a difference of kind not just degree   So what is a quantum computer and  qubits    especially as compared to a traditional computer and bits  What is Grover s Algorithm  And besides speed of processing  what are some of the new applications that wouldn t have been possible before  From how traditional computers work and quantum computers will work to why this all matters  a16z Deal and Research team head Frank Chen walks us through the basics of quantum computing in this slide presentation  And even though may feel like you finally understand after watching this  just remember what Feynman once said   If you think you understand quantum mechanics then you don t understand quantum mechanics    some sources and recommended further reading   Quantum Computing 101   University of Waterloo Institute for Quantum Computing  Quantum Computing Since Democritus   Scott Aronson  Quantum category   Scott Aronson s blog  Quantum Computing entry   Wikipedia  Grover s Quantum Search Algorithm   Craig Gidney  Twisted Oak Studios,"[672 994 453 516 335 186 1369 837 1284 102 1040]"
706,training-dataset/business/1005.txt,business,10 Data Acquisition Strategies for Startups   Moritz Mueller Freitag   Medium10 Data Acquisition Strategies for Startups  The  unreasonable effectiveness  of data for machine learning applications has been widely debated over the years  see here  here and here   It has also been suggested that many major breakthroughs in the field of Artificial Intelligence have not been constrained by algorithmic advances but by the availability of high quality datasets  see here   The common thread running through these discussions is that data is a vital component in doing state of the art machine learning   Access to high quality training data is critical for startups that use machine learning as the core technology of their business  While many algorithms and software tools are open sourced and shared across the research community  good datasets are usually proprietary and hard to build  Owning a large  domain specific dataset can therefore become a significant source of competitive advantage  especially if startups can jumpstart data network effects  a situation where more users   more data   smarter algorithms   better product   more users    Consequently  one of the key strategic decisions that machine learning startups have to make is how to build high quality datasets to train their learning algorithms  Unfortunately  startups often have limited or no labeled data in the beginning  a situation that precludes founders from making significant progress on building a data driven product  It is therefore worth exploring data acquisition strategies from the outset  before hiring the data science team or building up a costly core infrastructure   Startups can overcome the cold start problem of data acquisition in numerous ways  The choice of data strategy source usually goes hand in hand with the choice of business model  a startup s focus  consumer or enterprise  horizontal or vertical  etc   and the funding situation  The following list of strategies  while neither exhaustive nor mutually exclusive  gives a sense for the broad range of approaches available,"[706 1284 44 449 1078 646 186 837 453 1040 869]"
771,training-dataset/product/902.txt,product,The Future of Talking to TechnologyChristoph Auer Welsbach  Partner Innovation Leader at IBM Watson  introduces us to Watson   Talking with Technology  is an engaging and thought provoking talk about how Artificial Intelligence is changing the interface between humans and machines  The era of human machine relationships is transforming customer interactions and provides new opportunities to scale and enhance human expertise  As product managers  we continually work at the point where customers meet applications  so nearly everything we do is about how users engage with machines  How can we build with Artificial Intelligence in mind  at the same time as maintaining close customer relationships and protecting our brands  Christoph provides insight into where conversational applications might be used and how  as product managers  we can make them successful    Hello  How Can I Help    Agent type interfaces such as virtual assistants or chatbots can drive demand and performance and enable new workflows  The Intelligent Assistance space is already busy with over 200 conversational applications in use  These highly accurate human like interacting machines are leading us into a new age where  85  of customer interactions will be managed without a human by 2020   Conversational AI applications like these will become the human face of technology in the not too far future  meaning less and less involvement between you and the people using your products   Borrowing from Watson  Watson is a cognitive system designed to simulate human understanding  reasoning and learning  It makes use of machine learning and natural language processing to enable humans to interact more naturally with machines  Watson can simulate foundational cognitive skills  knowledge organization skills and higher reasoning skills  With Watson technology we can build interfaces which create value for users by responding more creatively to them  and building relationships with them   Christoph takes us on a quick tour of some areas where better interfaces between humans and machines will enhance services and experiences   In healthcare  doctors have access to vast quantities of information in the form of medical records  academic papers  scans  and images  which machines can easily scan and store  But without an intelligent  adaptive and responsive interface through which to access this information  they are limited by their own ability to search for and process this pile of unstructured data   Examples from the car  hospitality and retail ecommerce industries show how conversational applications are used not just to deliver customer value  such as providing navigation instructions or tourist information   but also to transfer brand values and company culture by becoming the human face and voice of the brand   Build Trust and Make Sense  Christoph s key take aways for product managers using AI are Trust and Interface  Build trust with your customers by inviting initial AI engagement in low risk scenarios  Build the best quality interface you can for your customers  so that when your robot talks  it makes sense,"[771 837 646 1040 44 1284 453 102 186 551 449]"
818,training-dataset/engineering/370.txt,engineering,Neural Networks Demystified,"[818 186 493 892 869 230 449 1284 995 620 1262]"
837,training-dataset/product/1375.txt,product,How to Build Great Products in the AI World56 Flares 56 Flares    What does technology do to society   When technology and human ingenuity gets together  everybody in society profits  If you look at graphs of GDP   capita over long time periods  850 years    the trend is always upwards  The only things that drag this progress down are severe periods of sickness  like the black death  and widespread war  such as WW2   Overall though  technology makes everyone healthier and richer   At this point in our society s history   technology has never been more important to the global economy  5 out of the top 6 companies in the world are technology companies   Apple  Alphabet  Microsoft  Amazon and Facebook   the non tech company being ExxonMobil  Only 10 years ago though  Microsoft was the only tech company in the top 6   clearly technology is only growing in importance   We have moved on from the age of agriculture  steam  oil or finance to the age of technology  as Azeem Azhar showed us at ProductTank London   The most important thing in technology is AI  There are three types of AI   ANI   Narrow AI   A system that doesn t have to be specifically programmed for all of its outcomes and can learn from its experience  It can still only solve a defined set of problems and cases though     A system that doesn t have to be specifically programmed for all of its outcomes and can learn from its experience  It can still only solve a defined set of problems and cases though  AGI   Human ish AI   What most people mean when they think about the Turing test  These types of systems can solve lots of different problems without being programmed specifically for them     What most people mean when they think about the Turing test  These types of systems can solve lots of different problems without being programmed specifically for them  ASI   Super AI   Intelligence that can think like us and solve general problems while defining its own goals  The key characteristic of this AI is that it will work out how to make itself smarter and will do so exponentially   In reality  ASI is a very long way off and nobody really knows what it will look like when it gets here  The most important research work is being done to build general problem solving systems that are in the AGI bracket   and even this is largely at the mathematical modelling stage   ANI systems are in production and use right now   from working out what objects are in a picture to predicting sports scores   these are the systems that most digital professionals will end up employing very soon if not already   AI is a blanket term  Traditional AI is made up of things like decision trees  natural language processing and predictive analytics  Machine learning starts to take these inputs and make more out of them than we had anticipated at the start of the system s programming  In order to describe the situation we re in  it s best to use AI as an umbrella term for all these activities   AI is already here  The most well known examples of recent AI success   We beat GO   an incredibly complex game with 10 170 move combinations which as recently as 1999  computer scientists thought we were 100 years away from beating using AI  Last year the world champion human was beaten 4 1 with the machine even creating a new type of move by going against conventional wisdom   Siri  Cortana and Google are all on the market providing voice recognition software to perform simple tasks for us that didn t exist 12 15 years ago   Why now   There are three main reasons for the explosion in the AI industry right now   Moore s Law is continuing to drive processing power to build these machines on demand and shows no sign of slowing down  We are able to provide these systems with enough Data and Information to make decisions  Businesses have been digitised and interfaces have been created to access their services using technology   2012 was the first time that GPUs were being combined with deep neural networks  a 40 year old technology  to allow us to do very interesting stuff and solve previously unimagined problems   Progress has come rapidly from that point with http   image net org  seeing computers overtake humans in image recognition 2014  Speak recognition saw the same milestone in the last 18 months   Pretty much anything that a normal person can do in less than a second  we can now automate with AI  Where Will we be in 5   7 Years   Conversational interfaces will run certain types of processes with people  Safe autonomous driving systems will become more commonplace  Power optimisation in data centres  and problems like it  will be optimised by machines where humans can t keep up  The Product Improvement Loop  Once you put AI into a product you should see an ongoing improvement of that product through the learning that the system can now generate  This leads to more people using the product which means the data that it has at its disposal to optimise itself increases  and you are hopefully set up with a self fulfilling cycle of improvement that leads to exponential success   Product Manager Considerations  Everyone needs to be considering how AI fits within their product or marketing space  The challenge for many people is that these technologies work best at a large scale with big data sets and are still very expensive to run at production levels  There are options available though   Buy off the shelf capabilities such as IBM Watson  Google Compute or AWS Experiment with creating scale even as a startup Find a niche to explore and exploit  and use that to build your product based on data sets that aren t broadly available  Finally  don t forget that you are now building products that fundamentally control how people experience and interact with the world  More likely than not you will end up making decisions that affect how happy or successful people are or could be   these need to be considered and taken seriously   One example of this is the way that Facebook looks to mediate its content for suitability based entirely on AI   leading to challenging situations which we haven t worked out a way to solve at scale yet  These weren t foreseen when their product was being designed  but need to be considered so they do not just focus on one person s opinion or political views,"[837 44 186 771 1284 646 102 1369 1078 1040 453]"
869,training-dataset/engineering/470.txt,engineering,June Update   The Mission   MediumUp to Speed on Deep Learning  June Update  By Isaac Madan  At the end of April  we published an article on getting up to speed on deep learning  which included 20  resources to catch up on rapid advancements in the field  Much has happened since then  so we thought we d pull together a few of the excellent resources that have emerged this month in June  As always  this list is not comprehensive  so let us know if there s something we should add  or if you re interested in discussing this area further   Announcements  Generative models by OpenAI  The team at OpenAI share five new projects that enhance or use generative models  a branch of unsupervised machine learning techniques   Improved techniques for training generative adversarial networks  paper    Improved techniques for training variational auto encoders  paper    Improved techniques for interpretable representation learning  paper    Curiosity driven exploration in deep reinforcement learning  paper    New approach for imitation learning  paper    Facebook introduces DeepText  its deep learning engine that understands textual content on Facebook with near human accuracy and at a speed of several thousand posts per second  in more than 20 languages   Google DeepMind learns to play Montezuma s Revenge via intrinsic motivation techniques  video   The game requires forward planning  Read the paper  Unifying Count Based Exploration and Intrinsic Motivation  here   NVIDIA announces its GPU Ventures Program in which it  provides marketing  operational  financial and other support to young ambitious companies founding their businesses around NVIDIA technologies   They plan to make  500K to  5M investments in these startups  Consider applying if you re working on a deep learning startup   DARPA announces its Data Driven Discovery of Models program  which is intended to help non experts build their own models using automated tools that facilitate data science  In effect  leveraging machine learning for machine learning   Explanation   Review  Neural Network Architectures by Eugenio Culurciello  The history of neural network design over the past few years to help us better craft neural network architectures in the future,"[869 1284 186 449 230 1040 404 1262 646 44 706]"
892,training-dataset/engineering/1345.txt,engineering,awesome deep learning papers README md at master   terryum awesome deep learning papers   GitHubAwesome   Most Cited Deep Learning Papers  A curated list of the most cited deep learning papers  since 2012   We believe that there exist classic deep learning papers which are worth reading regardless of their application domain  Rather than providing overwhelming amount of papers  We would like to provide a curated list of the awesome deep learning papers which are considered as must reads in certain research domains   Before this list  there exist other awesome deep learning lists  for example  Deep Vision and Awesome Recurrent Neural Networks  Also  after this list comes out  another awesome list for deep learning beginners  called Deep Learning Papers Reading Roadmap  has been created and loved by many deep learning researchers   Although the Roadmap List includes lots of important deep learning papers  it feels overwhelming for me to read them all  As I mentioned in the introduction  I believe that seminal works can give us lessons regardless of their application domain  Thus  I would like to introduce top 100 deep learning papers here as a good starting point of overviewing deep learning researches   To get the news for newly released papers everyday  follow my twitter or facebook page   Awesome list criteria  A list of top 100 deep learning papers published from 2012 to 2016 is suggested  If a paper is added to the list  another paper  usually from  More Papers from 2016  section  should be removed to keep top 100 papers   Thus  removing papers is also important contributions as well as adding papers  Papers that are important  but failed to be included in the list  will be listed in More than Top 100 section  Please refer to New Papers and Old Papers sections for the papers published in recent 6 months or before 2012    Citation criteria     6 months   New Papers  by discussion     New Papers  by discussion  2016    60 citations or  More Papers from 2016      60 citations or  More Papers from 2016  2015    200 citations     200 citations 2014    400 citations     400 citations 2013    600 citations     600 citations 2012    800 citations     800 citations  2012   Old Papers  by discussion   Please note that we prefer seminal deep learning papers that can be applied to various researches rather than application papers  For that reason  some papers that meet the criteria may not be accepted while others can be  It depends on the impact of the paper  applicability to other researches scarcity of the research domain  and so on   We need your contributions   If you have any suggestions  missing papers  new papers  key researchers or typos   please feel free to edit and pull a request   Please read the contributing guide for further instructions  though just letting me know the title of papers can also be a big contribution to us     Update  You can download all top 100 papers with this and collect all authors  names with this  Also  bib file for all top 100 papers are available  Thanks  doodhwala  Sven and grepinsight   Can anyone contribute the code for obtaining the statistics of the authors of Top 100 papers    More than Top 100   Understanding   Generalization   Transfer  Distilling the knowledge in a neural network  2015   G  Hinton et al   pdf    2015   G  Hinton et al   pdf  Deep neural networks are easily fooled  High confidence predictions for unrecognizable images  2015   A  Nguyen et al   pdf    2015   A  Nguyen et al   pdf  How transferable are features in deep neural networks   2014   J  Yosinski et al   pdf    2014   J  Yosinski et al   pdf  CNN features off the Shelf  An astounding baseline for recognition  2014   A  Razavian et al   pdf    2014   A  Razavian et al   pdf  Learning and transferring mid Level image representations using convolutional neural networks  2014   M  Oquab et al   pdf    2014   M  Oquab et al   pdf  Visualizing and understanding convolutional networks  2014   M  Zeiler and R  Fergus  pdf    2014   M  Zeiler and R  Fergus  pdf  Decaf  A deep convolutional activation feature for generic visual recognition  2014   J  Donahue et al   pdf   Optimization   Training Techniques  Batch normalization  Accelerating deep network training by reducing internal covariate shift  2015   S  Loffe and C  Szegedy  pdf    2015   S  Loffe and C  Szegedy  pdf  Delving deep into rectifiers  Surpassing human level performance on imagenet classification  2015   K  He et al   pdf    2015   K  He et al   pdf  Dropout  A simple way to prevent neural networks from overfitting  2014   N  Srivastava et al   pdf    2014   N  Srivastava et al   pdf  Adam  A method for stochastic optimization  2014   D  Kingma and J  Ba  pdf    2014   D  Kingma and J  Ba  pdf  Improving neural networks by preventing co adaptation of feature detectors  2012   G  Hinton et al   pdf    2012   G  Hinton et al   pdf  Random search for hyper parameter optimization  2012  J  Bergstra and Y  Bengio  pdf   Unsupervised   Generative Models  Pixel recurrent neural networks  2016   A  Oord et al   pdf    2016   A  Oord et al   pdf  Improved techniques for training GANs  2016   T  Salimans et al   pdf    2016   T  Salimans et al   pdf  Unsupervised representation learning with deep convolutional generative adversarial networks  2015   A  Radford et al   pdf    2015   A  Radford et al   pdf  DRAW  A recurrent neural network for image generation  2015   K  Gregor et al   pdf    2015   K  Gregor et al   pdf  Generative adversarial nets  2014   I  Goodfellow et al   pdf    2014   I  Goodfellow et al   pdf  Auto encoding variational Bayes  2013   D  Kingma and M  Welling  pdf    2013   D  Kingma and M  Welling  pdf  Building high level features using large scale unsupervised learning  2013   Q  Le et al   pdf   Convolutional Neural Network Models  Rethinking the inception architecture for computer vision  2016   C  Szegedy et al   pdf    2016   C  Szegedy et al   pdf  Inception v4  inception resnet and the impact of residual connections on learning  2016   C  Szegedy et al   pdf    2016   C  Szegedy et al   pdf  Identity Mappings in Deep Residual Networks  2016   K  He et al   pdf    2016   K  He et al   pdf  Deep residual learning for image recognition  2016   K  He et al   pdf    2016   K  He et al   pdf  Going deeper with convolutions  2015   C  Szegedy et al   pdf    2015   C  Szegedy et al   pdf  Very deep convolutional networks for large scale image recognition  2014   K  Simonyan and A  Zisserman  pdf    2014   K  Simonyan and A  Zisserman  pdf  Spatial pyramid pooling in deep convolutional networks for visual recognition  2014   K  He et al   pdf    2014   K  He et al   pdf  Return of the devil in the details  delving deep into convolutional nets  2014   K  Chatfield et al   pdf    2014   K  Chatfield et al   pdf  OverFeat  Integrated recognition  localization and detection using convolutional networks  2013   P  Sermanet et al   pdf    2013   P  Sermanet et al   pdf  Maxout networks  2013   I  Goodfellow et al   pdf    2013   I  Goodfellow et al   pdf  Network in network  2013   M  Lin et al   pdf    2013   M  Lin et al   pdf  ImageNet classification with deep convolutional neural networks  2012   A  Krizhevsky et al   pdf   Image  Segmentation   Object Detection  You only look once  Unified  real time object detection  2016   J  Redmon et al   pdf    2016   J  Redmon et al   pdf  Region based convolutional networks for accurate object detection and segmentation  2016   R  Girshick et al   pdf    2016   R  Girshick et al   pdf  Fully convolutional networks for semantic segmentation  2015   J  Long et al   pdf    2015   J  Long et al   pdf  Faster R CNN  Towards Real Time Object Detection with Region Proposal Networks  2015   S  Ren et al   pdf    2015   S  Ren et al   pdf  Fast R CNN  2015   R  Girshick  pdf    2015   R  Girshick  pdf  Rich feature hierarchies for accurate object detection and semantic segmentation  2014   R  Girshick et al   pdf    2014   R  Girshick et al   pdf  Semantic image segmentation with deep convolutional nets and fully connected CRFs   L  Chen et al   pdf     L  Chen et al   pdf  Learning hierarchical features for scene labeling  2013   C  Farabet et al   pdf   Image   Video   Etc  Image Super Resolution Using Deep Convolutional Networks  2016   C  Dong et al   pdf    2016   C  Dong et al   pdf  A neural algorithm of artistic style  2015   L  Gatys et al   pdf    2015   L  Gatys et al   pdf  Deep visual semantic alignments for generating image descriptions  2015   A  Karpathy and L  Fei Fei  pdf    2015   A  Karpathy and L  Fei Fei  pdf  Show  attend and tell  Neural image caption generation with visual attention  2015   K  Xu et al   pdf    2015   K  Xu et al   pdf  Show and tell  A neural image caption generator  2015    O  Vinyals et al   pdf     O  Vinyals et al   pdf  Long term recurrent convolutional networks for visual recognition and description  2015   J  Donahue et al   pdf    2015   J  Donahue et al   pdf  VQA  Visual question answering  2015   S  Antol et al   pdf    2015   S  Antol et al   pdf  DeepFace  Closing the gap to human level performance in face verification  2014   Y  Taigman et al   pdf     2014   Y  Taigman et al   pdf   Large scale video classification with convolutional neural networks  2014   A  Karpathy et al   pdf    2014   A  Karpathy et al   pdf  DeepPose  Human pose estimation via deep neural networks  2014   A  Toshev and C  Szegedy  pdf    2014   A  Toshev and C  Szegedy  pdf  Two stream convolutional networks for action recognition in videos  2014   K  Simonyan et al   pdf    2014   K  Simonyan et al   pdf  3D convolutional neural networks for human action recognition  2013   S  Ji et al   pdf   Recurrent Neural Network Models  Conditional random fields as recurrent neural networks  2015   S  Zheng and S  Jayasumana   pdf    2015   S  Zheng and S  Jayasumana   pdf  Memory networks  2014   J  Weston et al   pdf    2014   J  Weston et al   pdf  Neural turing machines  2014   A  Graves et al   pdf    2014   A  Graves et al   pdf  Generating sequences with recurrent neural networks  2013   A  Graves   pdf   Natural Language Processing  Neural Architectures for Named Entity Recognition  2016   G  Lample et al   pdf    2016   G  Lample et al   pdf  Exploring the limits of language modeling  2016   R  Jozefowicz et al   pdf    2016   R  Jozefowicz et al   pdf  Teaching machines to read and comprehend  2015   K  Hermann et al   pdf    2015   K  Hermann et al   pdf  Effective approaches to attention based neural machine translation  2015   M  Luong et al   pdf    2015   M  Luong et al   pdf  Neural machine translation by jointly learning to align and translate  2014   D  Bahdanau et al   pdf    2014   D  Bahdanau et al   pdf  Sequence to sequence learning with neural networks  2014   I  Sutskever et al   pdf    2014   I  Sutskever et al   pdf  Learning phrase representations using RNN encoder decoder for statistical machine translation  2014   K  Cho et al   pdf    2014   K  Cho et al   pdf  A convolutional neural network for modeling sentences  2014   N  Kalchbrenner et al   pdf    2014   N  Kalchbrenner et al   pdf  Convolutional neural networks for sentence classification  2014   Y  Kim  pdf    2014   Y  Kim  pdf  Glove  Global vectors for word representation  2014   J  Pennington et al   pdf    2014   J  Pennington et al   pdf  Distributed representations of sentences and documents  2014   Q  Le and T  Mikolov  pdf    2014   Q  Le and T  Mikolov  pdf  Distributed representations of words and phrases and their compositionality  2013   T  Mikolov et al   pdf    2013   T  Mikolov et al   pdf  Efficient estimation of word representations in vector space  2013   T  Mikolov et al   pdf    2013   T  Mikolov et al   pdf  Recursive deep models for semantic compositionality over a sentiment treebank  2013   R  Socher et al   pdf   Speech   Other Domain  End to end attention based large vocabulary speech recognition  2016   D  Bahdanau et al   pdf    2016   D  Bahdanau et al   pdf  Deep speech 2  End to end speech recognition in English and Mandarin  2015   D  Amodei et al   pdf    2015   D  Amodei et al   pdf  Speech recognition with deep recurrent neural networks  2013   A  Graves  pdf    2013   A  Graves  pdf  Deep neural networks for acoustic modeling in speech recognition  The shared views of four research groups  2012   G  Hinton et al   pdf    2012   G  Hinton et al   pdf  Context dependent pre trained deep neural networks for large vocabulary speech recognition  2012  G  Dahl et al   pdf    2012  G  Dahl et al   pdf  Acoustic modeling using deep belief networks  2012   A  Mohamed et al   pdf   Reinforcement Learning   Robotics  End to end training of deep visuomotor policies  2016   S  Levine et al   pdf    2016   S  Levine et al   pdf  Learning Hand Eye Coordination for Robotic Grasping with Deep Learning and Large Scale Data Collection  2016   S  Levine et al   pdf    2016   S  Levine et al   pdf  Asynchronous methods for deep reinforcement learning  2016   V  Mnih et al   pdf    2016   V  Mnih et al   pdf  Deep Reinforcement Learning with Double Q Learning  2016   H  Hasselt et al   pdf    2016   H  Hasselt et al   pdf  Mastering the game of Go with deep neural networks and tree search  2016   D  Silver et al   pdf    2016   D  Silver et al   pdf  Continuous control with deep reinforcement learning  2015   T  Lillicrap et al   pdf    2015   T  Lillicrap et al   pdf  Human level control through deep reinforcement learning  2015   V  Mnih et al   pdf    2015   V  Mnih et al   pdf  Deep learning for detecting robotic grasps  2015   I  Lenz et al   pdf    2015   I  Lenz et al   pdf  Playing atari with deep reinforcement learning  2013   V  Mnih et al   pdf    More Papers from 2016  Layer Normalization  2016   J  Ba et al   pdf    2016   J  Ba et al   pdf  Learning to learn by gradient descent by gradient descent  2016   M  Andrychowicz et al   pdf    2016   M  Andrychowicz et al   pdf  Domain adversarial training of neural networks  2016   Y  Ganin et al   pdf    2016   Y  Ganin et al   pdf  WaveNet  A Generative Model for Raw Audio  2016   A  Oord et al   pdf   web    2016   A  Oord et al   pdf   web  Colorful image colorization  2016   R  Zhang et al   pdf    2016   R  Zhang et al   pdf  Generative visual manipulation on the natural image manifold  2016   J  Zhu et al   pdf    2016   J  Zhu et al   pdf  Texture networks  Feed forward synthesis of textures and stylized images  2016   D Ulyanov et al   pdf    2016   D Ulyanov et al   pdf  SSD  Single shot multibox detector  2016   W  Liu et al   pdf    2016   W  Liu et al   pdf  SqueezeNet  AlexNet level accuracy with 50x fewer parameters and  1MB model size  2016   F  Iandola et al   pdf    2016   F  Iandola et al   pdf  Eie  Efficient inference engine on compressed deep neural network  2016   S  Han et al   pdf    2016   S  Han et al   pdf  Binarized neural networks  Training deep neural networks with weights and activations constrained to  1 or 1  2016   M  Courbariaux et al   pdf    2016   M  Courbariaux et al   pdf  Dynamic memory networks for visual and textual question answering  2016   C  Xiong et al   pdf    2016   C  Xiong et al   pdf  Stacked attention networks for image question answering  2016   Z  Yang et al   pdf    2016   Z  Yang et al   pdf  Hybrid computing using a neural network with dynamic external memory  2016   A  Graves et al   pdf    2016   A  Graves et al   pdf  Google s neural machine translation system  Bridging the gap between human and machine translation  2016   Y  Wu et al   pdf   New papers  Newly published papers    6 months  which are worth reading  Mask R CNN  2017   K  He   pdf   Learning to discover cross domain relations with generative adversarial networks  2017   T  Kim et al   pdf   Deep voice  Real time neural text to speech  2017   S  Arik et al    pdf   PixelNet  Representation of the pixels  by the pixels  and for the pixels  2017   A  Bansal et al   pdf   Batch renormalization  Towards reducing minibatch dependence in batch normalized models  2017   S  Ioffe   pdf   Wasserstein GAN  2017   M  Arjovsky et al   pdf   Understanding deep learning requires rethinking generalization  2017   C  Zhang et al   pdf   Least squares generative adversarial networks  2016   X  Mao et al   pdf   Old Papers  Classic papers published before 2012  An analysis of single layer networks in unsupervised feature learning  2011   A  Coates et al   pdf   Deep sparse rectifier neural networks  2011   X  Glorot et al   pdf   Natural language processing  almost  from scratch  2011   R  Collobert et al   pdf   Recurrent neural network based language model  2010   T  Mikolov et al   pdf   Stacked denoising autoencoders  Learning useful representations in a deep network with a local denoising criterion  2010   P  Vincent et al   pdf   Learning mid level features for recognition  2010   Y  Boureau  pdf   A practical guide to training restricted boltzmann machines  2010   G  Hinton  pdf   Understanding the difficulty of training deep feedforward neural networks  2010   X  Glorot and Y  Bengio  pdf   Why does unsupervised pre training help deep learning  2010   D  Erhan et al   pdf   Learning deep architectures for AI  2009   Y  Bengio   pdf   Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations  2009   H  Lee et al   pdf   Greedy layer wise training of deep networks  2007   Y  Bengio et al   pdf   Reducing the dimensionality of data with neural networks  G  Hinton and R  Salakhutdinov   pdf   A fast learning algorithm for deep belief nets  2006   G  Hinton et al   pdf   Gradient based learning applied to document recognition  1998   Y  LeCun et al   pdf   Long short term memory  1997   S  Hochreiter and J  Schmidhuber   pdf   HW   SW   Dataset  OpenAI gym  2016   G  Brockman et al   pdf   TensorFlow  Large scale machine learning on heterogeneous distributed systems  2016   M  Abadi et al   pdf   Theano  A Python framework for fast computation of mathematical expressions  R  Al Rfou et al   Torch7  A matlab like environment for machine learning  R  Collobert et al   pdf   MatConvNet  Convolutional neural networks for matlab  2015   A  Vedaldi and K  Lenc  pdf   Imagenet large scale visual recognition challenge  2015   O  Russakovsky et al   pdf   Caffe  Convolutional architecture for fast feature embedding  2014   Y  Jia et al   pdf   Book   Survey   Review  On the Origin of Deep Learning  2017   H  Wang and Bhiksha Raj   pdf   Deep Reinforcement Learning  An Overview  2017   Y  Li   pdf   Neural Machine Translation and Sequence to sequence Models 2017   A Tutorial  G  Neubig   pdf   Neural Network and Deep Learning  Book  Jan 2017   Michael Nielsen   html   Deep learning  Book  2016   Goodfellow et al   html   LSTM  A search space odyssey  2016   K  Greff et al   pdf   Tutorial on Variational Autoencoders  2016   C  Doersch   pdf   Deep learning  2015   Y  LeCun  Y  Bengio and G  Hinton  pdf   Deep learning in neural networks  An overview  2015   J  Schmidhuber  pdf   Representation learning  A review and new perspectives  2013   Y  Bengio et al   pdf   Video Lectures   Tutorials   Blogs   Lectures   CS231n  Convolutional Neural Networks for Visual Recognition  Stanford University  web   CS224d  Deep Learning for Natural Language Processing  Stanford University  web   Oxford Deep NLP 2017  Deep Learning for Natural Language Processing  University of Oxford  web    Tutorials   NIPS 2016 Tutorials  Long Beach  web   ICML 2016 Tutorials  New York City  web   ICLR 2016 Videos  San Juan  web   Deep Learning Summer School 2016  Montreal  web   Bay Area Deep Learning School 2016  Stanford  web    Blogs   OpenAI  web   Distill  web   Andrej Karpathy Blog  web   Colah s Blog  Web   WildML  Web   FastML  web   TheMorningPaper  web   Appendix  More than Top 100   2016   A character level decoder without explicit segmentation for neural machine translation  2016   J  Chung et al   pdf   Dermatologist level classification of skin cancer with deep neural networks  2017   A  Esteva et al   html   Weakly supervised object localization with multi fold multiple instance learning  2017   R  Gokberk et al   pdf   Brain tumor segmentation with deep neural networks  2017   M  Havaei et al   pdf   Professor Forcing  A New Algorithm for Training Recurrent Networks  2016   A  Lamb et al   pdf   Adversarially learned inference  2016   V  Dumoulin et al   web  pdf   Understanding convolutional neural networks  2016   J  Koushik  pdf   Taking the human out of the loop  A review of bayesian optimization  2016   B  Shahriari et al   pdf   Adaptive computation time for recurrent neural networks  2016   A  Graves  pdf   Densely connected convolutional networks  2016   G  Huang et al   pdf   Continuous deep q learning with model based acceleration  2016   S  Gu et al   pdf   A thorough examination of the cnn daily mail reading comprehension task  2016   D  Chen et al   pdf   Achieving open vocabulary neural machine translation with hybrid word character models  M  Luong and C  Manning   pdf   Very Deep Convolutional Networks for Natural Language Processing  2016   A  Conneau et al   pdf   Bag of tricks for efficient text classification  2016   A  Joulin et al   pdf   Efficient piecewise training of deep structured models for semantic segmentation  2016   G  Lin et al   pdf   Learning to compose neural networks for question answering  2016   J  Andreas et al   pdf   Perceptual losses for real time style transfer and super resolution  2016   J  Johnson et al   pdf   Reading text in the wild with convolutional neural networks  2016   M  Jaderberg et al   pdf   What makes for effective detection proposals   2016   J  Hosang et al   pdf   Inside outside net  Detecting objects in context with skip pooling and recurrent neural networks  2016   S  Bell et al   pdf    Instance aware semantic segmentation via multi task network cascades  2016   J  Dai et al   pdf   Conditional image generation with pixelcnn decoders  2016   A  van den Oord et al   pdf   Deep networks with stochastic depth  2016   G  Huang et al    pdf   Generative Short Term Stochastic Gibbs Networks 2016   I  Lenz et al   pdf    2015   Spatial transformer network  2015   M  Jaderberg et al    pdf   Ask your neurons  A neural based approach to answering questions about images  2015   M  Malinowski et al   pdf   Exploring models and data for image question answering  2015   M  Ren et al   pdf   Are you talking to a machine  dataset and methods for multilingual image question  2015   H  Gao et al   pdf   Mind s eye  A recurrent visual representation for image caption generation  2015   X  Chen and C  Zitnick   pdf   From captions to visual concepts and back  2015   H  Fang et al   pdf    Towards AI complete question answering  A set of prerequisite toy tasks  2015   J  Weston et al   pdf   Ask me anything  Dynamic memory networks for natural language processing  2015   A  Kumar et al   pdf   Unsupervised learning of video representations using LSTMs  2015   N  Srivastava et al   pdf   Deep compression  Compressing deep neural networks with pruning  trained quantization and huffman coding  2015   S  Han et al   pdf   Improved semantic representations from tree structured long short term memory networks  2015   K  Tai et al   pdf   Character aware neural language models  2015   Y  Kim et al   pdf   Grammar as a foreign language  2015   O  Vinyals et al   pdf   Trust Region Policy Optimization  2015   J  Schulman et al   pdf   Beyond short snippents  Deep networks for video classification  2015   pdf   Learning Deconvolution Network for Semantic Segmentation  2015   H  Noh et al   pdf   Learning spatiotemporal features with 3d convolutional networks  2015   D  Tran et al   pdf   Understanding neural networks through deep visualization  2015   J  Yosinski et al   pdf   An Empirical Exploration of Recurrent Network Architectures  2015   R  Jozefowicz et al   pdf   Training very deep networks  2015   R  Srivastava et al   pdf   Deep generative image models using a  laplacian pyramid of adversarial networks  2015   E Denton et al   pdf   Gated Feedback Recurrent Neural Networks  2015   J  Chung et al   pdf   Fast and accurate deep network learning by exponential linear units  ELUS   2015   D  Clevert et al   pdf   Pointer networks  2015   O  Vinyals et al   pdf   Visualizing and Understanding Recurrent Networks  2015   A  Karpathy et al   pdf   Attention based models for speech recognition  2015   J  Chorowski et al   pdf   End to end memory networks  2015   S  Sukbaatar et al   pdf   Describing videos by exploiting temporal structure  2015   L  Yao et al   pdf   A neural conversational model  2015   O  Vinyals and Q  Le   pdf   Improving distributional similarity with lessons learned from word embeddings  O  Levy et al    pdf    https   www transacl org ojs index php tacl article download 570 124   Transition Based Dependency Parsing with Stack Long Short Term Memory  2015   C  Dyer et al   pdf   Improved Transition Based Parsing by Modeling Characters instead of Words with LSTMs  2015   M  Ballesteros et al   pdf   Finding function in form  Compositional character models for open vocabulary word representation  2015   W  Ling et al   pdf     2014   Learning a Deep Convolutional Network for Image Super Resolution  2014  C  Dong et al   pdf   Recurrent models of visual attention  2014   V  Mnih et al   pdf   Empirical evaluation of gated recurrent neural networks on sequence modeling  2014   J  Chung et al   pdf   Addressing the rare word problem in neural machine translation  2014   M  Luong et al   pdf   On the properties of neural machine translation  Encoder decoder approaches  2014   K  Cho et  al   Recurrent neural network regularization  2014   W  Zaremba et al   pdf   Intriguing properties of neural networks  2014   C  Szegedy et al   pdf   Towards end to end speech recognition with recurrent neural networks  2014   A  Graves and N  Jaitly   pdf   Scalable object detection using deep neural networks  2014   D  Erhan et al   pdf   On the importance of initialization and momentum in deep learning  2013   I  Sutskever et al   pdf   Regularization of neural networks using dropconnect  2013   L  Wan et al   pdf   Learning Hierarchical Features for Scene Labeling  2013   C  Farabet et al   pdf   Linguistic Regularities in Continuous Space Word Representations  2013   T  Mikolov et al   pdf   Large scale distributed deep networks  2012   J  Dean et al   pdf   A Fast and Accurate Dependency Parser using Neural Networks  Chen and Manning   pdf   Thank you for all your contributions  Please make sure to read the contributing guide before you make a pull request   To the extent possible under law  Terry T  Um has waived all copyright and related or neighboring rights to this work,"[892 493 186 119 818 449 869 1284 230 453 1262]"
903,training-dataset/engineering/226.txt,engineering,Personalized Recommendations in LinkedIn LearningThe overall system architecture of the course recommendation engine is shown above  There are two sources of data that we use  the course database  which contains information related to courses like description  author  category  video transcription  and other metadata   and member information  both explicit profile information and behavior on LinkedIn   The resulting course recommendations for a member are stored in offline and online stores and are served through our online service   Mapping skills to courses and members  The central theme of our approach is to use skills as features to represent both members and courses  Courses can be mapped to a set of skill vectors that would be acquired by taking that course  Members also can be represented in this skill space by using various sources of information  While the explicit skills available on the member s profile is one such source of information  there are others that can be used to infer implicit skills  for instance using the member s activity on LinkedIn  or the connections the member has through our rich professional network  Once the courses and the members have been mapped to this skill space  we compute the affinity scores between members and course pairs   Some skills are more popular  and thus  less unique  than others  To ensure that skill based course recommendations for a member also take into account the uniqueness of their skills  we compute an  inverse document frequency   IDF  score for each skill  For computing a skill s IDF score  member profiles are treated as  documents  and the skills are treated as  words  appearing in those  documents   A smoothed IDF score for skills is then computed as IDF s    log 1   M N s    where M is the total number of members  and N s  is the number of members having skill  s  on their profile  Skills that are extremely rare  N s    100  are not considered  The skill vector representing a given member s mapping is weighted using IDF scores before being used in the dot product  providing a weighted similarity computation mechanism  In that way courses are recommended to users based on unique skills   There is also a post processing step we use to diversify our recommended courses in order to avoid having several very similar courses dominate the top of the list  Let s take an example to illustrate the point  Let s say there are three courses that have the same set of skills tagged  these could be newer versions of an older course  or the same course at a different difficulty level   In either case  the affinity score of all three courses will be exactly the same for a member who has a subset or all of those skills listed in his or her profile  If these three courses happen to have the highest affinity score  then the top three recommendations for the member will be those three courses  This is the classic problem of incorporating diversity in recommendations  To ensure our recommendations are diverse  we use a round robin algorithm  Let s say three courses have the exact same skill overlap with a member s skills  in that case  we put these courses in the same bucket and order the bucket by a secondary feature say course creation date  So given a set of skills for which a course recommendation needs to be made  the most recent course is picked from the bucket  After one recommendation has been drawn from that bucket  we draw the next course recommendation from the next bucket which has a different skill set  After picking a course recommendation from the last bucket  we return to the first bucket  This approach ensures diversity of skills in course recommendations   Along with relevant course recommendations  it is also important to provide explanations for why the recommendation is meaningful to the member  These explanations appear as  context annotations  for the course recommendations  Some examples of contexts for course recommendations are   Trending in your title    Because you have skill  Java     Stay sharp on  Object oriented programming     Because you watched Course  Java Essential Training     Based on your title  Software Engineering    etc  Each context annotation translates to a carousel in the grid of courses that are recommended on the LinkedIn Learning desktop interface,"[903 445 771 1284 1262 186 230 1412 404 449 837]"
994,training-dataset/business/1364.txt,business,Chad Rigetti on Building Quantum Computers   The MacroChad Rigetti  founder of Rigetti Quantum Computing  S14   at Startup School 2016   When Chad participated in YC he d yet to build a qubit   the fundamental building block of a quantum computer   and didn t know how he was going to do it  Two years later he and a team of 35 people have built a five qubit quantum computer in Berkeley   In his talk he outlines the current state of supercomputing and the possible uses for quantum computers in chemistry and machine learning  He differentiates hard tech companies from tech companies  hard tech companies have to deal with fundamental questions of possibility  And highlights how tech companies move into hard tech to create leverage and defensibility   Chad closed with an example of a small group of determined people creating tremendous leverage through hard tech   Subscribe to The Macro newsletter to receive Startup School talks as we release them,"[994 672 646 837 44 1078 186 1369 516 102 453]"
995,training-dataset/engineering/1492.txt,engineering,Evolution Strategies as a Scalable Alternative to Reinforcement LearningEvolution Strategies as a Scalable Alternative to Reinforcement Learning We ve discovered that evolution strategies  ES   an optimization technique that s been known for decades  rivals the performance of standard reinforcement learning  RL  techniques on modern RL benchmarks  e g  Atari MuJoCo   while overcoming many of RL s inconveniences  In particular  ES is simpler to implement  there is no need for backpropagation   it is easier to scale in a distributed setting  it does not suffer in settings with sparse rewards  and has fewer hyperparameters  This outcome is surprising because ES resembles simple hill climbing in a high dimensional space based only on finite differences along a few random directions at each step   Our finding continues the modern trend of achieving strong results with decades old ideas  For example  in 2012  the  AlexNet  paper showed how to design  scale and train convolutional neural networks  CNNs  to achieve extremely strong results on image recognition tasks  at a time when most researchers thought that CNNs were not a promising approach to computer vision  Similarly  in 2013  the Deep Q Learning paper showed how to combine Q Learning with CNNs to successfully solve Atari games  reinvigorating RL as a research field with exciting experimental  rather than theoretical  results  Likewise  our work demonstrates that ES achieves strong performance on RL benchmarks  dispelling the common belief that ES methods are impossible to apply to high dimensional problems   ES is easy to implement and scale  Running on a computing cluster of 80 machines and 1 440 CPU cores  our implementation is able to train a 3D MuJoCo humanoid walker in only 10 minutes  A3C on 32 cores takes about 10 hours   Using 720 cores we can also obtain comparable performance to A3C on Atari while cutting down the training time from 1 day to 1 hour   In what follows  we ll first briefly describe the conventional RL approach  contrast that with our ES approach  discuss the tradeoffs between ES and RL  and finally highlight some of our experiments   Reinforcement Learning  Let s briefly look at how RL works  Suppose we are given some environment  e g  a game  that we d like to train an agent on  To describe the behavior of the agent  we define a policy function  the brain of the agent   which computes how the agent should act in any given situation  In practice  the policy is usually a neural network that takes the current state of the game as an input and calculates the probability of taking any of the allowed actions  A typical policy function might have about 1 000 000 parameters  so our task comes down to finding the precise setting of these parameters such that the policy plays well  i e  wins a lot of games    Above  In the game of Pong  the policy could take the pixels of the screen and compute the probability of moving the player s paddle  in green  on right  Up  Down  or neither   The training process for the policy works as follows  Starting from a random initialization  we let the agent interact with the environment for a while and collect episodes of interaction  e g  each episode is one game of Pong   We thus obtain a complete recording of what happened  what sequence of states we encountered  what actions we took in each state  and what the reward was at each step  As an example  below is a diagram of three episodes that each took 10 time steps in a hypothetical environment  Each rectangle is a state  and rectangles are colored green if the reward was positive  e g  we just got the ball past our opponent  and red if the reward was negative  e g  we missed the ball    This diagram suggests a recipe for how we can improve the policy  whatever we happened to do leading up to the green states was good  and whatever we happened to do in the states leading up to the red areas was bad  We can then use backpropagation to compute a small update on the network s parameters that would make the green actions more likely in those states in the future  and the red actions less likely in those states in the future  We expect that the updated policy works a bit better as a result  We then iterate the process  collect another batch of episodes  do another update  etc   Exploration by injecting noise in the actions  The policies we usually use in RL are stochastic  in that they only compute probabilities of taking any action  This way  during the course of training  the agent may find itself in a particular state many times  and at different times it will take different actions due to the sampling  This provides the signal needed for learning  some of those actions will lead to good outcomes  and get encouraged  and some of them will not work out  and get discouraged  We therefore say that we introduce exploration into the learning process by injecting noise into the agent s actions  which we do by sampling from the action distribution at each time step  This will be in contrast to ES  which we describe next   Evolution Strategies  On  Evolution   Before we dive into the ES approach  it is important to note that despite the word  evolution   ES has very little to do with biological evolution  Early versions of these techniques may have been inspired by biological evolution and the approach can  on an abstract level  be seen as sampling a population of individuals and allowing the successful individuals to dictate the distribution of future generations  However  the mathematical details are so heavily abstracted away from biological evolution that it is best to think of ES as simply a class of black box stochastic optimization techniques   Black box optimization  In ES  we forget entirely that there is an agent  an environment  that there are neural networks involved  or that interactions take place over time  etc  The whole setup is that 1 000 000 numbers  which happen to describe the parameters of the policy network  go in  1 number comes out  the total reward   and we want to find the best setting of the 1 000 000 numbers  Mathematically  we would say that we are optimizing a function f w  with respect to the input vector w  the parameters   weights of the network   but we make no assumptions about the structure of f   except that we can evaluate it  hence  black box     The ES algorithm  Intuitively  the optimization is a  guess and check  process  where we start with some random parameters and then repeatedly 1  tweak the guess a bit randomly  and 2  move our guess slightly towards whatever tweaks worked better  Concretely  at each step we take a parameter vector w and generate a population of  say  100 slightly different parameter vectors w1     w100 by jittering w with gaussian noise  We then evaluate each one of the 100 candidates independently by running the corresponding policy network in the environment for a while  and add up all the rewards in each case  The updated parameter vector then becomes the weighted sum of the 100 vectors  where each weight is proportional to the total reward  i e  we want the more successful candidates to have a higher weight   Mathematically  you ll notice that this is also equivalent to estimating the gradient of the expected reward in the parameter space using finite differences  except we only do it along 100 random directions  Yet another way to see it is that we re still doing RL  Policy Gradients  or REINFORCE specifically   where the agent s actions are to emit entire parameter vectors using a gaussian policy   Above  ES optimization process  in a setting with only two parameters and a reward function  red   high  blue   low   At each iteration we show the current parameter value  in white   a population of jittered samples  in black   and the estimated gradient  white arrow   We keep moving the parameters to the top of the arrow until we converge to a local optimum  You can reproduce this figure with this notebook   Code sample  To make the core algorithm concrete and to highlight its simplicity  here is a short example of optimizing a quadratic function using ES  or see this longer version with more comments      simple example  minimize a quadratic around some solution point import numpy as np solution   np array  0 5  0 1   0 3   def f w   return  np sum  w   solution   2  npop   50   population size sigma   0 1   noise standard deviation alpha   0 001   learning rate w   np random randn 3    initial guess for i in range 300   N   np random randn npop  3  R   np zeros npop  for j in range npop   w_try   w   sigma N j  R j    f w_try  A    R   np mean R     np std R  w   w   alpha  npop sigma    np dot N T  A   Injecting noise in the parameters  Notice that the objective is identical to the one that RL optimizes  the expected reward  However  RL injects noise in the action space and uses backpropagation to compute the parameter updates  while ES injects noise directly in the parameter space  Another way to describe this is that RL is a  guess and check  on actions  while ES is a  guess and check  on parameters  Since we re injecting noise in the parameters  it is possible to use deterministic policies  and we do  in our experiments   It is also possible to add noise in both actions and parameters to potentially combine the two approaches   Tradeoffs between ES and RL  ES enjoys multiple advantages over RL algorithms  some of them are a little technical    No need for backpropagation   ES only requires the forward pass of the policy and does not require backpropagation  or value function estimation   which makes the code shorter and between 2 3 times faster in practice  On memory constrained systems  it is also not necessary to keep a record of the episodes for a later update  There is also no need to worry about exploding gradients in RNNs  Lastly  we can explore a much larger function class of policies  including networks that are not differentiable  such as in binary networks   or ones that include complex modules  e g  pathfinding  or various optimization layers      ES only requires the forward pass of the policy and does not require backpropagation  or value function estimation   which makes the code shorter and between 2 3 times faster in practice  On memory constrained systems  it is also not necessary to keep a record of the episodes for a later update  There is also no need to worry about exploding gradients in RNNs  Lastly  we can explore a much larger function class of policies  including networks that are not differentiable  such as in binary networks   or ones that include complex modules  e g  pathfinding  or various optimization layers   Highly parallelizable  ES only requires workers to communicate a few scalars between each other  while in RL it is necessary to synchronize entire parameter vectors  which can be millions of numbers   Intuitively  this is because we control the random seeds on each worker  so each worker can locally reconstruct the perturbations of the other workers  Thus  all that we need to communicate between workers is the reward of each perturbation  As a result  we observed linear speedups in our experiments as we added on the order of thousands of CPU cores to the optimization   ES only requires workers to communicate a few scalars between each other  while in RL it is necessary to synchronize entire parameter vectors  which can be millions of numbers   Intuitively  this is because we control the random seeds on each worker  so each worker can locally reconstruct the perturbations of the other workers  Thus  all that we need to communicate between workers is the reward of each perturbation  As a result  we observed linear speedups in our experiments as we added on the order of thousands of CPU cores to the optimization  Higher robustness  Several hyperparameters that are difficult to set in RL implementations are side stepped in ES  For example  RL is not  scale free   so one can achieve very different learning outcomes  including a complete failure  with different settings of the frame skip hyperparameter in Atari  As we show in our work  ES works about equally well with any frame skip   Several hyperparameters that are difficult to set in RL implementations are side stepped in ES  For example  RL is not  scale free   so one can achieve very different learning outcomes  including a complete failure  with different settings of the frame skip hyperparameter in Atari  As we show in our work  ES works about equally well with any frame skip  Structured exploration  Some RL algorithms  especially policy gradients  initialize with random policies  which often manifests as random jitter on spot for a long time  This effect is mitigated in Q Learning due to epsilon greedy policies  where the max operation can cause the agents to perform some consistent action for a while  e g  holding down a left arrow   This is more likely to do something in a game than if the agent jitters on spot  as is the case with policy gradients  Similar to Q learning  ES does not suffer from these problems because we can use deterministic policies and achieve consistent exploration   Some RL algorithms  especially policy gradients  initialize with random policies  which often manifests as random jitter on spot for a long time  This effect is mitigated in Q Learning due to epsilon greedy policies  where the max operation can cause the agents to perform some consistent action for a while  e g  holding down a left arrow   This is more likely to do something in a game than if the agent jitters on spot  as is the case with policy gradients  Similar to Q learning  ES does not suffer from these problems because we can use deterministic policies and achieve consistent exploration  Credit assignment over long time scales  By studying both ES and RL gradient estimators mathematically we can see that ES is an attractive choice especially when the number of time steps in an episode is long  where actions have longlasting effects  or if no good value function estimates are available   Conversely  we also found some challenges to applying ES in practice  One core problem is that in order for ES to work  adding noise in parameters must lead to different outcomes to obtain some gradient signal  As we elaborate on in our paper  we found that the use of virtual batchnorm can help alleviate this problem  but further work on effectively parameterizing neural networks to have variable behaviors as a function of noise is necessary  As an example of a related difficulty  we found that in Montezuma s Revenge  one is very unlikely to get the key in the first level with a random network  while this is occasionally possible with random actions   ES is competitive with RL  We compared the performance of ES and RL on two standard RL benchmarks  MuJoCo control tasks and Atari game playing  Each MuJoCo task  see examples below  contains a physically simulated articulated figure  where the policy receives the positions of all joints and has to output the torques to apply at each joint in order to move forward  Below are some example agents trained on three MuJoCo control tasks  where the objective is to move forward   We usually compare the performance of algorithms by looking at their efficiency of learning from data  as a function of how many states we ve seen  what is our average reward  Here are the example learning curves that we obtain  in comparison to RL  the TRPO algorithm in this case    Data efficiency comparison  The comparisons above show that ES  orange  can reach a comparable performance to TRPO  blue   although it doesn t quite match or surpass it in all cases  Moreover  by scanning horizontally we can see that ES is less efficient  but no worse than about a factor of 10  note the x axis is in log scale    Wall clock comparison  Instead of looking at the raw number of states seen  one can argue that the most important metric to look at is the wall clock time  how long  in number of seconds  does it take to solve a given problem  This quantity ultimately dictates the achievable speed of iteration for a researcher  Since ES requires negligible communication between workers  we were able to solve one of the hardest MuJoCo tasks  a 3D humanoid  using 1 440 CPUs across 80 machines in only 10 minutes  As a comparison  in a typical setting 32 A3C workers on one machine would solve this task in about 10 hours  It is also possible that the performance of RL could also improve with more algorithmic and engineering effort  but we found that naively scaling A3C in a standard cloud CPU setting is challenging due to high communication bandwidth requirements   Below are a few videos of 3D humanoid walkers trained with ES  As we can see  the results have quite a bit of variety  based on which local minimum the optimization ends up converging into   On Atari  ES trained on 720 cores in 1 hour achieves comparable performance to A3C trained on 32 cores in 1 day  Below are some result snippets on Pong  Seaquest and Beamrider  These videos show the preprocessed frames  which is exactly what the agent sees when it is playing   In particular  note that the submarine in Seaquest correctly learns to go up when its oxygen reaches low levels   Related Work  ES is an algorithm from the neuroevolution literature  which has a long history in AI and a complete literature review is beyond the scope of this post  However  we encourage an interested reader to look at Wikipedia  Scholarpedia  and J rgen Schmidhuber s review article  Section 6 6   The work that most closely informed our approach is Natural Evolution Strategies by Wierstra et al  2014  Compared to this work and much of the work it has inspired  our focus is specifically on scaling these algorithms to large scale  distributed settings  finding components that make the algorithms work better with deep neural networks  e g  virtual batch norm   and evaluating them on modern RL benchmarks   It is also worth noting that neuroevolution related approaches have seen some recent resurgence in the machine learning literature  for example with HyperNetworks   Large Scale Evolution of Image Classifiers  and  Convolution by Evolution    Conclusion  Our work suggests that neuroevolution approaches can be competitive with reinforcement learning methods on modern agent environment benchmarks  while offering significant benefits related to code complexity and ease of scaling to large scale distributed settings  We also expect that more exciting work can be done by revisiting other ideas from this line of work  such as indirect encoding methods  or evolving the network structure in addition to the parameters   Note on supervised learning  It is also important to note that supervised learning problems  e g  image classification  speech recognition  or most other tasks in the industry   where one can compute the exact gradient of the loss function with backpropagation  are not directly impacted by these findings  For example  in our preliminary experiments we found that using ES to estimate the gradient on the MNIST digit recognition task can be as much as 1 000 times slower than using backpropagation  It is only in RL settings  where one has to estimate the gradient of the expected reward by sampling  where ES becomes competitive   Code release  Finally  if you d like to try running ES yourself  we encourage you to dive into the full details by reading our paper or looking at our code on this Github repo,"[995 620 186 449 119 1284 1262 869 837 1140 1369]"
1035,training-dataset/engineering/763.txt,engineering,Confidence Splitting Criterions Can Improve Precision And Recall in Random Forest ClassifiersThe Trust and Safety Team maintains a number of models for predicting and detecting fraudulent online and offline behaviour  A common challenge we face is attaining high confidence in the identification of fraudulent actions  Both in terms of classifying a fraudulent action as a fraudulent action  recall  and not classifying a good action as a fraudulent action  precision    A classification model we often use is a Random Forest Classifier  RFC   However  by adjusting the logic of this algorithm slightly  so that we look for high confidence regions of classification  we can significantly improve the recall and precision of the classifier s predictions  To do this we introduce a new splitting criterion  explained below  and show experimentally that it can enable more accurate fraud detection   Traditional Node Splitting Criterions  A RFC is a collection of randomly grown  Decision Trees   A decision tree is a method for partitioning a multi dimensional space into regions of similar behaviour  In the context of fraud detection  identifying events as  0  for non fraud and  1  for fraud  a decision tree is binary and tries to find regions in the signal space that are mainly 0s or mainly 1s  Then  when we see a new event  we can look at which region it belongs to and decide if it is a 0s region or a 1s region   Typically  a Decision Tree is grown by starting with the whole space  and iteratively dividing it into smaller and smaller regions until a region only contains 0s or only contains 1s  Each final uniform region is called a  leaf   The method by which a parent region is partitioned into two child regions is often referred to as the  Splitting Criterion   Each candidate partition is evaluated and the partition which optimises the splitting criterion is used to divide the region  The parent region that gets divided is called a  node    Suppose a node has   N   observations and take   L_0   i     and   R_0   i     to denote the number of 0s in the left and right child respectively  and similarly   L_1   i     and   R_1   i     for 1s  So for each candidate partition   i   we have   N L_0   i   L_1   i   R_0   i   R_1   i      Now let   l_0   i   L_0   i    L_0   i   L_1   i      be the probability of selecting a 0 in the left child node for partition   i    and similarly denote the probabilities   l_1   i        r_0   i      and   r_1   i      The two most common splitting criterions are   A  Gini Impurity  Choose   i   to minimise the probability of mislabeling i e    i_ gini     arg  min_i H_ gini    i     where      H_ gini    i      frac L_0   i   L_1   i    N   big  l_0   i    1 l_0   i      l_1   i    1 l_1   i     big      frac R_0   i   R_1   i    N   big  r_0   i    1 r_0   i      r_1   i    1 r_1   i     big       B  Entropy  Choose   i   to maximise the informational content of the labeling i e    i_ entropy     arg  min_i H_ entropy    i     where      H_ entropy    i        frac L_0   i   L_1   i    N   big  l_0   i    log l_0   i      l_1   i    log l_1   i     big      frac R_0   i   R_1   i    N   big  r_0   i    log r_0   i      r_1   i    log r_1   i     big        However  notice that both of these criterions are scale invariant  A node with   N 300   observations and partition given by   L_1 100 L_0 100 R_1 100 R_0 0   achieves an identical spliting criterion score   H_ gini  1 3   to a node with   N 3   observations and   L_1 1 L_0 1 R_1 1 R_0 0    The former split is a far stronger result  more difficult partition to achieve  than the latter  It may be useful to have a criterion that is able to differentiate between the likelihood of each of these partitions   Confidence Splitting Criterion  Theory  Let   L   i   L_0   i   L_1   i     and   R   i   R_0   i   R_1   i      And let   p_0   be the proportion of 0s and   p_1   the proportion of 1s in the node we wish to split  Then we want to find partitions where the distribution of 0s and 1s is unlikely to have occured by random  If the null hypothesis is that each observation is a 0 with probability   p_0   then the probability of   L_0   i     or more 0s occuring in a partition of   L   i     observations is given by the Binomial random variable   X n p           mathbb P  X L   i   p_0     L_0   i      1   B L_0   i   L   i   p_0       where   B x N p    is the cumulative distribution function for a Binomial random variable   X x   with   N   trials and probability   p   of success  Similarly for   p_1   the probability of   L_1   i     or more 1s occuring in the left partition is   1 B L_1   i   L   i   p_1     Taking the minimum of these two probabilities gives us the equivalent of a two tailed hypothesis test  The probability is essentially the p value under the null hypothesis for a partition at least as extreme as the one given  We can repeat the statistical test for the right partition and take the product of these two p values to give an overall partition probability   Now  to enable biasing the splitting towards identifying high density regions of 1s in the observation space  one idea is to modify   B L_0   i   L   i   p_0    to only be non zero if it is sufficiently large  In other words  replace it with       mathbb 1 _    B L_0   i   L   i   p_0     C_0     B L_0   i   L   i   p_0       where   C_0  in  0 1    is the minimum confidence we require    C_0   might take value 0 5  0 9  0 95  0 99  etc  It should be chosen to match the identification desired  Similarly for   C_1    Thus we propose a new splitting criterion given by   C  Confidence  Choose   i   to minimise the probability of the partition being chance i e    i_ confidence     arg  min_i H_ confidence    i     where      H_ confidence    i      min_ j 0 1    1    mathbb 1 _     B L_j   i   L   i   p_j     C_j         B L_j   i   L   i   p_j         min_ j 0 1    1    mathbb 1 _     B R_j   i   R   i   p_j     C_j         B R_j   i   R   i   p_j          where   C_0   and   C_1   are to be chosen to optimise the identification of 0s or 1s respectively   Implementation  To run this proposed new splitting criterion  we cut a new branch of the Python opensource Scikit Learn repository and updated the Random Forest Classifier Library  Two modifications were made to the analytical   H_ confidence    i     function to optimise the calculation   Speed  Calculating the exact value of   B x N p    is expensive  especially over many candidate partitions for many nodes across many trees  For large values of   N   we can approximate the Binomial cumulative distribution function by the Normal cumulative distribution    Phi x Np  sqrt Np 1 p      which itself can be approximated using Karagiannidis   Lioumpas  2007  as a ratio of exponentials  Accuracy  for large values of   N   and small values of   p   the tails of the Binomial distribution can be very small so subtraction and multiplication of the tail values can be corrupted by the precision of the machine s memory  To overcome this we take the logarithm of the above approximation to calculate   H_ confidence    i       After these tweaks to the algorithm we find an insignificant change to the runtime of the Scikit Learn routines  The Python code with the new criterion looks something like this   from sklearn ensemble import RandomForestClassifier   using  C_0 C_1     0 95 0 95  rfc   RandomForestClassifier n_estimators 1000 criterion  conf  conf  0 95 0 95   rfc fit x_train y_train  pred   rfc predict_proba x_test   For more details on the Machine Learning model building process at Airbnb you can read previous posts such as Designing Machine Learning Models  A Tale of Precision and Recall and How Airbnb uses machine learning to detect host preferences  And for details on our architecture for detecting risk you can read more at Architecting a Machine Learning System for Risk   Evaluation  Data  To test the improvements the Confidence splitting criterion can provide  we use the same dataset we used in the previous post Overcoming Missing Values In A Random Forest Classifier  namely the adult dataset from the UCI Machine Learning Repository  As before the goal is predict whether the income level of the adult is greater than or less than  50k per annum using the 14 features provided   We tried 6 different combinations of    C_0 C_1    against the baseline RFC with Gini Impurity and looked at the changes in the Precision Recall curves  As always we holdout a training set and evaluate on the unused test set  We build a RFC of 1000 trees in each of the 7 scenarios   Results  Observe that   C_0 0 5    yellow and blue lines  offers very little improvement over the baseline RFC  modest absolute recall improvements of 5  at the 95  precision level  However  for   C_0 0 9    green and purple lines  we see a steady increase in recall from at precision levels of 45  and upwards  At 80  precision and above    C_0 0 9   improves recall by an absolute amount of 10   riing to 13  at 95  precision level  There is little variation between   C_1 0 9    green line  and   C_1 0 99    purple line  for   C_0 0 9   although    C_0 C_1   0 9 0 9     green line  does seem to be superior  For   C_0 0 9    pale blue and pink lines   the improvement is not so impressive or consistent   Final Thoughts  It would be useful to extend the analysis to compare the new splitting criterion against optimising existing hyper parameters  In the Scikit Learn implementation of RFCs we could experiment with min_samples_split or min_samples_leaf to overcome the scaling problem  We could also test different values of class_weight to capture the asymmetry introduced by non equal   C_0   and   C_1     More work can be done on the implementation of this methodology and there is still some outstanding analytical investigation on how the confidence thresholds   C_j   tie to the improvements in recall or precision  Note however that the methodology does already generalise to non binary classifiers  i e  where j 0 1 2 3    It could be useful to implement this new criterion into the Apache Spark RandomForest library also   Business Impact  For the dataset examined  the new splitting criterion seems to be able to better identify regions of higher density of 0s or 1s  Moreover  by taking into account the size of the partition and the probability of such a distribution of observations under the null hypothesis  we can better detect 1s  In the context of Trust and Safety  this translates into being able to more accurately detect fraudulent actions   The business implications of moving the Receiver Operating Characteristic outwards  equivalently moving the Precision Recall curve outwards  have been discussed in a previous post  As described in the  Efficiency Implications  section of Overcoming Missing Values In A Random Forest Classifier post  even decimal percentage point savings in recall or precision can lead to enormous dollar savings in fraud mitigation and efficiency respectively,"[1035 1262 620 119 995 1284 1140 449 186 516 837]"
1040,training-dataset/product/485.txt,product,How do you get started with machine learning Machine Learning for Product Managers How do you get started with machine learning   Arthur C  Clarke said   Any sufficiently advanced technology is indistinguishable from magic   I remember that every time I search for an old photo in Google Photos  filter an image in Prisma  talk to my Amazon Echo  or select a Smart Reply in Gmail  Machine learning is changing the face of computing  and faster than many of us ever imagined   Machine learning is going to upend your industry and your product   If you think ML is a fad  check in with someone who thought the web was a fad in 1998  or mobile in 2008   Product managers need to understand the current state of AI  and the opportunities that machine learning and techniques such as deep learning provide for transforming your products and delighting your users   With the help of some friends at Google I ve put together this list of resources  I m certain that this list is incomplete if I left anything out  please let me know and I ll keep the list updated on my website   Must Reads   Next Steps   Books    These artificial intelligence techniques are the present and the future of Google  maybe of all technology  Maybe of everything     Steven Levy  Product Jobs  Subscribe to the newsletter to see these job listings   Previous newsletters,"[1040 1284 646 1078 837 186 771 869 453 102 706]"
1078,training-dataset/engineering/1128.txt,engineering,The Product Edge in Machine Learning Startups   Andreessen HorowitzA lot of machine learning startups initially feel a bit of  impostor syndrome  around competing with big companies  because  the argument goes   those companies have all the data  surely we can t beat that  Yet there are many ways startups can  and do  successfully compete with big companies  You can actually achieve great results in a lot of areas even with a relatively small data set  argue the guests on this podcast  if you build the right product on top of it   So how do you go about building the right product  beyond machine learning algorithms in academic papers   It s about the whole system  the user experience  transparency  domain expertise  choosing the right tools  But what do you build  what do you buy  and do you bother to customize  Jensen Harris  CTO and co founder of Textio  and AJ Shankar  CEO and co founder of Everlaw  share their lessons learned here in this episode of the a16z Podcast   including what they wish they d known early on   Because  observes moderator  and a16z board partner  Steven Sinofsky   To achieve product market fit  there s a whole bunch of stuff beyond a giant corpus of data  and the latest deep learning algorithm   Machine learning is an ingredient  part of a modern software as a service company  going beyond the hype  it s really about figuring out the problem you re trying to solve  and then figuring out where machine learning fits in  as opposed to the other way around   Customers are paying you to help solve a problem for them  after all,"[1078 1284 706 646 1040 44 837 449 186 453 869]"
1140,training-dataset/engineering/871.txt,engineering,Machine Learning over 1M hotel reviews finds interesting insightsOn a previous post we learned how to train a machine learning classifier that is able to detect the different aspects mentioned on hotel reviews  With this aspect classifier  we were able to automatically know if a particular review was talking about cleanliness  comfort   facilities  food  Internet  location  staff and or value for money   We also learned how to combine this classifier with the sentiment analysis classifier to get interesting insights and answer questions like are guests loving the location of a particular hotel but complaining about its cleanliness   On this post we will cover how we can use these machine learning models to analyze millions of reviews from TripAdvisor and then compare how people feel about hotels in different cities to understand things like   Do people who stay in hotels in Bangkok complain more about cleanliness than those that stay in  say  hotels in Paris   What is the city with the worst facilities   Does the number of stars of a hotel impact its reviews   Do people have different standards when it comes to hotels of different classes   These are the kind of questions we aim to answer with this tutorial and that will lead us to some interesting insights  The source code used for this process is available in this repository   Scraping the hotel reviews  We created a new version of the TripAdvisor Spider that we had built on a previous post  one that collects more data from a review   The name of the hotel   The city where the hotel is located   The stars the hotel has  given by the reviewers    Creating a Pipeline to combine the models  After scraping more than 1 million of reviews from TripAdvisor with the new spider  we split the content into opinion units and classified them in a similar way we had done last time around   The big difference is that now we created a pipeline that combines both classifiers  Pipelines are very powerful and versatile tools that allows you to combine different modules within MonkeyLearn and thanks to them we can classify the reviews for both aspect and sentiment with a single request   This is what it looks like to classify opinion units using the pipeline   from monkeylearn import MonkeyLearn ml   MonkeyLearn   your api key here    data      texts      text    The room was very clean      text    very rude staff      res   ml pipelines run  pi_YKStimMw   data  sandbox False  1 2 3 4 5 6 from monkeylearn import MonkeyLearn ml   MonkeyLearn     your api key here     data      texts         text     The room was very clean         text     very rude staff        res   ml   pipelines   run    pi_YKStimMw    data   sandbox   False    Simple huh  Then  res result is a JSON that looks like this      tags       sentiment       category_id   102881   label    Good    probability   1 0      topic         category_id   1495678   label    Cleanliness    probability   1 0            sentiment       category_id   102882   label    Bad    probability   1 0      topic         category_id   1495676   label    Staff    probability   1 0           1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29    tags         sentiment         category_id    102881    label     Good     probability    1 0        topic           category_id    1495678    label     Cleanliness     probability    1 0              sentiment         category_id    102882    label     Bad     probability    1 0        topic           category_id    1495676    label     Staff     probability    1 0              There s an item for each review sent to the pipeline  and each one has a sentiment and a list of topics  aspects    When saving these results to a CSV file  we also added a link to its parent review with a key generated from a hash of the review text  So  we now had two files  one with the reviews  which included the metadata for each review that we scraped  city  hotel location  stars  etc   and one with the classified opinion units  the sentiment and topic of each unit  the probability of the sentiment  and a link to its parent review   Indexing the results with Elasticsearch   Kibana visualizations  But we didn t stop there  We then indexed the results with Elasticsearch and loaded them into Kibana in order to generate beautiful visualizations   This was pretty straightforward  First  we installed Elasticsearch and got it running as a local instance  Some trial and error followed for getting the best model for the data  we used sense for doing that  It s a great Chrome extension that lets you interact with elastic s API on a nice interface instead of using cURL  We created an index with two types  review and opinion_unit  The fields of each type were the fields of the csv  and using the _parent property we added a link from each opinion unit to its parent review  Finally  we indexed the data from the csv files using the python SDK   Using the SDK to index an opinion unit is very simple  You just need to create a Python dictionary with all the fields of the item  and send it to elasticsearch using the SDK  Doing it by bulk is faster than doing it an item at a time   es   Elasticsearch   http   localhost 9200    actions      parent_key    bd1ed398a8529d5ad010d927d5af7240  opinion_unit    The room was very clean  sentiment    Good  topic    Cleanliness  item    parent_key  opinion_unit  sentiment  topic  action      _index    index_hotels    _type    opinion_unit    _id   cont_id   _parent   parent_key   _source   item   actions append action  helpers bulk es  actions  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 es   Elasticsearch      http   localhost 9200      actions       parent_key    bd1ed398a8529d5ad010d927d5af7240  opinion_unit    The room was very clean  sentiment    Good  topic    Cleanliness  item     parent_key   opinion_unit   sentiment   topic   action      _index     index_hotels     _type     opinion_unit     _id    cont_id    _parent    parent_key    _source    item   actions   append   action   helpers   bulk   es   actions    Afterwards  we installed Kibana and configured it to point to our local Elasticsearch instance  In order to create the graphs shown in the following section  we used JSON based queries instead of Lucene because we needed to use the has_parent and has_child clauses   The following is a query that selects the opinion units from New York  It uses a filter with a has_parent clause  which means that it will only match elements  opinion units  that have a parent element  a review  that satisfy the requirement  being from New York   It also requires the opinion unit s sentiment to be classified with a probability above a certain threshold  which will improve the quality of the result      query      filtered      query      match_all      range      sent_probability      gt    0 501            filter      has_parent      type    review    query      match      city    New York City                1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25    query       filtered       query       match_all       range       sent_probability       gt     0 501             filter       has_parent       type     review     query       match       city     New York City                 Most of the queries used for creating the graphs are similar to this one  and they are available in Github   Insights from the reviews  Most reviews are positive  Looking at the overall picture  most reviews tend to be mostly positive  Out of all the opinion units analysed  82  had a  positive  sentiment  This means that  on average  82  of the things people write in hotel reviews are positive   London hotels have the worst reviews  However  things start to get interesting once you segment the sentiment of the reviews by city  London hotel reviews tend to be harsher than those of other cities  We were surprised by this result  as we were expecting London to perform at about the same level as New York or Paris  With millions of travelers visiting the city every year  it is remarkable that it scores differently to other similarly sized cities   This model doesn t give us any reasons for this behavior  only the facts  Could this be a symptom of the famously bad British customer service  Or maybe hotels in London are just worse than in other cities  Or is it because people are more demanding with the hotels in London   Share your opinion on this result in the comments below   London is dirtier than New York and has the worst food overall  Another interesting insight comes from comparing the overall sentiment of the particular aspects of the hotels across the different cities   Some aspects keep mostly the same level as the overall sentiment for the city   Comfort and Facilities  in London is lower than in New York  and so forth   Interestingly  not all aspects follow this pattern  For example  Location  tends to be overwhelmingly positive across all cities  meaning that when reviewers mention the hotel location  it s usually because they liked it and rarely to complain   Food  is a similar case  except for the fact that London is again remarkably lower than the other cities  This could be the English cuisine getting a bad rep   Sentiment on hotels with different number of stars  We did a final comparison to find out how people felt about the aspects in hotels of different class  stars   It seems that travelers keep the same standards no matter what hotel they stay in  since the positivity of the reviews goes up with the number of stars   Internet is always an issue  Interestingly   Internet  doesn t really rise above 70  in any hotel class  indicating that internet access is as bad in 3 star hotels as in 5 star hotels  One would expect that more expensive hotels would provide a better internet service  but If you ve ever stayed at a 5 star hotel you know that s not usually the case   Hotels with 3 stars provide the best Value for Money  The other aspect of hotel reviews that behaves differently is  Value for Money   which peaks at 3 stars and then goes down  This implies that hotels with three or even two stars have a better value for money overall even though they have more negative aspects than the hotels with a higher class   Context analysis with keyword extraction  Numbers are alright  but what about actual content  Out of the reviews about cleanliness  what is praised in each city  What are common complaints in the food of each hotel  In order to find out  we used the keyword extraction module  This is a public module that for given a text  extracts its keywords ordered by relevance   In order to get keywords representative from each segment  we combined several hundred opinion units with the same city  aspect and sentiment into a single text  and extracted keywords from that text  This is how you run the keyword extractor   ml   MonkeyLearn   your api key here    text     The carpets are a disgrace in the dining room and need replaced immediately  The room was frankly grim    old  saggy beds  all 3 of them   scuffed walls and decor  the hotel is SO old  smells old  and TINY    module_id    ex_y7BPYzNG  res   ml extractors extract module_id  text  1 2 3 4 ml   MonkeyLearn     your api key here     text      The carpets are a disgrace in the dining room and need replaced immediately  The room was frankly grim    old  saggy beds  all 3 of them   scuffed walls and decor  the hotel is SO old  smells old  and TINY     module_id    ex_y7BPYzNG  res   ml   extractors   extract   module_id   text    The result is a JSON that contains the extracted keywords and information about them  such as the relevance and the number of appearances   For example  these are the top ten keywords for New York hotels reviews with a  Bad  sentiment in the  Cleanliness  aspect and their relevance as returned by MonkeyLearn   for d in res result 0   print d  keyword    d  relevance   room 0 999 bathroom 0 790 carpet 0 407 towels 0 311 bed bugs 0 246 bed 0 232 hotel 0 196 shower 0 155 shared bathroom 0 150 walls 0 138 1 2 3 4 5 6 7 8 9 10 11 12 13 for d in res   result   0     print d    keyword      d    relevance    room 0 999 bathroom 0 790 carpet 0 407 towels 0 311 bed bugs 0 246 bed 0 232 hotel 0 196 shower 0 155 shared bathroom 0 150 walls 0 138  After the extraction was done for each segment  we compared the keywords obtained from the different cities  which ones were unique  and which ones were common to several destinations   Bangkok has a cockroach problem  This analysis gave us some very interesting insights about the differences and the similarities between the hotels in each city   For instance  cleanliness complaints common to each city were things like carpet  bed  hair  bed bugs  stains  However  cockroaches appeared only in Bangkok  which implies that the roach situation in Bangkok s hotels may be much worse than in other places   Shared bathroom appeared only in New York  which could mean that in NYC shared bathrooms are more common  and they are dirty to boot   The content of opinions about location changes widely from city to city  Keywords with the name of different landmarks come up in each city  in Rio de Janeiro comments mention things like Copacabana  Ipanema  in Beijing things like the Forbidden Palace and Tiananmen Square  in Madrid Puerta del Sol  and so forth  These are all places that are important tourist attractions in each city  so reviewers consider them important when it comes to how well located a hotel is  There are also mentions to other elements that are characteristic from a city without being a landmark  such as Tube Station for London  and Metro for Paris   Croissants are a huge disappointment  There are many more insights hidden in this dataset  but we d like to mention one more  Namely  the opinions about food in different cities  Breakfast is of course a common keyword  and so are coffee and tea  However  things get more interesting when we consider the keywords of a single city   For example  the keyword croissant appears only in reviews from Paris  In addition  it appears mostly on a Negative context  which was surprising  Why would such a staple French food be mentioned in a negative light  Looking at the content from these reviews  the answer became clear  croissant is mentioned in the context of a very basic breakfast  So if you go to Paris  a lackluster breakfast will surely include some croissants  but little else    Final words  On this tutorial we learned how to scrape millions of reviews  analyze them with pre trained classifiers within MonkeyLearn  indexed the results with Elasticsearch and visualize them using Kibana   Machine learning makes sense when you want to analyze big volumes of data in a cost effective way   We discovered some really interesting insights  some were expected  like Internet being always an issue  and some were a total surprise for us  London hotels seem to be some of the worst    If you have the opportunity  check out the code and perform your own analysis  you will find that playing with data and Machine Learning can be a lot of fun  And if you do  please share your opinions and results in the comments below  it would be awesome to hear your insights   Happy coding,"[1140 1262 995 1284 837 449 620 186 453 646 551]"
1156,training-dataset/business/611.txt,business,Mobile is eating the world   Benedict EvansAs we pass 2 5bn smartphones on earth and head towards 5bn  and mobile moves from creation to deployment  the questions change  What s the state of the smartphone  machine learning and  GAFA   and what can we build as we stand on the shoulders of giants,"[1156 620 453 1040 646 1284 1412 186 335 449 1078]"
1182,training-dataset/business/1355.txt,business,Mostly luck  not an accident   GV LibraryMostly luck  not an accident   This essay is based on my keynote talk at Techstars FounderCon on October 18  2016  I ll post the video once it s up    The United States Census of 1880 came at a pivotal time in U S  history  It was the last time the Census Office could identify anything resembling a U S  frontier  for the first time  just half of the country worked in agriculture  The census itself had grown so large that the government was collecting more data than it could tabulate   it took a full eight years and thousands of people to produce the twenty three volumes containing their analysis  At that rate  it was entirely likely that the 1900 Census would start before the 1890 Census results would be completed   Herman Hollerith  Source  Library of Congress   Enter Herman Hollerith  A teen graduate of the Columbia School of Mines  Hollerith s first job out of college was in the Census Office in 1880  He d seen first hand the challenges they d faced  experienced the frustration of knowing the information was contained in the voluminous records gathered by the census takers  unable to extract it  After just two years  he left to take a Mechanical Engineering professorship at MIT  He was 22   Hollerith had been experimenting with punch cards as a storage medium  by 1888 he d expanded on the idea by creating a corresponding machine to electrically tabulate the stored data  The idea of using punch cards to store information was not new   the Jacquard Loom  invented 80 years prior  used punch cards to store complex designs for textiles   But the counting of the stored data  That was new  And Hollerith thought it might be an advantage   Hollerith Machine  Source  U S  Census Bureau   In 1888  the Census Office   years late and over budget with the 1880 census   decided to hold a contest to solve the country s first  big data  problem  They needed more  and better  information about the growth of the country and they needed the information faster  The contest had two components  1  data capture and 2  data tabulation   Three people entered the contest  The third place submission captured the data in 145 hours  Hollerith s machine captured the data in half the time  The third place submission tabulated the data in just over 55 hours  Hollerith s machine needed just 5 5 hours  a 10X improvement  Hollerith had his first customer   Hollerith s machines were exactly what the Census Office had hoped for  the population count was completed in months  not years  The entire census   including analysis  demographic and economic data   finished in less time  contained 40  more information than the 1880 Census  and saved the U S  government  5M  Flush with his success in the U S   Hollerith founded the Tabulating Machine Company  and went on to capture and tabulate data for governments in Russia  Austria  France  Norway  Cuba  Canada  and the Philippines   There was just one problem  the Hollerith Machine produced data so quickly that many refused to accept the results initially   Useless machines   declared the Boston Herald  Local U S  politicians   who wanted more federal money   refused to accept the lower than expected population counts  The New York Herald complained   Slip shod  sic  work has ruined the Census,"[1182 706 449 1284 44 837 102 551 646 1412 453]"
1219,training-dataset/engineering/1153.txt,engineering,Open Sourcing TensorFlowOnSpark  Distributed Deep   By Lee Yang  Jun Shi  Bobbie Chern  and Andy Feng   afeng76   Yahoo Big ML team  Introduction  Today  we are pleased to offer TensorFlowOnSpark to the community  our latest open source framework for distributed deep learning on big data clusters   Deep learning  DL  has evolved significantly in recent years  At Yahoo  we ve found that in order to gain insight from massive amounts of data  we need to deploy distributed deep learning  Existing DL frameworks often require us to set up separate clusters for deep learning  forcing us to create multiple programs for a machine learning pipeline  see Figure 1 below   Having separate clusters requires us to transfer large datasets between them  introducing unwanted system complexity and end to end learning latency   Last year we addressed scaleout issues by developing and publishing CaffeOnSpark  our open source framework that allows distributed deep learning and big data processing on identical Spark and Hadoop clusters  We use CaffeOnSpark at Yahoo to improve our NSFW image detection  to automatically identify eSports game highlights from live streamed videos  and more  With the community s valuable feedback and contributions  CaffeOnSpark has been upgraded with LSTM support  a new data layer  training and test interleaving  a Python API  and deployment on docker containers  This has been great for our Caffe users  but what about those who use the deep learning framework TensorFlow  We re taking a page from our own playbook and doing for TensorFlow for what we did for Caffe   After TensorFlow s initial publication  Google released an enhanced TensorFlow with distributed deep learning capabilities in April 2016  In October 2016  TensorFlow introduced HDFS support  Outside of the Google cloud  however  users still needed a dedicated cluster for TensorFlow applications  TensorFlow programs could not be deployed on existing big data clusters  thus increasing the cost and latency for those who wanted to take advantage of this technology at scale   To address this limitation  several community projects wired TensorFlow onto Spark clusters  SparkNet added the ability to launch TensorFlow networks in Spark executors  DataBricks proposed TensorFrame to manipulate Apache Spark s DataFrames with TensorFlow programs  While these approaches are a step in the right direction  after examining their code  we learned we would be unable to get the TensorFlow processes to communicate with each other directly  we would not be able to implement asynchronous distributed learning  and we would have to expend significant effort to migrate existing TensorFlow programs   TensorFlowOnSpark  Our new framework  TensorFlowOnSpark  TFoS   enables distributed TensorFlow execution on Spark and Hadoop clusters  As illustrated in Figure 2 above  TensorFlowOnSpark is designed to work along with SparkSQL  MLlib  and other Spark libraries in a single pipeline or program  e g  Python notebook    TensorFlowOnSpark supports all types of TensorFlow programs  enabling both asynchronous and synchronous training and inferencing  It supports model parallelism and data parallelism  as well as TensorFlow tools such as TensorBoard on Spark clusters   Any TensorFlow program can be easily modified to work with TensorFlowOnSpark  Typically  changing fewer than 10 lines of Python code are needed  Many developers at Yahoo who use TensorFlow have easily migrated TensorFlow programs for execution with TensorFlowOnSpark   TensorFlowOnSpark supports direct tensor communication among TensorFlow processes  workers and parameter servers   Process to process direct communication enables TensorFlowOnSpark programs to scale easily by adding machines  As illustrated in Figure 3  TensorFlowOnSpark doesn t involve Spark drivers in tensor communication  and thus achieves similar scalability as stand alone TensorFlow clusters   TensorFlowOnSpark provides two different modes to ingest data for training and inference   TensorFlow QueueRunners  TensorFlowOnSpark leverages TensorFlow s file readers and QueueRunners to read data directly from HDFS files  Spark is not involved in accessing data  Spark Feeding  Spark RDD data is fed to each Spark executor  which subsequently feeds the data into the TensorFlow graph via feed_dict   Figure 4 illustrates how the synchronous distributed training of Inception image classification network scales in TFoS using QueueRunners with a simple setting  1 GPU  1 reader  and batch size 32 for each worker  Four TFoS jobs were launched to train 100 000 steps  When these jobs completed after 2  days  the top 5 accuracy of these jobs were 0 730  0 814  0 854  and 0 879  Reaching top 5 accuracy of 0 730 takes 46 hours for a 1 worker job  22 5 hours for a 2 worker job  13 hours for a 4 worker job  and 7 5 hours for an 8 worker job  TFoS thus achieves near linear scalability for Inception model training  This is very encouraging  although TFoS scalability will vary for different models and hyperparameters   RDMA for Distributed TensorFlow  In Yahoo s Hadoop clusters  GPU nodes are connected by both Ethernet and Infiniband  Infiniband provides faster connectivity and supports direct access to other servers  memories over RDMA  Current TensorFlow releases  however  only support distributed learning using gRPC over Ethernet  To speed up distributed learning  we have enhanced the TensorFlow C   layer to enable RDMA over Infiniband   In conjunction with our TFoS release  we are introducing a new protocol for TensorFlow servers in addition to the default  grpc  protocol  Any distributed TensorFlow program can leverage our enhancement via specifying protocol  grpc_rdma  in tf train ServerDef   or tf train Server     With this new protocol  a RDMA rendezvous manager is created to ensure tensors are written directly into the memory of remote servers  We minimize the tensor buffer creation  Tensor buffers are allocated once at the beginning  and then reused across all training steps of a TensorFlow job  From our early experimentation with large models like the VGG 19 network  our RDMA implementation has demonstrated a significant speedup on training time compared with the existing gRPC implementation   Since RDMA support is a highly requested capability  see TensorFlow issue  2916   we decided to make our current implementation available as an alpha release to the TensorFlow community  In the coming weeks  we will polish our RDMA implementation further  and share detailed benchmark results   Simple CLI and API  TFoS programs are launched by the standard Apache Spark command  spark submit  As illustrated below  users can specify the number of Spark executors  the number of GPUs per executor  and the number of parameter servers in the CLI  A user can also state whether they want to use TensorBoard   tensorboard  and or RDMA   rdma    spark submit  master   MASTER       TFoS_HOME  examples slim train_image_classifier py     model_name inception_v3     train_dir hdfs   default slim_train     dataset_dir hdfs   default data imagenet     dataset_name imagenet     dataset_split_name train     cluster_size   NUM_EXEC      num_gpus   NUM_GPU      num_ps_tasks   NUM_PS      sync_replicas     replicas_to_aggregate   NUM_WORKERS      tensorboard     rdma  TFoS provides a high level Python API  illustrated in our sample Python notebook    TFCluster reserve     construct a TensorFlow cluster from Spark executors  TFCluster start     launch Tensorflow program on the executors  TFCluster train   or TFCluster inference     feed RDD data to TensorFlow processes  TFCluster shutdown     shutdown Tensorflow execution on executors  Open Source  Yahoo is happy to release TensorFlowOnSpark at github com yahoo TensorFlowOnSpark and a RDMA enhancement of TensorFlow at github com yahoo tensorflow tree yahoo  Multiple example programs  including mnist  cifar10  inception  and VGG  are provided to illustrate the simple conversion process of TensorFlow programs to TensorFlowOnSpark  and leverage RDMA  An Amazon Machine Image is also available for applying TensorFlowOnSpark on AWS EC2   Going forward  we will advance TensorFlowOnSpark as we continue to do with CaffeOnSpark  We welcome the community s continued feedback and contributions to CaffeOnSpark  and are interested in thoughts on ways TensorFlowOnSpark can be enhanced,"[1219 230 1412 449 186 1284 706 1262 869 995 646]"
1262,training-dataset/engineering/8.txt,engineering,Learning when to skim and when to readLearning when to skim and when to read  The rise of Machine Learning  Deep Learning  and Artificial Intelligence more generally has been undeniable  and it has already had a massive impact on the field of computer science  By now  you might have heard how deep learning has surpassed super human performance in a number of tasks ranging from image recognition to the game of Go   The deep learning community is now eyeing natural language processing  NLP  as the next frontier of research and application   One beauty of deep learning is that advances tend to be very generic  For example  techniques that make deep learning work for one domain can often be transferred to other domains with little to no modification  More specifically  the approach of building massive  computationally expensive  deep learning models for image and speech recognition has spilled into NLP  One can see this in the case of the most recent state of the art translation system  which outperformed all previous results  but required an exorbitant amount of computers  Such demanding systems can capture very complex patterns occasionally found in real world data  but this has led many to apply these massive models to all tasks  This raises the question   Do all tasks always have the complexity that requires such models   Let s look at the innards of a two layered MLP trained on bag of words embeddings for sentiment analysis   The boundary boxes in the plot above offers some important insights  Real world data comes in different difficulties  some sentences are easily classified while others contain complex semantic structures  In the case of easily classified sentences  running a high capacity system might be unnessasary  A much simpler model could potentially do an equivalent job  This blog post will explore whether this is the case  It will show that we can often do with simple models   Deep learning with text  Most deep learning methods require floating point numbers as input and  unless you have been working with text before  you might wonder   How do I go from a piece of text to deep learning   A core issue with text is how to represent an arbitrarily large amount of information  given the length of the material  A popular solution has been tokenizing text into either words  sub words  or even characters  Each word is transformed into a floating point vector using well studied methods such as word2vec or GloVe  This provides for meaningful representations of a word through the implicit relationships between different words   By using tokenization and the word2vec methods we can turn a piece of text into a sequence of floating point representations of each word   Now  what can we use a sequence of word representations for   Bag of words  Now let s talk about the bag of words  BoW   perhaps one of the simplest machine learning algorithms you will ever learn   Simply take the mean of the words across each feature dimension  It turns out that simply averaging word embeddings  even though it completely ignores the order of the sentence  works well on many simple practical examples and will often give a strong baseline when combined with deep neural networks  shown later   Furthermore  taking the mean is a cheap operation and reduces the dimensionality of the sentence to a fixed sized vector   Recurrent Neural Networks  Some sentences require high precision or rely on sentence structure  Using a bag of words for these problems might not cut it  Instead  you might want to consider the amazing recurrent neural network   Each word embedding is  in order  fed to a recurrent neural network that then manages to store previously seen information and combine it with new words  When using an RNN powered by the famous memory cells such as the long short term memory cell  LSTM  or the gated recurrent unit  GRU   the RNN is capable of remembering what has happened in sentences with up to many words   because of the LSTM s success  the RNN with LSTM memory cells is often referred to as the LSTM   The biggest of these models stack eight of these on top of one another   However  the LSTM is much  much more expensive than the cheap bag of words model and will often require an experienced deep learning engineer to implement and support efficiently with high performance computing hardware   Example  Sentiment Analysis  Sentiment analysis is a type of document classification for quantifying polarity in subjective passages  Given a sentence  the model evaluates whether it is positive  negative or neutral   Want to find livid customers on twitter before they start trending  Well  Sentiment Analysis might be just what you re looking for   A great public dataset for this purpose  which we will use  is the Stanford sentiment treebank  SST   We have provided a publicly available data loader in pytorch  The SST provides not only the class  positive  negative  for a sentence  but also each of its grammatical subphrases  In our systems we do not utilize any tree information however  The original SST constitutes five classes  very positive  positive  neutral  negative and very negative  We consider the simpler task of binary classification where very positive is combined with positive  very negative is combined with negative and all neutrals are removed   We have provided a brief and technical description of our model architecture  The important point is not exactly how it is structured  but the fact that the cheap model gets 82  validation accuracy and takes 10 ms for a 64 sized batch  and the expensive LSTM achieves a significantly higher 88  validation accuracy but costs 87 ms for a 64 sized batch  Top models will be in the 88 90  accuracy ballpark    The cheap skim reader  On some tasks  algorithms can perform at near human level accuracy  but obtaining this performance might burn a hole in the server budget  You also know that if it is not always necessary to have an LSTM powerhouse with real world data  we might be just fine with the cheaper bag of words  But what happens when you get a sentence such as this    Horrible cast  complete lack of reality     but I loved it 9 10   The order agnostic bag of words will surely missclassify with the overwhelming amount of negative words  Completely switching to a crummy bag of words would drop our overall performance  which doesn t sound that compelling  So the question becomes   Can we learn to separate  easy  and  hard  sentences   And can we do so with a cheap model to save time   Exploring the innards  A popular way of exploring deep learning models is by plotting how each sentence is represented in the hidden layers  However  as the hidden layers are often high dimensional  we can use algorithms such as the T SNE to reduce dimensionality to 2D  allowing us to plot it for human inspection   T SNE plots are vulnerable to many over interpretations  but a few trends might strike you   Interpretations of T SNE  The sentences fall into clusters  The clusters consitutes different semantic types   Some clusters lie along a simple manifold with high confidence and accuracy   Other clusters are more scattered with low accuracy and low confidence   Sentences with positive and negative consituents are difficult   Let s now look at a similar plot for the LSTM   We can assess that many of these observations hold true for the LSTM as well  However  the LSTM only has relatively few examples with low confidence  and cooccurrence of positive and negative consituents in sentences does not look to be as challenging for the LSTM as it is for the bag of words   It seems the bag of words has been able to cluster sentences and use its probabillity to identify whether or not it is likely to give a correct prediction for the sentences in that cluster  From these observations  a reasonable hypothesis could be  Confident answers are more correct   To investigate this hypothesis  we can look at probability thresholds   Probability thresholding  The bag of words and LSTM are trained to give us probabilities for each class  which we can use as a measure of certainty  What do we mean by this  If the bag of words returns a 1  it is very confident in its prediction   Often when predicting we would take the class with the highest likelihood provided by our model  In the case of binary classification  e g  positive or negative  the likelihood has to be over 0 5  or else we would be predicting the opposite class    But a low likelihood for the predicted class might indicate that the model was in doubt  Say the model predicted 0 49 for negative and 0 51 for positive  it might not be so convincing that it actually is positive   When we say that we threshold  what we mean is that we compare the predicted probability to a value and assess whether or not to use it  E g  we could decide that we use all sentences with a probability above 0 7  Or we look at the interval 0 5 0 55 to see how accurate predictions with this confidence are  which is exactly what we will investigate in the next plot   From the bag of words plots it might occour to you that increasing the probability threshold increases the performance  From the LSTM plot it is not so obvious  which seems common as the LSTM overfits the training set and only provides confident answers   Use the BoW for easy examples  and the pristine LSTM for difficult ones   Thus  simply using the output probability could give us an indication of when a sentence is easy and when it is in need of guidance from a stronger system  like the powerful LSTM   Using the probability threshold  we create a strategy which we refer to as the  probability strategy   such that we threshold the probability of the bag of word system  and use the LSTM on all data points not reaching the threshold  Doing so provides us with an amount of data used for the bag of words  sentences above the threshold  and a set of data points where we have either chosen the BoW  above the threshold  or the LSTM  below the threshold   which we can use to find an accuracy and cost of computing  We then get a ratio between the BoW and the LSTM increasing from 0 0  only using LSTM  to 1 0  only using BoW   which we can use to calculate the accuracy and time to compute   Baseline  To construct a baseline we consider the ratio between the two models  e g  0 1 data used for BoW would correspond to 0 9 times LSTM accuracy and 0 1 times BoW accuracy  The purpose is to have a baseline with no guided strategy where the choice of using BoW or LSTM on a sentence is randomly assigned  However  there is a cost to using the strategies  We have to run all of the sentences through the bag of words model first  to determine if we should use the bag of words or the LSTM  In case that none of the sentences reaches the probability threshold  we could be running an extra model for no good reason  To incorporate this  we calculate the cost of our strategies and the ratio in the following way   The interesting discovery is that we find that using the bag of words thresholds significantly outperforms not having a guided strategy   We then measure the average value on the curve  which  we refer to as Speed Under the Curve  SUC   As shown in table below   Strategy Validation SUC Ratio between BoW and LSTM 84 84 Probability 86 03  std 0 3   Learning when to skim and when to read  Knowing when to switch between two different models is not enough  We want to build a more general system that learns when to switch between each model  Such a system would help us deal with the more complicated behaviour of  Can we learn when reading is strictly better than skimming in a supervised way   Where  reading  is using the LSTM which goes from left to right and stores a memory at each time step and  skimming  is using the BoW model  When operating on the probability from the bag of words model we make our decision based on the invariant that the more powerful LSTM will do a better job when the bag of word system is in doubt  but is that always the case   In fact  it turns out that it is only the case 12  of the time  whereas 6  of the sentences neither the bag of words or the LSTM get correct  In such case  we have no reason to run the LSTM and we might as well just save time by only using the bag of words   Learning to skim  the setup  So we don t always want to use the LSTM when the BoW is in doubt  Can we make our bag of word model understand when the LSTM also might be wrong and when we should spare our precious computational resources  Let us look at the T SNE plot again  but now with the confusion matrix between the BoW and the LSTM plotted  We hope to find a relationship between the elements of the confusion matrix  enspecially when the BoW is incorrect   From the comparison plot  we find that it is easy to assert when the BoW is correct and when it is in doubt  However  there is no clear relationship between when the LSTM might be right or wrong   Can we learn this relationship   Further  the probability strategy is quite restrictive as it relies on an inheritent binary decision and requires probabilities  Instead  we propose a trainable decision network that is based on a neural network  If we look at the confusion matrix  we can use that information to generate labels for a supervised decision network  In this way  we would only use the LSTM in the cases where the LSTM is correct and the BoW is wrong   To generate the dataset  we need a set of sentences having the true  underlying  prediction of our bag of words and the LSTM  However  during training the LSTM will often achieve upwards 99  training accuracy  significantly overfitting the training set  To avoid this  we split our training set into a model training set  80  of training data  and a decision training set  remaining 20  of training data  that the model has not yet seen  Afterwards we fine tune our model with the remaining 20   hoping that the decision network will still generalize to this new  unseen  but very related and slightly better system   To build our decision network  we tap into the last hidden layer of our cheap bag of words system  the same layer we used to generate our T SNE plots   We then stack a two layer MLP on top of our bag of words training on the model training set  We found that if we do not follow this recipe  the decision network will not be able to learn the tendencies of the BoW model and will not generalize well   The classes chosen on the validation set by the decision network  based on the models trained on the model training set  is then applied to the full  but very related  models on the full training set  The reason why we apply it on the model trained on the full training set  is that the models on the model training set will often be inferior and thus result in a lower accuracy  The decision network is trained with early stopping  based on maximizing the SUC on the validation set   How does our decision network perform   Let us start by looking at the predictions of the decision network   Notice how closely this resembles the probability cutoff of the bag of words  Now let us look at the T SNE of the last hidden layer of the decision network  to see if it is actually able to cluster some information of when the LSTM is correct or wrong   It seems the decision network is capable of picking up the clustering from the hidden states of the bag of words  However  it does not seem like it is able to understand when the LSTM might also be wrong  clustering yellows from reds    From the data accuracy over saved time curves  it is not obvious whether or not the decision network is better   Policy Validation SUC Test SUC Ratio between BoW and LSTM 84 84 83 77 Probability 86 03  std 0 3  85 49  std 0 3  Decision network 86 13  std 0 3  85 49  std 0 3   From prediction plot  data amount vs  accuracy and SUC score we can infer that the decision network is splendid at understanding when the BoW might be correct and when it is not  Further  it allows us to build a more general system that taps into the hidden states of deep learning models  However  it also goes to show that it was very difficult to make the decision network understand the behaviour of systems that it did not have access to  such as the more complex LSTM   Discussion  We now know that large powerful LSTMs can achieve near human level performance on text  that not all real world data needs near human level performance  that we can train a bag of words model to understand when a sentence is easy and that using bag of words for easy sentences allows us to save a significant amount of computation time with only a minor drop in performance  depending on how aggressive we threshold the bag of words    This approach is related to mean averaging usually performed when model ensembling as often the model with high confidence will be used  However  by having an adjustable confidence from the bag of words and not needing to run the LSTM  we can decide how much computation time vs  accuracy savings we are interested in  We believe that this method will be useful for deep learning engineers looking to save computational resources without having to sacrifice performance   Citation credit  If you use this blog post in published work  please cite   Alexander Rosenberg Johansen  Bryan McCann  James Bradbury  and Richard Socher  2017   Learning when to skim and when to read  arxiv paper coming soon   By having a better understanding of when a deep learning system might be wrong  we can make informed decisions about when to use which deep learning model  This allows us to save computational time by only running the bare minimum to complete a task,"[1262 449 186 1284 230 620 869 1140 119 995 453]"
1284,training-dataset/business/760.txt,business,Three things you need to know about machine learningThree things you need to know about machine learning  Machine learning is all the rage  and major improvements in infrastructure  data storage  and cloud adoption have led to growing interest in the space  Many consumer facing advancements reside within Google and Facebook  but other companies are investing in the field as well  However  given all the excitement around machine learning  it s important to understand some of the nuances and mechanics of what machine learning is and how it works  As an investor at Redpoint Ventures  I have the privilege of learning from people and companies on the cutting edge of this technology   So  what is machine learning and artificial intelligence  Artificial intelligence  AI  is software that does what humans do but faster and hopefully better  For example  transcribing notes from a meeting and highlighting all follow up tasks  Machine learning is one approach to achieve AI by using algorithms  instead of the traditional hand coded rules based decision trees  At a high level  there are three steps in machine learning  sensing  reasoning  and producing   Machine learning has increased in popularity and become more feasible in the last five years   There have been several advancements in the field of machine learning that have driven improvements in techniques  applications  and the overall accessibility of the technique such as the rise of cloud computing and the proliferation of data   There are four major factors that have led to the current proliferation of companies that are leveraging machine learning in their products   The machine learning landscape and its potential for impact on products and services in the future can be viewed in three specific ways   Takeway  1  Machine learning is not monolithic  Within machine learning  there are different types of learning  e g   supervised  unsupervised  and various techniques  e g   regression  neural nets   These techniques and types of learning do not exclusively map to each other but are used in different combinations depending on the situation  Below I ve shared how I visually mapped this out for myself   Types of learning   Supervised  Supervised learning identifies patterns in data given pre determined features and labeled data  e g   traditional insurance underwriting   Unsupervised  Unsupervised learning identifies patterns in data  which is particularly helpful for unlabeled and unstructured data  e g   Gmail spam   Semi supervised  A blend of supervised and unsupervised learning  Best in situations where there is some labeled data but not a lot  e g   customer segmentation   Reinforcement  Reinforcement learning provides feedback to the algorithm as it trains  it is essentially experience driven decision making  e g   playing chess   Takeaway  2  Deep learning has emerged as a technique with strong advantages  but also has important drawbacks as well  Deep learning involves many layers of neural nets  algorithms modeled on the structure of the human brain  making the network  deep   We often hear about the benefits of deep learning for unsupervised and reinforcement learning  which has in fact led to major advancements in both fields  although it is not always required  Deep learning may also be valuable in supervised learning with large  complex problems   Deep learning has three key advantages relative to other techniques  It is robust  generalizable  and scalable  It is robust because the features  the characteristics used to differentiate or classify the data  do not need to be predetermined  the optimal features are learned for a given task  It is generalizable because the same neural net approach can be used for different applications and data types  And it is scalable because the method is 1  parallelizable  i e   able to be run across multiple processors and 2  performance improves with more data  reducing the likelihood of that you will train the algorithm in a way that is accurate for only the specific training data  i e   overfitting    This has important consequences for three fields in particular  natural language processing  computer vision  and robotics   At the same time  there are drawbacks to keep in mind when using deep learning  One big one is that when a neural net determines certain features are important and makes decisions based on them  we do not know why  This means that if there is corrupted data or human bias in the system  we will not be able to pinpoint that it exists  which can be dangerous for specific use cases  such as various areas of finance and law enforcement  that could impact society   In addition  deep learning models require an extremely large amount of data and compute power to be effective  which is costly and time intensive  This is an important tradeoff to keep in mind  especially for young startups who are building their products on a budget   Therefore  deep learning is not always the best approach  but rather  for each specific use case  data scientists need to take the potential for bias  availability of computing resources  and access to data into account   Takeaway  3  Machine learning will have major implications for what products will look like going forward  Machine learning is not a solution in and of itself  but a tool to optimize the desired outcome  Therefore companies leveraging machine learning should focus on providing insights that are actionable  and move from helping customers manipulate data for analysis to focusing on strategy and recommendations to make decision making more efficient and accurate  It s likely that user interfaces will simplify to focus on recommending an action instead of providing a myriad of options to pick from  Below are two examples that illustrate early versions of this shift   Facebook s photo tagging engine has moved to recommended tagging  left   making it smarter and simpler to use than its previous version  right    Similarly  Google Now remembers where a user parks  left  and recommends when to leave for the airport  right  based on flight times  current location  and traffic information  This recommendation driven  simplified user experience will be the future of product user interfaces   These examples illustrate the massive impact that machine learning will have as it continues to enable products that are more effective and simpler to use   For further musings on machine learning and its implications for products and start ups  follow me on Twitter at  mkhandel and on Medium at  medhaa,"[1284 186 449 869 706 1040 404 646 1078 837 453]"
1369,training-dataset/business/23.txt,business,Machine intelligence  part 2This is part two of a a two part post the first part is here       THE NEED FOR REGULATION  Although there has been a lot of discussion about the dangers of machine intelligence recently  there hasn t been much discussion about what we should try to do to mitigate the threat   Part of the reason is that many people are almost proud of how strongly they believe that the algorithms in their neurons will never be replicated in silicon  and so they don t believe it s a potential threat  Another part of it is that figuring out what to do about it is just very hard  and the more one thinks about it the less possible it seems  And another part is that superhuman machine intelligence  SMI  is probably still decades away  1   and we have very pressing problems now   But we will face this threat at some point  and we have a lot of work to do before it gets here  So here is a suggestion   The US government  and all other governments  should regulate the development of SMI  In an ideal world  regulation would slow down the bad guys and speed up the good guys it seems like what happens with the first SMI to be developed will be very important   Although my general belief is that technology is often over regulated  I think some regulation is a good thing  and I d hate to live in a world with no regulation at all  And I think it s definitely a good thing when the survival of humanity is in question   Incidentally  there is precedent for classification of privately developed knowledge when it carries mass risk to human life  SILEX is perhaps the best known example    To state the obvious  one of the biggest challenges is that the US has broken all trust with the tech community over the past couple of years  We d need a new agency to do this   I am sure that Internet commentators will say that everything I m about to propose is not nearly specific enough  which is definitely true  I mean for this to be the beginning of a conversation  not the end of one   The first serious dangers from SMI are likely to involve humans and SMI working together  Regulation should address both the case of malevolent humans intentionally misusing machine intelligence to  for example  wreak havoc on worldwide financial markets or air traffic control systems  and the  accident  case of SMI being developed and then acting unpredictably   Specifically  regulation should   1  Provide a framework to observe progress  This should happen in two ways  The first is looking for places in the world where it seems like a group is either being aided by significant machine intelligence or training such an intelligence in some way   The second is observing companies working on SMI development  The companies shouldn t have to disclose how they re doing what they re doing  though when governments gets serious about SMI they are likely to out resource any private company   but periodically showing regulators their current capabilities seems like a smart idea   2  Given how disastrous a bug could be  require development safeguards to reduce the risk of the accident case  For example  beyond a certain checkpoint  we could require development happen only on airgapped computers  require that self improving software require human intervention to move forward on each iteration  require that certain parts of the software be subject to third party code reviews  etc  I m not very optimistic than any of this will work for anything except accidental errors humans will always be the weak link in the strategy  see the AI in a box thought experiments   But it at least feels worth trying   Being able to do this if it is possible at all will require a huge amount of technical research and development that we should start intensive work on now  This work is almost entirely separate from the work that s happening today to get piecemeal machine intelligence to work   To state the obvious but important point  it s important to write the regulations in such a way that they provide protection while producing minimal drag on innovation  though there will be some unavoidable cost    3  Require that the first SMI developed have as part of its operating rules that a  it can t cause any direct or indirect harm to humanity  i e  Asimov s zeroeth law   b  it should detect other SMI being developed but take no action beyond detection  c  other than required for part b  have no effect on the world   We currently don t know how to implement any of this  so here too  we need significant technical research and development that we should start now   4  Provide lots of funding for R D for groups that comply with all of this  especially for groups doing safety research   5  Provide a longer term framework for how we figure out a safe and happy future for coexisting with SMI the most optimistic version seems like some version of  the human machine merge   We don t have to figure this out today   Regulation would have an effect on SMI development via financing most venture firms and large technology companies don t want to break major laws  Most venture backed startups and large companies would presumably comply with the regulations   Although it s possible that a lone wolf in a garage will be the one to figure SMI out  it seems more likely that it will be a group of very smart people with a lot of resources  It also seems likely  at least given the current work I m aware of  it will involve US companies in some way  though  as I said above  I think every government in the world should enact similar regulations    Some people worry that regulation will slow down progress in the US and ensure that SMI gets developed somewhere else first  I don t think a little bit of regulation is likely to overcome the huge head start and density of talent that US companies currently have   There is an obvious upside case to SMI  it could solve a lot of the serious problems facing humanity but in my opinion it is not the default case  The other big upside case is that machine intelligence could help us figure out how to upload ourselves  and we could live forever in computers  Or maybe in some way  we can make SMI be a descendent of humanity   Generally  the arc of technology has been about reducing randomness and increasing our control over the world  At some point in the next century  we are going to have the most randomness ever injected into the system   In politics  we usually fight over small differences  These differences pale in comparison to the difference between humans and aliens  which is what SMI will effectively be like  We should be able to come together and figure out a regulatory strategy quickly           Thanks to Dario Amodei  especially Dario   Paul Buchheit  Matt Bush  Patrick Collison  Holden Karnofsky  Luke Muehlhauser  and Geoff Ralston for reading drafts of this and the previous post    1  If you want to try to guess when  the two things I d think about are computational power and algorithmic development  For the former  assume there are about 100 billion neurons and 100 trillion synapses in a human brain  and the average neuron fires 5 times per second  and then think about how long it will take on the current computing trajectory to get a machine with enough memory and flops to simulate that   For the algorithms  neural networks and reinforcement learning have both performed better than I ve expected for input and output respectively  e g  captioning photos depicting complex scenes  beating humans at video games the software has never seen before with just the ability to look at the screen and access to the controls   I am always surprised how unimpressed most people seem with these results  Unsupervised learning has been a weaker point  and this is probably a critical part of replicating human intelligence  But many researchers I ve spoken to are optimistic about current work  and I have no reason to believe this is outside the scope of a Turing machine,"[1369 102 837 453 186 1284 44 646 516 1040 620]"
1412,training-dataset/engineering/1071.txt,engineering,Building Venice with Apache HelixBackground  Like many internet companies  LinkedIn has faced data growth challenges  Naturally  distributed storage systems became the solution to handle larger volumes of data and queries per second  QPS   But  aside from scaling issues  the variability in access patterns also grew quickly  For example  some scenarios require no more than simple put get operations but insist on having low latency and high availability  On the contrary  other scenarios require stronger consistency guarantees  more complex query patterns  and secondary indexes but are willing to suffer brief periods of unavailability in order to maintain these guarantees  There is no one size fits all system to satisfy all needs perfectly  so LinkedIn has built specialized distributed storage systems that each offer different trade offs in order to cover various scenarios   Now  the question that comes up is  do we build all of these systems from scratch  The answer is a resounding no  Even though scenarios vary  there are common elements that are essential to any distributed system  cluster management  storage engines  security  etc  Thus  we build generic libraries and leverage them to accelerate our development work  For example  Helix is one of those generic tools that helps to manage large clusters at LinkedIn  This post presents how Helix s cluster management capabilities have been leveraged in Venice  a distributed derived data serving platform   Cluster management in a distributed system  All distributed storage systems are clusters composed of a bunch of machines  While managing a cluster  there are some typical issues you see   Data partitioning  If the dataset does not fit on a single machine  how do we distribute partitions among machines as evenly as possible  More interestingly  after machines join or leave the cluster  partitions need to be re balanced across the remaining machines  All of those mappings need to be persisted somewhere in order to recover from cluster failures   Fault tolerance  As we know  the possibility that one machine in the cluster might fail increases as we add more machines to the cluster  In a large cluster  we should therefore expect machine failures to happen frequently  In order to avoid data loss  we need sufficient redundancy  a mechanism to detect failure  and an automatic failover process  Obviously  the cluster manager must also be careful not to place multiple replicas of the same dataset within the same failure domain  machine  rack  etc     Replica state management  As a stateful service  the storage system typically cannot start serving requests immediately after joining the cluster  because it first needs to bootstrap the latest version of its state  On the flip side  before retiring a server  we need to make sure there is enough redundancy in the remaining replicas  Meanwhile  the data replica hosted on the server may have complex states during its lifecycle  For example  in a traditional master slave model  one machine hosts the master replica and receives all writes  and possibly all reads too  if strong consistency is required   so a replica assigned to a new machine must start from the bootstrap state to get all data from the master  then its state changes to slave and it may serve read requests  if stale reads are allowed   Furthermore  a slave replica might be elected to become the new master of a partition if the current master crashes  Aside from making sure the server applies the replica s state change  it s also necessary to notify other servers and clients about that change  which can happen using a broadcast protocol or by updating a centralized metadata database   Introducing Helix  Helix is a generic framework that we use to manage clusters at LinkedIn  It was initially developed for Espresso  a distributed document based NoSQL database  Helix is widely used at LinkedIn  not only for distributed storage systems but also for streaming processes  search infrastructure  and analytics platforms   The core concept of Helix is a state model  which is a finite state machine  FSM   In this FSM  you describe the possible states and transitions between states of your resource  Helix will help you to trigger the proper state transitions in order to achieve the ideal state or rollback to the initial state when retiring a server,"[1412 551 620 1219 445 1284 449 706 186 646 1369]"
