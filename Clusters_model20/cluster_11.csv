X1,file_name,category,text,recommendations
0,training-dataset/engineering/912.txt,engineering,Uber s case for incremental processing on HadoopFor more on how Uber uses Hadoop and Spark  check out the Strata   Hadoop World NY 2016 session with Praveen Murugesan  Big data processing with Hadoop and Spark  the Uber way    Uber s mission is to provide  transportation as reliable as running water  everywhere  for everyone   To fulfill this promise  Uber relies on making data driven decisions at every level  and most of these decisions can benefit from faster data processing  For example  using data to understand areas for growth or accessing of fresh data by the city operations team to debug each city  Needless to say  the choice of data processing systems and the necessary SLAs are the topics of daily conversations between the data team and the users at Uber   In this post  I would like to discuss the choices of data processing systems for near real time use cases  based on experiences building data infrastructure at Uber as well as drawing from previous experiences  In this post  I argue that by adding new incremental processing primitives to existing Hadoop technologies  we will be able to solve a lot more problems  at reduced cost  and in a unified manner  At Uber  we are building our systems to tackle the problems outlined here and are open to collaborating with like minded organizations interested in this space   Near real time use cases  First  let s establish the kinds of use cases we are talking about  cases in which up to one hour latency is tolerable are well understood and mostly can be executed using traditional batch processing via MapReduce Spark  coupled with incremental ingestion of data into Hadoop S3  On the other extreme  cases needing less than one to two seconds of latency typically involve pumping your data into a scale out key value store  having worked on one at scale  and querying that  Stream processing systems like Storm  Spark Streaming  and Flink have carved out a niche of operating really well at practical latencies of around one to five minutes  and are needed for things like fraud detection  anomaly detection  or system monitoring basically  those decisions made by machines with quick turnaround or humans staring at computer screens as their day job   That leaves us with a wide chasm of five minute to one hour end to end processing latency  which I refer to in this post as near real time  Most such cases are either powering business dashboards and or aiding some human decision making  Here are some examples where near real time could be applicable   Observing whether something was anomalous across the board in the last x minutes  Gauging how well the experiments running on the website performed in the last x minutes  Rolling up business metrics at x minute intervals Extracting features for a machine learning pipeline in the last x minutes   Figure 1  Different shades of processing latency with the typical technologies used therein  Source  Vinoth Chandar   Incremental processing via  mini  batches  The choices to tackle near real time use cases are pretty open ended  Stream processing can provide low latency  with budding SQL capabilities  but it requires the queries to be predefined to work well  Proprietary warehouses have a lot of features  e g   transactions  indexes  and can support ad hoc and predefined queries  but such proprietary warehouses are typically limited in scale and are expensive  Batch processing can tackle massive scale and provides mature SQL support via Spark SQL Hive  but the processing styles typically involve larger latency  With such fragmentation  users often end up making their choices based on available hardware and operational support within their organizations  We will circle back to these challenges at the conclusion of this post   For now  I d like to outline some technical benefits to tackling near real time use cases via  mini  batch jobs run every x minutes  using Spark MR as opposed to running stream processing jobs  Analogous to  micro  batches in Spark Streaming  operating at second by second granularity    mini  batches operate at minute by minute granularity  Throughout the post  I use the term  incremental processing  collectively to refer to this style of processing   Increased efficiency  Incrementally processing new data in  mini  batches could be a much more efficient use of resources for the organization  Let s take a concrete example  where we have a stream of Kafka events coming in at 10K sec and we want to count the number of messages in the last 15 minutes across some dimensions  Most stream processing pipelines use an external result store  e g   Cassandra  ElasticSearch  to keep aggregating the count  and keep the YARN Mesos containers running the whole time  This makes sense in the less than five minute latency windows such pipelines operate on  In practice  typical YARN container start up costs tend to be around a minute  In addition  to scale the writes to the result stores  we often end up buffering and batching updates  and this protocol needs the containers to be long running   Figure 2  Comparison of processing via stream processing engines vs incremental  mini batch  jobs  Source  Vinoth Chandar   However  in the near real time context  these decisions may not be the best ones  To achieve the same effect  you can use short lived containers and improve the overall resource utilization  In the figure above  the stream processor performs 6M updates over 15 minutes to the result store  But in the incremental processing model  we perform in memory merge once and only one update to the result store  while using the containers for only five minutes  The incremental processing model is three times more CPU efficient  several magnitudes more efficient on updating of the result store  Basically  instead of waiting for work and eating up CPU and memory  the processing wakes up often enough to finish up pending work  grabbing resources on demand   Built on top of existing SQL engines  Over time  a slew of SQL engines have evolved in the Hadoop big data space  e g   Hive  Presto  SparkSQL  that provide better expressibility for complex questions against large volumes of data  These systems have been deployed at massive scale and hardened over time in terms of query planning  execution  and so forth  On the other hand  SQL on stream processing is still in early stages  By performing incremental processing using existing  much more mature SQL engines in the Hadoop ecosystem  we can leverage the solid foundations that have gone into building them   For example  joins are very tricky in stream processing  in terms of aligning the streams across windows  In the incremental processing model  the problem naturally becomes simpler due to relatively longer windows  allowing more room for the streams to align across a processing window  On the other hand  if correctness is more important  SQL provides an easier way to expand the join window selectively and reprocess   Another important advancement in such SQL engines is the support for columnar file formats like ORC Parquet  which have significant advantages for analytical workloads  For example  joining two Kafka topics with Avro records would be much more expensive than joining two Hive Spark tables backed by ORC Parquet file formats  This is because with Avro records  you would end up de serializing the entire record  whereas columnar file formats only read the columns in the record that are needed by the query  For example  if we are simply projecting out 10 fields out of a total 1 000 in a Kafka Avro encoded event  we still end up paying the CPU and IO cost for all fields  Columnar file formats can typically be smart about pushing the projection down to the storage layer   Figure 3  Comparison of CPU IO cost of projecting 10 fields out of 1000 total  as Kafka events vs  columnar files on HDFS  Source  Vinoth Chandar   Fewer moving parts  The famed Lambda architecture that is broadly implemented today has two components  speed and batch layers  usually managed by two separate implementations  from code to infrastructure   For example  Storm is a popular choice for the speed layer  and MapReduce could serve as the batch layer  In practice  people often rely on the speed layer to provide fresher  and potentially inaccurate  results  whereas the batch layer corrects the results of the speed layer at a later time  once the data is deemed complete  With incremental processing  we have an opportunity to implement the Lambda architecture in a unified way at the code level as well as the infrastructure level   Figure 4  Computation of a result table  backed by a fast view via incremental processing and a more complete view via traditional batch processing  Source  Vinoth Chandar   The idea illustrated in the figure above is fairly simple  You can use SQL  as discussed  or the same batch processing framework as Spark to implement your processing logic uniformly  The resulting table gets incrementally built by way of executing the SQL on the  new data  just like stream processing  to produce a  fast  view of the results  The same SQL can be run periodically on all of the data to correct any inaccuracies  remember  joins are tricky    to produce a more  complete  view of the results  In both cases  we will be using the same Hadoop infrastructure for executing computations  which can bring down overall operational cost and complexity   Challenges of incremental processing  Having laid out the advantages of an architecture for incremental processing  let s explore the challenges we face today in implementing this in the Hadoop ecosystem   Trade off  Completeness vs  latency  In computing  as we traverse the line between stream processing  incremental processing  and batch processing  we are faced with the same fundamental trade off  Some applications need all the data and produce more complete accurate results  whereas some just need data at lower latency to produce acceptable results  Let s look at a few examples   Figure 5  Figure showing different Hadoop applications on their tolerance for latency and completeness  Source  Vinoth Chandar   Figure 5 depicts a few sample applications  placing them according to their tolerance for latency and  in completeness  Business dashboards can display metrics at different granularities because they often have the flexibility to show more incomplete data at lower latencies for recent times while over time getting complete  which also made them the marquee use case for Lambda architecture   For data science machine learning use cases  the process of extracting the features from the incoming data typically happens at lower latencies  and the model training itself happens at a higher latency with more complete data  Detecting fraud  on one hand  requires low latency processing on the data available thus far  An experimentation platform  on the other hand  needs a fair amount of data  at relatively lower latencies  to keep results of experiments up to date   The most common cause for lack of completeness is late arriving data  as explained in detail in this Google Cloud Dataflow deck   In the wild  late data can manifest in infrastructure level issues  such as data center connectivity flaking out for 15 minutes  or user level issues  such as a mobile app sending late events due to spotty connectivity during a flight  At Uber  we face very similar challenges  as we presented at Strata   Hadoop World earlier this year   To effectively support such a diverse set of applications  the programming model needs to treat late arrival data as a first class citizen  However  Hadoop processing has typically been batch oriented on  complete  data  e g   partitions in Hive   with the responsibility of ensuring completeness also resting solely with the data producer  This is simply too much responsibility for individual producers to take on in today s complex data ecosystems  Most producers end up using stream processing on a storage system like Kafka to achieve lower latencies  while relying on Hadoop storage for more  complete   re processing  We will expand on this in the next section   Lack of primitives for incremental processing  As detailed in this article on stream processing  the notions of event time versus arrival time and handling of late data are important aspects of computing with lower latencies  Late data forces recomputation of the time windows  typically  Hive partitions in Hadoop   over which results might have been computed already and even communicated to the end user  Typically  such recomputations in the stream processing world happen incrementally at the record event level by use of scalable key value stores  which are optimized for point lookups and updates  However  in Hadoop  recomputing typically just means rewriting the entire  immutable  Hive partition  or a folder inside HDFS for simplicity  and recomputing all jobs that consumed that Hive partition   Both of these operations are expensive in terms of latency as well as resource utilization  This cost typically cascades across the entire data flow inside Hadoop  ultimately adding hours of latency at the end  Thus  incremental processing needs to make these two operations much faster so that we can efficiently incorporate changes into existing Hive partitions as well as provide a way for the downstream consumer of the table to obtain only the new changes   Effectively supporting incremental processing boils down to the following primitives   Upserts  Conceptually  rewriting the entire partition can be viewed as a highly inefficient upsert operation  which ends up writing way more than the amount of incoming changes  Thus  first class support for  batch  upserts becomes an important tool to possess  In fact  recent trends like Kudu and Hive Transactions do point in this direction  The Google Mesa paper also talks about several techniques that can be applied in the context of ingesting quickly   Incremental consumption  Although upserts can solve the problem of publishing new data to a partition quickly  downstream consumers do not know what data has changed since a point in the past  Typically  consumers learn this by scanning the entire partition table and recomputing everything  which can take a lot of time and resources  Thus  we also need a mechanism to more efficiently obtain the records that have changed since the last time the partition was consumed   With the two primitives above  you can support a lot of common use cases by upserting one data set and then incrementally consuming from it to build another data set incrementally  Projections are the most simple to understand  as depicted in Figure 6   Figure 6  Simple example of building of table_1 by upserting new changes  and building a simple projected_table via incremental consumption  Source  Vinoth Chandar   Borrowing terminology from Spark Streaming  we can perform simple projections and stream data set joins much more efficiently at lower latency  Even stream stream joins can be computed incrementally  with some extra logic to align windows   Figure 7  More complex example that joins a fact table against multiple dimension tables  to produce a joined table  Source  Vinoth Chandar   This is actually one of the rare scenarios where we could save money with hardware while also cutting down the latencies dramatically   Shift in mindset  The final challenge is not strictly technical  Organizational dynamics play a central role in which technologies are chosen for different use cases  In many organizations  teams pick templated solutions that are prevalent in the industry  and teams get used to operating these systems in a certain way  For example  typical warehousing latency needs are on the order of hours  Thus  even though the underlying technology could solve a good chunk of use cases at lower latencies  a lot of effort needs to be put into minimizing downtimes or avoiding service disruptions during maintenance  If you are building toward lower latency SLAs  these operational characteristics are essential  On the one hand  teams that solve low latency problems are extremely good at operating those systems with strict SLAs  and invariably the organization ends up creating silos for batch and stream processing  which impedes realization of the aforementioned benefits to incremental processing on a system like Hadoop   This is in no way an attempt to generalize the challenges of organizational dynamics  but is merely my own observation as someone who has spanned the online services powering LinkedIn as well as the data ecosystem powering Uber   Takeaways  I would like to leave you with the following takeaways   Getting really specific about your actual latency needs can save you tons of money  Hadoop can solve a lot more problems by employing primitives to support incremental processing  Unified architectures  code   infrastructure  are the way of the future   At Uber  we have very direct and measurable business goals incentives in solving these problems  and we are working on a system that addresses these requirements  Please feel free to reach out if you are interested in collaborating on the project,"[0 334 1289 638 1414 413 563 397 1331 743 934]"
2,training-dataset/product/919.txt,product,How designers can use data to create amazing workAt Springboard  we re privileged to sit at the unique intersection between data science and UX design  Having spoken with hundreds of industry experts in both fields  we ve come to believe that there s a lot of common ground between the 2 fields  Data scientists check user behaviors through analytics tools  while UX designers look to see what best suits the large numbers of users they serve   It s a pity that UX designers and data scientists rarely talk they can learn a lot from each other  Content that combines insights from data science to UX design or vice versa is immensely valuable  and scarcer than we like   This led us to think about the data science principles that are helpful for UX designers  and ways you could implement these rules into your day to day workflow  So here s what we came up with   Embrace metrics and think of the return on investment  Most UX designers aspire to build something better for their users   While that s a noble goal  how do you determine if something is truly  better   Metrics can help you align closer with business goals and showcase the return on investment your company or client gets from your efforts   The metrics that businesses care about vary  In some cases it s revenue  In others  it s on page conversion  time spent on a page  or number of photos uploaded   By embracing the metrics your business cares about and contextualizing why good UX design matters to those metrics  you not only make it easier for business leaders to justify putting more resources into UX  you also start prioritizing what initiatives make the most sense  Speaking to how better user engagement ties to company goals helps frame your work in a context that s familiar to non designers   How you can apply this to your day to day  Ask which metrics matter to your company or client  Deeply examine how your design goals will help improve those metrics and the company s bottom line  Then reconcile your work with the impact it drives on the bottom line   Key resources  UX designers working in data driven organizations should start by reading Dave McClure s ARRRR Startup Metrics framework to understand how companies measure growth at various stages  The book Lean Analytics is a must read to understand how good use of data works to track impact   Experiment  measure  and repeat  You ll often find yourself debating between many design treatments  How do you make a decision   Good news  You don t always have to decide you can let users tell you  And to do that  you ll want to take a page from the data scientist s approach of experimentation   Data scientists tend to think in terms of experiments they re usually pretty structured about stating their hypothesis for an experiment  what they wish to measure and learn  and how they want to run the experiment   This mentality might already be part of your workflow  but you should employ some tricks data scientists use to make sure they re on track with their experiments  State and record your hypotheses  For instance   I believe that version 2 will lead to 20  more users making a purchase than version one    Next  build the minimum viable version of each design   Then put your design in front of users and track their behavior to learn if your hypothesis is true  Make sure to consider the concept of statistical significance  whether you have enough observations to make a statistically accurate prediction    How you can apply this to your day to day  Get familiar with deciding when to incorporate data in your decisions  and when data doesn t matter  Learn how to use tools like Optimizely  Mixpanel  or Google Analytics that can easily help you see how your UX choices impact user behavior   Whenever you re thinking of testing a new design idea  record your hypotheses  For example  if you want to change the color scheme and typography to make instructional content easier to read  say that you re testing to see if making instructions easier to read in a certain way leads to more user engagement   Key resources  Here are some quick tips on how you can get started with Google Analytics  And here s a recent InVision Blog article on A B testing  a form of experimentation that can help you determine what version of a webpage performs best for certain business goals   Segment your users and learn how they re different  You re likely already creating user personas that map onto the most common profiles of a company s users   Broad quantitative data from tools such as Google Analytics  which captures demographic  behavioral  and device data  can help flesh out those user personas in a way that s richer and ties more closely with users  actual behavior   With Google Analytics  you can quickly determine that older users may have different expectations from your site than younger ones  Males and females may respond differently to certain color schemes  You can even get a rough view of what interests your users have and how they might react differently accordingly   Don t stop at explicit data  either  Data scientists often infer data with clever techniques  They might map user first names to a database of the most common female and male first names to come up with a pretty precise picture of the gender split in a dataset  If there s data you need to flesh out for a persona  think about how you could get it  or work with technical team members to get it done   How you can apply this to your day to day  Ask about quantitative data that can help define which of your personas are the most common and which can flesh out more of who they are  Be thorough about who you re designing for  and learn how to segment your users  Ask for as much data as possible  any surveys people have collected around your target groups  and look as deeply as possible about who your users really are with a mix of interviews and a look at the quantitative data   Key resources  Read about how Google Analytics can supplement your audience data so you know what s possible  and read about what data scientists think are the most important ways you can segment users   Personalize at scale  You may have heard the term machine learning  One thing that machine learning can enable is a personalized experience to as many people as possible with the power of automation and algorithms   How does Amazon know exactly which book you might like to read next  It s through the power of machine learning algorithms  By using these algorithms to infer users  preferences  e g  assign a probability to how likely you are to find a book enjoyable   Amazon delights users by consistently providing them with something the company knows they ll be interested in   Don t lose a chance to delight your users by personalizing experiences and storytelling with the data they give you   Data scientists apply algorithms at scale to ensure that every user gets a personalized experience  They use complex mathematics and programming to do that  You can capture some of that magic by using tools like Visual Website Optimizer to offer different user experiences for different groups  The classic example of this is localization  Having your application load in a user s preferred language can mean a world of difference for them   How to implement this in your day to day  Understand how user experiences can be personalized and where the biggest benefits are  Use your understanding of different user groups and personas to build experiments so that every user walks away delighted   Key resources  Understand how to design for personalization but do it without being creepy   Get and understand feedback at scale  You ll likely rely heavily on one on one interviews and focus groups  While these are great at providing a rich context of how a few users feel  you can supplement that qualitative feedback with quantitative sources that measure users at scale   For instance  you can use user surveys to supplement your user research interviews  Similarly  you can use actual usage data to supplement user testing interviews   By combining both deep qualitative and quantitative insights about how users are truly interacting with your product  you can get a complete view of how users are reacting to your design and how you can improve it   How to implement this in your day to day  Use tools like Qualaroo to ask users questions when they re visiting your site  Use Hotjar to track heatmaps and on page analytics of how your users move through your app  Use tools like Promoter io to get an overall view of how users like your product at scale   Key resources  Read about the one number you need to grow  the Harvard Business Review article that introduced Net Promoter Score as the gold standard for measuring user satisfaction  Then delve into how you can analyze and visualize user feedback   Think of data as a tool you can use to sharpen your instincts about your users and what they like  It s a superpower that can help you create rich  immersive  and personalized experiences that will be maximally useful for your users   Suhail Doshi  the young founder of analytics powerhouse Mixpanel  once opined in a New Yorker feature that  most of the world will make decisions by either guessing or using their gut  They will be either lucky or wrong    Your commitment to using data to delight your users is an utter commitment to being right rather than lucky   Further reading  Check out Springboard s free 16 page guide to UX careers that can help you advance your UX skills,"[2 740 254 397 1331 1139 563 1107 668 334 413]"
59,training-dataset/engineering/848.txt,engineering,Powering Transactions Search with Elastic   Learnings from the FieldIntroduction  We see a lot of transactions at PayPal  Millions every day   These transactions originate externally  a customer using PayPal to pay for a purchase on a website  as well as internally  as money moves through our system  Regular reports of these transactions are delivered to merchants in the form of a csv or a pdf file  Merchants use these reports to reconcile their books   Recently  we set out to build a REST API that could return transaction data back to merchants  We also wanted to offer the capability to filter on different criteria such as name  email or transaction amount  Support for light aggregation insight use cases was a stretch goal  This API would be used by our partners  merchants and external developers   We choose Elastic as the data store  Elastic has proven  over the past 6 years  to be an actively developed product that constantly evolves to adapt to user needs  With a great set of core improvements introduced in version 2 x  memory mapped doc values  auto regulated merge throttling   we didn t need to look any further   Discussed below is our journey and key learnings we had along the way  in setting up and using Elastic for this project   Will It Tip Over  Once live  the system would have tens of terabytes of data spanning 40  billion documents  Each document would have over a hundred attributes  There would be tens of millions of documents added every day  Each one of the planned Elastic blades has 20TB of SSD storage  256 GB RAM and 48 cores  hyper    While we knew Elastic had all the great features we were looking for  we were not too sure if it would be able to scale to work with this volume  and velocity  of data  There are a host of non functional requirements that arise naturally in financial systems which have to be met  Let s limit our discussion in this post to performance   response time to be specific   Importance of Schema  Getting the fundamentals right is the key   When we initially setup Elastic  we turned on strict validation of fields in the documents  While this gave us a feeling of security akin to what we re used to with relational systems  strict field and type checks   it hurt performance   We inspected the content of the Lucene index Elastic created using Luke  With our initial index setup  Elastic was creating sub optimal indexes  For e g  in places where we had defined nested arrays  marked index  no    Elastic was still creating child hidden documents in Lucene  one per element in the array  This document explains why  but it was still using up space when we can t even query the property via the index  Due to this  we switched the  dynamic  index setting from strict to false  Avro helped ensure that the document conforms to a valid schema  when we prepared the documents for ingestion   A shard should have no more than 2 billion parent plus nested child documents  if you plan to run force merge on it eventually  Lucene doc_id is an integer   This can seem high but is surprisingly easy to exceed  especially when de normalizing high cardinality data into the source  An incorrectly configured index could have a large number of hidden Lucene documents being created under the covers   Too Many Things to Measure  With the index schema in place  we needed a test harness to measure the performance of the cluster  We wanted to measure Elastic performance under different load conditions  configurations and query patterns  Taken together  the dimensions total more than 150 test scenarios  Sampling each by hand would be near impossible  jMeter and Beanshell scripting really helped here to auto generate scenarios from code and have jMeter sample each hundreds of times  The results are then fed into Tableau to help make sense of the benchmark runs   Indexing Strategy 1 month data per shard  1 week data per shard  1 day data per shard    of shards to use  Benchmark different forms of the query  constant score  filter with match all etc    User s Transaction Volume Test with accounts having 10   100   1000   10000   1000000 transactions per day  Query time range 1   7   15   30 days  Store source documents in Elastic  Or store source in a different database and fetch only the matching IDs from Elastic   Establishing a Baseline  The next step was to establish a baseline  We chose to start with a single node with one shard  We loaded a month s worth of data  2 TB    Tests showed we could search and get back 500 records from across 15 days in about 10 seconds when using just one node  This was good news since it could only get better from here  It also proves an Elastic  Lucene segments  shard can handle 2 billion documents indexed into it  more than what we ll end up using   One take away was high segment count increases response time significantly  This might not be so obvious when querying across multiple shards but is very obvious when there s only one  Use force merge if you have the option  offline index builds   Using a high enough value for refresh_interval and translog flush_threshold_size enables Elastic to collect enough data into a segment before a commit  The flip side was it increased the latency for the data to become available in the search results  we used 4GB   180 seconds for this use case   We could clock over 70 000 writes per second from just one client node   Nevertheless  data from the recent past is usually hot and we want all the nodes to chip in when servicing those requests  So next  we shard   Sharding the Data  The same one month s data  2 TB  was loaded onto 5 nodes with no replicas  Each Elastic node had one shard on it  We choose 5 nodes to have a few unused nodes  They would come in handy in case the cluster started to falter and needed additional capacity  and to test recovery scenarios  Meanwhile  the free nodes were used to load data into Elastic and acted as jMeter slave nodes   With 5 nodes in play   Response time dropped to 6 seconds  40  gain  for a query that scanned 15 days  Filtered queries were the most consistent performers  As a query scanned more records due to an increase in the date range  the response time also grew with it linearly  A force merge to 20 segments resulted in a response time of 2 5 seconds  This showed a good amount of time was being spent in processing results from individual segments  which numbered over 300 in this case  While tuning the segment merge process is largely automatic starting with Elastic 2 0  we can influence the segment count  This is done using the translog settings discussed before  Also  remember we can t run a force merge on a live index taking reads or writes  since it can saturate available disk IO  Be sure to set the  throttle max_bytes_per_sec  param to 100 MB or more if you re using SSDs  the default is too low  Having the source documents stored in Elastic did not affect the response time by much  maybe 20ms  It s surely more performant than having them off cluster on say Couchbase or Oracle  This is due to Lucene storing the source in a separate data structure that s optimized for Elastic s scatter gather query format and is memory mapped  see fdx and fdt files section under Lucene s documentation   Having SSDs helped  of course  Final Setup  The final configuration we used had 5 9 shards per index depending on the age of the data  Each index held a week s worth of data  This got us a reasonable shard count across the cluster but is something we will continue to monitor and tweak  as the system grows   We saw response times around the 200 ms mark to retrieve 500 records after scanning 15 days  worth of data with this setup  The cluster had 6 months of data loaded into it at this point   Shard counts impact not just read times  they impact JVM heap usage due to Lucene segment metadata as well as recovery time  in case of node failure or a network partition  We ve found it helps Elastic during rebalance if there are a number of smaller shards rather than a few large ones   We also plan to spin up multiple instances per node to better utilize the hardware  Don t forget to look at your kernel IO scheduler  see hardware recommendations  and the impact of NUMA and zone reclaim mode on Linux   Conclusion  Elastic is a feature rich platform to build search and data intensive solutions  It removes the need to design for specific use cases  the way some NoSQL databases require  That s a big win for us as it enables teams to iterate on solutions faster than would ve been possible otherwise   As long as we exercise due care when setting up the system and validate our assumptions on index design  Elastic proves to be a reliable workhorse to build data platforms on,"[59 1289 563 1382 334 823 397 831 668 934 855]"
88,training-dataset/engineering/24.txt,engineering,Designing Euclid to Make Uber Engineering Marketing Savvyby Mrina Natarajan   Hohyun Shim  Fast  granular  reliable ROI on ad performance was our bugle call to build Euclid  Uber s in house marketing platform  Early this year  Euclid replaced a legacy system  which processed ROI data somewhat manually as it struggled to keep up with Uber s scale and data complexity   Unlike any solution out of the box  the Hadoop  Spark based Euclid ecosystem lets us scale for Uber s growth with a channel agnostic API plugin architecture called MaRS as well as a custom ETL pipeline that streams heterogenous data into a single schema for easy querying  A visual layer on top of Euclid lets marketers pull ROI metrics to optimize ad spend  Euclid s pattern recognition capabilities powers marketing intelligence further  With these capabilities  marketers can detect fraud  automatically allocate ad budget  by channel country city product   automate ad spend  manage campaigns  target audience profiles at a granular level  and bid on ads real time   Problem Solving with Euclid  The reason Euclid is what it is today is because of long standing issues we addressed in the marketing data world   Automating ad spend   To do away with collecting and processing data manually  we needed a system to not only analyze data reliably at high speeds  but show spending at event specific granular levels such as impressions  clicks  installs from hundreds of external ad networks all over the world   Dealing with data complexity   Data at Uber comes from a hundred plus channels per ad network each with ten thousand plus campaigns  millions of keywords  and many creatives  Throw in multiple currencies and various Uber product types on top of that  Besides scale  data variety too plays a part different kinds of ads  social  display  search  job boards  and so on  result in complex schema and data mapping  When delivering complex data at such scale and volume  SLA matters  We have to ensure we provide data at promised intervals on an hourly  daily basis   Reporting accurate ROI   In the past  teams measured channel level ROI with time lags  Euclid s challenge lay in not just closing the lag but also measuring ROI at a granular level at the ad  creative and keyword level for example  To get there  we needed to join internal with external data sources and apply predictive analytics to report future trends and multi touch attribution   Euclid Architecture  The Euclid system has three main parts  MaRS  Marketing Report Service   an email service  and an ETL data pipeline   MaRS  Marketing Report Service   A plugin based report ingestion microservice  MaRS is the reason we can scale fast  up to 80 channels in just a couple of months  Before Euclid  marketing managers manually generated high level weekly spend data for hundreds of channels around the globe  Data was nongranular  it had the potential for human error  and marketing teams could not act fast on spending goals nor optimize ad spend accurately   MaRS solved this problem with a standard API interface to import ad campaign data via plugins  The plugins isolate ad network logic from the rest of the ETL data pipeline  This isolation lets us develop and test ad network dependent logic independently from the Hadoop ETL pipeline  The ETL data pipeline calls the MaRS API  it passes a network ID as a parameter to get various ad spend data without knowing any ad network specific logic   Such a design gives us the advantage of outsourcing plugin development to external vendors who can add plugins for many more ad networks  It s one of the ways of how we scaled the number of API channel imports for Euclid  Currently  Euclid supports 30  plugins including Facebook  Twitter  and AdWords   It s straightforward to implement a plugin  All an external plugin developer needs to do is inherit three base classes  define the Avro schema  and apply a normalization rule   Auth class  Handles network API authentication   Extractor class  Extracts data through an ad network API   Transform class  Converts the API data format into Avro   Avro schema  Stores the raw ad network data schema   Normalization rule  Applies a mapping rule to convert fields from the raw Avro schema to a uniform model table   Engineers implement plugins independently without worrying about underlying changes to the service environment or data infrastructure  The config based design of the Hadoop Hive Pipeline combined with the API plugin based MaRS architecture  allows us to add new channel plugins rapidly without making code changes   Euclid Email Service  For channels that lack APIs  Euclid provides an email attachment based ingestion framework  Using this system  ad channels can directly send CSV attachments in an expected format  Included in the MaRS architecture  the service allows ad networks to push their daily spend as email attachments to a predefined collection point  Euclid then automatically validates and lands the data into the ETL pipeline  This is how Euclid imports data from dozens of small channels every day   ETL Data Pipeline Workflow  Euclid s ETL data pipeline  which is Hive based  uses a workflow management tool to extract  normalize  and join ingested data   Extractor  Requests MaRS to ingest data from the ad network APIs   Normalizer  Normalizes ad network specific raw spend data into a unified Hive table called fact_marketing_spend_ad   ROI joiner  Joins spend with conversion data such as user level signups and first trips to get granular ROI metrics   With MaRS handling the ad network logic  the channel agnostic ETL pipeline lets us engineer Uber marketing data at scale while minimizing costs to operate and maintain the pipeline  Since each channel differs in its campaign and report schema  it s hard to query and analyze data directly  But the config based normalization step in Euclid aligns heterogeneous data in various Avro schema into a single model table in Parquet  which makes the data easy to aggregate  compare  and analyze across dimensions and slices   That said  daily ingestion is still a tricky process given varying data schema and readiness SLAs  For example  APIs can break when a version deprecates  when credentials or the upstream channel data format changes  or if upstream data gets corrupt  To get around this problem  Euclid s custom MySQL monitoring and alerting does all kinds of data health checks  Its automated backfill policy  failure logging  and on call alerting pinpoint the cause of data integrity issues  In this way  the Euclid pipeline ingests hundreds of millions of records daily into Hadoop   Before and After Euclid Optimized Uber Marketing ROI  Granular ROI Ad M etrics  Besides standard acquisition metrics  the cost of a user making their first Uber trip is an important ad performance ROI metric  Previously  when teams measured ad ROI there were time gaps  Now with Euclid  marketers access granular ROI ad data quickly and reliably  Powered by Euclid s hierarchical campaign spend dataset  marketers drill down deeper   to the creative level for display ads on social channels  to the keyword level for search channels  to the job board level for job board channels  and so forth   Here are three display ad creatives marketers tested for the same campaign  Which ad do you think performed better   To see which ad results in a better click through rate  CTR   marketers drill down to the creative level performance data via Euclid s visualization layer  They see ROI granular spending data Euclid ingested and joined with user conversion metrics  In this example  the zoomed in version  Variant B  with its significantly higher CTR works better than either the Control version or the zoomed out creative  Variant A    Given thousands of ads running across hundreds of channels  where external agencies maintain many of them  it s humanly impossible to check the status of every single ad  For such cases  Euclid lets Uber channel managers easily surface underperforming ads   Predicting First Trips  The conversion journey from Uber app install to first trip usually involves time gaps  and if we don t know when a first trip occurs we can t attribute a particular marketing activity as the motivating trigger  Yesterday s ad spend could result in clicks or installs  but not drive signups or first trips yet  How then can we measure marketing ROI  After Euclid ingests data from impressions  clicks  and installs  it applies patterns of where and when partners and riders sign up  types of device they use  ad channels they engage in  and so on  Mapping this type of information to historical conversion data  Euclid statistically predicts when and whether a user is likely to make their first trip  As Euclid continues to accumulate more data into its training models in this fashion  prediction accuracy increases   Solving Multi touch Attribution  When a user sees a social network ad  then searches  Uber  on the web  clicks a search ad  and later signs up with Uber  it s not fair to give all the credit to the search ad  Multi touch attribution  another challenge Euclid tackles  involves looking at the impression level data  analyzing the complete user conversion journey  and then attributing the right weight of credit for each conversion to multiple channels  Clear multi touch attribution allows marketers to allocate the right budget to the right channels  Euclid ingests impression level data every day  then analyzes and trains multi touch attribution models using its predictive engine  Powered by Hadoop and Spark  Euclid soon plans to deploy marketing multi touch attribution models in the production data pipeline   What s Next  What we ve developed so far in the Euclid tech stack has helped marketers understand which creatives  search keywords  and ad campaigns work best for a particular channel  We re looking to make the next incarnation of Euclid an even more advanced marketing engine involving data management platforms  DMP  and demand side platforms  DSP   So if you have experience with Hadoop  Hive  Spark  Kafka  Vertica  or an understanding of SQL databases to implement ETL pipelines  check out the engineering talent we re looking to hire to further Euclid s development  Come be part of our story   Hohyun Shim  software engineer in Uber Engineering s business platform group who leads Euclid s architecture  wrote this article in conjunction with Mrina Natarajan,"[88 397 334 934 1331 823 0 1414 413 1155 1139]"
184,training-dataset/engineering/570.txt,engineering,Online migrations at scaleEngineering teams face a common challenge when building software  they eventually need to redesign the data models they use to support clean abstractions and more complex features  In production environments  this might mean migrating millions of active objects and refactoring thousands of lines of code   Stripe users expect availability and consistency from our API  This means that when we do migrations  we need to be extra careful  objects stored in our systems need to have accurate values  and Stripe s services need to remain available at all times   In this post  we ll explain how we safely did one large migration of our hundreds of millions of Subscriptions objects   Why are migrations hard   Scale Stripe has hundreds of millions of Subscriptions objects  Running a large migration that touches all of those objects is a lot of work for our production database  Imagine that it takes one second to migrate each subscription object  in sequential fashion  it would take over three years to migrate one hundred million objects   Uptime Businesses are constantly transacting on Stripe  We perform all infrastructure upgrades online  rather than relying on planned maintenance windows  Because we couldn t simply pause the Subscriptions service during migrations  we had to execute the transition with all of our services operating at 100    Accuracy Our Subscriptions table is used in many different places in our codebase  If we tried to change thousands of lines of code across the Subscriptions service at once  we would almost certainly overlook some edge cases  We needed to be sure that every service could continue to rely on accurate data   A pattern for online migrations  Moving millions of objects from one database table to another is difficult  but it s something that many companies need to do   There s a common 4 step dual writing pattern that people often use to do large online migrations like this  Here s how it works   Dual writing to the existing and new tables to keep them in sync  Changing all read paths in our codebase to read from the new table  Changing all write paths in our codebase to only write to the new table  Removing old data that relies on the outdated data model   Our example migration  Subscriptions  What are Subscriptions and why did we need to do a migration   Stripe Subscriptions helps users like DigitalOcean and Squarespace build and manage recurring billing for their customers  Over the past few years  we ve steadily added features to support their more complex billing models  such as multiple subscriptions  trials  coupons  and invoices   In the early days  each Customer object had  at most  one subscription  Our customers were stored as individual records  Since the mapping of customers to subscriptions was straightforward  subscriptions were stored alongside customers   class Customer Subscription subscription end  Eventually  we realized that some users wanted to create customers with multiple subscriptions  We decided to transform the subscription field  for a single subscription  to a subscriptions field allowing us to store an array of multiple active subscriptions   class Customer array  Subscription subscriptions end  As we added new features  this data model became problematic  Any changes to a customer s subscriptions meant updating the entire Customer record  and subscriptions related queries scanning through customer objects  So we decided to store active subscriptions separately   As a reminder  our four migration phases were   Dual writing to the existing and new tables to keep them in sync  Changing all read paths in our codebase to read from the new table  Changing all write paths in our codebase to only write to the new table  Removing old data that relies on the outdated data model   Let s walk through these four phases looked like for us in practice   Part 1  Dual writing  We begin the migration by creating a new database table  The first step is to start duplicating new information so that it s written to both stores  We ll then backfill missing data to the new store so the two stores hold identical information   In our case  we record all newly created subscriptions into both the Customers table and the Subscriptions table  Before we begin dual writing to both tables  it s worth considering the potential performance impact of this additional write on our production database  We can mitigate performance concerns by slowly ramping up the percentage of objects that get duplicated  while keeping a careful eye on operational metrics   At this point  newly created objects exist in both tables  while older objects are only found in the old table  We ll start copying over existing subscriptions in a lazy fashion  whenever objects are updated  they will automatically be copied over to the new table  This approach lets us begin to incrementally transfer our existing subscriptions   Finally  we ll backfill any remaining Customer subscriptions into the new Subscriptions table   The most expensive part of backfilling the new table on the live database is simply finding all the objects that need migration  Finding all the objects by querying the database would require many queries to the production database  which would take a lot of time  Luckily  we were able to offload this to an offline process that had no impact on our production databases  We make snapshots of our databases available to our Hadoop cluster  which lets us use MapReduce to quickly process our data in a offline  distributed fashion   We use Scalding to manage our MapReduce jobs  Scalding is a useful library written in Scala that makes it easy to write MapReduce jobs  you can write a simple one in 10 lines of code   In this case  we ll use Scalding to help us identify all subscriptions  We ll follow these steps   Write a Scalding job that provides a list of all subscription IDs that need to be copied over   Run a large  multi threaded migration to duplicate these subscriptions with a fleet of processes efficiently operating on our data in parallel   Once the migration is complete  run the Scalding job once again to make sure there are no existing subscriptions missing from the Subscriptions table   Part 2  Changing all read paths  Now that the old and new data stores are in sync  the next step is to begin using the new data store to read all our data   We need to be sure that it s safe to read from the new Subscriptions table  our subscription data needs to be consistent  We ll use GitHub s Scientist to help us verify our read paths  Scientist is a Ruby library that allows you to run experiments and compare the results of two different code paths  alerting you if two expressions ever yield different results in production  With Scientist  we can generate alerts and metrics for differing results in real time  When an experimental code path generates an error  the rest of our application won t be affected   We ll run the following experiment   Use Scientist to read from both the Subscriptions table and the Customers table   If the results don t match  raise an error alerting our engineers to the inconsistency   After we verified that everything matched up  we started reading from the new table   Part 3  Changing all write paths  Next  we need to update write paths to use our new Subscriptions store  Our goal is to incrementally roll out these changes  so we ll need to employ careful tactics   Up until now  we ve been writing data to the old store and then copying them to the new store   We now want to reverse the order  write data to the new store and then archive it in the old store  By keeping these two stores consistent with each other  we can make incremental updates and observe each change carefully   Refactoring all code paths where we mutate subscriptions is arguably the most challenging part of the migration  Stripe s logic for handling subscriptions operations  e g  updates  prorations  renewals  spans thousands of lines of code across multiple services   The key to a successful refactor will be our incremental process  we ll isolate as many code paths into the smallest unit possible so we can apply each change carefully  Our two tables need to stay consistent with each other at every step   For each code path  we ll need to use a holistic approach to ensure our changes are safe  We can t just substitute new records with old records  every piece of logic needs to be considered carefully  If we miss any cases  we might end up with data inconsistency  Thankfully  we can run more Scientist experiments to alert us to any potential inconsistencies along the way   Our new  simplified write path looks like this   We can make sure that no code blocks continue using the outdated subscriptions array by raising an error if the property is called   class Customer def subscriptions hard_assertion_failed   Accessing subscriptions array on customer    end end  Part 4  Removing old data  Our final  and most satisfying  step is to remove code that writes to the old store and eventually delete it   Once we ve determined that no more code relies on the subscriptions field of the outdated data model  we no longer need to write to the old table   With this change  our code no longer uses the old store  and the new table now becomes our source of truth   We can now remove the subscriptions array on all of our Customer objects  and we ll incrementally process deletions in a lazy fashion  We first automatically empty the array every time a subscription is loaded  and then run a final Scalding job and migration to find any remaining objects for deletion  We end up with the desired data model   Conclusion  Running migrations while keeping the Stripe API consistent is complicated  Here s what helped us run this migration safely   We laid out a four phase migration strategy that would allow us to transition data stores while operating our services in production without any downtime   We processed data offline with Hadoop  allowing us to manage high data volumes in a parallelized fashion with MapReduce  rather than relying on expensive queries on production databases   All the changes we made were incremental  We never attempted to change more than a few hundred lines of code at one time   All our changes were highly transparent and observable  Scientist experiments alerted us as soon as a single piece of data was inconsistent in production  At each step of the way  we gained confidence in our safe migration   We ve found this approach effective in the many online migrations we ve executed at Stripe  We hope these practices prove useful for other teams performing migrations at scale   Like this post  Join the Stripe engineering team  View Openings,"[184 428 734 413 845 334 563 823 1289 0 397]"
193,training-dataset/business/975.txt,business,Making MRR Actionable  How We re Fine Tuning Our Financial ModelKey Stats  Team members 79  Revenue in cash receipts  996k  1 2   MRR  987k  4 6   ARR  11 8m  5 0   Debits  854 5k  6 1   Profit  122k  1 9   We have  1 56m in the bank as of September 30  which is a good signal  We continue to be cashflow positive  and plan to remain as such   In September  we ve worked hard on a few topics  especially improving at forecasting our cash planning  Here is a bit more about it all   Short term cash planning  New focus on cash receipts  First  we have a new focus on  cash receipts  for a given month or the record of payment for the sale of a Buffer subscription to have higher degree of certainty on where revenue comes from   The goal here is to know how much money hits the bank on a given month  It s an addition to several metrics we keep close track of  together with revenue  on accrual basis and MRR   These are the cash receipts for August and September   With Stripe such an important part of our revenue  we took it a step further with this source  I went back to match all transfers from Stripe to a calendar day   What we re looking for here is an accurate view of what happened over the last few month   Exporting our Stripe data was very helpful here   we tracked down our daily transfers  including those that have a 1  to   business day delay  and we can also check which ones went through versus those that are pending   As a result  we re managing to get more granular results for our forecast  and using more data points for forecasting revenue   Long term cash planning  Understanding unit economics  Buffer has been lucky to grow through content marketing  word of mouth and our freemium model over the last few years  We don t have a sales team  nor do we have a paid acquisition strategy   As a consequence  we haven t had the most granular metrics in place in terms of Unit Economics trying to account for every customer in every plan  and how those fluctuate over time  Startups often focus on this to answer questions such as    How many users churned from Buffer for Business v1 plan in August  Does it help forecast how many may churn in September     What is the lifetime value of a Buffer Awesome yearly subscriber  How much can we pay in social media advertising to get more of them     What is the seasonality of business  in terms of subscriptions  How can we hire and staff customer support accordingly   With the assistance of the SurePath Capital team  we have started to pay more attention to Unit Economics  Accounting for our customers properly is one of the first steps to build a successful forecasting model   We re now backfilling data on our largest six plans by volume in a model so that we can estimate fluctuations more accurately  This helps break the metric of MRR into to actionable insights   Here are some definitions we ve been using to be very explicit about what we are looking at  in terms of MRR   This is an ongoing effort  and our friends over at Baremetrics have been kind enough to provide us with a lot of data and insights plugged in from Stripe  in addition to the data we get from Looker to perfect our understanding of MRR and all of its components   We also hope to get into scenario planning  with best and worst case scenarios to help us plan for the future in either extreme   Other items of the formula we d like to explore include  past due subscription  refunds and cost of sales on a per plan basis  This feels like a great challenge to keep improving on   The first steps of organizing our next retreat  Exciting news   we ve started to plan for our next retreat  Last time we met  it was in February 2016 where the entire team headed down to Hawaii   We ve shared before how valuable our in person retreats are for collaboration and building deeper relationships  and we ve started thinking about how to continue connecting as a team  while being mindful of our spending   Our last meetup in Hawaii cost around  420k  We decided to cancel our last retreat for financial reasons  but it was initially planned to happen in Berlin  Germany in August 2016 with a  400k budget   Right now  we re planning to spend a bit less on our upcoming retreat  though planning is still in the early stages   Our first order of business is to check on teammates  availability  Here s a look at the survey we shared this month   In early October  we will submit a set of 5 cities for all team members to vote on   Over to you  Is there anything you d love to learn more about  Anything we could share more of  We d love to hear from you in the comments   Check out other reports from September,"[193 385 1139 184 2 397 668 374 1331 254 1414]"
254,training-dataset/product/1422.txt,product,How design thinking can inform data problemsThere is no questioning the usefulness of data in design  Data helps us understand our customers  behavior  something that s fundamental to good design  But the link between data and design can be a two way street   Our design principles underpin our approach to problem solving and help us make sure we are designing the best solutions for our customers   And although these principles were crafted with the design team in mind  we have started to see great value in applying them to problems right across the company  For the analytics team  those are data problems   Here s how I put three of them   designing from first principles  design in systems  not pages  and value reusage   to work on some recent analytics projects   Design from first principles  One of the biggest challenges for an analytics team is democratizing data across a company  you want everyone in your organisation to easily access data  and for that data to be structured in a way that maps to their understanding of how the product works   Being able to think in systems is not just important for designers  it s crucial for anyone working on complex problems   Initially  the way we described our product was very different to how our data was structured  making it confusing and difficult to understand for anyone who wasn t on the analytics team  The easiest  quickest fix would have been to iterate on the data we already had  But  designing from first principles  meant exploring the idea that you should never iterate out from the middle of a problem  We advocate starting at the foundational level  building up the solution one clear piece at a time  This means you re not constrained by the way things currently are   Rather than patching up the most confusing aspects of our data model  I began by thinking deeply about the best way to structure it so it aligns to the way we talk about our product  I built upon work done by our research team to define a clear system model of how Intercom works  Having this clarity made it easier to begin transforming our current data  because there was no ambiguity as to what we were aiming for  The result  Getting a step closer to data that is democratic  and easier for everyone across the company to understand   When time is tight  it can be tempting to hack together a quick fix to a problem that is either based on your current way of solving problems or the standard industry approach  But before long you will end up with a solution comprised of many quick fixes messily patched together  Whether the problem is rooted in design or data  it s important to check that you aren t simply defaulting to a known solution  but that you are thinking thoughtfully about the best possible solution   Design systems  not pages  Systems thinking is an essential design skill  It means understanding how components interact with each other   designers at Intercom don t design in a silo  they need to consider how their work connects our product together   Anyone working to solve complex problems is in essence a designer   But being able to think in systems is not just important for designers  it s crucial for anyone working on complex problems  Thinking of   or better still  drawing   the system forces you to think holistically  to consider interdependencies and the impact even one change can have   That  system  can be your product  your company  your codebase or even your physical workspace   When we started overhauling our analytics database  we physically drew out our infrastructure system  This allowed us to understand the order of certain processes and how changes would affect different teams  We could clearly see dependencies early on in the project and be thoughtful about the best way to add new tasks and processes into our current system   Value reusage  One of our guiding principles is about valuing consistency over change for change s sake  This encourages you to work in a way that allows your teammates to recycle and adapt your work and it s also why things like pattern libraries are so important   Within our analytics team  we found the exact same principle applies  But instead of colours  fonts and UX components  the reusable objects are queries  metrics and data definitions  Data definitions happen once in the codebase  and further logic is built from these reusable building blocks  This makes the codebase both easy to maintain   if a definition needs to be updated it only needs to happen in one place   and unambiguous  as there is only one definition for each table  field or metric   Design is not simply about how something looks  but a holistic approach to solving problems  Anyone working to solve complex problems is in essence a designer  Having a set of considered  guiding principles that reflect how you aspire to work can provide an invaluable framework not just in design or analytics but across the whole company,"[254 2 1331 397 563 428 865 740 1139 334 823]"
330,training-dataset/engineering/364.txt,engineering,Barcode recovery using a priori constraintsIf you ever connected to the Internet before the 2000s  you probably remember that it made a peculiar sound  But despite becoming so familia,"[521 330 1139 413 668 59 1226 1348 855 638 334]"
331,training-dataset/engineering/548.txt,engineering,Visualize Data Sets on the Web with Uber Engineering s deck gl Frameworkby Nicolas Garcia Belmonte  No matter what you work on  data drives decisions these days  At Uber  we like to see this data move in order to really understand it  Now anyone with a big data set can do the same on the Web   Today  we open sourced deck gl  a WebGL powered framework specifically designed for exploring and visualizing data sets at scale  deck gl lets us visualize it without compromises   Data  Visualized Fast  While deck gl s architecture suits both abstract and scientific visualization needs  map related data is our biggest asset at Uber  We ve stretched and tested deck gl in many different mapping environments prior to this first release   At Uber  the many GPS points we process every day serves needs across the company  If we have a problem in our platform  we can diagnose it by exploring historical geolocation data  If there s an accident on the road  we can explore GPS traces for a given trip to get full context  If there are pain points around pickup locations in a city  we can communicate plans to city authorities by visualizing them  We need this data to be web based  real time  and shareable so other teams at Uber can use it  To meet all these needs  we developed deck gl   deck gl focuses on the following key aspects for users   Performance   Get high performance rendering of large data sets  millions of points or vertices   including features like on the fly aggregation and visual exploration  based on latest WebGL technologies   Accuracy  Achieve high precision numerical computations on the GPU  thanks to our custom fp64 math library  To our knowledge  no WebGL based library currently provides this functionality  which is crucial for full interactivity of geo data sets   Extensibility   Use the latest coding standards  including ES2016   and a rich ecosystem of libraries and settings that enable easy debugging and profiling of WebGL applications  What deck gl Offers to Mapping Use Case  deck gl s set of features adapts to a wide range of use cases  including mapping  As an example  deck gl can be used in conjunction with MapboxGL s camera system  so data can be optionally displayed on top of a Mapbox map and visualized in perspective mode  You can also jump start any data visualization project with deck gl s set of core layers  including the scatter plot  line  arc  choropleth  and grid layers  or you can optionally connect to other third party libraries like our popular open source framework react map gl  a thin React wrapper around the MapboxGL JavaScript API   To demonstrate  we developed a set of simple examples that focus on mapping use cases  Check out this demo to see the performance of deck gl with large data sets  2M points and 36K taxi trips in NYC with live GPU interpolation   We ve also developed layers to visualize 3D building structures  street segments  point cloud data  and more  which we will be open sourcing in future releases  Let s dive deeper into some of the design and architecture decisions behind deck gl   Instancing   Layering  By combining instances and layers  deck gl makes complex visualizations possible without crashing computers  If you ve worked with D3 visualizations in the past  then you re already familiar with the concept of instancing  Instancing is when you render multiple copies of a single object  with little tweaks among the copies  You end up with what appear to be different objects  but all of these are displayed inexpensively because of how the WebGL API works  For example  to create a scatter plot  you have circle elements and apply modifiers on the radius  position  and color of these circles based on the values of each row in the data     To make use of this instancing feature  deck gl has a layering system to overlay different data sets  with blending modes  clipping  etc    You can reason about different types of data with clear semantic separations in one big picture  adding and removing layers as you need to  For example  a common Uber deck gl application might include layers for pickups  dropoffs  requests  elevation  3D buildings  and point cloud data  all rendered with high performance because the geometric primitives are all just copies   Layers can also create computations like grouping and aggregations  This is useful to create on the fly heatmaps of population density  median ETAs  etc  This example shows dynamic aggregation in a grid  The computations are performed live so that granularity changes with the zoom level   High Precision GPU Computations  One deck gl feature we re really proud of is its ability to handle high precision transformations and other numerical calculations on the GPU  WebGL and many other graphics libraries like Apple s Metal support only 32 bit single precision floating point numbers  WebGL is particularly lenient about numerical accuracy on many of its supporting platforms   As deck gl aims to provide both accuracy and performance for high dynamic range  HDR   scientific grade data  we implemented a powerful extension to emulate 64 bit double precision floating point numbers on all platforms that support WebGL 1 0  With twice the significant digits  46 bits  in the mantissa  the emulated high precision data type enables rendering of primitives that can never be handled by the GPU  using intrinsic single precision floating point numbers  This is incredibly useful for rendering geographic elements specified in lat long pairs at the centimeter level or for rendering interactive data dense visualizations that span from the city level down to the intersection level of a map   Most other libraries that handle high zoom levels either use different coordinate systems  like UTM   which trade dynamic range for precision  or perform CPU based computations for alignment with the overall map  which trade performance for precision  By doing all math in this high precision type of floating point numbers  we re able to move over all computations to the GPU and attain accuracy  performance  and interactivity at the same time  This example showcases the high precision GPU float operations   Built with Latest Coding Standards  We built deck gl to be as relevant as possible to other users like us  deck gl therefore uses the latest JavaScript coding standards  with classes and other concepts from ES6  ES2015  and ES7  ES2016   supported via Babel transpilation  Naturally  it comes packaged as an npm module and is compatible with standard JavaScript bundlers like Browserify and webpack   To make deck gl easy to use  we created a companion utility library called luma gl  which uses the same coding and design conventions as deck gl  luma gl comes with the following   Latest JavaScript coding standards  including ES2016  Well supported features from the coming WebGL 2 0 standard  such as instanced arrays in WebGL1 applications  Classes that expose WebGL 2 0 features  like transform feedback   enabling you to start experimenting today on supporting browsers   Advanced debugging  tracing  error checking  and facilities for inspecting buffers and memory to simplify the notoriously difficult process of debugging WebGL applications  Shader library with a 64 bit floating point emulation package for the GPU  tested and made to work across the majority of the industry s GPU drivers  the fp64 library is heavily used in deck gl for creating high precision layers    It will soon enable the use of WebGL to run compute shaders for general purpose computation   Interoperability with React and Mapbox  We built deck gl with reactive programming model principles  popularized by React  and worked on full interoperability with Mapbox s camera system  As a result  perspective mode and alignment work out of the box for mapping applications  This example showcases perspective mode in Mapbox with the core Arc Layer in deck gl  It s displaying county to county migration in the United States  Use shift   drag to change the perspective of the map   We have future plans of integrating with other aspects of Mapbox  but we re also keeping the library decoupled so that it can be used with other packages of your choice   More to Come  With this first announcement of deck gl we also are working on an ambitious roadmap  tightening the interoperability with Mapbox but also pushing in another direction  making deck gl a building block for abstract data visualizations or other types of scientific visualization  Feel free to reach out to the team at data viz uber com if you have any comments  Looking forward to seeing what you build with this   Nicolas Garcia Belmonte leads the data visualization team within Uber Engineering,"[331 397 0 1331 823 855 563 334 413 1289 668]"
334,training-dataset/engineering/791.txt,engineering,Data Wrangling at Slack   Several People Are CodingData Wrangling at Slack  By Ronnie Chen and Diana Pojar  For a company like Slack that strives to be as data driven as possible  understanding how our users use our product is essential   The Data Engineering team at Slack works to provide an ecosystem to help people in the company quickly and easily answer questions about usage  so they can make better and data informed decisions   Based on a team s activity within its first week  what is the probability that it will upgrade to a paid team   or  What is the performance impact of the newest release of the desktop app    The Dream  We knew when we started building this system that we would need flexibility in choosing the tools to process and analyze our data  Sometimes the questions being asked involve a small amount of data and we want a fast  interactive way to explore the results  Other times we are running large aggregations across longer time series and we need a system that can handle the sheer quantity of data and help distribute the computation across a cluster  Each of our tools would be optimized for a specific use case  and they all needed to work together as an integrated system   We designed a system where all of our processing engines would have access to our data warehouse and be able to write back into it  Our plan seemed straightforward enough as long as we chose a shared data format  but as time went on we encountered more and more inconsistencies that challenged our assumptions   The Setup  Our central data warehouse is hosted on Amazon S3 where data could be queried via three primary tools  Hive  Presto and Spark   To help us track all the metrics that we want  we collect data from our MySQL database  our servers  clients  and job queues and push them all to S3  We use an in house tool called Sqooper to scrape our daily MySQL backups and export the tables to our data warehouse  All of our other data is sent to Kafka  a scalable  append only message log and then persisted on to S3 using a tool called Secor   For computation  we use Amazon s Elastic MapReduce  EMR  service to create ephemeral clusters that are preconfigured with all three of the services that we use   Presto is a distributed SQL query engine optimized for interactive queries  It s a fast way to answer ad hoc questions  validate data assumptions  explore smaller datasets  create visualizations and use it for some internal tools  where we don t need very low latency   When dealing with larger datasets or longer time series data  we use Hive  because it implicitly converts SQL like queries into MapReduce jobs  Hive can handle larger joins and is fault tolerant to stage failures  and most of our jobs in our ETL pipelines are written this way   Spark is a data processing framework that allows us to write batch and aggregation jobs that are more efficient and robust  since we can use a more expressive language  instead of SQL like queries  Spark also allows us to cache data in memory to make computations more efficient  We write most of our Spark pipelines in Scala to do data deduplication and write all core pipelines   Tying It All Together  How do we ensure that all of these tools can safely interact with each other   To bind all of these analytics engines together  we define our data using Thrift  which allows us to enforce a typed schema and have structured data  We store our files using Parquet which formats and stores the data in a columnar format  All three of our processing engines support Parquet and it provides many advantages around query and space efficiency   Since we process data in multiple places  we need to make sure that our systems always are aware of the latest schema  thus we rely on the Hive Metastore to be our ground truth for our data and its schema   CREATE TABLE IF NOT EXISTS server_logs   team_id BIGINT   user_id BIGINT   visitor_id STRING   user_agent MAP STRING  STRING    api_call_method STRING   api_call_ok BOOLEAN     PARTITIONED BY  year INT  month INT  day INT  hour INT   STORED AS PARQUET  LOCATION  s3   data server_logs   Both Presto and Spark have Hive connectors that allow them to access the Hive Metastore to read tables and our Spark pipelines dynamically add partitions and modify the schema as our data evolves   With a shared file format and a single source for table metadata  we should be able to pick any tool we want to read or write data from a common pool without any issues  In our dream  our data is well defined and structured and we can evolve our schemas as our data needs evolve  Unfortunately  our reality was a lot more nuanced than that   Communication Breakdown  All three processing engines that we use ship with libraries that enable them to read and write Parquet format  Managing the interoperation of all three engines using a shared file format may sound relatively straightforward  but not everything handles Parquet the same way  and these tiny differences can make big trouble when trying to read your data   Under the hood  Hive  Spark  and Presto are actually using different versions of the Parquet library and patching different subsets of bugs  which does not necessarily keep backwards compatibility  One of our biggest struggles with EMR was that it shipped with a custom version of Hive that was forked from an older version that was missing important bug fixes   What this means in practice is that the data you write with one of the tools might not be read by other tools  or worse  you can write data which is read by another tool in the wrong way  Here are some sample issues that we encountered   Absence of Data  One of the biggest differences that we found between the different Parquet libraries was how each one handled the absence of data   In Hive 0 13  when you use use Parquet  a null value in a field will throw a NullPointerException  But supporting optional fields is not the only issue  The way that data gets loaded can turn a block of nulls  harmless by themselves   into an error if no non null values are also present  PARQUET 136    In Presto 0 147  the complex structures were the ones that made us uncover a different set of issues   we saw exceptions being thrown when the keys of a map or list are null   The issue was fixed in Hive  but not ported in the Presto dependency  HIVE 11625    To protect against these issues  we sanitize our data before writing to the Parquet files so that we can safely perform lookups   Schema Evolution Troubles  Another major source of incompatibility is around schema and file format changes  The Parquet file format has a schema defined in each file based on the columns that are present  Each Hive table also has a schema and each partition in that table has its own schema  In order for data to be read correctly  all three schemas need to be in agreement   This becomes an issue when we need to evolve custom data structures  because the old data files and partitions still have the original schema  Altering a data structure by adding or removing fields will cause old and new data partitions to have their columns appears with different offsets  resulting in an error being thrown  Doing a complete update will require re serializing all of the old data files and updating all of the old partitions  To get around the time and computation costs of doing a complete rewrite for every schema update  we moved to a flattened data structure where new fields are appended to the end of the schema as individual columns   These errors that will kill a running job are not as dangerous as invisible failures like data showing up in incorrect columns  By default  Presto settings use column location to access data in Parquet files while Hive uses column names  This means that Hive supports the creation of tables where the Parquet file schema and the table schema columns are in different order  but Presto will read those tables with the data appearing in different columns   File schema    fields      name   user_id   type   long       name   server_name   type   string       name   experiment_name    type   string     Table schema    user_id BIGINT  experiment_name STRING  server_name STRING                     Hive                     user_id experiment_name server_name  1 test1 slack 1  2 test1 slack 2                   Presto                    user_id experiment_name server_name  1 slack 1 test1  2 slack 2 test1  It s a simple enough problem to avoid or fix with a configuration change  but easily something that can slip through undetected if not checked for   Upgrading EMR  Upgrading versions is an opportunity to fix all of the workarounds that were put in earlier  But it s very important to do this thoughtfully  As we upgrade EMR versions to resolve bugs or to get performance improvements  we also risk exchanging one set of incompatibilities with another  When libraries get upgraded  it s expected that the new libraries are compatible with the older versions  but changes in implementation will not always allow older versions to read the upgraded versions   When upgrading our cluster  we must always make sure that the Parquet libraries being used by the analytics engines we are using are compatible with each other and with every running version of those engines on our cluster  A recent test cluster to try out a newer version of Spark resulted in some data types being unreadable by Presto   This leads to us being locked into certain versions until we implement workarounds for all of the compatibility issues and that makes cluster upgrades a very scary proposition  Even worse  when upgrades render our old workarounds unnecessary  we still have a difficult decision to make  For every workaround we remove  we have to decide if it s more effective to backfill our data to remove the hack or perpetuate it to maintain backwards compatibility  How can we make that process easier   A Common Language  To solve some of these issues and to enable us to safely perform upgrades  we wrote our own Hive InputFormat and Parquet OutputFormat to pin our encoding and decoding of files to a specific version  By bringing control of our serialization and deserialization in house  we can safely use out of the box clusters to run our tooling without worrying about being unable to read our own data   These formats are essentially forks of the official version which bring in the bug fixes across various builds   Final Thoughts  Because the various analytics engines we use have subtly different requirements about serialization and deserialization of values  the data that we write has to fit all of those requirements in order for us to read and process it  To preserve the ability use all of those tools  we ended up limiting ourselves and building only for the shared subset of features   Shifting control of these libraries into a package that we own and maintain allows us to eliminate many of the read write errors  but it s still important to make sure that we consider all of the common and uncommon ways that our files and schemas can evolve over time  Most of our biggest challenges on the data engineering team were not centered around writing code  but around understanding the discrepancies between the systems that we use  As you can see  those seemingly small differences can cause big headaches when it comes to interoperability  Our job on the data team is to build a deeper understanding of how our tools interact with each other  so we can better predict how to build for  test  and evolve our data pipelines,"[334 397 0 823 1331 1414 1289 428 563 413 934]"
374,training-dataset/business/234.txt,business,Beginning with OurselvesIn a recent post  we offered some insights into how we scaled Airbnb s data science team in the context of hyper growth  We aspired to build a team that was creative and impactful  and we wanted to develop a lasting  positive culture  Much of that depends on the points articulated in that previous post  however there is another part of the story that deserves its own post   on a topic that has been receiving national attention  diversity   For us  this challenge came into focus a year ago  We d had a successful year of hiring in terms of volume  but realized that in our push for growth we were not being as mindful of culture and diversity as we wanted to be  For example  only 10  of our new data scientists were women  which meant that we were both out of sync with our community of guests and hosts  and that the existing female data scientists at Airbnb were quickly becoming outnumbered  This was far from intentional  but that was exactly the problem   our hiring efforts did not emphasize a gender balanced team   There are  of course  many ways to think about team balance  gender is just one dimension that stood out to us  And there are known structural issues that form a headwind against progress in achieving gender balance  source   So  in a hyper growth environment where you re under pressure to build your team  it is easy to recruit and hire a larger proportion of male data scientists   But this was not the team we wanted to build  Homogeneity brings a narrower range of ideas and gathers momentum toward a vicious cycle  in which it becomes harder to attract and retain talent within a minority group as it becomes increasingly underrepresented  If Airbnb aspires to build a world where people can belong anywhere  we needed to begin with our team   We worried that some form of unconscious bias had infiltrated our interviews  leading to lower conversion rates for women  But before diving into a solution  we decided to treat this like any problem we work on   begin with research  identify an opportunity  experiment with a solution  and iterate   Over the year since  the results have been dramatic  47  of hires were women  doubling the overall ratio of female data scientists on our team from 15  to 30   The effect this has had on our culture is clear   in a recent internal survey  our team was found to have the highest average employee satisfaction in the company  In addition  100  of women on our team indicated that they expect to still be here a year from now and felt like they belonged at Airbnb   Our work is by no means done  There s still more to learn and other dimensions of diversity to improve  but we feel good enough about our progress to share some insights  We hope that teams at other companies can adopt similar approaches and build a more balanced industry of data scientists   Addressing the top of funnel  When we analyze the experience of a guest or host on Airbnb  we break it into two parts  the top of funnel  are there enough guests looking for places to stay and enough hosts with available rooms  and conversion  did we find the right match and did it result in a booking   Analyzing recruiting experiences is quite similar   And  like any project  our first task was to clean our data  We used the EEOC reporting in Greenhouse  our recruiting tool  to better understand the diversity of our applicants  doing our own internal audit of data quality as well  One issue we faced is that while Greenhouse collects diversity data on applicants who apply directly through the Airbnb jobs page  it does not collect information on the demographics of referrals  candidates who were recommended for the job by current Airbnb employees   which represent a large fraction of hires  Then we combined this with data from an internal audit of our teams history and from Workday  our HR tool  in order to compare the composition of applicants to the composition of our team   When we dug in  we found that historically about 30  of our applicants   the top of the funnel   had been women  This told us that there were opportunities for improvement on both fronts  Our proportion of female applicants was twice that of employees  so there was clearly room for improvement in our hiring process   the conversion portion  However  there wasn t male female parity in our applicant pool so this could also prove a meaningful lever   In addition  we wanted to ensure that our efforts to diversify our data science team didn t end with us  Making changes to the top of the funnel   to how many women want to and feel qualified to apply for data science jobs   could help us do that  Our end goal is to create a world where there is diversity across the entire data science field  not just at Airbnb   We decided that the best way to achieve these goals would be to look beyond our own applicants to inspire and support women in the broader field  One observation was that while there were a multitude of meetups for women who code  and many great communities of women in engineering  we hadn t seen the same proliferation of events for women in data science   We decided to create a series of lightning talks featuring women in data  under the umbrella of the broader Airbnb  Taking Flight  initiative  The goals were twofold  to showcase the many contributions of women in the field  and to create a forum for celebrating the contributions of women to data science  At the same time  we wanted to highlight diversity on multiple dimensions  For each lightning talk  we created a panel of women from many different racial and ethnic backgrounds  practicing different types of data science  The talks were open to anyone who supported women in data science   We came up with the title  Small Talks  Big Data  and started with an event in November 2014 where we served food and created a space and time for mingling  The event sold out  with over 100 RSVPs  Afterward we ran a survey to see what our attendees thought we could improve in subsequent events and turned  Small Talks  Big Data  into a series  all of which have continued to sell out  Given this level of interest  several of the women on our team volunteered to write blog posts about their accomplishments  for example  Lisa s analysis of NPS and Ariana s overview of machine learning  in order to circulate their stories beyond San Francisco  and to give talks and interviews  for example  Get to know Data Science Panelist Elena Grewal   Many applicants to our team have cited these talks and posts as inspirations to consider working at Airbnb   In parallel to these large community events we put together smaller get together for senior women in the field to meet  support one another  and share best practices  We hosted an initial dinner at Airbnb and were amazed at what wonderful conversations and friendships were sparked by the event  This group has continued to meet informally  with women from other companies taking the lead on hosting events at their companies  further exposing this group to the opportunities in the field   Increasing conversion  Alongside our efforts to broaden our applicant pool  we scrutinized our approach to interviewing  As with any conversion funnel  we broke our process down into discrete steps  allowing us to isolate where the drop off was occurring   There are essentially three stages to interviewing for a data science role at Airbnb  a take home challenge used to assess technicality and attention to detail  an onsite presentation demonstrating communication and analytical rigor  and a set of 1 1 conversations with future colleagues where we evaluate compatibility with our culture and fit for the role itself  Conversion in the third step was relatively equal  but quite different in steps one and two   We wanted to keep unconscious bias from affecting our grading of take home challenges  either relating to reviewers being swayed by the name and background of the candidate  via access to their resume  or to subjective views of what constitutes success  To combat this  we removed access to candidate names 1  and implemented a binary scoring system for the challenge  tracking whether candidates did or did not do certain tasks  in an effort to make ratings clearer and more objective  We provided graders with a detailed description of what to look for and how to score  and trained them on past challenges before allowing them to grade candidates in flight  The same challenge would circulate through multiple graders to ensure consistency   Our hypothesis for the onsite presentation was that we had created an environment that catered more to men  Often  a candidate would be escorted into a room where there would be a panel of mostly male data scientists who would scrutinize their approach to solving the onsite challenge  The most common critique of unsuccessful candidates was that they were  too junior   stemming from poor communication or a lack of confidence  Our assumption was that this perception was skewed by the fact that they were either nervous or intimidated by the presentation atmosphere we had created   A few simple changes materially improved this experience  We made it a point to ensure women made up at least half of the interview panel for female candidates  We also began scheduling an informal coffee chat for the candidate and a member of the panel before the presentation  so they would have a familiar face in the room  we did this for both male and female candidates and both said they appreciated this change   And  in our roundup discussions following the presentation  we would focus the conversation on objective traits of the presentation rather than subjective interpretations of overall success   Taken together  these efforts had a dramatic effect on conversion rates  While our top of funnel initiatives increased the relative volume of female candidates  our interviewing initiatives helped create an environment in which female candidates would be just as likely to succeed as any male candidate  Furthermore  these changes to our process didn t just help with diversity  they improved the candidate experience and effectiveness of hiring data scientists in general   Why this is important  The steps we took over the last year grew the gender balance on our team from 15  to 30   which has made our team stronger and our work more impactful  How   First  it makes us smarter  source  by allowing for divergent voices  opinions  and ideas to emerge  As Airbnb scales  it has access to more data and increasingly relies upon the data science team s creativity and sophistication for making strategic decisions about our future  If we were to maintain a homogenous team  we would continue to rely upon the same approaches to the challenges we face  investing in the diversity of data scientists is an investment in the diversity of perspectives and ideas that will help us jump from local to global maxima  Airbnb is a global company and people from a multitude of backgrounds use Airbnb  We can be smarter about how we understand that data when our team better reflects the different backgrounds of our guests and hosts   Second  a diverse team allows us to better connect our insights with the company  The impact of a data science team is dependent upon its ability to influence the adoption of its recommendations  It is common for new members of the field to assume that statistical significance speaks for itself  however  colleagues in other fields tend to assume the statistical voodoo of a data scientist s work is valid and instead focus on the way their ideas are conveyed  Our impact is therefore limited by our ability to connect with our colleagues and convince them of the potential our recommendations hold  Indeed  the pairing of personalities between data scientists and partners is often more impactful than the pairing of skillsets  especially at the leadership level  Increasing diversity is an investment in our ability to influence a broader set of our company s leadership   Finally  and perhaps most importantly  increasing our team s diversity has improved our culture  The women on the data science team feel that they belong and that their careers can grow at Airbnb  As a result  they are more likely to stay with the company and are more invested in helping to build this team  referring people in their networks for open roles  We are not done  but we have reversed course from a vicious to virtuous cycle  Additionally  the results aren t just restricted to women   the culture of the team as a whole has improved significantly over past years  in our annual internal survey  the data science team scores the highest in employee satisfaction across the company   Of course  gender is only one dimension of diversity that we aim to balance within the team  In 2015 it was our starting point  As we look to 2016 and beyond  we will use this playbook to enhance diversity in other respects  and we expect this will strengthen our team  our culture  and our company,"[374 397 2 1139 734 1331 254 385 823 334 413]"
385,training-dataset/business/381.txt,business,The Limitations of Data and BenchmarksThe Limitations of Data and Benchmarks  Numbers provide us a certain certainty  With their precision  they offer a sense of black and white  in or out  But  metrics alone aren t enough  All the quantitative analysis in the world won t lead me to the next great idea for startup  Those figures can t create empathy  develop the right culture  or hire the right people  I ve been thinking about this quite a bit because in both the recent Software Engineering Daily podcast I did with Jeff  and the presentation I gave at Launch Conference  the question of the limits of metrics surfaced   In those conversations  we discussed two shortcomings of data  On Software Engineering Daily  Jeff asked whether metrics can lead us into a local maximum or minimum  And the answer is yes  Data is not a way to create new ideas  Pixar never ran linear regressions to create Woody the Cowboy  Rather  data is a way to optimize a funnel  whittle down a series of options  evaluate experiments  It is a filtering tool  not an ideation tool  Startup idea generation has always been closer to poetry  with a healthy addition of user research  than accounting   At the Launch Conference  an audience member asked whether a single metric  even a proxy metric  is enough to determine the viability of an idea  The answer is no  Most metrics we evaluate are rear view mirror metrics  And each metric only describes a facet of the business  To describe a publicly traded company  you might use five or six  market cap  revenue multiple  gross margin  cash flow  revenue growth rate  profitability  Even then  those figures provide only the foggiest outline of a company   Like historians  investors use numbers to compare and contrast  to categorize and critique  We identify unusual companies  those with best in class sales efficiency or revenue growth  Management teams employ metrics to identify when a particular part of a company is performing in an unexpected way  Sagging quota attainment suggests sales recruiting and t practices are worth investigating  Often  data is a filter   We have shown in analysis on this blog how revenue growth is not correlated with series A pre money valuation  And  at least one third of premium SaaS companies raise capital before generating a dollar of revenue  That means that the early stages  while we can look at metrics to evaluate companies  these numbers don t tell the majority of the story   It might be the case that as a company grows and matures and mechanizes its business model and its go to market strategy  that numbers capture more and more of the business  But even then  data is just one way to describe a business   I hope the metrics I publish inspire  They show what can be done  but not how to do it  They show that there are many different ways of building a company  whether it is the astronomical growth rate of Slack and Salesforce or the brick by brick execution of Atlassian or Concur  But they will never capture the entirety of the story  And for every one path trod by a business  there is another path less taken that a founding team will take to redefine all the rules and observations   We can measure elephant s height  the length of its tusks  its weight  how fast it runs  even sequence its genome  But like the six blind men who disagree on which animal stands before them  no one perspective  even a data driven one  is not sufficient to fully describe it   Published 2016 11 21 in benchmarks,"[385 397 193 2 1139 374 668 740 254 855 1107]"
394,training-dataset/engineering/982.txt,engineering,Introducing Ohana  Uber Engineering s iOS Contacts Libraryby Adam Zethraeus  Within Uber s mobile ecosystem  we are always trying to engineer our architecture for reuse  We benefit greatly from properly modularized code because we are constantly running experiments to help us make product decision across our varied markets and types of people we serve  Over the past year  we have iteratively worked on Ohana  an iOS framework for retrieving and formatting contact information  which we have open sourced for others to build on   Ohana  which means family in Hawaiian  provides tools for easily organizing and presenting contacts information from the iOS address book  It s a data toolset  not a UI widget  Ohana is part of a broader effort to modularize both the logic and the UI around displaying contacts to the users of the Uber iOS driver app  Here s more on what Ohana is about  and how it can be a useful part of iOS app architecture  We will present an overview of Ohana and what its components do  with some of the source code interface in case you want to use it for your application   Architecture  Ohana s high level architecture is about filtering data from multiple sources  abstracted to Data Providers   through an ordered set of post processors to transform the data for a specific user need  such as displaying it within the app   Ohana uses UberSignals  an implementation of the observable pattern  as its primary data flow mechanic  Ohana is structured with a small set of implementable protocols  DataProvider  PostProcessor  SelectionFilter  which allow the consuming app to customize its behavior  We ve ordered the explanation of its components  in order of precedence for importance for how you would use Ohana   OHContact  This is the data model for a contact and is the output from the system  the end result  OHContact contains properties for fields like first and last name which are common on most contacts  as well as a list of OHContactFields for the arbitrarily named fields like  iPhone  or  fax     interface OHContact   NSObject  NSCopying    property  nonatomic  nullable  NSString  firstName    property  nonatomic  nullable  NSString  lastName    property  nonatomic  nullable  NSOrderedSet OHContactField     contactFields      properties omitted     BOOL isEqualToContact  OHContact   contact    end  OHContactsDataSource  The Data Source is where you retrieve the contacts from  It s a Data Source in the Cocoa sense  it s what provides the final contacts data to your application  The Data Source is the class which contains the core of Ohana s internal implementation  It triggers contact loading via the Data Providers and uses the Post Processors to configure their data  Once the contacts have been loaded and processed  the onContactsDataSourceReadySignal is fired and the contacts list is populated  Consumers of the framework plug this custom Data Providers and Post Processors into this object to configure Ohana for their use case    interface OHContactsDataSource   NSObject   property  nonatomic  readonly  OHContactsDataSourceReadySignal  onContactsDataSourceReadySignal    property  nonatomic  readonly  nullable  NSOrderedSet OHContact     contacts      instancetype initWithDataProviders  NSOrderedSet id OHContactsDataProviderProtocol     dataProviders postProcessors  NSOrderedSet id OHContactsPostProcessorProtocol     postProcessors      void loadContacts      properties omitted   end  OHContactsDataProviderProtocol  The data provider protocol describes an interface to retrieve contact information from a source of your choice  Ohana provides default implementations to use the iOS s address book via the ABAddressBook or CNContacts API  You can use multiple data providers at once  You could  for example  implement your own data provider which retrieves contacts from Facebook and interleaves them with the system contacts    protocol OHContactsDataProviderProtocol  NSObject    property  nonatomic  readonly  OHContactsDataProviderFinishedLoadingSignal  onContactsDataProviderFinishedLoadingSignal    property  nonatomic  readonly  nullable  NSOrderedSet OHContact     contacts      void loadContacts      properties omitted   end  OHContactsPostProcessorProtocol  The Post Process protocol describes an interface for filtering the data to be exposed to the consumer  A Post Processor could filter for only contacts which have associated phone numbers  or reorder input contacts to be displayed in alphabetical order  You could then chain these Post Processors so that the data reflects both transformations  These and many other useful Post Processors are available by default in Ohana   The real power of the system emerges when the consumer implements their own Post Processors for their custom needs  Do you want to filter for contacts with a certain area code  Dedupe contacts by shared address  Ensure you only display contacts that have all of the fields you happen to care about  All of these use cases fit nicely within the API   Once you ve built them  consider making a pull request     protocol OHContactsPostProcessorProtocol  NSObject      NSOrderedSet OHContact      processContacts  NSOrderedSet OHContact      preProcessedContacts    end  Here s an example of filtering if you had the following data set      first    Marge    last    Simpson    image    UIImage        first    Mom    last   nil   image    UIImage        first    Wednesday    last    Addams    image    UIImage        first    Homer    last    Simpson    image    UIImage        first    Maggie    last    Simpson    image   nil       first    Morticia    last    Addams    image    UIImage        first    Gomez    last    Addams    image    UIImage        first    bae    last   nil   image   nil     and wanted to filter for complete data  and change the data to be sent to a server in a different format such as the following      full_name    Morticia Addams    image    UIImage        full_name    Gomez Addams    image    UIImage        full_name    Wednesday Addams    image    UIImage        full_name    Homer Simpson    image    UIImage        full_name    Marge Simpson    image    UIImage     You could use three Post Processors to execute the following steps   Filter for contacts with data for all of the   first     last    and  image  fields    OHSplitOnFieldTypePostProcessor  Order the contacts by  last    then  first    OHAlphabeticalSortPostProcessor   Merge the  first  and  last  fields into a  full_name  field  your custom Post Processor implementation   OHContactsSelectionFilterProtocol  Since selecting and deselecting contacts in a UI is an exceedingly common use case  Ohana provides first class support for statefully filtering selections and notifying when they happen  For example  a contact picker might only allow for selecting three contacts at any one time  OHMaximumSelectedCountSelectionFilter   and may want to show an clear error state and cancel selection when the user attempts to select a fourth contact   We won t further describe this API here  but in our open source documentation we have some example implementations to learn more about how contact selection in Ohana works   Using Ohana as Part of Your Project  Setting up Ohana as a simple consumer of the iOS system address book is easy  This code snippet from a ViewController instantiates a single Data Provider for the system address book and a single Post Processor to order those contacts by their full names   let alphabeticalSortProcessor   OHAlphabeticalSortPostProcessor sortMode   fullName   var dataProvider   OHABAddressBookContactsDataProvider delegate  self   let dataSource   OHContactsDataSource dataProviders  NSOrderedSet objects  dataProvider   postProcessors  NSOrderedSet object  alphabeticalSortProcessor    Finally  we indicate which code to trigger when the contacts are returned  and we call to load them   dataSource onContactsDataSourceReadySignal addObserver self  callback     weak self   observer  in  self  tableView  reloadData       The table view reads contacts from dataSource contacts      dataSource loadContacts    For a basic contact picker view  that s it  All that remains is to hook your OHContactsDataSource up as the data source for your table view   Ohana includes an example app with a large set of examples in both Swift and Objective C  The example app shows how to make more complex transformations  as well as how Ohana seamlessly handles asynchronous issues like auth challenges   Using Ohana at Uber  The effort that produced Ohana started on the driver signups team  whose mission is to increase partner signups via the driver app  We found that componentization and the careful separation of concerns in code was essential in allowing us to maintain codebase quality while doing the rapid experimentation we were tasked with   Once properly componentized  our contacts handing logic became extensible enough that other engineers wanted to use it  Teams responsible for contacts display UI in other parts of our apps  e g  fare splitting  share my ETA  invites  quickly consolidated on using the same architecture  The contacts data model that we d created became the de facto format for interacting with contact information across every layer of the app   We ve since open sourced Ohana  Uber has a flexible open source policy  The code we release must be of general interest to the open source community  and we must be able to license it to allow it to have utility to others  Importantly  there should also be a clear set of owners for the open source project who can commit to handling the project s lifecycle as the open source community becomes involved  and as the technical environment around the project changes   Since open sourcing Ohana  we ve added support for managing dependencies with Carthage  added features  and fixed some bugs in Swift 3 support   Making iOS contacts more accessible than the core APIs allow gives you more creative control how you display contacts from within the app  It will be useful for handling contacts in your project if your needs go beyond showing the default iOS contact picker  Take a look at Ohana  see if it matches your project s needs  We welcome pull requests  and we at Uber Engineering would love to see the wider iOS community give the tool a try   Adam Zethraeus is an iOS engineer on Uber Engineering s Driver Team  who developed Ohana alongside Maxwell Elliot  Doug Togno  and Nick Entin  The Driver Team is hiring talented mobile engineers to help make driving with Uber a better experience,"[394 397 823 563 1331 413 0 334 428 934 997]"
397,training-dataset/engineering/242.txt,engineering,The State of Data EngineeringTotal Number of Data Engineers How many data engineers are there  It s easy to understand why data engineers are in such high demand  we re currently in the development phase of  the big data stack   There isn t consensus yet on how the stack will mature  and difficult technological problems arise at every turn  Because of this  it requires serious software engineering chops to build and deploy this technology today  and there just aren t a lot of people with these skills  Additionally  because these individuals are building the data infrastructure that companies like Uber  Spotify  and Slack rely on to deliver their products  the role couldn t be more critical  We found a grand total of 6 500 people who call themselves  data engineers  on Linkedin  We have no doubt that plenty of folks are doing the work of the data engineer who aren t using this title  but in this report we focus specifically on people who self report having this title  There s plenty of potential fuzziness around the definition of  data engineer   all software engineers work with data in some fashion   and we don t think there s a perfect answer  We felt it was best to let the practitioners speak for themselves  6 500 is not a big number  In fact  we were a little surprised at just how small it is  For comparison  as of this writing  there are 6 600 data engineering job postings on Indeed  And that s just in the San Francisco Bay area  Salary data also confirms that data engineers are in demand  Anecdotally  top data engineering positions at tech giants like Facebook  Amazon  and Google can exceed  500k  Indeed s data shows a more modest distribution  but salaries well into the six figures none the less   Expert Insight  The Demand for Data Engineering Talent Jonathan Coveney  Data Engineer at Stripe For nearly a decade  Jonathan Coveney has been deep in the data world  building data systems at Twitter  Spotify  and even doing a stint at The Apache Software Foundation before joining Stripe  From his perspective  there are three important trends driving up the demand for data engineering talent  New sophistication in how companies think about data and the people who manage it   There s a growing sense today that data isn t just a byproduct   Jonathan says   but rather it s a core part of what a company does   In the past  a company might run a one off analysis of a database log  today  data engineers own the analysis and organization of that data  They identify with data processes  which leads to a more nuanced understanding of their data architecture   Push towards machine learning  Thanks to machine learning advances  access to proprietary data has become a major competitive advantage for firms in almost all industries  Collecting and making this data available has therefore become a key strategic function   Companies building data products  There s some overlap here with machine learning  but Jonathan uses the example of maps to describe the difference   The machine learning aspects of maps include things like traffic detection and routing  but the infrastructure of maps relies on managing and organizing massive volumes of data that s data engineering   Today s tech companies need to play well in both areas  How has the number of data engineers changed over time  Linkedin profiles show an individual s self reported employment history as a list of titles with start and end dates  This information allows us to construct a timeline of the job market  Take a look at the chart below  it s hard to overstate just how quickly this space is growing   The number of data engineers more than doubled from 2013 2015  And based on the job posting data from earlier  this growth isn t about to slow down  For comparison  there are currently about 2x the number of data scientists  roughly 11 400   but the growth rate of data engineers is much faster than anything the data scientist job market ever experienced  In this same period  the number of data scientists grew by a little over 50   This is particularly interesting when you consider the saturated press around data scientist hiring  The feature length article on data engineers has yet to be published  Where do data engineers come from  The rapid influx of data engineers begs an obvious question  who are these people  What did they do beforehand  We looked at the data  asking the specific question  What was the job title held by this person just prior to them taking their first role as a data engineer   This is instructive in that it tells us about the DNA of data engineers  We had a few theories of what we would find when we looked at this question  Data engineers bridge the boundary between software engineering and data science in that they create the production code that allows data science to scale reliably  We expected to see both software engineering and data science represented in the data   Because so much of data engineering is about scale  data engineers bridge the gap between software engineering and devops  Because of this we anticipated seeing some devops specific titles   Database administrators have historically played a very similar role within companies  We anticipated seeing some DBAs who have transitioned to this more modern role  We found that our three hypotheses played out to some extent  but one thing was very clear  data engineers share most of their DNA with software engineers  Here are their top ten prior job titles   This makes intuitive sense  data engineering is a subspecialty of software engineering  The two fields share methodology and tools  While individuals from other disciplines do transition into the role  the most common path starts at the more general  Software Engineer  title  and progresses to the more specialized  Data Engineer   Where are data engineers located  50  of all data engineers live in the US  This isn t entirely surprising  as the term itself and much of the foundational technology comes from technology companies and universities in America   This is interesting particularly because it validates conventional wisdom within the data engineering field  Most of the space s technology has either come out of a small set of universities most especially Berkeley or from the software engineering teams of the biggest internet companies in the world  Google  Facebook  Linkedin  and Amazon were struggling with big data and had resources to throw at the problem long before the rest of the industry  Not only have they invented much of the technology  they ve also acted as training grounds for talent  However  this chart is slightly misleading  While the US has the most data engineers by far  they also have the most profiles in the world  nearly 4x that of the next country  India  To normalize the data  we broke out the top ten countries from the chart above and looked at how their data engineer population relates to the number of LinkedIn profiles from that country  as well as the population as a whole   Missing from this list is Israel  which in our previous benchmark  ranked highest in terms of data scientists per million of their population  As we mentioned  Israel has long been known as a startup nation with a strong tech presence in  Silicon Wadi   It s surprising that this doesn t translate to a higher density of data engineering talent   Top Employers of Data Engineers What industries employ the most data engineers  Companies that experience challenges related to scaling the storage  transmission  and processing of data are those in need of data engineering talent  These challenges arise mostly within tech companies  but what about industries like telecom  biotech  and insurance  Don t these industries need data scaling help as well  When we looked at where these data engineers are working  we found that a wide range of industries require a data role   Telecom and financial services are up towards the top  as we expected  but the petabytes of DNA being sequenced in biotech today don t seem to be pushing it towards the top of the list  The takeaway from this chart shouldn t be that other industries don t need or don t employ people who function as data engineers  Rather  the title  Data Engineer  has been popularized within a certain industry internet tech and the usage of this particular title is still nascent  The technology  process  and mindset within this space is beginning to spread to other industries  Expert Insight  Data Engineering in Healthcare Alyssa Kwan  Data Engineer at Clara Lending Alyssa Kwan has built data infrastructure in a variety of industries including Financial Services at Citizen s Bank and Clara Lending  Marketing and Advertising at Nanigans  and healthcare at Clover Health  She s seen first hand how much the work of data engineering changes depending on the industry one is working in   For most tech companies  the primary concerns in data engineering are around issues of scale  But in industries like healthcare  your primary concern is complexity  You need detailed models that conform to both compliance and auditability requirements  And then there s the human perspective to that industry as well you can t risk messing up someone s healthcare   This has a big impact on the specific data engineering skills companies hire for   If you re doing web analytics  in the case of marketing and advertising platforms  you want a developer experienced working with off the shelf tools  likely a DBA type person  The model for this role is more set   But in more complex industries like Financial Services and Healthcare   You re looking for someone that s more of a classic computer science person  There are very few organizations in protected industries that have done this work or are talking about it  So you want someone that s interested in the space  a generalist that is comfortable with the amount of complexity   What companies employ the most data engineers  The popularity of data engineers in tech becomes even more clear when looking at companies employing these data engineers  Within the top ten companies there are only two companies not specifically in technology or data  a telecom company  Verizon  and a financial institution  Capital One    It s interesting to pick out companies who employ a disproportionate number of data engineers  For example  Spotify  1600  employees  is far smaller than Pitney Bowes  16k employees   but employs roughly the same number of data engineers  The data clearly shows that some of today s tech  unicorns  value the data engineer role very highly  And  considering that there are 6 600 companies in San Francisco currently looking to hire a data engineer  it doesn t seem like this is about to change in the very near future  Expert Advice  Recruiting   Retaining Data Engineers Maggie Xiong  Director of Data Engineering at The Huffington Post Maggie has been working with data for over a decade  In that time  she s learned some important lessons about how to recruit and retain the engineering talent needed to build data technology  She points to three factors  Build an enviable team   I hired top notch architects on my team early on  and that made recruiting easier  Smart data engineers want to work with people they can learn from    Remove the wall between data engineering and data science   A lot of companies separate the data platform team from the data science team  I took the job at Huffington Post because they gave me the authorization to remove that wall  For algorithms minded folks  they want to see their work in production  They re not thinking about monitoring  recovery scenarios  documentation  A data engineer brings that rigor to data science that makes both groups successful  When teams are working separately  a lot of each of their work never sees the light of day    Create an environment of creativity   Big data people on both sides of the equation   systems and algorithms   tend to be highly creative  New technologies and approaches are coming out every month  people want the space to explore and learn    Skillset of Data Engineers We ve gotten to know a lot about data engineers at this point  but what  exactly  do they do  Earlier in the report we editorialized on this topic  The common understanding of the role of the data engineer is two fold  Make data available to consumers throughout the business   Production ize  algorithms that can be turned into data products  While this seems like a fair assessment of the role  we d prefer to let data engineers speak for themselves  Fortunately  Linkedin profiles have an entire section devoted to skills  which can say a lot about a person s role  While this section is often the least well maintained of the sections of a profile  we re confident that you ll find the conclusions that can be drawn from this data extremely interesting  What are the primary skills of a data engineer  The skillset of a data engineer obviously trends heavily towards data  while keeping some of the core software skills that many developed in prior roles  Take a look at the top 20   There are three specific things that we find notable on this list  SQL  a declarative language that most software engineers think of as little more than something to wrap up in an ORM  is the most common skill for data engineers  This is big  For years  SQL was a bit of an ugly duckling within data tech with the ascendancy of NoSQL approaches  However  SQL isn t going anywhere in fact  it s enjoying a renewal as SQL based interfaces for unstructured data  e g  Impala  Drill  Hive  and Presto  increase in popularity   Java is by far the most popular programming language on the list  This makes complete sense  since the original founding of Hadoop in the mid 2000 s  the JVM has been at the heart of data processing   Python is extremely common  but R doesn t even make the list  Python is frequently used both for data engineering workloads as well as analytics workloads  whereas R is specifically used for analytics  Within the data science community  both of these skills are roughly equal in weight  The difference in these two populations is striking  Beyond that  there s a tight focus on the highly relevant technical skills needed to work with data  Expert Insight  Know the Type of Data Engineer You Need Mike Xu  Data Architect at Looker Part of Mike Xu s job at Looker involves developer relations  which means he gets to hear quite a bit about what makes data engineers both happy  and not so happy  One of the most common complaints  Companies don t understand the nuances between data engineering roles  For the most part  Mike sees data engineers fall into four buckets  Warehouses  Focuses on optimizing warehouses for analytics  writing and managing data transformations  Tools  Highly skilled at using a specific tool in the big data toolbox  Architecture  Talented end to end thinkers  planning everything from data collection  to how teams will use the data  Ops  Focuses on building database and data tool instances  and managing permissions and security This distinction is particularly important during the hiring process   A lot of data engineering candidates I talk to are frustrated by the lack of insight into the projects they would be working on  Without that information  they re unable to evaluate if the role aligns with the nuance of their skill set   If you want to attract data engineering talent in this market  make it clear what type of data engineer you need  How do the skills of data engineers change with company size  It s a core function of data engineers to deal with the scalability challenges that arise with increases in dataset size  As such  we thought it would be instructive to look at how skills changed with company size  given that larger companies will often have more data  The chart below shows the relative difference in prevalence of skills based on the size of the company employing the data engineer  Skills at the top are more prevalent with data engineers at small companies  skills at the bottom are more prevalent in companies with 1 000 or more employees   We anticipated that as company size increased  so would the focus on scaling related skill  However  that s not the story the data told  Instead  data engineers at larger companies tend to be more focused on  enterprise  skills like ETL  BI  and data warehousing  whereas data engineers at smaller companies focus more on core technologies  Expert Insight  Data Engineering in the Enterprise vs  at Startups Will Smith  Principal Data Engineer Architect at MIT Will Smith has built data technology at companies like Nokia and Warner Bros Games  and in his experience  the skill set of a data engineer has less to do with whether they re working for an enterprise company or startup  and much more about how they answer one question  is their data schema on write  or schema on read   Enterprise companies are experienced working with Informatica  Oracle  SAP the business intelligence  BI  side of data engineering   Will says   In these environments  there s a team that has defined a schema of what the data will look like  schema on write  BI teams know exactly how the data will look in the warehouse   But companies building data technology today are functioning in a schema on read environment  Will shares this example   Imagine you have terabytes of log data from ad impressions in JSON  The data engineer has no idea what they will find in that data  The skillset now requires the developer to do data discovery and develop code  rather than just using straight SQL  This is a very different skill set than is needed in the schema on write environment   Will saw this first hand while working on Nokia s big data initiatives in 2011  While working with enterprise size data  the design pattern they used  learned from ex Yahoo developers  was built for schema on read  an approach more typically adopted by smaller companies   Building this way is how today s data engineers get economies of scale  They can ingest data from any source  Legacy BI systems can t do that  You point them at a data lake and  because of schema on write  they can t do anything with it because they don t know what s in there and we don t know what we don t know about the data   How do the skills of data engineers and data scientists compare  With this dataset  we re able to compare the skills of data engineers vs  those of data scientists  And the data paints a very clear difference between the two roles  Think of data engineers and data scientists appearing at opposites sides of a spectrum  This chart shows where skills are on that spectrum  with the the top representing skills more prevalent on data engineer profiles  and the bottom highlighting skills reported mostly by data scientists   Data engineers focus on making data available and processing it in production environments  which explains why data warehousing appears at the very top  followed closely by Java  the language often used to productionize algorithms  Data scientists  on the other hand  focus at the top of the stack  As we alluded to earlier  usage of R is a huge difference between these two groups  but that s followed very closely by other analytical skills like machine learning  stats  and modeling  How do the skills of data engineers and software engineers compare  The difference between data engineers and data scientists is clear  but what about data engineers and their software engineering compatriots  After all  as we showed earlier  a plurality of data engineers come from a software engineering background   The skills that are the most data engineer centric are Hadoop  data warehousing  and BI exactly what you would expect  And conversely  almost all of the skills listed on the software engineer end of the spectrum are focused on front end web development  The two biggest exceptions to that  C and C    are both languages not commonly used in the modern big data stack  While many data engineers may come from a software engineer role  they haven t simply changed to a trendy new job title for a pay raise  they ve had to differentiate themselves by learning new skills along the way  Expert Insight  Building Better Relationships Between Data Engineers and Data Scientists Ryan Orban  Chief Technology Officer at Galvanize  Think about the relationship between designers and front end developers   Ryan Orban says   One comes up with the ideas  the other implements  And it can cause a lot of tension   The relationship between data engineers and data scientists is similar in Ryan s opinion  and so is the solution to easing that tension   Just like designers are always told to learn to write some code  and front end developers are told to get more comfortable building mockups  I encourage data scientists and data engineers to learn some subset of their counterparts  skills   So just how deep should a data engineer go into the data science world   Data engineers should have a basic understanding of machine learning   Ryan says   They don t need to know all the mathematical theory  but they should be able to judge efficacy and accuracy  Conversely  data scientists need to understand infrastructure  how things scale  and have a rudimentary understanding of production level languages   This shift toward going deeper into the expertise of the other s discipline is happening in other areas as well  Companies used to hire data scientists that would report into marketing  product  or business analytics  while data engineers would report into the broader engineering function  This creates a misalignment of goals  Ryan says this trend is changing   The concept of the  data team  a group of both data scientists and data engineers  is growing in popularity  Just this simple change does a lot to improve the relationship between these two teams,"[397 1331 823 1139 334 2 668 934 855 413 374]"
413,training-dataset/engineering/343.txt,engineering,Real Architectural Patterns   Camille Fournier   MediumMicroservices  Real Architectural Patterns  A dissection of our favorite folk architecture  Introduction  I m fascinated by the lore and mystery behind microservices  As a concept  microservices feels like one of the most interesting folk architectures of the modern era  It s useful enough to be applied widely across different usage patterns and also vague enough to mean many different things   I ve been struggling for a while with understanding what people really mean when they discuss  microservices   Despite deploying what I would consider to be a version of that pattern in my last gig  it s quite clear to me that the architecture we used is not the same as the pattern that all other companies use  Recently I finally interrogated someone who has deployed the pattern in a very different way than I have  and so I decided it would be illustrative to compare and contrast the circumstances of our architectures for those in the larger technical audience   This article is going to have two examples  The first is the rough way  microservices  was deployed in my last gig  and why I made the decisions I made in the architecture  The second is an example of an architecture that is much closer to the  beautiful dream  microservices as I have heard it preached  for architectures that are stream focused   Microservices Basics  I think that microservices as an architecture evolved due to a few factors   A bunch of startups in the late 2000s started on monoliths like rails  scaled their business and team quickly  and hit the wall on what could reasonably be done in that monolith The cloud made it significantly easier to get access to a new server instance to run software We all got much more comfortable with the idea that we were dealing with distributed systems and in particular got comfortable making network calls as part of our systems  This combination of factors   scaling woes  easy access to new hardware  distributed systems and network access   played a huge part in what I might call  microservices for CRUD   If you have managed to scale a company to a certain level of success on a monolith but you are having trouble scaling the technology and or the engineering team  breaking the monolith into a services style architecture makes sense  This is a situation I encountered first hand   The arguments for microservices here look something like   Services allow for independent axes of scaling  If you have a part of the system with higher load or capacity requirements than other parts  you can scale to meet its needs  This is certainly doable in a monolith  but somewhat more complicated to reason about  Services allow for independent failure domains  to a more limited extent  Insofar as parts of your system are independently operable  you may want to allow for partial availability by splitting them out into services  For example  in a commerce app  if you can serve the checkout flow even when the product search flow is down  that might be considered a good thing  This is much more complicated in practice than it is in theory  and people make many silly claims about microservices that imply that any overlap in services means that they are not valuable  Independent failure domains are sometimes more of a  nice to have  than a necessity  and making the architecture truly account for this is not easy  Services allow for teams to work independently on parts of the system  Again  you can do this in a monolith  I have done this in a monolith  But the challenge with monolith  and a related challenge with services in a monorepo  single source repository   is that humans struggle to tangibly understand domains that are theoretically separate when they are presented as colocated by the source code  If I can see all of the code and it all compiles together and feels like a single thing  my tendency is to want to use it as a single thing  Grab code from here to use there  grab data from there to use here  etc   A few more notes   Monolith  and  monorepo  often get tangled up when talking about this world  A monolithic application is one where you have a set of code that compiles into a single main server artifact  possibly with some additional client artifacts produced   You can use configuration to make monoliths do almost anything you can imagine  including all of the services type things above  but the image produced tends to include most if not all of the code in the repository  This does get fuzzy because sometimes teams evolve their monoliths to compile to a couple of specialized server artifacts via a combination of build tooling and configuration  I would generally still call this a monolithic architecture   Monorepo  or monolith repository  is the model where you have a single repository that holds all of the code for any system you are actively changing  so  possibly excluding the source code for your OSS external dependencies   The repository itself contains source code that accounts for multiple artifacts that are run as separate applications  and which can be compiled packaged and tested separately without using the entire repository  Often this is used to enable certain shared libraries to change across all of the services that use those libraries  so that developers who support shared libraries can more easily evolve them instead of having to wait for each dependent team to adopt the newest version  The biggest downside of the monorepo model is that there s not much OSS tooling that supports this  because most OSS is not built this way  so large investments in tooling are usually needed to make this work   Microservices for CRUD based Applications  Before I get to how to evolve a CRUD monolith to microservices  let me further articulate the architecture needed to build your traditional mid sized CRUD platform  This type of platform covers a use case that is pretty well trod  that of  transactions  and  metadata    Transactions  User does an action that you want to persist  consistency of data is very valuable  The  Create  Update  Delete  of CRUD  Much less frequent than the  Read  actions of CRUD   Metadata  Information that describes things to the users  but is usually only modified by internal content creators  or rarely by external users  reviews  for example   Changes less frequently  often highly cacheable  Even more  can often tolerate a degree of temporary inconsistency  showing stale data    Are there more things that CRUD heavy companies want to do  especially in the analytical space here  Sure  You may want to adjust results frequently based on user behavior as the user is browsing the site  and other personalization actions  However  that is a hard thing to do real time and you don t always have the volume of data you need from the user to actually do that well  so it isn t generally the first order concern of the system   The process for moving off of a monolith in this type of architecture is relatively straightforward   Identify independent entities  This paper by Pat Helland   Life Beyond Txns   has some useful and interesting definitions there  It s better to go a little bit too big early than to go too small and end up having to implement significant distributed transactions  You probably want data owning services for the major business objects products  users  etc   and then sets of integration services that implement aggregations and logic over those objects  Pull out the logic into services entity by entity  Try not to change the data model as much as possible in this process  Redirect the monolith to call APIs in the new services as functionality is moved   That s basically it  You pull pieces out until you have enough to cover a particular set of user functionality in data and integration terms  then you can start to evolve that part of the user functionality to do new things in the services   These services are not classic SOA  but nor are they teeny tiny microservices  The services that own the data may be fairly sophisticated  You may not want to have too many services because you want to be able to satisfy requests from the user without having to make a ton of network hops  and ideally  without needing to do distributed transactions   You are probably not making new services every day  and especially if you have a sub 50 person engineering team and a long product roadmap  you may not want to invest extensive engineering time into complex orchestration and tooling that enables people to dynamically add new services at the click of a button  nb  the products to support this are getting better all the time  and so at some point this will be worth doing even for that smaller team  It is unclear to me whether that time is now or not     The equation to apply for determining how much to invest in tooling is pretty straightforward  how much time does it cost devs to have a less automated process for adding a new service  vs how long does it take to implement and maintain the automation for doing it easily  and how many new services do you expect to want to deploy over time  You re making a guess  Obviously  if you think there is value to enabling people to spin up tiny services fast and frequently  it is better to invest time and tooling into this  As with all engineering process optimization decisions  it s not a matter of getting it perfectly right  but rather  of deciding for the foreseeable future and periodically re evaluating   There are many microservices  must haves  in this instance that I have found to be anything but  I mentioned extensive orchestration above  Dynamic service discovery is also not needed if you are not automatically spinning up services or moving services around frequently  load balancers are pretty nice for doing this at a basic level    Allowing teams to choose their ideal language  framework  and data store per service is also certainly not a must have and in fact it s likely to be far more of a headache than a boon to your team   Having independent data stores for the services is also not a must have  although it does mean that you will have a high risk SPOF on the shared database  As I was writing this piece I discovered a section of some writing on microservices from 2015   Create a Separate Data Store for Each Microservice  Do not use the the same back end data store across microservices    Moreover  with a single data store it s too easy for microservices written by different teams to share database structures  perhaps in the name of reducing duplication of work  You end up with the situation where if one team updates a database structure  other services that also use that structure have to be changed too   This is true  but for smaller teams you can prevent sharing of database structures by convention  process and code review  and automated testing and checking for such access if it is a huge worry   When you carefully define the data owner services  it s less likely this will happen  And the alternative is the next paragraph   Breaking apart the data can make data management more complicated  because the separate storage systems can more easily get out sync or become inconsistent  and foreign keys can change unexpectedly  You need to add a tool that performs master data management  MDM  by operating in the background to find and fix inconsistencies  For example  it might examine every database that stores subscriber IDs  to verify that the same IDs exist in all of them  there aren t missing or extra IDs in any one database   You can write your own tool or buy one  Many commercial relational database management systems  RDBMSs  do these kinds of checks  but they usually impose too many requirements for coupling  and so don t scale  original   This paragraph probably leads to sighs of exhaustion from anyone with experience doing data reconciliation  It s due to this overhead that I encourage those of you in smaller organizations to at least evaluate a convention based approach before deciding to use entirely independent and individual data stores  This is a decision you can delay as needed   This version of the microservices architecture is very compelling for the scaled CRUD world because it lets you do a rewrite piece by piece  You can do the whole system  or you can simply take out pieces that are most sensitive to scaling  You proactively engage with many of the bits of distributed systems complexity by thinking carefully about the data and where transactions on that data will be needed  You probably don t need a ton of fancy data pipelines floating around  You know where the data will be modified   Do you have to go to microservices to scale this  Probably not  but that doesn t mean using microservices to scale such systems is a bad idea  However  going extreme with the microservices model may be a bad idea  because you really don t want to slice your data up in a way that ends up in distributed transaction land   Microservices For Data Stream Processing  Now  let s talk about a very different use case  This use case is not your classic CRUD application  thick with business rules around transactionally updated objects  Instead  this use case has a large pipeline of data  It has small bits of data flowing into it from many different sources  a very large volume of many bits of data  This large volume of input data sources also has many different services that will consume it  modify it  and pass it along for further processing   The major concern of this application is ingesting large quantities of ever changing data  processing it in various ways  and showing a view of it to customers  CRUD concerns are secondary to the larger concerns of keeping up with the data stream and recalculating information based on what is happening on that stream   Let s take a metrics aggregating SaaS application  for example  This application has customers all over the world with various applications  services  and machines that are reporting out metrics to the aggregator  These customers only need to see their data  although the combined total of data for any one customer may be very large  Our aggregator needs to consume these metrics and send them off to the application that is going to show them to the customer  The customer facing application may be operating on a combination of incoming metrics in real time plus historical data that comes from cache or a backing storage system  A large part of the value of the data is in the moving window of what is happening right now recently   This architecture from the start has considerations of volume that even our scaled CRUD world may not care about for a very  very long time  Additionally  the data itself is mostly a stream of updates over time  The notion of the  stateful  data that is transactionally updated is minimal  the most useful data is more like a timeseries or log of events  The transactional data  say  stored user views and user configuration  may be more like the  metadata  of our CRUD application in the first example  infrequently changed compared to the updates coming in from the stream  The majority of developer time is most likely spent not in dealing with these transactional changes but rather in managing the streams of inputs  providing new types of inputs  applying new calculations to the stream of inputs  and changing the calculations   In this example  you can imagine a service that wants to run an experiment by doing a different calculation across a particular element on the stream  Instead of modifying the existing code  the experimental service listens to the data stream at the same point as the existing calculation  provides a new calculation value  and pushes that calculation value back into the data pipeline on a different channel  At some point an experiment service pulls this data out for the customers who are assigned to the experimental treatment and shows the results of that calculation instead of the standard calculation  In all of these places you need a record of what happened in order to do analysis of experiment success and debugging  but that record does not need to be strongly  transactionally related to the record of other events in the system at this time  even across related users   In this example  it may very well be much more effective to spin up new services as needed  in order to run quick experiments  rather than changing existing services  Especially in cases where the service can do this without needing to worry about coordinating the data consumption or production with any existing service  This is the world of what I would like to call  stream centric microservices    If there is enormous value to your business to manage real time data streams  and you are going to have a lot of developers consuming those streams by creating new services to listen to them and produce results  then you absolutely must be willing to commit to the investment in tooling to make the process of creating services and putting them into production as easy as possible  You will probably use this for all of your services over time  once you have it  but realize that the clear value is that you have dynamic data that can be processed and manipulated and experimented on independently   Cron Jobs as Microservices  I d be remiss if I didn t mention this pattern  When it becomes very easy to make anything a microservice  everything becomes a microservice  including things we would traditionally run as cron jobs   But cron jobs are a nice concept  and not everything has to be a  service   You can use CloudWatch Events from AWS for this purpose  or scheduled Lambda functions  Use Gearman  a queue and async job runner  to schedule cron jobs  Remember your cron jobs need to be idempotent  can be run twice on the same input without changing the outcome   If you have an easy way to spin up services and it s easy to create tiny services that are basically cron jobs  no problem  but cron jobs in and of themselves are not a great reason to create a large  orchestrated services environment   Conclusion  I hope that this has been a useful breakout across a few axes of the wild world of microservices  Going through the thought experiment was very useful for me  personally  It helped me understand how what seems obvious to people at one extreme  say those who spend most of their time focused on stream processing  doesn t make as much sense for people who are more focused on the world of CRUD application scaling,"[413 823 660 397 563 428 668 1289 0 334 1331]"
428,training-dataset/engineering/138.txt,engineering,Evolutionary Database DesignIn the past decade  we ve seen the rise of agile methodologies  Compared to their predecessors they change the demands on database design  One of the most central of these demands is the idea of evolutionary architecture  On an agile project you assume that you cannot fix the requirements of the system up front  As a result having a detailed design phase at the beginning of a project becomes impractical  The architecture of the system has to evolve through the various iterations of the software  Agile methods  in particular extreme programming  XP   have a number of practices that make this evolutionary architecture practical   When we and our ThoughtWorks colleagues started doing agile projects  we realized that we needed to solve the problem of how to evolve a database to support this evolution of architecture  We began around 2000 with a project whose database ended up with around 600 tables  As we worked on this project we developed techniques that allowed to change the schema and migrate existing data comfortably  This allowed our database to be completely flexible and evolvable  We described these techniques in an earlier version of this article  a description that s inspired other teams and toolsets  Since then we ve used and further developed these techniques on hundreds of projects world wide  from small teams to large multi national programs of work  We ve been long intending to bring this article up to date  and now we ve had the chance to give it a proper refresh   A small user story like this has only a single database migration  larger stories are often broken down into several separate migrations for each change to the database  Our usual rule is to make each database change as small as possible  The smaller it is  the easier it is to get right  and any errors are quick to spot and debug  Migrations like this compose easily  so it s best to do many small ones   Once the changes are in mainline they will be picked up by the Continuous Integration server  It will run the migration scripts on the mainline copy of the database  and then run all the application tests  If all is green  this process will be repeated across the whole Deployment Pipeline   including QA and Staging environments  The same code finally is run against production  now updating the live database s schema and data   If Jen isn t too familiar with making this change  she s fortunate that it s a common one to make to a database  So she can look up it the database refactoring book   there s also a summary online    Jen runs this migration script on a local copy of database on her machine  She then proceeds to update the application code to use these new columns  As she does this she runs the existing test suite against this code to detect any changes in the application s behavior  Some tests  those that relied on the combined column need to be updated  Some tests may need to be added  Once Jen has done all this  and the application has all its tests green on her machine  Jen pushes all her changes to the shared project version control repository   which we call the mainline   These changes include the migration scripts and the application code changes   To add new columns and migrate the data  Jen writes a SQL migration script that she can run against the current schema  This will both change the schema and also migrate all the existing data in the inventory   To get a feel for how all of this works  lets outline what happens when a developer  Jen  writes some code to implement a new user story  The story states that the user should be able to see  search  and update the location  batch  and serial numbers of a product in inventory  Looking at the database schema  Jen sees that currently there s no such fields in the inventory table  just a single inventory_code field which is the concatenation of these three fields  She has to take this single code and split it into three separate fields  location_code   batch_number and serial_number    We don t consider these problems to be inherently unsolvable  After all when we wrote the original version of this article we hadn t solved the problems of 24 7 uptime or integration databases  We found ways to deal with those  and expect we ll push the limits of evolutionary database design further too  But until we do  we won t claim we can solve such problems   Increasingly we are seeing people use multiple schemas as part of a single database environment  We ve worked with projects using a handful of schemas like this  but haven t yet cranked this up to tens or hundreds of schemas  This is a situation that we anticipate having to deal with in the next few years   We have had projects with hundreds of retail stores having their own database  all of which need to upgraded together  However we haven t yet explored the situation where there is a lot of customization on such a large group of sites  An example might be a small business application that allows customization of the schema  deployed into thousands of different small companies   Before we dive into the techniques  it s important to state that we haven t solved all the problems of evolutionary database design   Since the early days we have tried to spread these techniques over more of our projects  gaining more experience from more cases and now all our projects use this approach  We ve also found inspiration  ideas  and experience from other agile practitioners   Over the course of the last decade and half we ve been involved in many large projects that have used evolutionary database design and made it work  Some projects involved more than 100 people in multiple sites world wide  Others involved over half million lines of code  over 500 tables  Some had multiple versions of the application in production and applications that needed 24 7 uptime  During these projects we have seen iterations of a month and 1 week duration  shorter iterations worked better  The techniques we describe below are the ones that we used to make this work   While these techniques have grown in use and interest  one of the biggest questions is how to make evolutionary design work for databases  For a long time  people in the database community considered database design as something that absolutely needs up front planning  Changing the database schema late in the development tended to cause wide spread breakages in application software  Furthermore changing a schema after deployment resulted in painful data migration problems   An important part of this approach is iterative development  where you run the entire software life cycle many times during the life of a project  Agile processes run complete life cycles in each iteration  completing the iteration with working  tested  integrated code for a small subset of the requirements of the final product  These iterations are short  anywhere from a few hours to a couple of weeks  more skillful teams use shorter iterations   One of the vital contributions of agile methods is that they have come up with practices that allow evolutionary design to work in a controlled manner  So instead of the common chaos that often happens when design isn t planned up front  these methods provide techniques to control evolutionary design and make them practical   In order to make this work  you need a different attitude to design  Instead of thinking of design as a phase  which is mostly completed before you begin construction  you look at design as an on going process that is interleaved with construction  testing  and even delivery  This is the contrast between planned and evolutionary design   Agile processes approach change differently  They seek to embrace change  allowing changes to occur even late in a development project  Changes are controlled  but the attitude of the process is to enable change as much as possible  Partly this is in response to the inherent instability of requirements in many projects  partly it is to better support dynamic business environments by helping them change with the competitive pressures   Such approaches look to minimize changes by doing extensive up front work  Once the early work is done  changes cause significant problems  As a result such approaches run into trouble if requirements are changing  and requirements churn is a big problem for such processes   As agile methods have spread in popularity in the early 2000s  one of their most obvious characteristics is their towards change  Before they appeared on the scene most of the thinking about software process was about understanding requirements early  signing off on these requirements  using the requirements as a basis for design  signing off on that  and then proceeding with construction  This is a plan driven cycle  often referred to  usually with derision  as the waterfall approach  The Practices  Our approach to evolutionary database design depends on several important practices   DBAs collaborate closely with developers One of the tenets of agile methods is that people with different skills and backgrounds need to collaborate very closely together  They can t communicate mainly through formal meetings and documents  Instead they need to be out talking with each other and working with each other all the time Everybody is affected by this  analysts  PMs  domain experts  developers    and DBAs  Every task that a developer works on potentially needs a DBA s help  Both the developers and the DBA need to consider whether a development task is going to make a significant change to the database schema  If so  the developer needs to consult with the DBA to decide how to make the change  The developer knows what new functionality is needed  and the DBA has a global view of the data in the application and other surrounding applications  Many times developers have visibility into the application they are working on and not necessarily into all the other upstream or downstream dependencies on the schema  Even if it s a single database application  there could be dependencies in the database that a developer isn t aware of  At any time a developer can call on the DBA and ask to pair to sort out a database change  By pairing  the developer learns about how the database works  and the DBA learns the context of the demands on the database  For most changes it s up to the developers to call on the DBA if they are concerned about the database impact of changes  But DBAs also take initiative  When they see stories that they think are likely to have a significant data impact  they can seek out the developers involved to talk through the database impact  DBAs can also review the migrations as they get committed into version control  While it s annoying to reverse a migration  we again gain from each migration being small  which makes it easier to reverse  To make this happen the DBA has to make herself approachable and available  She needs to make it easy for a developer to just pop over for a few minutes and ask some questions  maybe on the slack channel or hipchat room or whatever communication medium the developers are using  When setting up the project space  make sure the DBAs and developers sit close to each other so they can easily get together  Ensure that DBAs are told about any application design sessions so they can pop in easily  In many environments we see people erecting barriers between the DBA and application development functions  These barriers must come down for an evolutionary database design process to work   All database artifacts are version controlled with application code Developers benefit a lot from using version control for all their artifacts  application code  unit and functional tests  and other code such as build scripts  and puppet or chef scripts used to create an environment  Figure 1  All database artifacts are in version control along with other project artifacts Similarly all the database artifacts should be in version control  in the same repository that s being used by everyone else  The benefits for this are  There s only one place to look  making it easier for anyone on the project to find things   Every change to the database is stored  allowing easy audit if any problems occur  We can trace every deployment of the database to the exact state of the schema and supporting data   We prevent deployments where the database is out of sync with the application  which leads to errors retrieving and updating data   We can easily create new environments  for development  testing  and indeed production  Everything needed to create a running version of the software should be in a single repository  so it can be quickly checked out and built   All database changes are migrations In many organizations we see a process where developers make changes to a development database using schema editing tools and ad hoc SQL for standing data  Once they ve finished their development task  then DBAs compare the development database to the production database and make the corresponding changes to the production database when promoting the software to live  But doing this at production time is tricky as the context for the changes in development is lost  The purpose of the changes needs be understood again  by a different group of people  To avoid this we prefer to capture the change during development  and keep the change as a first class artifact that can be tested and deployed to production with the same process and control as changes to application code  We do this by representing every change to the database as a database migration script which is version controlled together with application code changes  These migration scripts include  schema changes  database code changes  reference data updates  transaction data updates  and fixes to production data problems caused by bugs  Here is a change adding min_insurance_value and max_insurance_value to the equipment_type table  with some default values  ALTER TABLE equipment_type ADD  min_insurance_value NUMBER 10 2   max_insurance_value NUMBER 10 2     UPDATE equipment_type SET min_insurance_value   3000  max_insurance_value   10000000  This change adds some standing data to the location and equipment_type tables     Create new warehouse locations  Request 497 INSERT INTO location  location_code  name   location_address_id  created_by  created_dt  VALUES   PA PIT 01    Pittsburgh Warehouse   4567   APP_ADMIN    SYSDATE   INSERT INTO location  location_code  name   location_address_id  created_by  created_dt  VALUES   LA MSY 01    New Orleans Warehouse   7134   APP_ADMIN    SYSDATE      Create new equipment_type  Request 562 INSERT INTO equipment_type  equipment_type_id  name  min_insurance_value  max_insurance_value  created_by  created_dt  VALUES  seq_equipment_type nextval   Lift Truck   40000  4000000   APP_ADMIN   SYSDATE   With this way of working  we never use schema editing tools such as Navicat  DBArtisan or SQL Developer to alter schemas  nor do we ever run ad hoc DDL or DML in order to add standing data or fix problems  Other than updates to the database that occur due to the application software  all changes are made by migrations  Defining migrations as sets of SQL commands is part of the story  but in order to apply them properly we need some extra things to manage them  Each migration needs a unique identification   We need to track which migrations have been applied to the database  We need to manage the sequencing constraints between the migrations  In the example above  we must apply the ALTER TABLE migration first  otherwise the second migration will fail to insert the equipment type  We handle these demands by giving each migration a sequence number  This acts as a unique identifier and ensures we can maintain the order that they re applied to the database  When a developer creates a migration she puts the SQL into a text file inside a migrations folder within the project s version control repository  She looks up the highest currently used number in the migrations folder  and uses that number together with a description to name the file  Thus the earlier pair of migrations might be called 0007_add_insurance_value_to_equipment_type sql and 0008_data_location_equipment_type    1  To track the application of the migrations to the database we use a changelog table  Database migration frameworks typically create this table and automatically update it whenever a migration is applied  This way the database can always report which migration it is synchronized with  If we don t use such a framework  after all they didn t exist when we starting doing this  we automate this with a script  Figure 2  changelog table maintained by database migration frameworks With this numbering scheme in place  we can then track changes as they apply to the many databases that we manage  Figure 3  Life of a migration script from its creation to its deployment in production Some of these data migrations may have to be released more frequently than the migrations related to new features  in that scenario  we have found it to be useful to have separate migration repository or folder for data related bug fixes  Figure 4  Separate folders to manage new feature database changes and production data fixes Each of these folders can be tracked separately by the database migration tools such as Flyway  dbdeploy  MyBatis or similar tools  with a separate table to store the migration numbers  The property flyway table in Flyway is used to change the name of the table where the migration metadata is stored  Everybody gets their own database instance Most development organizations share a single development database  which is used by all members of the organization  Perhaps a separate database is used for QA or staging  but the notion is to limit how many databases are running live  Sharing databases like this is a consequence of database instances being difficult to set up and manage  leading organizations to minimize how many there are  Controls on who can alter the schema in such situations varies  some places require all changes to be made through the DBA team  others allow any developers to change the schema of the development database  and the DBAs get involved when changes are promoted downstream  When we started work with agile database projects  we noted that application developers usually follow a pattern where they work in a private working copy of the code  People learn by trying things out  so in programming terms developers experiment with how to implement a certain feature and may make a few attempts before picking one  It s important to be able to experiment in private workspace and pushing to a shared area when things are more stable  If everyone is working in a shared area  then they are constantly interrupting each other with half done changes  Although we favor Continuous Integration  where integrations occur after no more than a few hours  the private working copy is still important  Version control systems support this work  allowing developers to work independently while supporting integrating their work in a mainline copy  This separate working works with files  but it can also work with databases  Each developer gets their own database instance which they can freely modify without touching other people s work  When they are ready they can push and share their changes  as we ll see in the next section  These separate databases can either be separate schemas on a shared server or  more commonly these days  a separate database running on a developer s laptop or workstation  A decade ago  database licensing costs could make individual database instances prohibitively expensive   but these days this is rarely the case  particularly as open source databases have grown in popularity  We ve found it handy to run a database in a virtual machine running on a developer s machine  We define the build of the database VM using Vagrant and Infrastructure As Code  so the developer doesn t need to know the details of setting up the database VM  or have to do it manually  Figure 5  Problem using a single database schema for all members on the team in development Figure 6  Every member of the team gets their own database schema for development and testing Many DBAs still see multiple databases as anathema  too difficult to work in practice  but we ve found that you can easily manage a hundred or so application database instances  The vital thing is to have tools to allow you to manipulate databases much as you would manipulate files   target name  create_schema  description  create a schema as defined in the user properties    echo message  Admin UserName    admin username      echo message  Creating Schema    db username      sql password    admin password   userid    admin username   url    db url   driver    db driver   classpath    jdbc classpath     CREATE USER   db username  IDENTIFIED BY   db password  DEFAULT TABLESPACE   db tablespace   GRANT CONNECT RESOURCE  UNLIMITED TABLESPACE TO   db username   GRANT CREATE VIEW TO   db username   ALTER USER   db username  DEFAULT ROLE ALL    sql    target  Creation of developer schemas can be automated  using the build script to reduce workload on the DBA  This automation can also be restricted just to environments in development   target name  drop_schema    echo message  Admin UserName    admin username      echo message  Working UserName    db username      sql password    admin password   userid    admin username   url    db url   driver    db driver   classpath    jdbc classpath     DROP USER   db username  CASCADE    sql    target  For example  a developer joins a project  checks out the code base and starts to setup her local development environment  She uses the template build properties file and makes changes  such as setting db username to Jen and so forth for the rest of the settings  Once these settings are done she can just run ant create_schema and get a schema of her own on the team development database server or database server on his laptop  With the schema created  she can then run the database migration script to build all the database content to populate her database instance  tables  indexes  views  sequences  stored procedures  triggers  synonyms and other database specific objects  Similarly  there are scripts to delete schemas   either because they are no longer needed  or merely because the developer wishes to clean up and start again with a fresh schema  Database environments should be phoenixes   regularly burnt down and rebuilt at will  That way there s less danger of environments accumulating characteristics that aren t reproducible  or audited  This need for a private workspace is true for developers  but also true for everyone else on the team  QA staff should create their own databases  so they also can work without danger of getting confused by changes outside their knowledge  DBAs should be able to experiment with their own database copy as they explore modeling options  or performance tuning   Developers continuously integrate database changes Although developers can experiment frequently in their own sandbox  it s vital to integrate their different changes back together frequently using Continuous Integration  CI   CI involves setting up an integration server that automatically builds and tests the mainline software  Our rule of thumb is that each developer should integrate into mainline at least once a day  Many tools exist to help with CI including  GoCD  Snap CI  Jenkins  Bamboo and Travis CI Figure 7  Database changes  migrations being developed and integrated just like application code Figure 7 shows the flow of how database migrations are developed  tested locally  checked into source control  picked up by the CI server and applied to the integration database  tested again  and packaged for downstream use  Let s take an example 1 Jen starts a development that include a database schema change  If the change is easy  such as adding a column  Jen decides how to make the change directly  If it s complicated she grabs the DBA and talks it over with her  Once she has sorted out the change  she writes the migration  ALTER TABLE project ADD projecttypeid NUMBER 10  NULL  ALTER TABLE project ADD  CONSTRAINT fk_project_projecttype FOREIGN KEY  projecttypeid  REFERENCES projecttype DEFERRABLE INITIALLY DEFERRED   UPDATE project SET projecttypeid    SELECT projecttypeid FROM projecttype WHERE name  Integration    Adding a nullable column is a backwards compatible change  so she can integrate the change without needing to change any application code  However  if it isn t a backwards compatible change  such as splitting a table  then Jen would need to modify the application code too  2 Once Jen has finished her changes  she is ready to integrate  The first step in integration is updating her local copy from mainline  These are changes other members of the team have done while she s been working on her task  She then checks her changes work with these updates by rebuilding the database and running all the tests  If she runs into problems  due to the other developers  changes interfering with hers  she needs to fix those problems on her copy  Usually such clashes are easy to sort out  but occasionally they are more involved  Often these more complex conflicts trigger a conversation between Jen and her teammates so they can sort out how to resolve overlapping changes  Once she has her local copy working again  she checks to see if any more changes have been pushed to master while she s working  if so she needs to repeat the integration with the new changes  Usually  however  it doesn t take more than one or two of these cycles before her code is fully integrated with mainline  3 Jen pushes the change to mainline  Since the change is backwards compatible with the existing application code  she can integrate the database change before updating the application code to use it   a common example of Parallel Change  4 The CI server detects the change in mainline and starts a new build which contains the database migration  5 The CI server uses its own database copy for the build  so applies the database migration script to this database to apply the changes in the migration  In addition it runs the rest of the build steps  compile  unit tests  functional tests etc  6 Once the build finishes successfully  the CI server packages the build artifacts and publishes them  These build artifacts contain the database migration scripts  so that they can be applied to the databases in downstream environments  such as a Deployment Pipeline  The build artifacts also contain the application code packaged into a jar  war  dll etc  This is exactly the practice of Continuous Integration  which is commonly used with application source code management  The steps above are just about treating the database code as another piece of source code  As such the database code   DDL  DML  Data  views  triggers  stored procedures   is kept under configuration management in the same way as the source code  Whenever we have a successful build  by packaging the database artifacts along with the application artifacts  we have a complete and synchronized version history of both application and database  With application source code  much of the pain of integrating with changes can be handled by source code control systems and using various tests in local environments  For databases there s a bit more effort involved as there is data  state  in the database that needs to preserve its business meaning   We ll talk more about automated database refactorings like this shortly   In addition the DBA needs to look at any database changes and ensure that they fit within the overall scheme of the database schema and data architecture  For all this to work smoothly  big changes shouldn t come as surprises at integration time   hence the need for the DBA to collaborate closely with the developers  We emphasize integrating frequently because we ve found that it s much easier to do frequent small integrations rather than infrequent large integrations   a case of Frequency Reduces Difficulty  The pain of integration increases exponentially with the size of the integration  so doing many small changes is much easier in practice  even though it appears counter intuitive to many   A database consists of schema and data When we talk about a database here  we mean not just the schema of the database and database code  but also a fair amount of data  This data consists of common standing data for the application  such as the inevitable list of all the states  countries  currencies  address types and various application specific data  We may also include some sample test data such as a few sample customers  orders etc  This sample data would not make it to production  unless specifically needed for sanity testing or semantic monitoring  This data is there for a number of reasons  The main reason is to enable testing  We are great believers in using a large body of automated tests to help stabilize the development of an application  Such a body of tests is a common approach in agile methods  For these tests to work efficiently  it makes sense to work on a database that is seeded with some sample test data  which all tests can assume is in place before they run  This sample data needs to be version controlled  so we know where to look for it when we need to populate a new database  and so we have a record of changes that s synchronized with the tests and application code  As well as helping test the code  this sample test data also allows to test our migrations as we alter the schema of the database  By having sample data  we are forced to ensure that any schema changes also handle sample data  In most projects we ve seen this sample data be fictional  However in a few projects we ve seen people use real data for the samples  In these cases this data s been extracted from prior legacy systems with automated data conversion scripts  Obviously you can t convert all the data right away  as in early iterations only a small part of the new database is actually built  But we can use Incremental Migration to develop the conversion scripts to provide necessary data just in time  Not just does this help flush out data conversion problems early  it makes it much easier for domain experts to work with the growing system as they are familiar with the data they are looking at and can often help to identify cases that may cause problems for the database and application design  As a result we are now of the view that you should try to introduce real data from the very first iteration of your project  We ve found Jailer to be a useful tool to help with this process   All database changes are database refactorings The changes that we make to the database alter the way the database stores information  introduces new ways to store information  or removes storage that s no longer needed  But none of the database changes  on their own  change the overall behavior of the software  Consequently we can see them as fitting the definition of a refactoring  a change made to the internal structure of software to make it easier to understand and cheaper to modify without changing its observable behavior    Refactoring  chapter 2  Recognizing this  we collected and documented many of these refactorings  By writing such a catalog  we make it easier to make these changes correctly since we can follow the steps we ve successfully used before  Scott and Pramod s book detail the steps needed for most of the database refactorings you ll need  One of the big differences about database refactorings is that they involve three different changes that have to be done together Changing the database schema  Migrating the data in the database  Changing the database access code Thus whenever we describe a database refactoring  we have to describe all three aspects of the change and ensure that all three are applied before we apply any other refactorings  Like code refactoring  database refactorings are very small  The concept of chaining together a sequence of very small changes is much the same for databases as it is for code  The three dimensional nature of the change makes it all the more important to keep to small changes  Many database refactorings  such as Introduce New Column  can be done without having to update all the code that accesses the system  If code uses the new schema without being aware of it  the column will just go unused  Many changes  however don t have this property  we call these destructive changes  Destructive changes need a bit more care  the degree of which depends on the degree of destruction involved  An example of a minor destructive change is Make Column Non Nullable  which changes a nullable column to not nullable  This is destructive because if any existing code doesn t set it to a value  then we ll get an error  We ll also get problems if there are any nulls in the existing data  We can avoid the problem with existing nulls  at the cost of slightly different ones  by assigning default data to any rows that have nulls here  For the problem of application code not assigning  or assigning null  we have two options  One is to set a default value to the column  ALTER TABLE customer MODIFY last_usage_date DEFAULT sysdate  UPDATE customer SET last_usage_date    SELECT MAX order_date  FROM order WHERE order customer_id   customer customer_id  WHERE last_usage_date IS NULL  UPDATE customer SET last_usage_date   last_updated_date WHERE last_usage_date IS NULL  ALTER TABLE customer MODIFY last_usage_date NOT NULL  The other way of dealing with a lack of assignment is to change the application code as part of the refactoring  This is the option we prefer if we can confidently get to all the code that updates the database  which is usually easy if the database is only used by a single application  but is hard if it s a shared database  A more complex case is Split Table  particularly if the access to the table is spread widely across the application code  If this is the case it s important to let everyone know that the change is coming up so they can prepare themselves for it  It may also be wise to wait for a relatively quiet moment  such as the start of an iteration  Any destructive change is much easier if database access is all channeled through a few modules of the system  That makes it easier to find and update the database access code  The important thing overall is to choose a procedure that s appropriate for the kind of change that you re making  If in doubt try to err on the side of making changes easier  Our experience is that we got burned much less frequently than many people would think  and with a strong configuration control of the entire system it s not difficult to revert should the worst happen  Taking care of database changes including DDL  DML and data migration during development  provides the most context for the data team  avoiding batch migration of all changes by data team during deployment without context  Transition phase We ve already alluded to the difficulties we run into when we get a destructive database refactoring and we can t easily change the access code  These problems grow horns and big sharp teeth when you have a shared database  which may have many applications and reports using it  In this situation  you have to take much more care over something like a Rename Table  To keep us safe from such horns and teeth  we turn to the transition phase  A transition phase is a period of time when the database supports both the old access pattern and the new ones simultaneously  This allows older systems time to migrate over to the new structures at their own pace  Figure 8  Database refactorings  being applied to legacy database and the phases it needs to take before being implemented ALTER TABLE customer RENAME to client  CREATE VIEW customer AS SELECT id  first_name  last_name FROM client  For the Rename Table example  the developer would create a script that renames the table customer to client and also creates a view named customer that existing applications can use  This Parallel Change supports new and old access  It does add complexity  so it s important that it gets removed once downsteam systems have had time to migrate  In some organizations this can be done in a couple of months and in some other organizations it may take years  Views are one technique to enable transition phases  We also make use of database triggers  which are handy for things like Rename Column  Automate the refactorings Since refactoring became well known for application code  many languages have seen good support for automated refactorings  These simplify and speed up refactoring by swiftly carrying out the various steps with no human involved to make mistakes  Such automation is also available for databases  Frameworks like Liquibase and Active Record Migrations provide a DSL to apply database refactorings  allowing a standard way to apply database migrations  However these kinds of standardized refactorings don t work so well for databases since the rules for dealing with data migration and legacy data are very dependent on a team s specific context  So we prefer to handle database refactoring by writing scripts for migration and focus on tools to automate how to apply them  We write each script  as we ve shown so far  by combining SQL DDL  for the schema change  and DML  for the data migration  and putting the result in a folder in our version controlled repository  Our automation ensures we never apply these changes manually  they are only applied by the automation tooling  That way we maintain the ordering of the refactorings and update the database metadata  We can apply the refactorings to any database instance  to bring them up to date with the latest master  or to any previous version  The tooling uses the metadata information in the database to find out its current version  then applies each refactoring between it and the desired version  We can use this approach to update development instances  test instances  and production databases  Updating production databases isn t any different to test databases  we run the same set of scripts against different data  We do prefer releasing frequently as that keeps the updates small  as that means that the updates occur more quickly and it is easier to deal with any problems that come up  The easiest way to do these updates is to take the production database down while we apply the updates  this works well for most situations  If we have to do them while keeping the application live  it is possible  but the techniques we use will need another article to explain  So far  we ve found that this technique has worked remarkably well  By breaking down all the database changes into a sequence of small  simple changes  we ve been able to make quite large changes to production data without getting ourselves in trouble  As well as automating the forward changes  you can consider automating reverse changes for each refactoring  If you do this you ll be able to back out changes to a database in the same automated way  We haven t found this to be cost effective and beneficial enough to try all the time  also we ve not had much demand for it  but it s the same basic principle  On the whole we prefer to write our migrations so that the database access section can work with both the old and new version of the database  This allows us to update the database to support a future need and make it live  have it running in production for a while  and then only push the update that uses the new data structures once we ve found they have settled down without problems  These days there are many tools that automate applying database migrations  including  Flyway  Liquibase  MyBatis migrations  DBDeploy  Here s applying a migration with Flyway  psadalag flyway 4     flyway migrate Flyway 4 0 3 by Boxfuse Database  jdbc oracle thin  localhost 1521 xe  Oracle 11 2  Successfully validated 9 migrations  execution time 00 00 021s  Creating Metadata table   JEN_DEV   schema_version  Current version of schema  JEN_DEV      Empty Schema    Migrating schema  JEN_DEV  to version 0   base version Migrating schema  JEN_DEV  to version 1   asset Migrating schema  JEN_DEV  to version 2   asset type Migrating schema  JEN_DEV  to version 3   asset parameters Migrating schema  JEN_DEV  to version 4   inventory Migrating schema  JEN_DEV  to version 5   split inventory Migrating schema  JEN_DEV  to version 6   equipment type Migrating schema  JEN_DEV  to version 7   add insurance value to equipment type Migrating schema  JEN_DEV  to version 8   data location equipment type Successfully applied 9 migrations to schema  JEN_DEV   execution time 00 00 394s   psadalag flyway 4    Clearly separate all database access code To understand the consequences of database refactorings  it s important to be able to see how the database is used by the application  If SQL is scattered willy nilly around the code base  this is very hard to do  As a result it s important to have a clear database access layer to show where the database is being used and how  To do this we suggest following one of the data source architectural patterns from P ofEAA  Having a clear database layer has a number of valuable side benefits  It minimizes the areas of the system where developers need SQL knowledge to manipulate the database  which makes life easier to developers who often are not particularly skilled with SQL  For the DBA it provides a clear section of the code that he can look at to see how the database is being used  This helps in preparing indexes  database optimization  and also looking at the SQL to see how it could be reformulated to perform better  This allows the DBA to get a better understanding of how the database is used,"[428 668 184 1289 563 823 413 845 334 562 254]"
521,training-dataset/engineering/356.txt,engineering,Barcode recovery using a priori constraintsIf you ever connected to the Internet before the 2000s  you probably remember that it made a peculiar sound  But despite becoming so familia,"[521 330 1139 413 668 59 1226 1348 855 638 334]"
562,training-dataset/engineering/1506.txt,engineering,Scaling our infrastructure to multiple data centersInstagration Pt  2  Scaling our infrastructure to multiple data centers  In 2013  about a year after we joined Facebook  200m people were using Instagram every month and we were storing 20b photos  With no slow down in sight  we began Instagration   our move from AWS servers to Facebook s infrastructure  Two years later  Instagram has grown to be a community of 400 million monthly active users with 40b photos and videos  serving over a million requests per second  To keep supporting this growth  and to make sure the community has a reliable experience on Instagram  we decided to scale our infrastructure geographically  In this post  we ll talk about why we expanded our infrastructure from one to three data centers and some of the technical challenges we met along the way   Motivation  Mike Krieger  Instagram s co founder and CTO  recently wrote a post that included a story about the time in 2012 when a huge storm in Virginia brought down nearly half of our instances  The small team spent the next 36 hours rebuilding almost all of our infrastructure  which was an experience they never wanted to repeat  Natural disasters like this have the potential to temporarily or permanently damage a data center   and we need to make sure to sustain the loss with minimal impact on user experience   Other motivations for scaling out geographically include   Resilience to regional issues  More common than natural disasters are network disconnects  power issues  etc  For example  soon after we expanded our services to Oregon  one of the racks containing memcache and async tier servers was powered off  which caused large exceptions to user requests  With our new infrastructure in place  we were able to shift traffic away from the region to mitigate the issue while we recovered from the power failure   More common than natural disasters are network disconnects  power issues  etc  For example  soon after we expanded our services to Oregon  one of the racks containing memcache and async tier servers was powered off  which caused large exceptions to user requests  With our new infrastructure in place  we were able to shift traffic away from the region to mitigate the issue while we recovered from the power failure  Flexible capacity expansion  Facebook has a few data centers  It is much easier to expand Instagram s capacity where it is available when our infrastructure is ready to expand beyond one region and even when there is considerable delay in network latency  It helps to make quick decisions on getting new features ready for users without having to scramble for infrastructure resources to support them   From One to Two  So how did we start spreading things out  First let s take a look at Instagram s overall infrastructure stack   The key to expanding to multiple data centers is to distinguish global data and local data  Global data needs to be replicated across data centers  while local data can be different for each region  for example  the async jobs created by web server would only be viewed in that region    The next consideration is hardware resources  These can be roughly divided into three types  storage  computing and caching   Storage  Instagram mainly uses two backend database systems  PostgreSQL and Cassandra  Both PostgreSQL and Cassandra have mature replication frameworks that work well as a globally consistent data store   Global data neatly maps to data stored in these servers  The goal is to have eventual consistency of these data across data centers  but with potential delay  Because there are vastly more read than write operations  having read replica each region avoids cross data center reads from web servers   Writing to PostgreSQL  however  still goes across data centers because they always write to the primary   CPU Processing  Web servers  async servers are both easily distributed computing resources that are stateless  and only need to access data locally  Web servers can create async jobs that are queued by async message brokers  and then consumed by async servers  all in the same region   Caching  The cache layer is the web servers  most frequently accessed tier  and they need to be collocated within a data center to avoid user request latency  This means that updates to cache in one data center are not reflected in another data center  therefore creating a challenge for moving to multiple data centers   Imagine a user commented on your newly posted photo  In the one data center case  the web server that served the request can just update the cache with the new comment  A follower will see the new comment from the same cache   In the multi data center scenario  however  if the commenter and the follower are served in different regions  the follower s regional cache will not be updated and the user will not see the comment   Our solution is to use PgQ and enhance it to insert cache invalidation events to the databases that are being modified   On the primary side   Web server inserts a comment to PostgreSQL DB   Web server inserts a cache invalidation entry to the same DB   On the replica side   Replicate primary DB  including both the newly inserted comment as well as the cache invalidation entry  Cache invalidation process reads the cache invalidation entry and invalidates regional cache  Djangos will read from DB with the newly inserted comment and refill the cache  This solves the cache consistency issue  On the other hand  compared to the one region case where django servers directly update cache without re reading from DB  this would create increased read load on databases  In order to mitigate this problem  we took two approaches  1  reduce computational resources needed for each read by denormalizing counters  2  reduce number of reads by using cache leases   De normalizing Counters  The most commonly cached keys are counters  For example  we would use a counter to determine the number of people who liked a specific post from Justin Bieber  When there was just one region  we would update the memcache counters by incrementing from web servers  therefore avoiding a  select count     call to the database  which would take hundreds of milliseconds   But with two regions and PgQ invalidation  each new like creates a cache invalidation event to the counter  This will create a lot of  select count      especially on hot objects   To reduce the resources needed for each of these operations  we denormalized the counter for likes on the post  Whenever a new like comes in  the count is increased in the database  Therefore  each read of the count will just be a simple  select  which is a lot more efficient   There is also an added benefit of denormalizing counters in the same database where the liker to the post is stored  Both updates can be included in one transaction  making the updates atomic and consistent all the time  Whereas before the change  the counter in cache could be inconsistent with what was stored in the database due to timeout  retries etc   Memcache Lease  In the above example of a new post from Justin Bieber  during the first few minutes of the post  both the viewing of the new post and likes for the post spikes  With each new like  the counter is deleted from cache  It is very common that multiple web servers would try to retrieve the same counter from cache  but it will have a  cache miss   If they all go to the database server for retrieval  it would create a thundering herd problem   We used memcache lease mechanism to solve this problem  It works likes this   Web server issues a  lease get   not the normal  get  to memcache server   Memcache server returns the value if it s a hit  In this case  it is no different than a normal  get    If the memcache server does not find the key  it returns a  first miss  to just one web server within  n  seconds  any other  lease get  requests during that time will get a  hot miss   In the case of  hot miss where the key had been deleted from cache  recently   it would return stale value  If the cache key is not filled within  n  seconds  it again issues a  first miss  to a  lease get  request   When a web server receives  first miss   it goes to the database to retrieve data and fill the cache   When a web server receives  hot miss  with a stale value  it can typically use that value  If it receives  hot miss  without any value  it can choose to wait for the cache to be filled by the  first miss  web server   In summary  with both of the above implementations  we can mitigate the increased database load by reducing the number of accesses to the database  as well as the resources required for each access   It also improved the reliability of our backend in the cases when some hot counters fall out of cache  which wasn t an infrequent occurrence in early days of Instagram  Each of these occurrence would cause some hurried work from an engineer to manually fix the cache  With these changes  those incidents have become memories for old timer engineers   From 10ms Latency to 60ms  So far  we have focused mostly on cache consistency when caches become regional  Network latency between data centers across the continent was another challenge that impacted multiple designs  Between data centers  a 60ms network latency can cause problems in database replication as well as web servers  updates to the database  We needed to solve the following problems in order to support a seamless expansion   PostgreSQL Read Replicas Can t Catch up  As a Postgres primary takes in writes  it generates delta logs  The faster the writes come in  the more frequent these logs are generated  The primaries themselves store the most recent log files for occasional needs from the replicas  but they archive all the logs to storage to make sure that they are saved and accessible by any replicas that need older data than what the primary has retained  This way  the primary does not run out of disk space   When we build a new readreplica  the readreplica starts to read a snapshot of the database from the primary  Once it s done  it needs to apply the logs that have happened since the snapshot to the database  When all the logs are applied  it will be up to date and can stream from the primary and serve reads from web servers   However  when a large database s write rate is quite high  and there is a lot of network latency between the replica and storage device  it is possible that the rate at which the logs are read is slower than the log creation rate  The replica will fall further and further behind and never catch up   Our fix was to start a second streamer on the new readreplica as soon as it starts to transfer the base snapshot from the primary  It streams logs and stores it on local disk  When snapshot finishes transfer  the readreplica can read the logs locally  making it a much faster recovery process   This not only solved our database replication issues across the US  but also cut down the time it took to build a new replica by half  Now  even if the primary and replica are in the same region  operational efficiency is drastically increased   Summary  Instagram is now running in multiple data centers across the US  giving us more flexible capacity planning and acquisition  higher reliability  and better preparedness for the kind of natural disaster that happened in 2012  In fact  we recently survived a staged  disaster   Facebook regularly tests its data centers by shutting them down during peak hours  About a month ago  right as we had finished migrated our data to a new data center  Facebook ran a test and took it down  It was a high stakes simulation  but fortunately we survived the loss of capacity without users noticing  Instagration part 2 was a success,"[562 563 668 1289 428 397 831 413 823 1331 1414]"
563,training-dataset/engineering/561.txt,engineering,donnemartin system design primer  Learn how to design large scale systems  Prep for the system design interview The System Design Primer      Learn how to design large scale systems  Prep for the system design interview   Learn how to design large scale systems  Learning how to design scalable systems will help you become a better engineer   System design is a broad topic  There is a vast amount of resources scattered throughout the web on system design principles   This repo is an organized collection of resources to help you learn how to build systems at scale   Learn from the open source community  This is an early draft of a continually updated  open source project   Contributions are welcome   Prep for the system design interview  In addition to coding interviews  system design is a required component of the technical interview process at many tech companies   Practice common system design interview questions and compare your results with sample solutions  discussions  code  and diagrams   Additional topics for interview prep   Anki flashcards      The provided Anki flashcard decks use spaced repetition to help you retain key system design concepts   Great for use while on the go   Learn from the community   Feel free to submit pull requests to help   Fix errors  Improve sections  Add new sections  Content that needs some polishing is placed under development   Review the Contributing Guidelines   Interested in translating  Please see the following ticket   Index of system design topics  Summaries of various system design topics  including pros and cons  Everything is a trade off  Each section contains links to more in depth resources       Study guide  Suggested topics to review based on your interview timeline  short  medium  long    Q  For interviews  do I need to know everything here   A  No  you don t need to know everything here to prepare for the interview   What you are asked in an interview depends on variables such as   How much experience you have  What your technical background is  What positions you are interviewing for  Which companies you are interviewing with  Luck  More experienced candidates are generally expected to know more about system design  Architects or team leads might be expected to know more than individual contributors  Top tech companies are likely to have one or more design interview rounds   Start broad and go deeper in a few areas  It helps to know a little about various key system design topics  Adjust the following guide based on your timeline  experience  what positions you are interviewing for  and which companies you are interviewing with   Short timeline   Aim for breadth with system design topics  Practice by solving some interview questions     Aim for with system design topics  Practice by solving interview questions  Medium timeline   Aim for breadth and some depth with system design topics  Practice by solving many interview questions     Aim for and with system design topics  Practice by solving interview questions  Long timeline   Aim for breadth and more depth with system design topics  Practice by solving most interview questions   How to approach a system design interview question  How to tackle a system design interview question   The system design interview is an open ended conversation  You are expected to lead it   You can use the following steps to guide the discussion  To help solidify this process  work through the System design interview questions with solutions section using the following steps   Step 1  Outline use cases  constraints  and assumptions  Gather requirements and scope the problem  Ask questions to clarify use cases and constraints  Discuss assumptions   Who is going to use it   How are they going to use it   How many users are there   What does the system do   What are the inputs and outputs of the system   How much data do we expect to handle   How many requests per second do we expect   What is the expected read to write ratio   Step 2  Create a high level design  Outline a high level design with all important components   Sketch the main components and connections  Justify your ideas  Step 3  Design core components  Dive into details for each core component  For example  if you were asked to design a url shortening service  discuss   Generating and storing a hash of the full url MD5 and Base62 Hash collisions SQL or NoSQL Database schema  Translating a hashed url to the full url Database lookup  API and object oriented design  Step 4  Scale the design  Identify and address bottlenecks  given the constraints  For example  do you need the following to address scalability issues   Load balancer  Horizontal scaling  Caching  Database sharding  Discuss potential solutions and trade offs  Everything is a trade off  Address bottlenecks using principles of scalable system design   Back of the envelope calculations  You might be asked to do some estimates by hand  Refer to the Appendix for the following resources   Source s  and further reading  Check out the following links to get a better idea of what to expect   System design interview questions with solutions  Common system design interview questions with sample discussions  code  and diagrams  Solutions linked to content in the solutions  folder   Question Design Pastebin com  or Bit ly  Solution Design the Twitter timeline  or Facebook feed   Design Twitter search  or Facebook search  Solution Design a web crawler Solution Design Mint com Solution Design the data structures for a social network Solution Design a key value store for a search engine Solution Design Amazon s sales ranking by category feature Solution Design a system that scales to millions of users on AWS Solution Add a system design question Contribute  Design Pastebin com  or Bit ly   View exercise and solution  Design the Twitter timeline and search  or Facebook feed and search   View exercise and solution  Design a web crawler  View exercise and solution  Design Mint com  View exercise and solution  Design the data structures for a social network  View exercise and solution  Design a key value store for a search engine  View exercise and solution  Design Amazon s sales ranking by category feature  View exercise and solution  Design a system that scales to millions of users on AWS  View exercise and solution  Object oriented design interview questions with solutions  Common object oriented design interview questions with sample discussions  code  and diagrams  Solutions linked to content in the solutions  folder   Note  This section is under development  Question Design a hash map Solution Design a least recently used cache Solution Design a call center Solution Design a deck of cards Solution Design a parking lot Solution Design a chat server Solution Design a circular array Contribute Add an object oriented design question Contribute  System design topics  start here  New to system design   First  you ll need a basic understanding of common principles  learning about what they are  how they are used  and their pros and cons   Step 1  Review the scalability video lecture  Scalability Lecture at Harvard  Topics covered  Vertical scaling Horizontal scaling Caching Load balancing Database replication Database partitioning    Step 2  Review the scalability article  Scalability  Topics covered  Clones Databases Caches Asynchronism    Next steps  Next  we ll look at high level trade offs   Performance vs scalability  vs Latency vs throughput  vs Availability vs consistency  Keep in mind that everything is a trade off   Then we ll dive into more specific topics such as DNS  CDNs  and load balancers   Performance vs scalability  A service is scalable if it results in increased performance in a manner proportional to resources added  Generally  increasing performance means serving more units of work  but it can also be to handle larger units of work  such as when datasets grow 1  Another way to look at performance vs scalability   If you have a performance problem  your system is slow for a single user   problem  your system is slow for a single user  If you have a scalability problem  your system is fast for a single user but slow under heavy load   Source s  and further reading  Latency vs throughput  Latency is the time to perform some action or to produce some result   Throughput is the number of such actions or results per unit of time   Generally  you should aim for maximal throughput with acceptable latency   Source s  and further reading  Availability vs consistency  CAP theorem    Source  CAP theorem revisited  In a distributed computer system  you can only support two of the following guarantees   Consistency   Every read receives the most recent write or an error    Every read receives the most recent write or an error Availability   Every request receives a response  without guarantee that it contains the most recent version of the information    Every request receives a response  without guarantee that it contains the most recent version of the information Partition Tolerance   The system continues to operate despite arbitrary partitioning due to network failures  Networks aren t reliable  so you ll need to support partition tolerance  You ll need to make a software tradeoff between consistency and availability   CP   consistency and partition tolerance  Waiting for a response from the partitioned node might result in a timeout error  CP is a good choice if your business needs require atomic reads and writes   AP   availability and partition tolerance  Responses return the most recent version of the data  which might not be the latest  Writes might take some time to propagate when the partition is resolved   AP is a good choice if the business needs allow for eventual consistency or when the system needs to continue working despite external errors   Source s  and further reading  Consistency patterns  With multiple copies of the same data  we are faced with options on how to synchronize them so clients have a consistent view of the data  Recall the definition of consistency from the CAP theorem   Every read receives the most recent write or an error   Weak consistency  After a write  reads may or may not see it  A best effort approach is taken   This approach is seen in systems such as memcached  Weak consistency works well in real time use cases such as VoIP  video chat  and realtime multiplayer games  For example  if you are on a phone call and lose reception for a few seconds  when you regain connection you do not hear what was spoken during connection loss   Eventual consistency  After a write  reads will eventually see it  typically within milliseconds   Data is replicated asynchronously   This approach is seen in systems such as DNS and email  Eventual consistency works well in highly available systems   Strong consistency  After a write  reads will see it  Data is replicated synchronously   This approach is seen in file systems and RDBMSes  Strong consistency works well in systems that need transactions   Source s  and further reading  Availability patterns  There are two main patterns to support high availability  fail over and replication   With active passive fail over  heartbeats are sent between the active and the passive server on standby  If the heartbeat is interrupted  the passive server takes over the active s IP address and resumes service   The length of downtime is determined by whether the passive server is already running in  hot  standby or whether it needs to start up from  cold  standby  Only the active server handles traffic   Active passive failover can also be referred to as master slave failover   In active active  both servers are managing traffic  spreading the load between them   If the servers are public facing  the DNS would need to know about the public IPs of both servers  If the servers are internal facing  application logic would need to know about both servers   Active active failover can also be referred to as master master failover   Disadvantage s   failover  Fail over adds more hardware and additional complexity   There is a potential for loss of data if the active system fails before any newly written data can be replicated to the passive   Master slave and master master  This topic is further discussed in the Database section   Domain name system    Source  DNS security presentation  A Domain Name System  DNS  translates a domain name such as www example com to an IP address   DNS is hierarchical  with a few authoritative servers at the top level  Your router or ISP provides information about which DNS server s  to contact when doing a lookup  Lower level DNS servers cache mappings  which could become stale due to DNS propagation delays  DNS results can also be cached by your browser or OS for a certain period of time  determined by the time to live  TTL    NS record  name server    Specifies the DNS servers for your domain subdomain     Specifies the DNS servers for your domain subdomain  MX record  mail exchange    Specifies the mail servers for accepting messages     Specifies the mail servers for accepting messages  A record  address    Points a name to an IP address     Points a name to an IP address  CNAME  canonical    Points a name to another name or CNAME  example com to www example com  or to an A record   Services such as CloudFlare and Route 53 provide managed DNS services  Some DNS services can route traffic through various methods   Weighted round robin Prevent traffic from going to servers under maintenance Balance between varying cluster sizes A B testing  Latency based  Geolocation based  Disadvantage s   DNS  Accessing a DNS server introduces a slight delay  although mitigated by caching described above   DNS server management could be complex  although they are generally managed by governments  ISPs  and large companies   DNS services have recently come under DDoS attack  preventing users from accessing websites such as Twitter without knowing Twitter s IP address es    Source s  and further reading  Content delivery network    Source  Why use a CDN  A content delivery network  CDN  is a globally distributed network of proxy servers  serving content from locations closer to the user  Generally  static files such as HTML CSS JS  photos  and videos are served from CDN  although some CDNs such as Amazon s CloudFront support dynamic content  The site s DNS resolution will tell clients which server to contact   Serving content from CDNs can significantly improve performance in two ways   Users receive content at data centers close to them  Your servers do not have to serve requests that the CDN fulfills  Push CDNs  Push CDNs receive new content whenever changes occur on your server  You take full responsibility for providing content  uploading directly to the CDN and rewriting URLs to point to the CDN  You can configure when content expires and when it is updated  Content is uploaded only when it is new or changed  minimizing traffic  but maximizing storage   Sites with a small amount of traffic or sites with content that isn t often updated work well with push CDNs  Content is placed on the CDNs once  instead of being re pulled at regular intervals   Pull CDNs  Pull CDNs grab new content from your server when the first user requests the content  You leave the content on your server and rewrite URLs to point to the CDN  This results in a slower request until the content is cached on the server   A time to live  TTL  determines how long content is cached  Pull CDNs minimize storage space on the CDN  but can create redundant traffic if files expire and are pulled before they have actually changed   Sites with heavy traffic work well with pull CDNs  as traffic is spread out more evenly with only recently requested content remaining on the CDN   Disadvantage s   CDN  CDN costs could be significant depending on traffic  although this should be weighed with additional costs you would incur not using a CDN   Content might be stale if it is updated before the TTL expires it   CDNs require changing URLs for static content to point to the CDN   Source s  and further reading  Load balancer    Source  Scalable system design patterns  Load balancers distribute incoming client requests to computing resources such as application servers and databases  In each case  the load balancer returns the response from the computing resource to the appropriate client  Load balancers are effective at   Preventing requests from going to unhealthy servers  Preventing overloading resources  Helping eliminate single points of failure  Load balancers can be implemented with hardware  expensive  or with software such as HAProxy   Additional benefits include   SSL termination   Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations Removes the need to install X 509 certificates on each server    Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations Session persistence   Issue cookies and route a specific client s requests to same instance if the web apps do not keep track of sessions  To protect against failures  it s common to set up multiple load balancers  either in active passive or active active mode   Load balancers can route traffic based on various metrics  including   Layer 4 load balancing  Layer 4 load balancers look at info at the transport layer to decide how to distribute requests  Generally  this involves the source  destination IP addresses  and ports in the header  but not the contents of the packet  Layer 4 load balancers forward network packets to and from the upstream server  performing Network Address Translation  NAT    layer 7 load balancing  Layer 7 load balancers look at the application layer to decide how to distribute requests  This can involve contents of the header  message  and cookies  Layer 7 load balancers terminates network traffic  reads the message  makes a load balancing decision  then opens a connection to the selected server  For example  a layer 7 load balancer can direct video traffic to servers that host videos while directing more sensitive user billing traffic to security hardened servers   At the cost of flexibility  layer 4 load balancing requires less time and computing resources than Layer 7  although the performance impact can be minimal on modern commodity hardware   Horizontal scaling  Load balancers can also help with horizontal scaling  improving performance and availability  Scaling out using commodity machines is more cost efficient and results in higher availability than scaling up a single server on more expensive hardware  called Vertical Scaling  It is also easier to hire for talent working on commodity hardware than it is for specialized enterprise systems   Disadvantage s   horizontal scaling  Scaling horizontally introduces complexity and involves cloning servers Servers should be stateless  they should not contain any user related data like sessions or profile pictures Sessions can be stored in a centralized data store such as a database  SQL  NoSQL  or a persistent cache  Redis  Memcached   Downstream servers such as caches and databases need to handle more simultaneous connections as upstream servers scale out  Disadvantage s   load balancer  The load balancer can become a performance bottleneck if it does not have enough resources or if it is not configured properly   Introducing a load balancer to help eliminate single points of failure results in increased complexity   A single load balancer is a single point of failure  configuring multiple load balancers further increases complexity   Source s  and further reading  Reverse proxy  web server     Source  Wikipedia    A reverse proxy is a web server that centralizes internal services and provides unified interfaces to the public  Requests from clients are forwarded to a server that can fulfill it before the reverse proxy returns the server s response to the client   Additional benefits include   Increased security   Hide information about backend servers  blacklist IPs  limit number of connections per client    Hide information about backend servers  blacklist IPs  limit number of connections per client Increased scalability and flexibility   Clients only see the reverse proxy s IP  allowing you to scale servers or change their configuration    Clients only see the reverse proxy s IP  allowing you to scale servers or change their configuration SSL termination   Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations Removes the need to install X 509 certificates on each server    Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations Compression   Compress server responses    Compress server responses Caching   Return the response for cached requests    Return the response for cached requests Static content   Serve static content directly HTML CSS JS Photos Videos Etc    Serve static content directly  Load balancer vs reverse proxy  Deploying a load balancer is useful when you have multiple servers  Often  load balancers route traffic to a set of servers serving the same function   Reverse proxies can be useful even with just one web server or application server  opening up the benefits described in the previous section   Solutions such as NGINX and HAProxy can support both layer 7 reverse proxying and load balancing   Disadvantage s   reverse proxy  Introducing a reverse proxy results in increased complexity   A single reverse proxy is a single point of failure  configuring multiple reverse proxies  ie a failover  further increases complexity   Source s  and further reading  Application layer    Source  Intro to architecting systems for scale  Separating out the web layer from the application layer  also known as platform layer  allows you to scale and configure both layers independently  Adding a new API results in adding application servers without necessarily adding additional web servers   The single responsibility principle advocates for small and autonomous services that work together  Small teams with small services can plan more aggressively for rapid growth   Workers in the application layer also help enable asynchronism   Related to this discussion are microservices  which can be described as a suite of independently deployable  small  modular services  Each service runs a unique process and communicates through a well defined  lightweight mechanism to serve a business goal  1  Pinterest  for example  could have the following microservices  user profile  follower  feed  search  photo upload  etc   Service Discovery  Systems such as Consul  Etcd  and Zookeeper can help services find each other by keeping track of registered names  addresses  and ports  Health checks help verify service integrity and are often done using an HTTP endpoint  Both Consul and Etcd have a built in key value store that can be useful for storing config values and other shared data   Disadvantage s   application layer  Adding an application layer with loosely coupled services requires a different approach from an architectural  operations  and process viewpoint  vs a monolithic system    Microservices can add complexity in terms of deployments and operations   Source s  and further reading    Source  Scaling up to your first 10 million users  Relational database management system  RDBMS   A relational database like SQL is a collection of data items organized in tables   ACID is a set of properties of relational database transactions   Atomicity   Each transaction is all or nothing    Each transaction is all or nothing Consistency   Any transaction will bring the database from one valid state to another    Any transaction will bring the database from one valid state to another Isolation   Executing transactions concurrently has the same results as if the transactions were executed serially    Executing transactions concurrently has the same results as if the transactions were executed serially Durability   Once a transaction has been committed  it will remain so  There are many techniques to scale a relational database  master slave replication  master master replication  federation  sharding  denormalization  and SQL tuning   Master slave replication  The master serves reads and writes  replicating writes to one or more slaves  which serve only reads  Slaves can also replicate to additional slaves in a tree like fashion  If the master goes offline  the system can continue to operate in read only mode until a slave is promoted to a master or a new master is provisioned     Source  Scalability  availability  stability  patterns  Disadvantage s   master slave replication  Additional logic is needed to promote a slave to a master   See Disadvantage s   replication for points related to both master slave and master master   Master master replication  Both masters serve reads and writes and coordinate with each other on writes  If either master goes down  the system can continue to operate with both reads and writes     Source  Scalability  availability  stability  patterns  Disadvantage s   master master replication  You ll need a load balancer or you ll need to make changes to your application logic to determine where to write   Most master master systems are either loosely consistent  violating ACID  or have increased write latency due to synchronization   Conflict resolution comes more into play as more write nodes are added and as latency increases   See Disadvantage s   replication for points related to both master slave and master master   Disadvantage s   replication  There is a potential for loss of data if the master fails before any newly written data can be replicated to other nodes   Writes are replayed to the read replicas  If there are a lot of writes  the read replicas can get bogged down with replaying writes and can t do as many reads   The more read slaves  the more you have to replicate  which leads to greater replication lag   On some systems  writing to the master can spawn multiple threads to write in parallel  whereas read replicas only support writing sequentially with a single thread   Replication adds more hardware and additional complexity   Source s  and further reading  replication    Source  Scaling up to your first 10 million users  Federation  or functional partitioning  splits up databases by function  For example  instead of a single  monolithic database  you could have three databases  forums  users  and products  resulting in less read and write traffic to each database and therefore less replication lag  Smaller databases result in more data that can fit in memory  which in turn results in more cache hits due to improved cache locality  With no single central master serializing writes you can write in parallel  increasing throughput   Disadvantage s   federation  Federation is not effective if your schema requires huge functions or tables   You ll need to update your application logic to determine which database to read and write   Joining data from two databases is more complex with a server link   Federation adds more hardware and additional complexity   Source s  and further reading  federation    Source  Scalability  availability  stability  patterns  Sharding distributes data across different databases such that each database can only manage a subset of the data  Taking a users database as an example  as the number of users increases  more shards are added to the cluster   Similar to the advantages of federation  sharding results in less read and write traffic  less replication  and more cache hits  Index size is also reduced  which generally improves performance with faster queries  If one shard goes down  the other shards are still operational  although you ll want to add some form of replication to avoid data loss  Like federation  there is no single central master serializing writes  allowing you to write in parallel with increased throughput   Common ways to shard a table of users is either through the user s last name initial or the user s geographic location   Disadvantage s   sharding  You ll need to update your application logic to work with shards  which could result in complex SQL queries   Data distribution can become lopsided in a shard  For example  a set of power users on a shard could result in increased load to that shard compared to others  Rebalancing adds additional complexity  A sharding function based on consistent hashing can reduce the amount of transferred data   Joining data from multiple shards is more complex   Sharding adds more hardware and additional complexity   Source s  and further reading  sharding  Denormalization attempts to improve read performance at the expense of some write performance  Redundant copies of the data are written in multiple tables to avoid expensive joins  Some RDBMS such as PostgreSQL and Oracle support materialized views which handle the work of storing redundant information and keeping redundant copies consistent   Once data becomes distributed with techniques such as federation and sharding  managing joins across data centers further increases complexity  Denormalization might circumvent the need for such complex joins   In most systems  reads can heavily number writes 100 1 or even 1000 1  A read resulting in a complex database join can be very expensive  spending a significant amount of time on disk operations   Disadvantage s   denormalization  Data is duplicated   Constraints can help redundant copies of information stay in sync  which increases complexity of the database design   A denormalized database under heavy write load might perform worse than its normalized counterpart   Source s  and further reading  denormalization  SQL tuning  SQL tuning is a broad topic and many books have been written as reference   It s important to benchmark and profile to simulate and uncover bottlenecks   Benchmark   Simulate high load situations with tools such as ab     Simulate high load situations with tools such as ab  Profile   Enable tools such as the slow query log to help track performance issues   Benchmarking and profiling might point you to the following optimizations   Tighten up the schema  MySQL dumps to disk in contiguous blocks for fast access   Use CHAR instead of VARCHAR for fixed length fields  CHAR effectively allows for fast  random access  whereas with VARCHAR   you must find the end of a string before moving onto the next one   instead of for fixed length fields  Use TEXT for large blocks of text such as blog posts  TEXT also allows for boolean searches  Using a TEXT field results in storing a pointer on disk that is used to locate the text block   for large blocks of text such as blog posts  also allows for boolean searches  Using a field results in storing a pointer on disk that is used to locate the text block  Use INT for larger numbers up to 2 32 or 4 billion   for larger numbers up to 2 32 or 4 billion  Use DECIMAL for currency to avoid floating point representation errors   for currency to avoid floating point representation errors  Avoid storing large BLOBS   store the location of where to get the object instead     store the location of where to get the object instead  VARCHAR 255  is the largest number of characters that can be counted in an 8 bit number  often maximizing the use of a byte in some RDBMS   is the largest number of characters that can be counted in an 8 bit number  often maximizing the use of a byte in some RDBMS  Set the NOT NULL constraint where applicable to improve search performance   Use good indices  Columns that you are querying   SELECT   GROUP BY   ORDER BY   JOIN   could be faster with indices           could be faster with indices  Indices are usually represented as self balancing B tree that keeps data sorted and allows searches  sequential access  insertions  and deletions in logarithmic time   Placing an index can keep the data in memory  requiring more space   Writes could also be slower since the index also needs to be updated   When loading large amounts of data  it might be faster to disable indices  load the data  then rebuild the indices   Avoid expensive joins  Denormalize where performance demands it   Partition tables  Break up a table by putting hot spots in a separate table to help keep it in memory   Tune the query cache  In some cases  the query cache could lead to performance issues   Source s  and further reading  SQL tuning  NoSQL is a collection of data items represented in a key value store  document store  wide column store  or a graph database  Data is denormalized  and joins are generally done in the application code  Most NoSQL stores lack true ACID transactions and favor eventual consistency   BASE is often used to describe the properties of NoSQL databases  In comparison with the CAP Theorem  BASE chooses availability over consistency   Basically available   the system guarantees availability     the system guarantees availability  Soft state   the state of the system may change over time  even without input     the state of the system may change over time  even without input  Eventual consistency   the system will become consistent over a period of time  given that the system doesn t receive input during that period   In addition to choosing between SQL or NoSQL  it is helpful to understand which type of NoSQL database best fits your use case s   We ll review key value stores  document stores  wide column stores  and graph databases in the next section   Key value store  Abstraction  hash table  A key value store generally allows for O 1  reads and writes and is often backed by memory or SSD  Data stores can maintain keys in lexicographic order  allowing efficient retrieval of key ranges  Key value stores can allow for storing of metadata with a value   Key value stores provide high performance and are often used for simple data models or for rapidly changing data  such as an in memory cache layer  Since they offer only a limited set of operations  complexity is shifted to the application layer if additional operations are needed   A key value store is the basis for more complex systems such as a document store  and in some cases  a graph database   Source s  and further reading  key value store  Document store  Abstraction  key value store with documents stored as values  A document store is centered around documents  XML  JSON  binary  etc   where a document stores all information for a given object  Document stores provide APIs or a query language to query based on the internal structure of the document itself  Note  many key value stores include features for working with a value s metadata  blurring the lines between these two storage types   Based on the underlying implementation  documents are organized in either collections  tags  metadata  or directories  Although documents can be organized or grouped together  documents may have fields that are completely different from each other   Some document stores like MongoDB and CouchDB also provide a SQL like language to perform complex queries  DynamoDB supports both key values and documents   Document stores provide high flexibility and are often used for working with occasionally changing data   Source s  and further reading  document store  Wide column store    Source  SQL   NoSQL  a brief history  Abstraction  nested map ColumnFamily RowKey  Columns ColKey  Value  Timestamp    A wide column store s basic unit of data is a column  name value pair   A column can be grouped in column families  analogous to a SQL table   Super column families further group column families  You can access each column independently with a row key  and columns with the same row key form a row  Each value contains a timestamp for versioning and for conflict resolution   Google introduced Bigtable as the first wide column store  which influenced the open source HBase often used in the Hadoop ecosystem  and Cassandra from Facebook  Stores such as BigTable  HBase  and Cassandra maintain keys in lexicographic order  allowing efficient retrieval of selective key ranges   Wide column stores offer high availability and high scalability  They are often used for very large data sets   Source s  and further reading  wide column store  Graph database    Source  Graph database  Abstraction  graph  In a graph database  each node is a record and each arc is a relationship between two nodes  Graph databases are optimized to represent complex relationships with many foreign keys or many to many relationships   Graphs databases offer high performance for data models with complex relationships  such as a social network  They are relatively new and are not yet widely used  it might be more difficult to find development tools and resources  Many graphs can only be accessed with REST APIs   Source s  and further reading  graph  Source s  and further reading  NoSQL  SQL or NoSQL    Source  Transitioning from RDBMS to NoSQL  Reasons for SQL   Structured data  Strict schema  Relational data  Need for complex joins  Transactions  Clear patterns for scaling  More established  developers  community  code  tools  etc  Lookups by index are very fast  Reasons for NoSQL   Semi structured data  Dynamic or flexible schema  Non relational data  No need for complex joins  Store many TB  or PB  of data  Very data intensive workload  Very high throughput for IOPS  Sample data well suited for NoSQL   Rapid ingest of clickstream and log data  Leaderboard or scoring data  Temporary data  such as a shopping cart  Frequently accessed   hot   tables  Metadata lookup tables  Source s  and further reading  SQL or NoSQL    Source  Scalable system design patterns  Caching improves page load times and can reduce the load on your servers and databases  In this model  the dispatcher will first lookup if the request has been made before and try to find the previous result to return  in order to save the actual execution   Databases often benefit from a uniform distribution of reads and writes across its partitions  Popular items can skew the distribution  causing bottlenecks  Putting a cache in front of a database can help absorb uneven loads and spikes in traffic   Client caching  Caches can be located on the client side  OS or browser   server side  or in a distinct cache layer   CDN caching  CDNs are considered a type of cache   Web server caching  Reverse proxies and caches such as Varnish can serve static and dynamic content directly  Web servers can also cache requests  returning responses without having to contact application servers   Database caching  Your database usually includes some level of caching in a default configuration  optimized for a generic use case  Tweaking these settings for specific usage patterns can further boost performance   Application caching  In memory caches such as Memcached and Redis are key value stores between your application and your data storage  Since the data is held in RAM  it is much faster than typical databases where data is stored on disk  RAM is more limited than disk  so cache invalidation algorithms such as least recently used  LRU  can help invalidate  cold  entries and keep  hot  data in RAM   Redis has the following additional features   Persistence option  Built in data structures such as sorted sets and lists  There are multiple levels you can cache that fall into two general categories  database queries and objects   Row level  Query level  Fully formed serializable objects  Fully rendered HTML  Generally  you should try to avoid file based caching  as it makes cloning and auto scaling more difficult   Caching at the database query level  Whenever you query the database  hash the query as a key and store the result to the cache  This approach suffers from expiration issues   Hard to delete a cached result with complex queries  If one piece of data changes such as a table cell  you need to delete all cached queries that might include the changed cell  Caching at the object level  See your data as an object  similar to what you do with your application code  Have your application assemble the dataset from the database into a class instance or a data structure s    Remove the object from cache if its underlying data has changed  Allows for asynchronous processing  workers assemble objects by consuming the latest cached object  Suggestions of what to cache   User sessions  Fully rendered web pages  Activity streams  User graph data  When to update the cache  Since you can only store a limited amount of data in cache  you ll need to determine which cache update strategy works best for your use case     Source  From cache to in memory data grid  The application is responsible for reading and writing from storage  The cache does not interact with storage directly  The application does the following   Look for entry in cache  resulting in a cache miss  Load entry from the database  Add entry to cache  Return entry  def get_user self  user_id   user   cache get  user  0    user_id  if user is None  user   db query  SELECT   FROM users WHERE user_id    0    user_id  if user is not None  key    user  0   format user_id  cache set key  json dumps user   return user  Memcached is generally used in this manner   Subsequent reads of data added to cache are fast  Cache aside is also referred to as lazy loading  Only requested data is cached  which avoids filling up the cache with data that isn t requested   Disadvantage s   cache aside  Each cache miss results in three trips  which can cause a noticeable delay   Data can become stale if it is updated in the database  This issue is mitigated by setting a time to live  TTL  which forces an update of the cache entry  or by using write through   When a node fails  it is replaced by a new  empty node  increasing latency     Source  Scalability  availability  stability  patterns  The application uses the cache as the main data store  reading and writing data to it  while the cache is responsible for reading and writing to the database   Application adds updates entry in cache  Cache synchronously writes entry to data store  Return  Application code   set_user 12345    foo   bar     Cache code   def set_user user_id  values   user   db query  UPDATE Users WHERE id    0    user_id  values  cache set user_id  user   Write through is a slow overall operation due to the write operation  but subsequent reads of just written data are fast  Users are generally more tolerant of latency when updating data than reading data  Data in the cache is not stale   Disadvantage s   write through  When a new node is created due to failure or scaling  the new node will not cache entries until the entry is updated in the database  Cache aside in conjunction with write through can mitigate this issue   Most data written might never read  which can be minimized with a TTL   Write behind  write back     Source  Scalability  availability  stability  patterns  In write behind  the application does the following   Add update entry in cache  Asynchronously write entry to the data store  improving write performance  Disadvantage s   write behind  There could be data loss if the cache goes down prior to its contents hitting the data store   It is more complex to implement write behind than it is to implement cache aside or write through     Source  From cache to in memory data grid  You can configure the cache to automatically refresh any recently accessed cache entry prior to its expiration   Refresh ahead can result in reduced latency vs read through if the cache can accurately predict which items are likely to be needed in the future   Disadvantage s   refresh ahead  Not accurately predicting which items are likely to be needed in the future can result in reduced performance than without refresh ahead   Disadvantage s   cache  Need to maintain consistency between caches and the source of truth such as the database through cache invalidation   Need to make application changes such as adding Redis or memcached   Cache invalidation is a difficult problem  there is additional complexity associated with when to update the cache   Source s  and further reading    Source  Intro to architecting systems for scale  Asynchronous workflows help reduce request times for expensive operations that would otherwise be performed in line  They can also help by doing time consuming work in advance  such as periodic aggregation of data   Message queues  Message queues receive  hold  and deliver messages  If an operation is too slow to perform inline  you can use a message queue with the following workflow   An application publishes a job to the queue  then notifies the user of job status  A worker picks up the job from the queue  processes it  then signals the job is complete  The user is not blocked and the job is processed in the background  During this time  the client might optionally do a small amount of processing to make it seem like the task has completed  For example  if posting a tweet  the tweet could be instantly posted to your timeline  but it could take some time before your tweet is actually delivered to all of your followers   Redis is useful as a simple message broker but messages can be lost   RabbitMQ is popular but requires you to adapt to the  AMQP  protocol and manage your own nodes   Amazon SQS  is hosted but can have high latency and has the possibility of messages being delivered twice   Task queues  Tasks queues receive tasks and their related data  runs them  then delivers their results  They can support scheduling and can be used to run computationally intensive jobs in the background   Celery has support for scheduling and primarily has python support   Back pressure  If queues start to grow significantly  the queue size can become larger than memory  resulting in cache misses  disk reads  and even slower performance  Back pressure can help by limiting the queue size  thereby maintaining a high throughput rate and good response times for jobs already in the queue  Once the queue fills up  clients get a server busy or HTTP 503 status code to try again later  Clients can retry the request at a later time  perhaps with exponential backoff   Disadvantage s   asynchronism  Use cases such as inexpensive calculations and realtime workflows might be better suited for synchronous operations  as introducing queues can add delays and complexity   Source s  and further reading    Source  OSI 7 layer model  Hypertext transfer protocol  HTTP   HTTP is a method for encoding and transporting data between a client and a server  It is a request response protocol  clients issue requests and servers issue responses with relevant content and completion status info about the request  HTTP is self contained  allowing requests and responses to flow through many intermediate routers and servers that perform load balancing  caching  encryption  and compression   A basic HTTP request consists of a verb  method  and a resource  endpoint   Below are common HTTP verbs   Verb Description Idempotent  Safe Cacheable GET Reads a resource Yes Yes Yes POST Creates a resource or trigger a process that handles data No No Yes if response contains freshness info PUT Creates or replace a resource Yes No No PATCH Partially updates a resource No No Yes if response contains freshness info DELETE Deletes a resource Yes No No   Can be called many times without different outcomes   The difference between PUT and PATCH is explained by example here   HTTP is an application layer protocol relying on lower level protocols such as TCP and UDP   Transmission control protocol  TCP     Source  How to make a multiplayer game  TCP is a connection oriented protocol over an IP network  Connection is established and terminated using a handshake  All packets sent are guaranteed to reach the destination in the original order and without corruption through   Sequence numbers and checksum fields for each packet  Acknowledgement packets and automatic retransmission  If the sender does not receive a correct response  it will resend the packets  If there are multiple timeouts  the connection is dropped  TCP also implements flow control and congestion control  These guarantees cause delays and generally result in less efficient transmission than UDP   To ensure high throughput  web servers can keep a large number of TCP connections open  resulting in high memory usage  It can be expensive to have a large number of open connections between web server threads and say  a memcached server  Connection pooling can help in addition to switching to UDP where applicable   TCP is useful for applications that require high reliability but are less time critical  Some examples include web servers  database info  SMTP  FTP  and SSH   Use TCP over UDP when   You need all of the data to arrive intact  You want to automatically make a best estimate use of the network throughput  User datagram protocol  UDP     Source  How to make a multiplayer game  UDP is connectionless  Datagrams  analogous to packets  are guaranteed only at the datagram level  Datagrams might reach their destination out of order or not at all  UDP does not support congestion control  Without the guarantees that TCP support  UDP is generally more efficient   UDP can broadcast  sending datagrams to all devices on the subnet  This is useful with DHCP because the client has not yet received an IP address  thus preventing a way for TCP to stream without the IP address   UDP is less reliable but works well in real time use cases such as VoIP  video chat  streaming  and realtime multiplayer games   Use UDP over TCP when   You need the lowest latency  Late data is worse than loss of data  You want to implement your own error correction  Source s  and further reading  TCP and UDP  Remote procedure call  RPC     Source  Crack the system design interview  In an RPC  a client causes a procedure to execute on a different address space  usually a remote server  The procedure is coded as if it were a local procedure call  abstracting away the details of how to communicate with the server from the client program  Remote calls are usually slower and less reliable than local calls so it is helpful to distinguish RPC calls from local calls  Popular RPC frameworks include Protobuf  Thrift  and Avro   RPC is a request response protocol   Client program   Calls the client stub procedure  The parameters are pushed onto the stack like a local procedure call     Calls the client stub procedure  The parameters are pushed onto the stack like a local procedure call  Client stub procedure   Marshals  packs  procedure id and arguments into a request message     Marshals  packs  procedure id and arguments into a request message  Client communication module   OS sends the message from the client to the server     OS sends the message from the client to the server  Server communication module   OS passes the incoming packets to the server stub procedure     OS passes the incoming packets to the server stub procedure  Server stub procedure   Unmarshalls the results  calls the server procedure matching the procedure id and passes the given arguments     Unmarshalls the results  calls the server procedure matching the procedure id and passes the given arguments  The server response repeats the steps above in reverse order   Sample RPC calls   GET  someoperation data anId POST  anotheroperation    data   anId    anotherdata    another value     RPC is focused on exposing behaviors  RPCs are often used for performance reasons with internal communications  as you can hand craft native calls to better fit your use cases   Choose a Native Library aka SDK when   You know your target platform   You want to control how your  logic  is accessed  You want to control how error control happens off your library  Performance and end user experience is your primary concern  HTTP APIs following REST tend to be used more often for public APIs   Disadvantage s   RPC  RPC clients become tightly coupled to the service implementation   A new API must be defined for every new operation or use case   It can be difficult to debug RPC   You might not be able to leverage existing technologies out of the box  For example  it might require additional effort to ensure RPC calls are properly cached on caching servers such as Squid   Representational state transfer  REST   REST is an architectural style enforcing a client server model where the client acts on a set of resources managed by the server  The server provides a representation of resources and actions that can either manipulate or get a new representation of resources  All communication must be stateless and cacheable   There are four qualities of a RESTful interface   Identify resources  URI in HTTP    use the same URI regardless of any operation     use the same URI regardless of any operation  Change with representations  Verbs in HTTP    use verbs  headers  and body     use verbs  headers  and body  Self descriptive error message  status response in HTTP    Use status codes  don t reinvent the wheel     Use status codes  don t reinvent the wheel  HATEOAS  HTML interface for HTTP    your web service should be fully accessible in a browser   Sample REST calls   GET  someresources anId PUT  someresources anId   anotherdata    another value    REST is focused on exposing data  It minimizes the coupling between client server and is often used for public HTTP APIs  REST uses a more generic and uniform method of exposing resources through URIs  representation through headers  and actions through verbs such as GET  POST  PUT  DELETE  and PATCH  Being stateless  REST is great for horizontal scaling and partitioning   Disadvantage s   REST  With REST being focused on exposing data  it might not be a good fit if resources are not naturally organized or accessed in a simple hierarchy  For example  returning all updated records from the past hour matching a particular set of events is not easily expressed as a path  With REST  it is likely to be implemented with a combination of URI path  query parameters  and possibly the request body   REST typically relies on a few verbs  GET  POST  PUT  DELETE  and PATCH  which sometimes doesn t fit your use case  For example  moving expired documents to the archive folder might not cleanly fit within these verbs   Fetching complicated resources with nested hierarchies requires multiple round trips between the client and server to render single views  e g  fetching content of a blog entry and the comments on that entry  For mobile applications operating in variable network conditions  these multiple roundtrips are highly undesirable   Over time  more fields might be added to an API response and older clients will receive all new data fields  even those that they do not need  as a result  it bloats the payload size and leads to larger latencies   RPC and REST calls comparison  Operation RPC REST Signup POST  signup POST  persons Resign POST  resign      personid    1234     DELETE  persons 1234 Read a person GET  readPerson personid 1234 GET  persons 1234 Read a person s items list GET  readUsersItemsList personid 1234 GET  persons 1234 items Add an item to a person s items POST  addItemToUsersItemsList      personid    1234     itemid    456     POST  persons 1234 items      itemid    456     Update an item POST  modifyItem      itemid    456     key    value     PUT  items 456      key    value     Delete an item POST  removeItem      itemid    456     DELETE  items 456  Source  Do you really know why you prefer REST over RPC  Source s  and further reading  REST and RPC  This section could use some updates  Consider contributing   Security is a broad topic  Unless you have considerable experience  a security background  or are applying for a position that requires knowledge of security  you probably won t need to know more than the basics   Encrypt in transit and at rest   Sanitize all user inputs or any input parameters exposed to user to prevent XSS and SQL injection   Use parameterized queries to prevent SQL injection   Use the principle of least privilege   Source s  and further reading  You ll sometimes be asked to do  back of the envelope  estimates  For example  you might need to determine how long it will take to generate 100 image thumbnails from disk or how much memory a data structure will take  The Powers of two table and Latency numbers every programmer should know are handy references   Powers of two table  Power Exact Value Approx Value Bytes                                                                 7 128 8 256 10 1024 1 thousand 1 KB 16 65 536 64 KB 20 1 048 576 1 million 1 MB 30 1 073 741 824 1 billion 1 GB 32 4 294 967 296 4 GB 40 1 099 511 627 776 1 trillion 1 TB  Source s  and further reading  Latency numbers every programmer should know  Latency Comparison Numbers                            L1 cache reference 0 5 ns Branch mispredict 5 ns L2 cache reference 7 ns 14x L1 cache Mutex lock unlock 100 ns Main memory reference 100 ns 20x L2 cache  200x L1 cache Compress 1K bytes with Zippy 10 000 ns 10 us Send 1 KB bytes over 1 Gbps network 10 000 ns 10 us Read 4 KB randomly from SSD  150 000 ns 150 us  1GB sec SSD Read 1 MB sequentially from memory 250 000 ns 250 us Round trip within same datacenter 500 000 ns 500 us Read 1 MB sequentially from SSD  1 000 000 ns 1 000 us 1 ms  1GB sec SSD  4X memory Disk seek 10 000 000 ns 10 000 us 10 ms 20x datacenter roundtrip Read 1 MB sequentially from 1 Gbps 10 000 000 ns 10 000 us 10 ms 40x memory  10X SSD Read 1 MB sequentially from disk 30 000 000 ns 30 000 us 30 ms 120x memory  30X SSD Send packet CA  Netherlands  CA 150 000 000 ns 150 000 us 150 ms Notes       1 ns   10  9 seconds 1 us   10  6 seconds   1 000 ns 1 ms   10  3 seconds   1 000 us   1 000 000 ns  Handy metrics based on numbers above   Read sequentially from disk at 30 MB s  Read sequentially from 1 Gbps Ethernet at 100 MB s  Read sequentially from SSD at 1 GB s  Read sequentially from main memory at 4 GB s  6 7 world wide round trips per second  2 000 round trips per second within a data center  Latency numbers visualized  Source s  and further reading  Additional system design interview questions  Common system design interview questions  with links to resources on how to solve each   Question Reference s  Design a file sync service like Dropbox youtube com Design a search engine like Google queue acm org  stackexchange com  ardendertat com  stanford edu Design a scalable web crawler like Google quora com Design Google docs code google com  neil fraser name Design a key value store like Redis slideshare net Design a cache system like Memcached slideshare net Design a recommendation system like Amazon s hulu com  ijcai13 org Design a tinyurl system like Bitly n00tc0d3r blogspot com Design a chat app like WhatsApp highscalability com Design a picture sharing system like Instagram highscalability com  highscalability com Design the Facebook news feed function quora com  quora com  slideshare net Design the Facebook timeline function facebook com  highscalability com Design the Facebook chat function erlang factory com  facebook com Design a graph search function like Facebook s facebook com  facebook com  facebook com Design a content delivery network like CloudFlare cmu edu Design a trending topic system like Twitter s michael noll com  snikolov  wordpress com Design a random ID generation system blog twitter com  github com Return the top k requests during a time interval ucsb edu  wpi edu Design a system that serves data from multiple data centers highscalability com Design an online multiplayer card game indieflashblog com  buildnewgames com Design a garbage collection system stuffwithstuff com  washington edu Add a system design question Contribute  Real world architectures  Articles on how real world systems are designed     Source  Twitter timelines at scale  Don t focus on nitty gritty details for the following articles  instead   Identify shared principles  common technologies  and patterns within these articles  Study what problems are solved by each component  where it works  where it doesn t  Review the lessons learned  Type System Reference s  Data processing MapReduce   Distributed data processing from Google research google com Data processing Spark   Distributed data processing from Databricks slideshare net Data processing Storm   Distributed data processing from Twitter slideshare net Data store Bigtable   Distributed column oriented database from Google harvard edu Data store HBase   Open source implementation of Bigtable slideshare net Data store Cassandra   Distributed column oriented database from Facebook slideshare net Data store DynamoDB   Document oriented database from Amazon harvard edu Data store MongoDB   Document oriented database slideshare net Data store Spanner   Globally distributed database from Google research google com Data store Memcached   Distributed memory caching system slideshare net Data store Redis   Distributed memory caching system with persistence and value types slideshare net File system Google File System  GFS    Distributed file system research google com File system Hadoop File System  HDFS    Open source implementation of GFS apache org Misc Chubby   Lock service for loosely coupled distributed systems from Google research google com Misc Dapper   Distributed systems tracing infrastructure research google com Misc Kafka   Pub sub message queue from LinkedIn slideshare net Misc Zookeeper   Centralized infrastructure and services enabling synchronization slideshare net Add an architecture Contribute  Company architectures  Company engineering blogs  Architectures for companies you are interviewing with  Questions you encounter might be from the same domain   Source s  and further reading  Under development  Interested in adding a section or helping complete one in progress  Contribute   Distributed computing with MapReduce  Consistent hashing  Scatter gather  Contribute  Credits and sources are provided throughout this repo   Special thanks to   Contact info  Feel free to contact me to discuss any issues  questions  or comments   My contact info can be found on my GitHub page,"[563 1289 562 668 428 413 997 831 2 254 59]"
638,training-dataset/engineering/367.txt,engineering,The Story of Batching to Streaming Analytics at OptimizelyWednesday  November 16  2016 at 8 56AM  The original contributor to this article was David Yu  Distributed Systems Engineer at Optimizely   Our mission at Optimizely is to help decision makers turn data into action  This requires us to move data with speed and reliability  We track billions of user events  such as page views  clicks and custom events  on a daily basis  To provide our customers with immediate access to key business insights about their users has always been our top most priority  Because of this  we are constantly innovating on our data ingestion pipeline   In this article we will introduce how we transformed our data ingestion pipeline from batching to streaming to provide our customers with real time session metrics   Motivations  Unification  Previously  we maintained two data stores for different use cases   HBase is used for computing Experimentation metrics  whereas Druid is used for calculating Personalization results  These two systems were developed with distinctive requirements in mind   Experimentation Personalization Instant event ingestion Delayed event ingestion ok Query latency in seconds Query latency in subseconds Visitor level metrics Session level metrics  As our business requirements evolve  however  things quickly became difficult to scale  Maintaining a Druid   HBase Lambda architecture  see below  to satisfy these business needs became a technical burden for the engineering team  We need a solution that reduces backend complexity and increases development productivity  More importantly  a unified counting infrastructure creates a generic platform for many of our future product needs   Consistency  As mentioned above  the two counting infrastructures provide different metrics and computational guarantees  For example  Experimentation results show you the number of visitors visited your landing page whereas Personalization shows you the number of sessions instead  We want to bring consistent metrics to our customers and support both type of statistics across our products   Real time results  Our session based results are computed using MR jobs  which can be delayed up to hours after the events are received  A real time solution will provide our customers with more up to date view of their data   Druid   HBase  In our earlier posts  we introduced our backend ingestion pipeline and how we use Druid and MR to store transactional stats based on user sessions  One biggest benefit we get from Druid is the low latency results at query time  However  it does come with its own set of drawbacks  For example  since segment files are immutable  it is impossible to incrementally update the indexes  As a result  we are forced to reprocess user events within a given time window if we need to fix certain data issues such as out of order events  In addition  we had difficulty scaling the number of dimensions and dimension cardinality  and queries expanding long period of time became expensive   On the other hand  we also use HBase for our visitor based computation  We write each event into an HBase cell  which gave us maximum flexibility in terms of supporting the kind of queries we can run  When a customer needs to find out  how many unique visitors have triggered an add to cart conversion   for example  we do a scan over the range of dataset for that experimentation  Since events are pushed into HBase  through Kafka  near real time  data generally reflect the current state of the world  However  our current table schema does not aggregate any metadata associated with each event  These metadata include generic set of information such as browser types and geolocation details  as well as customer specific tags used for customized data segmentation  The redundancy of these data prevents us from supporting large number of custom segmentations  as it increases our storage cost and query scan time   SessionDB  Since it became difficult to optimize our Druid indexes  we decided to obsolete the Druid route and focus on improving our HBase data representation  Pre aggregating events and compacting away redundant information became the most obvious next step  This was when we turned to Samza for help   Samza is a perfect fit for our needs thanks to its seamless integration with Kafka   our distributed message queue  We will get to the details of how this real time aggregation works in part two  But on a very high level  Samza continuously bundles events into sessions and periodically streams out snapshots of the pending sessions into HBase  With this approach  each HBase cell becomes a consolidated view of a group of events   There are several advantages to this  First being that  our core logic for computing various statistics very much stays the same  Given the fact that majority of the base calculations we do are summations  an oversimplification of course   adding a bunch of ones together is equivalent of summing a list of cumulative values   The second benefit we get is that  with session level information immediately available  we can start querying session metrics right off of HBase and answer questions  such as  what is the average revenue generated per user session   in real time  This newly created HBase schema is unsurprisingly named SessionDB  which became the basis of backend unification   Last but not least  the HBase storage requirement is drastically reduced and queries run much faster  By aggregating session level metadata  we no longer have to replicate information  such as browser types  locations and user dimensions  across each cell  The graph below shows the average query latency  x axis  given different number of user dimensions  y axis   With an average of 10 events per session  for example  the median query latency drops to 5 ms as opposed to 40  ms  the yellow line    Background  Session aggregation  AKA sessionization  was not the first stream processing use case at Optimizely  We have been applying stream processing for various ETL tasks  such as   Data enrichment  backfilling missing metadata for events   Event stream repartition  for various downstream use cases   Real time experiment metrics  count number of events received for each experiment   There are several production ready stream processing frameworks available today  including Samza  Spark  Storm and Flink  We chose Samza because of several reasons  First  Samza allows you to create a processing topology by chaining together kafka topics  which offers high isolation  As we encourage our engineers to explore different processing ideas  this pluggability creates the minimal ripple effect to our overall data pipeline  Secondly  Samza can be easily integrated into Kafka and YARN  which are what we use heavily  In addition  Samza is very low latency  It provides a simple programming model and easy to use state management framework  all of which fit well with many our needs   What is Apache Samza   On a very high level  a Samza job consumes a stream of immutable events from one or more Kafka topics  It then does some kind of computation to these events  one at a time  and writes outputs into one or more downstream Kafka topics   One noticeable characteristic of Samza jobs is that  they are highly composable  To help you visualize what Samza jobs are capable of  you can think of them as a bunch of real time MR jobs  chained together by Kafka topics  The above diagram shows you how a job can have multiple input streams and how several jobs are connected to form a complex workflow   Samza is scalable  fault tolerant and stateful  We are going to briefly touch each of these three aspects  since our sessionization job takes advantage of all these features to some extend   Scalability  Just like MR jobs  Samza jobs are also highly concurrent and scalable  The difference lies in that  Samza parallelism is partition oriented as opposed to file oriented  A job is divided into multiple tasks  As analogous to MR  a Samza task is roughly equivalent to a mapper   reducer  A task  maps  events by funneling them into designated Kafka topic partitions  At the same time  events from an input stream partition all go to a single task  mimicking the behavior of a reducer   One important aspect to keep in mind is that  an input topic partition is statically assigned to a task instance  The number of task instances  thus  is directly related to the max number of partitions of any given input topic  illustrated in an example below   The first time you submit a job  this mapping is created and persisted into a Kafka topic for fault tolerance  The coupling helps simplify Samza s state management model  which we ll cover later   since each task only needs to manage its own state   With a fixed number of tasks  how can we scale out  A Samza job can scale out by increasing the number of running containers  When running on a YARN cluster  the only distributed environment supported to run Samza currently  each task instance executes in a container   a single unit of computational resource  Depending on how much computational power a task demands  you can run everything within a single container  or you can create as many containers as tasks  providing the maximum power for each task  The diagram above demonstrates this kind of elasticity   Fault tolerance  Samza promises that  if a container dies for whatever reason  tasks within that container will be able to recover from where they left off once a new container launches  To do this correctly  Samza periodically bookmarks checkpoints the offsets at which the tasks are consuming from  It also does this for changes made to its internal state stores  With all of this information logged to Kafka  a job recovery can be done by consuming and replaying this stream of changes to reconstruct various internal job states   Statefulness  Most stream processing needs to store some kind of states  whether it be the number of events for a given user if you are generating user metrics  or in our case  all the events triggered within a given session  To make stream processing truely real time  it is impractical to store these states anywhere other than local to the task  Out of the box  Samza provides RocksDB as an efficient KV store  like an embedded HBase  to track states too large to fit into the local memory  As mentioned above  this store is also recoverable on a task failure  thanks to the changes it persists in a changelog topic   With all of the basic concepts of Samza in mind  we can now move on to introduce how we leverage this tool to continuously roll up events into sessions   Implementation  A session is simply a series of user events triggered in a relatively quick succession  To provide different types of session demarcations  we ask our upstream clients to assign a session id for each event  Typically  all events are assigned the same session id if they are created less than 30 minutes apart   The main Samza event loop for aggregating events then becomes quite straightforward  The job maintains a per task KV store using the session ids as keys  Upon receiving an event  the job does a lookup in the KV store  It either creates a new session if the session id does not exist or updates the session otherwise  The update is where most of the consolidation happens  The common set of metadata  e g  ip address  location information  browser version  etc  is aggregated at the top level  whereas event specific ones  e g  event type  add to cart dollar amount  etc  are preserved as a list   As introduced in the previous post  each HBase cell now stores a session  To keep overwriting the cells with the latest copies of the sessions  we need to continuously flush snapshots of the KV store into a Kafka topic   One important question we asked earlier in our design was  how frequently should we snapshot the ongoing sessions to keep our HBase updated  We could do this as soon as a session is updated after processing an event  The benefit of doing so is that it minimizes session update latency and produces truly real time results  The downside  however  is that it also creates a large number of session updates  which significantly increases our Kafka load  Instead  we leveraged the windowing feature of Samza and batch one minute worth of session updates before flushing into HBase  If a user triggers 10 click events within 60 seconds  for example  we write out a single session snapshot with 10 events at the end of the minute  instead of 10 session snapshots  This greatly reduced the amount of data sent through the wire   Challenges and lessons  As straightforward as the sessionization logic sounds  we did encounter various challenges during our sessionization development   Iterating through RocksDB affects job throughput  As mentioned above  Samza windowing needs to traverse the list of all pending sessions when writing out the session snapshots  Due to the fact that each Samza process is single threaded  future Samza releases will introduce multi threaded processing   the actual event processing will be blocked until windowing is finished  This means that  in extreme cases  if windowing takes longer than the configured windowing interval  the event loop will not have a chance to process any actual events  When that happens  you will observe a high window ns and zero process envelopes count  To prevent this  you will have to make windowing run as efficiently as possible and probably want to avoid frequent full scans of RocksDB  In our case  we achieved this by keeping a stripped down version of the KV store in memory just for this purpose  Traversing a plain old hash table is a lot faster than over the on disk KV store after all   Be careful with Kafka log compaction  The log compaction feature allows Kafka to strategically remove log entries instead of a simple truncate at the tail of each partition  A log compacted topic will only retain the last known value of each key  If that value is null  Kafka removes that key after a configured retention time  First of all  if you are using an older version of Kafka  e g  0 8 2   make sure that log cleaner enable is set to true  Otherwise  some of the Samza topics such as checkpoints and changelogs will not be compacted  We figured this out when during a redeployment  the job took hours to resume processing  It turned out that it needed to consume TBs of uncompacted changelogs to materialize each KV store   Another lesson we learned regarding log compaction is that  you should not produce to a log compacted topic if no deletion takes place  The gotcha here is that  log retention bytes does not apply to log compacted topics at all  We learned this the hard way when our output topic grows unbounded and exhausted our Kafka disk space   Use SSDs for RocksDB  During our development  we noticed that  once in awhile  a subset of the tasks would have a hard time keeping up with the rate at which events are received  Further investigation revealed somewhat longer process ns and window ns time on these tasks  It turned out that these tasks were having slower interactions with the underlying RocksDB stores due to the type of disks configured on the YARN instances  The containers of these tasks were deployed on Node Managers that used EBS drives  Since RocksDB is highly optimized to run fast compactions in fast storage like SSD or RAM  using EBS significantly reduced performance for RocksDB IO,"[638 1281 0 743 1031 1289 563 413 660 1382 1414]"
657,training-dataset/product/1203.txt,product,Design words with data   Dropbox Design   MediumAt Dropbox  we realized we were using different terms to refer to our  version history  feature   We knew we wanted to fix these inconsistencies  but we weren t sure which term to use  Should it be  version history    file history   or maybe even  revision history   There were a number of things we had to consider  but we used Google Trends as one data point to help inform us   Google Trends showed us people were more likely to search for  version history   and that s one big reason why we now call it  version history  throughout our product   2  Google Ngram Viewer  Ngram Viewer is similar to Google Trends  except it searches published books  scanned by Google  You can use this data to see which terms are more commonly used in your language   Dropbox recently launched a new signature tool in our iOS app  On the screen where you draw your signature  the screen showed  Sign Your Signature  before we had a chance to review it   We knew that  sign your signature  sounded funny  But  sounds funny  isn t a great reason for changing it  How could we convince the team to change it   That s when we headed over to Ngram Viewer and compared  sign your signature  with  sign your name   It showed us that  sign your signature  wasn t really used at all  When we shared this data with the team  they quickly changed it to  Sign your name    Comparing terms using Ngram Viewer  3  Readability tests  Over the years  language experts have developed a number of readability tests that measure how easy it is to understand your words   Many of these tests give you a grade level for your writing  For example  a grade of 8 means that a typical 8th grader in the U S  should be able to understand your writing   I ran one of my Medium stories  How to design words  through one of these tests  Here s what it told me   Results from Readability Score com  There s a lot of interesting data you can get from here  For example   I wrote the story at a 6th grade level      My tone was neutral but slightly positive   but I averaged 10 7 words per sentence   At Dropbox  we try to keep our sentences to 15 words or less    If you want to give one of these tests a spin  below are a few you can try  Some of these tests even give you suggested edits to make your writing more readable   4  Research surveys  Trying to figure out what to name your new feature  Or what value prop to focus on  In cases like these  it can help to set up a research survey   Many survey tools allow you to choose your target audience  so you can easily get feedback from potential users   Here are a few places where you can set up research surveys   Back in the day  Dropbox ran a survey to figure out what was the biggest benefit to using our product  Most people mentioned  access    the ability to get to your files from any device  As a result  a lot of the messaging we used in our landing page redesign focused on access   5  User studies  User studies are a great way to get valuable feedback about your writing  In a typical user study  you invite a number of people to read your text or try out a product  and then you ask them questions about it  This can be incredibly helpful for seeing whether your writing makes sense or not   One of our researchers recently ran a study where we tested a new flow  There was one step that said   Select  Remove local copy  to save space   We asked participants when they might use this feature  Most had a tough time understanding this feature and didn t think it was useful  So then we flipped the order of the words by putting the user benefit at the front of the sentence   Save space by selecting  Remove local copy    This time  participants had a much easier time telling us when they d use this feature  And all we really did was change the order of the words   This shows how a writer s hunch can turn into an experiment  and you can test it just like any other design decision   Write with your heart  edit with your head  Data can be useful when you re trying to make specific writing choices  But that doesn t mean you should write like a machine   The way I see it  your first draft should always come from the heart  Trust your gut  After you ve written out your ideas  that s when you can turn to research and data to refine your words   Writing is both an art and a science  By writing with your heart and editing with your head  you can craft something that s both authentic and informative   Data gives you confidence as a writer  Data is what makes your writing  right,"[657 2 865 1274 397 563 334 428 1331 413 254]"
660,training-dataset/engineering/576.txt,engineering,How Urban Airship Scaled to 2 5 Billion Notifications During the U S  ElectionMonday  November 14  2016 at 8 56AM  This is a guest post by Urban Airship  Contributors  Adam Lowry  Sean Moran  Mike Herrick  Lisa Orr  Todd Johnson  Christine Ciandrini  Ashish Warty  Nick Adlard  Mele Sax Barnett  Niall Kelly  Graham Forest  and Gavin McQuillan  Urban Airship is trusted by thousands of businesses looking to grow with mobile  Urban Airship is a seven year old SaaS company and has a freemium business model so you can try it for free  For more information  visit www urbanairship com  Urban Airship now averages more than one billion push notifications delivered daily  This post highlights Urban Airship notification usage for the 2016 U S  election  exploring the architecture of the system  the Core Delivery Pipeline  that delivers billions of real time notifications for news publishers   2016 U S  Election  In the 24 hours surrounding Election Day  Urban Airship delivered 2 5 billion notifications its highest daily volume ever  This is equivalent to 8 notification per person in the United States or 1 notification for every active smartphone in the world  While Urban Airship powers more than 45 000 apps across every industry vertical  analysis of the election usage data shows that more than 400 media apps were responsible for 60  of this record volume  sending 1 5 billion notifications in a single day as election results were tracked and reported   Notification volume was steady and peaked when the presidential election concluded   HTTPS ingress traffic to the Urban Airship API peaked at nearly 75K per second during the election  Most of this traffic comes from the Urban Airship SDK communicating with the Urban Airship API   Push notification volume has been rapidly increasing  Key recent drivers have been Brexit  the Olympics  and the U S  election  October 2016 monthly notification volume has increased 150  year over year   Core Delivery Pipeline Architecture Overview  The Core Delivery Pipeline  CDP  is the Urban Airship system responsible for materializing device addresses from audience selectors  and delivering notifications  Low latency is expected for all the notifications we send  whether they go to tens of millions of users simultaneously  target multiple complex sub segments  contain personalized content  or anything in between  Here s an overview of our architecture  and some of the things we ve learned along the way   How We Started  What initially started in 2009 as a webapp and some workers has transformed into a service oriented architecture  As pieces of the old system began to run into scale issues  we extracted them into one or more new services that were designed to satisfy the same feature set  but at a larger scale with better performance  Many of our original APIs and workers were written in Python  which we extracted into high concurrency Java services  Where we originally stored device data in a set of Postgres shards  our scale quickly outpaced our capacity to add new shards  so we moved to a multiple database architecture using HBase and Cassandra   The CDP is a collection of services that handle segmentation and push notification delivery  These services provide the same type of data in response to requests  but each service has that data indexed in a very different way for performance reasons  For example  we have a system that is responsible for handling broadcast messages  delivering the same notification payload to every device registered to the associated application  This service and its underlying datastore are designed very differently than the services we have that is responsible for delivering notifications based on location or user profile attributes   We consider any long lived process a service  These long lived processes follow a general template regarding metrics  configuration  and logging for ease of deployment and operation  Typically our services fall into one of two groups  RPC services  or consumer services  RPC services provide commands to synchronously interact with the service using an in house library very similar to GRPC  while consumer services process messages off of Kafka streams and perform service specific operations on those messages   Databases  To meet our performance and scale requirements we rely heavily on HBase and Cassandra for our data storage needs  While HBase and Cassandra are both columnar NoSQL stores  they have very different trade offs that influence which store we use and for what purpose   HBase is very good at high throughput scans where the expected cardinality of the response is very high  while Cassandra is good at lower cardinality lookups where the response is expected to contain just a handful of results  Both allow for high volumes of write throughput  which is a requirement for us  because all metadata updates from users  phones are applied in real time   Their failure characteristics differ as well  HBase favors consistency and partition tolerance in the event of failure  while Cassandra favors availability and partition tolerance  Each of the CDP services has a very specific use case and as such has a highly specialized schema designed to facilitate the required access pattern as well as limiting the storage footprint  As a general rule  each database is only accessed by a single service  which is responsible for providing database access to other services via a less specialized interface   Enforcing this 1 1 relationship between service and its backing database has a number of benefits   By treating the backing datastore of a service as an implementation detail  and not a shared resource  we gain flexibility   We can adjust the data model for a service while only changing that service s code   Usage tracking is more straightforward  which makes capacity planning easier   Troubleshooting is easier  Sometimes an issue lies with the service code  other times it is a backing database issue  Having the service and database be a logical unit vastly simplifies the troubleshooting process  We don t have to wonder  Who else could be accessing this database to make it behave in such a way   Instead  we can rely on our application level metrics from the service itself and only worry about one set of access patterns   Because there is only one service interacting with a database  we can perform nearly all of our maintenance activities without taking downtime  Heavy maintenance tasks become a service level concern  data repair  schema migration and even switching to a completely different database can be done without disrupting service   It s true that there can be some performance tradeoffs when breaking out applications into smaller services  However  we ve found that the flexibility we gain in meeting our high scalability and high availability requirements more than worth it   Data Modeling  Most of our services deal with the same data  just in different formats  Everything has to be consistent  To keep all of these services  data up to date we rely heavily on Kafka  Kafka is extremely fast and is also durable  Speed comes with certain tradeoffs  Kafka messages are only guaranteed to be sent at least once  and they aren t guaranteed to arrive in order   How do we deal with this  We ve modeled all mutation paths to be commutative  operations can be applied in any order and end up with the same result  They re also idempotent  This has a nice side effect that we can replay Kafka streams for one off data repair jobs  backfills  or even migrations   To do this we take advantage of the concept of a  cell version   which exists in both HBase and Cassandra  Typically this is a timestamp  but it can be any number you d like  there are some exceptions  for example  MAX_LONG can cause some strange behavior depending on your version of HBase or Cassandra and how your schema deals with deletes    For us  the general rule for these cells is that they can have multiple versions  and the way we order versions is by their provided timestamp  With this behavior in mind  we can decompose our incoming messages into a specific set of columns  and combine that layout with custom application logic for tombstoning  taking into account the timestamp  That allows blind writes to the underlying datastore while maintaining the integrity of our data   Just blindly writing changes to both Cassandra and HBase isn t without its issues  A great example is the case of repeated writes of the same data in the event of a replay  While the state of the data won t change due to the effort we put in to make the records idempotent  the duplicate data will have to be compacted out  In the most extreme cases  these extra records can cause significant compaction latency and backup  Because of this detail  we monitor our compaction times and queue depths closely as getting behind on compaction in both Cassandra and HBase can cause serious problems   By ensuring messages from the stream follow a strict set of rules  and designing the consuming service to expect out of order and repeated messages  we can keep a very large number of disparate services in sync with only a second or two of lag on updates   Service Design  Most of our services are written in Java  but in a very opinionated and modern style  We have a set of general guidelines that we take into consideration when designing a Java service   Do one thing  do it well   When designing a service  it should have a single responsibility  It is up to the implementer to decide what s included in a responsibility  but she or he will need to be ready to justify in a code review situation   No shared operational state   When designing a service  assume there will always be at least three instances of it running  A service needs to be able to handle the same exact request any other instance can without any external coordination  Those familiar with Kafka will note that Kafka consumers externally coordinate partition ownership for a topic group pair  This guideline refers to service specific external coordination  not utilizing libraries or clients that may coordinate externally under the covers   Bound your queues   We use queues all over our services  they re a great way to batch up requests and fan things out to workers that will end up doing tasks that block externally  All queues should be bounded  Bounding queues does raise a number of questions  however  What happens to producers when the queue is full  Should they block  Except  Drop  How big should my queue be  To answer this question it helps to assume the queue is always full  How do I shut down cleanly  Each service will have a different answer for these questions depending on the exact use case     Name custom thread pools and register an UncaughtExceptionHandler   If we end up creating our own thread pool  we use the constructor or helper method from Executors to allow us to provide a ThreadFactory  With that ThreadFactory we can properly name our threads  set their daemon state  and register an UncaughtExceptionHandler to handle the case where an exception makes it to the top of the stack  These steps make debugging our service much easier  and spare us some late night frustration   Prefer immutable data objects over mutable state   In a highly concurrent environment  mutable state can be dangerous  In general we use immutable data objects that can be passed between internal subsystems and queues  Having immutable objects be the main form of communication between subsystems makes designing for concurrent usage much more straightforward  and makes troubleshooting easier   Where Do We Go From Here   With Urban Airship s ability to send notifications through mobile wallet passes  its new support for web notifications and Apple News notifications  and its open channels capability to send notifications to any platform  device or marketing channel  we anticipate notification volume will grow exponentially  To meet this demand  we are continuing to invest heavily in our Core Delivery Pipeline architecture  services  databases  and infrastructure  To learn more about our technology and where we are headed please see GitHub  developer resources  documentation  and our jobs page,"[660 413 1289 831 563 428 638 934 823 1031 1281]"
668,training-dataset/business/1310.txt,business,Your Database is Your Prison   Here s How Expensify Broke FreeWhen expense management company Expensify first designed its database architecture  it was building its own corporate card for the masses  There were strict requirements to hit to work with financial institutions  response times within milliseconds  multiple servers replicated in real time and every transaction logged and authenticated  At the time  it felt like overkill for a startup to build a high level  enterprise grade architecture  When it pivoted to expense reporting  it still had a robust technology at its disposal  Startups often don t know until later just how valuable their early decisions turn out to be  Only in hindsight did David Barrett realize how the early constraints on Expensify s database architecture led to many of its key competitive advantages   Over eight years as the founder and CEO of Expensify  Barrett has seen the advantages of over investing in early database design for not just the product s functionality  but also to enable a radical business model that acquires customers very differently than its competitors  It grew over 120  in 2015  nearly 70  more than its closest competitor  The expense management startup now assists over 20 000 companies  Barrett credits a large part of the company s ability to scale to its technology decisions  in particular its unusually powerful database architecture   In this exclusive interview  Barrett explains how startups fall into damaging defaults when it comes to database architecture and deconstructs the specific mistakes to avoid  He shares the three key steps to follow to set yourself up for both technological and business model scale and success  Lastly  he shares how to course correct midstream if you need to change your database architecture   Where Many Startups Falter With Database Architecture  Startups are told they are supposed to be  data driven   but they rarely know what that means at the time they are making their key technology decisions   When you first start  you have no data and no customers  Everything you do easily fits in a single database  powered by a single web server  You choose the tools you ve used before or are currently in vogue and hope for the best   says Barrett   As you grow  very slowly you start to need multiple servers because either the performance or reliability of a single server is no longer adequate  So one by one you shoehorn another server into your sole data center  Bit by bit  you add capacity as you scale    Here s why that approach traps many startups   Squirrels abound  In Pixar s movie Up  the protagonist s talking golden retriever  Dug  stops mid sentence to avert his eyes and exclaim  Squirrel   whenever one crosses his line of vision  Don t be Dug   Most companies start with whatever is the most familiar to them  This is a safe choice  and it s easy to argue for minimizing risk at the start  Unfortunately  people can only be familiar with the past   and the world belongs to the future   says Barrett   The bolder companies step back and see what s new and better than they are familiar with  This can be good  But too many companies mistake  trendy  for  better   and choose unproven technologies that just happen to be popular when they are deciding what to use  This can kill your startup  in a slow and painful way    Years ago  Barrett once worked at a company that tried to build an earlier version of Dropbox   All the technology was there  but they insisted on using this horrible P2P technology called JXTA rather than a simple series of centralized servers  Despite us having no customers and thus no data to store our CTO bet the company on a fancy distributed system claiming it was the only way to scale to petabytes of data   says Barrett   Like most sexy   hyper scalable  solutions  in practice it couldn t even scale to megabytes because it was a nightmare to use and had never been used in a production environment  The company pivoted several times  and then four years later Dropbox built a much more obvious solution involving a more sane mix of new and old and took over the world    The key challenge for any new startup is to retain the benefits of the past with a dash of the future so as to stay cutting edge without getting killed by the bleeding edge   You re not Google  That s not to say you aren t able to   or will   be a giant like Google  But don t start with that assumption   What s popular is usually some very novel technology driven by a sexy company with very complex needs that appear at scale  The likelihood that your needs as a brand new startup are the same as Google s is slim   says Barrett   There are good databases to choose out there such as Postgres  but they re losing their shine because they re too boring  It s much more exciting to say   Google uses this really diverse database technology  I want to be like Google so I m going to use its technology     But people forget that the reason Google developed its technology foundation the way it did is because it s running the largest search engine in the world   The technology that powers a search engine is not the same as that which powers a lot of other businesses  Google needs to absorb an enormous quantity of data every day  of which only a tiny fraction is changing or interesting  Back in the day  Google would essentially make a copy of the internet each day  This means it was able to use a method of data indexing that is very fast to search and add data to  but extremely slow to selectively remove or change data once added  That s fine for Google  because every day they d just throw the whole thing out and start from scratch again   says Barrett   The vast majority of businesses aren t doing something like this  Instead  they re slowly accumulating a much more dense set of data   accounts  passwords and editable documents   that you can t just throw out and start over  but need to update going forward over time    Is your startup making a copy of the internet daily  If not  stop trying to emulate Google s database architecture   How To Go In Eyes Wide Open on Database Architecture  The danger of copying a lot of the technology foundations that are popular or of companies you admire is that they are likely very different from your company  The decisions that they make can actually place constraints upon you  But while they ll navigate around those constraints  those parameters may become showstoppers to you  Here s how to mitigate that risk when designing your database architecture   Do your math before specs seduce you  It s easy to be impressed by the performance of databases without thinking of what you actually need   Don t swoon when you hear a database can be split over a thousand computers and process petabytes of data  First  ask yourself   How much data do I really expect to have   The likelihood that you ll need even within 1000 times that much data is very unlikely   says Barrett   Even today  if we didn t care about reliability and maintainability  we could probably run all of Expensify from a single 5 year old database server  People forget just how ridiculously powerful and cheap computers are today  NASA went to the moon with a computer 10 000 times less powerful than your iPhone  and even a cheap server can process 1000 times more data than that  For the average startup  the likelihood that it ll need more than a single server capacity wise is very low for years    Prioritize maintenance over capacity   Today  the cheapest Dell server available has a dual core 2 8GHz processor and 500GB of RAID storage for just over  700  Upgrade to an 8 core 3 7GHz box with 64GB of RAM and 10TB of storage for about  3 200   far  far less than Expensify spends on coffee every month  Most startups will fail or be acquired before outgrowing the cheapest server on the market   especially since storage costs are falling far faster than most startups grow  And even though EC2 charges a premium for being cloud managed  it s still a steal at twice the price  Regardless of whether you rent or buy your hardware  capacity is not and will never be a real problem for the overwhelming majority of startups  But maintenance will be  If everything goes wrong on your single server   or you just need to upgrade   what happens when you reboot it  Your entire service vanishes  Bottom line  computers are absurdly cheap   get multiple  not for capacity  but for redundancy    Expensify Founder and CEO David Barrett  Split hairs on security   Most databases can be accessed in two ways  The first is doing a generic query upon the database  The other way is via what s called a stored procedure  Granted  all databases have security measures  but they re not created equal   says Barrett   Most of the trendy  new databases rely upon security built into web server code  The problem with that is your web servers are the most likely to be compromised  because they sit directly on the internet  And if they do  they can bypass the security measures and get unrestricted access to the database  A stored procedure executes inside the database itself  enabling you to build security into the database layer  outside the reach of web hackers  Unknowingly  the vast majority of startups   especially consumer companies   are choosing technology that simply can t be made in a secure fashion  And once they do have customers  the cost to retrofit on security without downtime is so much more expensive than had they just done it from the start    Let s buy a safe once we ve got cash piled up in stacks  That s how startups often think about database security   Here s What To Do  Early stage  growth stage  Consumer  enterprise  Every startup has its set of circumstances  but Barrett contends that there s a rule of thumb plan that can serve a spectrum of technology companies seeking intelligent and intentional database architecture  Here s how to do it   Start with three data centers  Given that today s technology makes it possible  Barrett believes that every startup should start in three data centers on Day One   Three is the magic number   he says   One isn t good enough  because it s only a matter of time before it goes down  The internet will die or the power will fail  Whatever it is  something will go wrong  so a single database won t cut it  The problem with two is that you re vulnerable to what s called a  split brain syndrome   If the servers at each data center lose contact with each other  it s unclear if the other s gone entirely or just inaccessible temporarily  If  at the same time  they both think that the other one s down  then they both think that they re in charge and duplicate efforts  In our world  that might mean that they both reimbursed the same expense report and double paid  That s not good    With one clock  you always know the time  With two different clocks  you never do  because you don t know which is right   Since one data center is susceptible and two can easily cross wires  start with servers in three data centers   If you have three  that means at any point  you can lose an entire data center  and there s still two remaining  Two can be a quorum and determine a decision   says Barrett   This may sound like a lot to set up on Day One  but it could be worse  Tackling this problem upfront is much better when no one s breathing down your neck  Investors aren t upset with you or the customers aren t freaking out  Build the foundation to scale upfront  Get on three different data centers or three different availability zones in AWS  It s cheap and easy  It just takes foresight    Find and use a replication technology  Why don t more startups start with three data centers  Building for three data centers means that on Day One   before you have any data   you need to deal with a replication problem  Each server in each data center needs to continuously share data with the others  such that each server shares the same level of information    But the classic technology for replication is really optimized for having a standby or backup server that everybody  fails over  to when when the primary is down  This failover process is either manual or hacked together with custom scripts that  drop  requests along the way  Either way  failure is a serious all hands on deck problem that affects live customers   says Barrett   More modern solutions are built with replication and failover in mind  such that a server going down is an unremarkable occurrence with no dropped requests  no manual action required  and no customer impact  Furthermore  unlike the classic solutions designed for very fast  very reliable networks inside a single datacenter  new solutions are optimized to work across relatively slow and unreliable internet connections between datacenters around the world    Part of the challenge is the predominance of an outdated relational database management system   If you try to do it with MySQL  it s going to be very hard because it s an old database designed for a time when disks were super slow and small  The file system couldn t be larger than 4GB  It was designed for a completely different world than the one we live in today   says Barrett   Today  everything s on SSDs or even cached in RAM   and therefore super fast  That s the conundrum with MySQL  It s still in use but optimized for a series of constraints that no longer exist  so it has all of the baggage of the old world  but it doesn t really have the advantages of the new world    There are tools that can help synchronize servers in different data centers  but the open source  easy to implement ones are just emerging   It s still tough to launch with three data centers with some of the classic solutions  and almost impossible back in 2007 without something huge like Oracle  To be fair  Percona might have worked  but it was so new I didn t discover it at the time   says Barrett   Since I had a background in P2P software  we decided to build a solution ourselves  The resulting technology  called Bedrock  makes it easy to operate a real time replicated  geo redundant distribution without the complexity of Oracle or MySQL  So far as each server is concerned  it just has a single database in its local environment  but the technology links them and takes care of all the replication  And just for kicks  we also made it speak the MySQL protocol so it can nearly be a drop in replacement    Expensify plans to give this technology away for free given the few options that existed when it was looking for a solution   Initially it was just a proprietary solution to a specific problem we couldn t find off the shelf  But over time  we realized just how powerful it was  and how really anybody could use it  Because the idea of seamless clustering with automatic  lossless failover is relevant to anyone who cares about performance and availability   which is about everyone   says Barrett   But very few people would ve had a reason to build it themselves because its core technology  the Paxos distributed consensus algorithm  is really tricky to get right  This is what enables a group of otherwise equal servers to reliably elect a master to orchestrate distributed transactions and rebalance traffic  and to do it within milliseconds of the failure occurring  But we ve spent eight years honing it through several orders of magnitude  so it s pretty hardened against a wide range of real world conditions and thus we think ready for broader use    Decide whether or not to partition your data  After selecting three servers from different data centers and selecting a replication technology  determine whether or not you ll partition your data  Partitioning involves breaking up a database into distinct independent parts of the whole  This choice affects a lot of future decisions   With partitioning  ask  do I want to allow every user to share data with every other user or split them into disjointed groups    Virtually everybody starts out with  disjointed groups   because it s conceptually the simplest  Whether you have a consumer product for individuals or enterprise products for companies  the relationships between users seem pretty obvious at the start   says Barrett   For example  if you re doing enterprise document storage  it may initially seem obvious that you only need to share documents between employees inside the company  You might even sign contracts with enterprise customers requiring that data be physically isolated   after all  what s the harm    The risk is one day you encounter an unexpected use case that links people who previously you thought were unlinkable   Imagine a law firm works for two different clients  each of which is hosted on two different databases  Suddenly the  disjointed groups  model breaks down  because putting the law firm on one database means it can t  see  the documents in the other  Your technology is now blocking your product from supporting this key use case   and you might ve cemented this technology in place by signing enterprise agreements that depend on it    The alternative approach is to assume out of the gate that any two users might someday want to share data and thus design for a single shared database from the very start   This puts you on a very different technology path  because you can no longer just  throw hardware  at the problem  If your database gets full  you can t just spin up a new one for your next batch of customers  you need to find a way to upgrade the entire thing   and do it without taking it down for everyone   says Barrett   The upside of maintaining a single continuous database for all your users is that you eliminate constraints of who can share with who  whether right away or any time in the future  The downside is a single giant database is much  much harder to maintain than a bunch of small databases   especially if you re using a classic database solution    A Primer to Course Correct  Not every startup will have the luxury of starting from scratch  If you ve already made some of these other choices   intentionally or unknowingly   there are ways to get on the right track  It may take some effort to hit the railroad switch  but the train hasn t left the station   Choose a database that replicates across multiple availability zones   Most startups will say   I ve got a single EC2 instance which runs my web server  a single RDS instance which is my database  or I store some data in S3  That s all in a single availability zone   Don t paint yourself into a corner on day one  build to support multiple availability zones with Amazon or distinct data centers    Try Expensify s replication technology    Switching from MySQL to Bedrock is easy  if you re small  you can likely take down your server in the middle of the night  export your data  re import it into Bedrock and your web servers won t know the difference because Bedrock speaks the MySQL protocol    Start with stored procedures   Ask yourself   How bad would it be if a hacker rooted my webserver   If the answer is  real bad   then move your authentication logic to a stored procedure inside the database  It s more secure  maintains better layering  and higher performance for the end user    Bringing it Together  Most wouldn t describe database architecture as sexy  But it s vital to get right before you have customers or their data  How you organize  scale and secure this data has an impact on not only your technology but also the scalability of your business model  Start with at least a server in three different data centers or availability zones  Choose a replication technology that allows them to precisely and reliably communicate with each other  Examine the relationship between your users to decide whether you should partition your data  Consider stored procedures to secure the data you have  The biggest challenge is not making a decision to take this approach  but getting comfortable veering from popular database management systems like MySQL    Don t make decisions around database architecture as if you re Google  But also don t put constraints on your startup that ll eliminate its chance to be  a Google  in the future  The truth is that many startups will fail well before some of these issues matter  But ask yourself  are you optimizing for success or failure   asks Barrett   It was rigorous requirements and chance early on that got us thinking more thoughtfully about database architecture  We now know that foundation helped us not only scale with the requirements we anticipated  but support all the amazing deep data and artificial intelligence opportunities we could have never imagined  That s only possible given the decisions we first made with our database architecture  which keep all our data in one giant bucket that can be sliced  diced  and intermingled in any conceivable fashion  The alternative is grim  If you don t realize you re building a prison on your first day  those bars get really hard to move,"[668 428 563 397 562 1289 413 772 845 2 823]"
734,training-dataset/engineering/1339.txt,engineering,Scientist  Measure Twice  Cut Over OnceToday we re releasing Scientist 1 0 to help you rewrite critical code with confidence   As codebases mature and requirements change  it is inevitable that you will need to replace or rewrite a part of your system  At GitHub  we ve been lucky to have many systems that have scaled far beyond their original design  but eventually there comes a point when performance or extensibility break down and we have to rewrite or replace a large component of our application   Problem  A few years ago when we were faced with the task of rewriting one of the most critical systems in our application   the permissions code that controls access and membership to repositories  teams  and organizations   we began looking for a way to make such a large change and have confidence in its correctness   There is a fairly common architectural pattern for making large scale changes known as Branch by Abstraction  It works by inserting an abstraction layer around the code you plan to change  The abstraction simply delegates to the existing code to begin with  Once you have the new code in place  you can flip a switch in the abstraction to begin substituting the new code for the old   Using abstractions in this way is a great way to create a chokepoint for calls to a particular code path  making it easy to switch over to the new code when the time comes  but it doesn t really ensure that the behavior of the new system will match the old system   just that the new system will be called in all places where the old system was called  For such a critical piece of our system architecture  this pattern only fulfilled half of the requirements  We needed to ensure not only that the new system would be used in all places that the old system was  but also that its behavior would be correct and match what the old system did   Why tests aren t enough  If you want to test correctness  you just write some tests for your new system  right  Well  not quite  Tests are a good place to start verifying the correctness of a new system as you write it  but they aren t enough  For sufficiently complicated systems  it is unlikely you will be able to cover all possible cases in your test suite  If you do  it will be a large  slow test suite that slows down development considerably   There s also a more concerning reason not to rely solely on tests to verify correctness  Since software has bugs  given enough time and volume  your data will have bugs  too  Data quality is the measure of how buggy your data is  Data quality problems may cause your system to behave in unexpected ways that are not tested or explicitly part of the specifications  Your users will encounter this bad data  and whatever behavior they see will be what they come to rely on and consider correct  If you don t know how your system works when it encounters this sort of bad data  it s unlikely that you will design and test the new system to behave in the way that matches the legacy behavior  So  while test coverage of a rewritten system is hugely important  how the system behaves with production data as the input is the only true test of its correctness compared to the legacy system s behavior   Enter Scientist  We built Scientist to fill in that missing piece and help test the production data and behavior to ensure correctness  It works by creating a lightweight abstraction called an experiment around the code that is to be replaced  The original code   the control   is delegated to by the experiment abstraction  and its result is returned by the experiment  The rewritten code is added as a candidate to be tried by the experiment at execution time  When the experiment is called at runtime  both code paths are run  with the order randomized to avoid ordering issues   The results of both the control and candidate are compared and  if there are any differences in that comparison  those are recorded  The duration of execution for both code blocks is also recorded  Then the result of the control code is returned from the experiment   From the caller s perspective  nothing has changed  But by running and comparing both systems and recording the behavior mismatches and performance differences between the legacy system and the new one  you can use that data as a feedback loop to modify the new system  or sometimes the old   to fix the errors  measure  and repeat until there are no differences between the two systems  You can even start using Scientist before you ve fully implemented the rewritten system by telling it to ignore experiments that mismatch due to a known difference in behavior   The diagram below shows the happy path that experiments follow   Happy paths are only part of a system s behavior  though  so Scientist can also handle exceptions  Any exceptions encountered in either the control or candidate blocks will be recorded in the experiments observations  An exception in the control will be re raised at the end of the experiment since this is the  return value  of that block  exceptions in candidate blocks will not be raised since that would create an unexpected side effect of the experiment  If the candidate and control blocks raise the same exception  this is considered a match since both systems are behaving the same way   Example  Let s say we have a method to determine whether a repository can be pulled by a particular user   class Repository def pullable_by    user   self   is_collaborator    user   end end  But the is_collaborator  method is very inefficient and does not perform well  so you have written a new method to replace it   class Repository def has_access    user         end end  To declare an experiment  wrap it in a science block and name your experiment   def pullable_by    user   science  repository pullable by  do   experiment         end end  Declare the original body of the method to be the control branch   the branch to be returned by the entire science block once it finishes running   def pullable_by    user   science  repository pullable by  do   experiment   experiment   use   is_collaborator    user     end end  Then specify the candidate branch to be tried by the experiment   def pullable_by    user   science  repository pullable by  do   experiment   experiment   use   is_collaborator    user     experiment   try   has_access    user     end end  You may also want to add some context to the experiment to help debug potential mismatches   def pullable_by    user   science  repository pullable by  do   experiment   experiment   context  repo    id    user    user   id experiment   use   is_collaborator    user     experiment   try   has_access    user     end end  Enabling  By default  all experiments are enabled all of the time  Depending on where you are using Scientist and the performance characteristics of your application  this may not be safe  To change this default behavior and have more control over when experiments run  you ll need to create your own experiment class and override the enabled  method  The code sample below shows how to override enabled  to enable each experiment a percentage of the time   class MyExperiment include ActiveModel    Model include Scientist    Experiment attr_accessor  percentage def enabled  rand   100     percentage end end  You ll also need to override the new method to tell Scientist create new experiments with your class rather than the default experiment implementation   module Scientist  Experiment def self   new   name   MyExperiment   new   name  name   end end  Publishing results  Scientist is not opinionated about what you should do with the data it produces  it simply makes the metrics and results available and leaves it up to you to decide how and whether to store it  Implement the publish method in your experiment class to record metrics and store mismatches  Scientist passes an experiment s result to this method  A Scientist  Result contains lots of useful information about the experiment such as   whether an experiment matched  mismatched  or was ignored  the results of the control and candidate blocks if there was a difference  any additional context added to the experiment  the duration of the candidate and control blocks  At GitHub  we use Brubeck and Graphite to record metrics  Most experiments use Redis to store mismatch data and additional context  Below is an example of how we publish results   class MyExperiment def publish   result   name   result   experiment_name  stats   increment  science     name    total   stats   timing  science     name    control    result   control   duration  stats   timing  science     name    candidate    result   candidates   first   duration if result   mismatched   stats   increment  science     name    mismatch  store_mismatch_data   result   end end end def store_mismatch_data   result   payload      name    name    context    context    control    observation_payload   result   control     candidate    observation_payload   result   candidates   first     execution_order    result   observations   map      name     Redis   lpush  science     name    mismatch    payload       end end  By publishing this data  we get graphs that look like this   And mismatch data like     context  repo  3 user  1 name   repository pullable by  execution_order     candidate     control    candidate  duration  0   00156 89999999999999 exception  nil value  true control  duration  0   000735 exception  nil value  false    Using the data to correct the system  Once you have some mismatch data  you can begin investigating individual mismatches to see why the control and candidate aren t behaving the same way  Usually you ll find that the new code has a bug or is missing a part of the behavior of the legacy code  but sometimes you ll find that the bug is actually in the legacy code or in your data  After the source of the error has been corrected  you can start the experiment again and repeat this process until there are no more mismatches between the two code paths   Finishing an experiment  Once you are able to conclude with reasonable confidence that the control and candidate are behaving the same way  it s time to wrap up your experiment  Ending an experiment is as simple as disabling it  removing the science code and control implementation  and replacing it with the candidate implementation   def pullable_by    user   has_access    user   end  Caveats  There are a few cases where Scientist is not an appropriate tool to use  The most important caveat is that Scientist is not meant to be used for any code that has side effects  A candidate code path that writes to the same database as the control  invalidates a cache  or otherwise modifies data that affects the original  production behavior is dangerous and incorrect  For this reason  we only use Scientist on read operations   You should also be mindful that you take a performance hit using Scientist in production  New experiments should be introduced slowly and carefully and their impact on production performance should be closely monitored  They should run for just as long as is necessary to gain confidence rather than being left to run indefinitely  especially for expensive operations   Conclusion  We make liberal use of Scientist for a multitude of problems at GitHub  This development pattern can be used for something as small as a single method or something as large as an external system  The Move Fast and Fix Things post is a great example of a short rewrite made easier with Scientist  Over the last few years we ve also used Scientist for projects such as   a large  multi year long rewrite and clean up of our permission code  switching to a new code search cluster  optimizing queries   this allows us to ensure not only better performance of the new query  but that it is still correct and doesn t unintentionally return more or less or different data  refactoring risky parts of the codebase   to ensure no unintentional changes have been introduced  If you re about to make a risky change to your Ruby codebase  give the Scientist gem a try and see if it can help make your work easier  Even if Ruby isn t your language of choice  we d still encourage you to apply Scientist s experiment pattern to your system  And of course we would love to hear about any open source libraries you build to accomplish this,"[734 184 428 413 2 1139 374 563 397 823 845]"
740,training-dataset/product/214.txt,product,The matrimony of qualitative and quantitative analytics115 Flares 115 Flares    As mobile app technology evolves  it seems logical that our mobile analytics capabilities should evolve proportionally  Yet for the most part  any evolution in the mobile analytics realm is happening at a much more glacial pace  Now that s not to discount improvements in areas such as data visualisation  product integrations  and real time capabilities  which have helped product managers gather and dissect their data better than ever before  These advancements are valuable  but do not supersede the underlying disproportion between mobile app technology and our capability to analyse mobile app usage that exists today   Interestingly  this disparity is due to the data itself   you ve all heard the quote  the devil is in the data   But what if I was to tell you that the quantitative data you have been gathering is actually functioning more like a prologue to an important story than the story itself   in this case  your users  story  This quantitative data gives you a powerful introduction into what users are doing in your mobile app  but it doesn t allow you to explore their specific experiences  Mobile product managers need data that provides them with the ability to actually see and understand specific user behaviour instead of having to define it by aggregate  numerical data   However  a few mobile analytics companies  including Appsee  have recognised this need and brought a new type of analytics to market   qualitative analytics   And as you probably guessed  once you combine qualitative analytics with your quantitative data  you are able to obtain that epic  complete story on your mobile users  But how exactly   The shortcomings of quantitative analytics  In order to understand the potency of this union  we first need to understand why relying solely on analytics that provides quantitative data  traditional analytics  simply does not cut it   Let s just review the definition of quantitative for a moment  Merriam Webster notes the definition as follows   1  of  relating to  or expressible in terms of quantity  2  of  relating to  or involving the measurement of quantity or amount  3  based on quantity  specifically of classical verse  based on temporal quantity or duration of sounds  Numbers  numbers  numbers   that is the core of the definition  So when it comes to quantitative analytics  basically all of the data and information it collects can be measured with numbers   This is no bad thing  in fact it s extremely important  Quantitative data can help you gather insights on overall user actions and usage trends  such as the length of the average user session or how many users completed a certain conversion funnel  But these numbers don t answer the pivotal question of  why    Quantitative analytics can only answer your number based inquiries  Numbers have an extremely important story to tell  but how do you figure out and communicate that story   Enter qualitative analytics   What is qualitative analytics and why is it needed   While quantitative analytics focuses on aspects of your app that can be measured by numbers  qualitative analytics zones in on the one essential element of your mobile app that cannot be delineated by numbers  That element is the user experience  your user s unique story within your app   At the moment  how do you know whether your users are frustrated with a certain unresponsive button or confused by a particular feature  To put it simply  no number on a dashboard can effectively describe those specific in app experiences  In order to fully understand and assess your users  stories  you need data that enables you to see what your users are experiencing and how they behave  This is the essence of qualitative analytics   With features such as user session recordings and touch heatmaps  qualitative analytics allows you to actually step into the shoes of your real users  not beta testers  and examine how they truly interact with your app  This is the best way to analyse a KPI as subjective and nuanced as user experience   Yet the value of qualitative analytics is not limited to inspecting user experience  It also serves as an extremely powerful compliment to your quantitative data   How quantitative and qualitative make the perfect couple  Quantitative analytics allows you to identify on a numerical basis important trends  issues  and actions within your mobile app  Then  qualitative analytics  such as unique user session recordings  augments this data by supplying the crucial  whys  behind those numbers   Let s look at some compelling use cases of this power couple in action   In app crashes  Your quantitative analytics tells you that your daily app crash rate has increased by 50   This is very important  but now you need to understand why this is happening  To obtain valuable visual context behind your crashes  you turn to your qualitative analytics and watch session recordings of crashed sessions from that specific day  This allows you to accurately reproduce a crash and discern the sequence of user actions that led to a crash   Conversion funnels  You have an ecommerce app with a conversion funnel in place for purchase completion  Your quantitative data tells you that over a seven day period  74 4  of your users that visited the  My Cart  screen  dropped out of the funnel and did not trigger the event  Purchase Complete   These stats alert you to the fact that your users might be encountering a potential issue or multiple issues within the  My Cart  screen  What are the issues exactly  By drilling down to specific session recordings of users that dropped out of the funnel  you can see exactly what might have caused friction within their experience   In a nutshell  this combination of quantitative data and qualitative information allows you to streamline the process of turning data into information  and information into insights   actionable insights  No more scenarios of drowning in copious amounts of quantitative data and guesswork   To top it off  by using qualitative analytics to distill quantitative data  you can save valuable time and resources   which product managers often are low on  At the end of the day  this quantitative and qualitative union should empower you to separate the  wheat from the chaff  within your data and make key decisions regarding your product with more confidence  We can t wait to hear what insights you obtain,"[740 2 1107 1331 254 397 865 334 385 934 1139]"
743,training-dataset/engineering/1382.txt,engineering,uReplicator  Uber Engineering s Robust Kafka Replicatorby Chinmay Soman  Yuanchi Ning  Xiang Fu   Hongliang Xu  Uber s Analytics Pipeline  At Uber  we use Apache Kafka as a message bus for connecting different parts of the ecosystem  We collect system and application logs as well as event data from the rider and driver apps  Then we make this data available to a variety of downstream consumers via Kafka   Data in Kafka feeds both real time pipelines and batch pipelines  The former data is for activities like computing business metrics  debugging  alerting  and dashboarding  The batch pipeline data is more exploratory  such as ETL into Apache Hadoop and HP Vertica   In this article  we describe uReplicator  Uber s open source solution for replicating Apache Kafka data in a robust and reliable manner  This system extends the original design of Kafka s MirrorMaker to focus on extremely high reliability  a zero data loss guarantee  and ease of operation  Running in production since November 2015  uReplicator is a key piece of Uber s multi data center infrastructure   What s a Mirror Maker  and Why Do We Need One   Given the large scale use of Kafka within Uber  we end up using multiple clusters in different data centers  For a variety of use cases  we need to look at the global view of this data  For instance  in order to compute business metrics related to trips  we need to gather information from all data centers and analyze it in one place  To achieve this  we have historically used the open source MirrorMaker tool shipped with the Kafka package to replicate data across data centers  as shown below   MirrorMaker  as part of Kafka 0 8 2  itself is quite simple  It uses a high level Kafka consumer to fetch the data from the source cluster  and then it feeds that data into a Kafka producer to dump it into the destination cluster   Kafka s MirrorMaker Limitations at Uber  Although our original MirrorMaker setup started out sufficient  we soon ran into scalability issues  As the number of topics and the data rate  bytes second  grew  we started seeing delayed data delivery or complete loss of data coming into the aggregate cluster  resulting in production issues and reducing data quality  Some of the major issues with the existing MirrorMaker tool  as of 0 8 2  for Uber s particular use cases are listed below   Expensive rebalancing  As mentioned before  each MirrorMaker worker uses a high level consumer  These consumers often go through a process of rebalance  They negotiate among themselves to decide who gets to own which topic partition  done via Apache Zookeeper    This process can take a long time  we ve observed about 5 10 minutes of inactivity in certain situations  This is a problem  as it violates our end to end latency guarantee  In addition  the consumers can give up after 32 rebalancing attempts and get stuck forever  Unfortunately  we saw this happen firsthand a few times  After every rebalance attempt  we saw a similar traffic pattern   After the inactivity during the rebalance  MirrorMaker had a massive backlog of data that it had to catch up with  This resulted in a traffic spike on the destination cluster and  subsequently  all downstream consumers  leading to production outages and increased end to end latency   Difficulty adding topics  At Uber  we must specify a whitelist of topics within our mirroring tool to control how much data flows across the WAN link  With Kafka MirrorMaker  this whitelist was completely static  and we needed to restart the MirrorMaker cluster to add new topics  Restart is expensive  since it forces the high level consumers to rebalance  This became an operational nightmare   Possible data loss  The old MirrorMaker had a problem it seems to be fixed in the latest release with automatic offset commit that could have resulted in data loss  The high level consumer automatically committed the offsets for fetched messages  If a failure were to occur before MirrorMaker could verify that it actually wrote the messages to the destination cluster  then those messages would be lost   The old MirrorMaker had a problem it seems to be fixed in the latest release with automatic offset commit that could have resulted in data loss  The high level consumer automatically committed the offsets for fetched messages  If a failure were to occur before MirrorMaker could verify that it actually wrote the messages to the destination cluster  then those messages would be lost  Metadata sync issues  We also ran into an operational issue with the way config was updated  To add or delete topics from the whitelist  we listed all the final topic names in a config file  which was read during MirrorMaker initialization  Sometimes the config failed to update on one of the nodes  This brought down the entire cluster  since the various MirrorMaker workers did not agree on the list of topics to replicate   Why We Developed uReplicator  We considered the following alternatives for solving the aforementioned problems   A  Split into multiple MirrorMaker clusters  Most of the problems listed above resulted from the high level consumer rebalance process  One way to reduce its impact is to restrict the number of topic partitions replicated by one MirrorMaker cluster  Thus  we would end up with several MirrorMaker clusters  each replicating a subset of the topics to be aggregated   Pros     Adding new topics is easy  Just create a new cluster     MirrorMaker cluster restart happens quickly   Cons     It s another operational nightmare  we have to deploy and maintain multiple clusters   B  Use Apache Samza for replication  Since the problem is with the high level consumer  as of 0 8 2   one solution is using the Kafka SimpleConsumer and adding the missing pieces of leader election and partition assignment  Apache Samza  a stream processing framework  already statically assigns partitions to workers  We can then simply use a Samza job to replicate and aggregate data to the destination   Pros     It s highly stable and reliable     It s easy to maintain  We can replicate a lot of topics using one job     Job restart has minimal impact on replication traffic   Cons     It s still very static  We need to restart the job to add and or delete topics     We need to restart the job to add more workers  as of Samza 0 9      Topic expansion needs to be explicitly handled   C  Use an Apache Helix based Kafka consumer  Ultimately  we decided to use a Helix based Kafka consumer  In this case  we re using Apache Helix to assign partitions to workers  and each worker uses the SimpleConsumer to replicate data   Pros     Adding and deleting topics is very easy     Adding and deleting nodes to the MirrorMaker cluster is very easy     We never need to restart the cluster for an operational reason  just for upgrades      It s highly reliable and stable   Cons     This introduces a dependency on Helix   This is fine  since Helix itself is very stable and we can use one Helix cluster for multiple MirrorMaker clusters    uReplicator Overview  uReplicator s various components work in different ways toward reliability and stability   1  The Helix uReplicator controller  actually a cluster of nodes  has several responsibilities   Distribute and assign topic partitions to each worker process  Handle addition deletion of topics partitions  Handle addition deletion of uReplicator workers  Detect node failures and redistribute those specific topic partitions  The controller uses Zookeeper to accomplish all of these tasks  It also exposes a simple REST API in order to add delete modify topics to be mirrored   2  A uReplicator worker  similar to a worker process in Kafka s mirroring feature  replicates a certain set of topic partitions from source cluster to destination cluster  Instead of a rebalance process  uReplicator controller determines uReplicator s assignment  Also  instead of using the Kafka high level consumer  we use a simplified version called DynamicKafkaConsumer   3  A Helix agent for each uReplicator worker gets notified whenever there s a change  addition deletion of topic partitions   In turn  it notifies the DynamicKafkaConsumer to add remove topic partitions   4  A DynamicKafkaConsumer instance  which is a modification of the high level consumer  exists on each uReplicator worker  It removes the rebalance part and adds a mechanism to add delete topic partitions on the fly   For instance  let s say we want to add a new topic to an existing uReplicator cluster  The flow of events is as follows   Kafka admin adds the new topic to the controller using the following command   uReplicator controller figures out the number of partitions for testTopic and maps topic partitions to active workers  It then updates the Zookeeper metadata to reflect this mapping   Each corresponding Helix agent receives a callback with notification of the addition of these topic partitions  In turn  this agent invokes the addFetcherForPartitions function of DynamicKafkaConsumer    The DynamicKafkaConsumer subsequently registers these new partitions  finds the corresponding leader brokers  and adds them to fetcher threads to start the data mirroring   For more details regarding the implementation  please refer to the uReplicator Design wiki   Impact on Overall Stability  Since uReplicator s initial launch in production at Uber about eight months ago  we haven t seen a single prod issue with it  contrasted with an outage of some sort almost every week before its implementation   The graph below depicts the scenario of adding new topics to the mirroring tool whitelist in production  The first graph shows the total topic partitions owned by each uReplicator worker  This count increases for each new topic being added   The second graph shows the corresponding uReplicator traffic flowing to the destination cluster  There s no period of inactivity or load spikes  as with the old Kafka MirrorMaker   Overall  the benefits of uReplicator include   Stability   Rebalancing now happens only during startup and when a node is added or deleted  In addition  it only affects a subset of the topic partitions instead of causing complete inactivity like before   Easier scalability   Adding a new node to an existing cluster is now much simpler  Since partition assignment is now static  we can intelligently move only a subset of partitions to the new node  Other topic partitions remain unaffected   Easier operation   Uber s new mirroring tool supports dynamic whitelisting  We now don t need to restart the cluster when adding deleting expanding Kafka topics   Zero data loss   uReplicator guarantees zero data loss  since it commits checkpoints only after the data has been persisted on the destination cluster   Since inception  uReplicator has been a valuable addition to the streaming platform team s mission of connecting different parts of the Uber Engineering ecosystem together via messaging and the publish subscribe model  using Kafka   As part of this mission  we are building a novel analytics platform for computing business metrics on top of streaming data  Sound interesting  See our real time data infrastructure openings on the Uber Careers page if you re interested in participating in the next chapter of this story   Chinmay Soman is a software engineer on Streaming Platform in the Core Infrastructure group within Uber Engineering and wrote this article with fellow Streaming Platform engineers Yuanchi Ning  Xiang Fu  and Hongliang Xu   Photo Credit  Zebra waterhole reflections by Conor Myhrvold  Etosha National Park  Namibia   Editor s note Sep 30  2016  Uber s uReplicator tool was formerly called uMirrorMaker    Conor Myhrvold,"[743 1031 638 934 1414 1281 0 334 397 563 831]"
772,training-dataset/engineering/692.txt,engineering,Game of Thrones can teach you valuable security lessonsWith new hacking techniques  malware  viruses and threats being created faster than Melisandre s demon babies  the web is indeed dark and full of terrors  Here are seven lessons for security managers pulled straight out of Westeros   1  Small things can become huge problems  In the age of big data  risk once deemed minimal may pose serious threats to companies concerned with keeping the information they ve collected private  but that begins and ends within the companies and the parameters and protocols they have in place to keep data secure   Nobody took the dragons or dire wolves seriously in the beginning of Game of Thrones  but by season 3 they were capable of wreaking havoc and wiping out armies   Small issues can grow into serious complications If left unchecked   Everything from employee access to information  to the changing of passwords on a regular basis is uniquely important  Businesses are using mobile systems more often everyday  but mobile security isn t quite up to par with larger network security endpoints    I think it s more dangerous in some ways with mobile systems that business endpoints do  Even home systems are better monitored to resist attack  Android has critical vulnerabilities   said Gene Spafford  professor and executive director at Purdue University  Computer Software Consultant   The trend is generally making mobile devices more powerful  all purpose computer systems  so the threat increases    2  Faceless men are everywhere  Anonymous has become synonymous with a global network of hackers  connected through common causes  and faceless men attempting to breach network security is nothing new  Legislators are almost always one step behind  while cybercriminals and hackers are always looking toward tomorrow and how to breach the security of tomorrow   Much like the faceless assassins of the house of black and white who approach their victims anonymously through seemingly friendly interactions  Season 5 Episode 2   cybercriminals make common practice of seeking out and learning everything they can about a target before phishing for their information   They may procure the information they seek by phishing for personal information via email  text messages and even phone calls  They will engage their victims slowly but surely taking each step as it comes  and using every bit of information given to their advantage in the retrieval of more   While a skilled and more often than not lone hacker will often use their talents to breach the gates of companies and corporations alike for the simple purpose of retrieving information for the sake of access to information  networks of cybercriminals  or a particularly malicious individual will break into a network with the intent of interference  surveillance  counter surveillance  cyberlaundering  and the overall goal of bringing a company to its knees   In the world of Game of Thrones  the many faced god is a just god  who takes a life for a life  In the real world  faceless attackers have far more disguises at their disposal  and will use them to their advantage at every turn made available to them  While the ends differ  the means remain the same   These days cyber attacks are more common and becoming more sophisticated every day   What they re after isn t always clear  but for every method used by cybercriminals and hackers seeking information  The implementation of new technology  hybrid cloud storage systems  data splitting  cryptography and centralized storage databases are becoming the norm   3  Walls of fire don t always help  Modern firewalls are complex and take months to become familiar with  but even the most complex firewall is only software and by its very nature has defects  Unidirectional gateways block attacks from untrusted networks no matter what their IP address is  but without them  it s easy to bypass firewalls with forged IP addresses  especially if someone has access to the same LAN segment as the network they re trying to breach   In Game of Thrones  the seven kingdoms of Westeros are protected by a 700 foot  300 mile wide wall of solid ice that was built by  Bran The Builder    It has magical spells woven into it to White Walkers out  but many of those spells have been undone by Bran Stark  Now the wall is just a wall   Sometimes all hackers need to breach a firewall are the magic words   Password theft is the easiest way to break into a network  and the methods attackers have devised to steal passwords have become far more devious   Spear phishers use extremely convincing emails targeted at people with access to passwords and protocols  Encryption and two way factor authentication are practically useless against attacks from within a network  but unidirectional gateways block outside communication and attacks into plant networks   4  Keeping your friends far and your enemies farther  Access to data by individuals within a network  or by trusted employees isn t always safe  From Mark Abene and Julian Assange  to Chelsea Manning and Edward Snowden  people with access to networks can gather massive amounts of data with limited resources and small windows of time   As seen on Game of Thrones  as Lord  Littlefinger  Baelish and Varys  The Spider  use their networks of information in the form of  Little Birds  to grasp and grip in the power struggle between kingdoms  even the weakest link can bring down  or at the very least contribute to the fall of kings   In September of 2015  Morgan Stanley realized that 730 000 account numbers were stolen by an employee  whom had been gathering account numbers over a period of three years and had them transferred to a private server at his home  It would be wise for companies with sensitive information to implement a  trust but verify  model  storing data in digital safes and data secure repositories  as well as developing and enforcing  need to know  policies among employees   5  The dead can come back to haunt you  Many small businesses  midsize companies and even large corporations assume that once the hard drives on their computer systems are wiped  they can sell the computers or throw them away without worry  but as we ve learned from Game Of Thrones  dead doesn t always mean dead  Some ATA  IDE and SATA hard drive manufacture designs include support for the ATA secure erase standard and have been since the dawn of the 21st century  But research in 2011 found that four out of eight manufacturers did not implement ATA Secure Erase correctly   If we ve learned anything from Game of Thrones  it s that death doesn t always mean forever   Much like Melisandre and Thoros use magic words to resurrect the dead   cybercriminals and hackers alike can resurrect data from sources long thought to be dead   All data has value  and the retrieval of most trivial data from major corporations can be valuable to a company from its infancy to the big leagues   Small businesses and midsize companies may not be concerned with hackers or intelligence agencies attempting to retrieve data from their hard drives after they ve been wiped  Larger companies and corporations however  would do best to ensure that data they want gone stays gone The Gutmann method  a 35 pass overwrite technique  may be considered overkill by some  but it s been tried and true for years and may work for years to come   6  The iron price  The biggest issue among leading information security experts is a lack of understanding of cloud based security  The vast majority of web based companies put more of their financial resources into security software than they put into hardware and the people working for them  A trend among elite web based companies in big data is hybrid storage  private cloud storage  hyperscale compute storage and centralized storage  all of which combine yesterday s technology with the technology of tomorrow  The value of data continues to rise  while the value of human beings with access and control of data has remained stagnant   From software to hardware  the cost of information security can be expensive  but it s worth it  In Game of Thrones  Valyrian Steel is a rare commodity  but it s one of the few things that shatter a White Walker into ice dust    It comes down to valuation and people s understanding  said Spafford   If people better understood the cost involved  Centralized storage may cost more  but it comes down to valuation of the data  There are some things being tried by organizations using data splitting and cryptography  it requires extra processing and can be hard to audit  What is the real cost of sharing  valued with operational cost  A number of people aren t willing to spend to protect the information they are trying to protect    7  The Old Gods  Or The New Gods  In Game of Thrones  there are many different religions and gods the inhabitants of Westeros and the seven kingdoms pray to  and everyone seems certain that their deities are the greatest  but who can we turn to for protection in the real world   From mom and pop small businesses to corporate giants  with each new advance in information technology  new threats arise  From mobile applications to quantum computing  security must develop and adapt in order to cope with the changing times  but how can cloud based security storage handle the massive amounts of data captured without corruption or interference    Technology is always evolving  And very fast  This causes a lot of consumer products  whether hardware or software  to be released without having gone through proper security testing as the latter takes time  is costly and could cause delays in product releases which would in turn have a company fall behind competitors   said Khalil Sehnaoui  founder of Krypton Security  an information security consulting firm   The future of data protection is safe storage and strong encryption  Safe storage is a wide subject but basically I usually do not like anything cloud based  as we say in InfoSec  Cloud storage is just your data stored in someone else s computer    Obviously small to midsize businesses  as well as a majority of single users  have no choice when it comes to using data storage companies as it is cost effective  In that case  those organizations may want to pay extra attention to security practices  redundancy and multi layer security and encryption procedures    Hybrid Data Storage is for now one of the best solutions as it is cost effective  offers high capacity and good manageability  Hybrid Hard Drives mix old Hard Disk Drive  HDD  storage capacity with speedier Solid State Drives  SSD  on a single drive  This allows the most used data to be cached and accessed quickly  Only a small SSD volume is needed to get high performance gains  Booting times are also improved   said Khalil   So what is the best solution for companies trying to ensure their data is secure  The best solution it seems is a combination of the old and the new   From small businesses to big data giants  hybrid data storage  repositories and a better understanding of cloud based security systems will become the new normal,"[772 668 397 1289 1139 563 413 1331 855 1414 823]"
823,training-dataset/engineering/816.txt,engineering,Data Leverage   Cognitect BlogYou can take a look at the schema reference if you re interested in the details   To me  the most interesting part is what I can do with that metadata  In most databases  data and metadata are entirely separate worlds  You ve got one language for manipulating the database schema and a totally different one for manipulating the contents of the database  The query language can only query contents and cannot touch schema  Datomic puts data and metadata on the same level  Suppose I wanted to make a documentation generator that ran across my code and documented the schema  Can you imagine scanning for every string  then parsing the string to see if it is a SQL statement  then extracting the details of the tables  Absolutely not  Can you imagine mapping a function across a list of maps  to extract the  db ident and  db doc keys  Easiest thing in the world   This is why I like working with Datomic  My transaction is data  just a vector of vectors or a vector of maps  What I get back as a result is data  a set of tuples  If I want to look at any of the built in attributes of any of the attributes I ve added any of the metadata I just do an ordinary query and get back ordinary data  My query is specified as data  Results come back as data   I eliminate the loop of generating and parsing strings  I can use data in my program and send the data directly to the database  This creates a great simplicity  I never need an object relational mapping layer  If I need to look at something  I can interact with it through the REPL or through a debugger and I just see the data  I don t see some opaque driver specific object like I would with JDBC   Data Orientation Facilitates Change  Data orientation comes into play when we re building services or interacting with services  Anytime we need to use JSON or XML to communicate with something  the typical approach in Java or C  is to write a class that represents the unit of data that we re passing around  This is when versioning gets difficult and people say   it s hard to support multiple versions of your API   Again  it s a problem we ve created for ourselves by choosing the object oriented approach  Working with data as data  it becomes easier to work with multiple versions  and easier to manage change  which builds antifragility   With Clojure  it s very easy to support multiple versions of our API  In fact  it so easy  we can even have an API about APIs  Now  our data orientation approach takes everything up a level  By using data rather than classes or annotations we give ourselves the ability to  go meta  with no more difficulty than writing things  straight    If we wanted to do that in Java  we would need to build some kind of a new language that emitted Java as its byte code  We don t need to do that with Clojure   By focusing on the data  and not to doing a lot of the data modeling inherent in the old OOP paradigm  the data becomes very easy to represent  open up and unlock  For example  it s easy to make the data available to mobile and web applications used by other systems via APIs   The Power of Data driven Microservices  With a data driven microservice  instead of writing code that describes the data that you re going to send via JSON  or writing code that has annotations all over it to turn it into REST paths  we simply send a chunk of data to a service  That data describes a data format and it describes end points to expose via http  nonREST   This data orientation paves a direct path to maneuverable microservices and increased business value   In a previous post  I asserted that by working with a maneuverable microservices architecture and open source technology and tools such as Clojure and Datomic  a pair of skilled developers could create a meaningful service in minutes or hours versus days and weeks   My colleague Paul deGrandis has done some compelling work in the area of data driven microservices  Working with Consumer Reports  who was in the process of reviewing new technologies and platforms  Paul proposed and developed three proof of concept  POC  systems  all produced with Clojure  ClojureScript and Datomic  The last POC rebuilt an existing system  providing a benchmark for comparison   Code Metrics,"[823 397 413 334 1331 428 934 1139 845 563 668]"
831,training-dataset/engineering/1366.txt,engineering,How Discord Stores Billions of Messages   Discord BlogHow Discord Stores Billions of Messages  Discord continues to grow faster than we expected and so does our user generated content  With more users comes more chat messages  In July  we announced 40 million messages a day  in December we announced 100 million  and as of this blog post we are well past 120 million  We decided early on to store all chat history forever so users can come back at any time and have their data available on any device  This is a lot of data that is ever increasing in velocity  size  and must remain available  How do we do it  Cassandra   What we were doing  The original version of Discord was built in just under two months in early 2015  Arguably  one of the best databases for iterating quickly is MongoDB  Everything on Discord was stored in a single MongoDB replica set and this was intentional  but we also planned everything for easy migration to a new database  we knew we were not going to use MongoDB sharding because it is complicated to use and not known for stability   This is actually part of our company culture  build quickly to prove out a product feature  but always with a path to a more robust solution   The messages were stored in a MongoDB collection with a single compound index on channel_id and created_at   Around November 2015  we reached 100 million stored messages and at this time we started to see the expected issues appearing  the data and the index could no longer fit in RAM and latencies started to become unpredictable  It was time to migrate to a database more suited to the task   Choosing the Right Database  Before choosing a new database  we had to understand our read write patterns and why we were having problems with our current solution   It quickly became clear that our reads were extremely random and our read write ratio was about 50 50   Voice chat heavy Discord servers send almost no messages  This means they send a message or two every few days  In a year  this kind of server is unlikely to reach 1 000 messages  The problem is that even though this is a small amount of messages it makes it harder to serve this data to the users  Just returning 50 messages to a user can result in many random seeks on disk causing disk cache evictions   Private text chat heavy Discord servers send a decent number of messages  easily reaching between 100 thousand to 1 million messages a year  The data they are requesting is usually very recent only  The problem is since these servers usually have under 100 members the rate at which this data is requested is low and unlikely to be in disk cache   Large public Discord servers send a lot of messages  They have thousands of members sending thousands of messages a day and easily rack up millions of messages a year  They almost always are requesting messages sent in the last hour and they are requesting them often  Because of that the data is usually in the disk cache   We knew that in the coming year we would add even more ways for users to issue random reads  the ability to view your mentions for the last 30 days then jump to that point in history  viewing plus jumping to pinned messages  and full text search  All of this spells more random reads    Next we defined our requirements   Linear scalability   We do not want to reconsider the solution later or manually re shard the data   We do not want to reconsider the solution later or manually re shard the data  Automatic failover   We love sleeping at night and build Discord to self heal as much as possible   We love sleeping at night and build Discord to self heal as much as possible  Low maintenance   It should just work once we set it up  We should only have to add more nodes as data grows   It should just work once we set it up  We should only have to add more nodes as data grows  Proven to work   We love trying out new technology  but not too new   We love trying out new technology  but not too new  Predictable performance   We have alerts go off when our API s response time 95th percentile goes above 80ms  We also do not want to have to cache messages in Redis or Memcached   We have alerts go off when our API s response time 95th percentile goes above 80ms  We also do not want to have to cache messages in Redis or Memcached  Not a blob store   Writing thousands of messages per second would not work great if we had to constantly deserialize blobs and append to them   Writing thousands of messages per second would not work great if we had to constantly deserialize blobs and append to them  Open source   We believe in controlling our own destiny and don t want to depend on a third party company   Cassandra was the only database that fulfilled all of our requirements  We can just add nodes to scale it and it can tolerate a loss of nodes without any impact on the application  Large companies such as Netflix and Apple have thousands of Cassandra nodes  Related data is stored contiguously on disk providing minimum seeks and easy distribution around the cluster  It s backed by DataStax  but still open source and community driven   Having made the choice  we needed to prove that it would actually work   Data Modeling  The best way to describe Cassandra to a newcomer is that it is a KKV store  The two Ks comprise the primary key  The first K is the partition key and is used to determine which node the data lives on and where it is found on disk  The partition contains multiple rows within it and a row within a partition is identified by the second K  which is the clustering key  The clustering key acts as both a primary key within the partition and how the rows are sorted  You can think of a partition as an ordered dictionary  These properties combined allow for very powerful data modeling   Remember that messages were indexed in MongoDB using channel_id and created_at   channel_id became the partition key since all queries operate on a channel  but created_at didn t make a great clustering key because two messages can have the same creation time  Luckily every ID on Discord is actually a Snowflake  chronologically sortable   so we were able to use them instead  The primary key became  channel_id  message_id    where the message_id is a Snowflake  This meant that when loading a channel we could tell Cassandra exactly where to range scan for messages   Here is a simplified schema for our messages table  this omits about 10 columns    While Cassandra has schemas not unlike a relational database  they are cheap to alter and do not impose any temporary performance impact  We get the best of a blob store and a relational store   When we started importing existing messages into Cassandra we immediately began to see warnings in the logs telling us that partitions were found over 100MB in size  What gives   Cassandra advertises that it can support 2GB partitions  Apparently  just because it can be done  it doesn t mean it should  Large partitions put a lot of GC pressure on Cassandra during compaction  cluster expansion  and more  Having a large partition also means the data in it cannot be distributed around the cluster  It became clear we had to somehow bound the size of partitions because a single Discord channel can exist for years and perpetually grow in size   We decided to bucket our messages by time  We looked at the largest channels on Discord and determined if we stored about 10 days of messages within a bucket that we could comfortably stay under 100MB  Buckets had to be derivable from the message_id or a timestamp   Cassandra partition keys can be compounded  so our new primary key became   channel_id  bucket   message_id     To query for recent messages in the channel we generate a bucket range from current time to channel_id  it is also a Snowflake and has to be older than the first message   We then sequentially query partitions until enough messages are collected  The downside of this method is that rarely active Discords will have to query multiple buckets to collect enough messages over time  In practice this has proved to be fine because for active Discords enough messages are usually found in the first partition and they are the majority   Importing messages into Cassandra went without a hitch and we were ready to try in production   Dark Launch  Introducing a new system into production is always scary so it s a good idea to try to test it without impacting users  We setup our code to double read write to MongoDB and Cassandra   Immediately after launching we started getting errors in our bug tracker telling us that author_id was null  How can it be null  It is a required field   Eventual Consistency  Cassandra is an AP database which means it trades strong consistency for availability which is something we wanted  It is an anti pattern to read before write  reads are more expensive  in Cassandra and therefore everything that Cassandra does is essentially an upsert even if you provide only certain columns  You can also write to any node and it will resolve conflicts automatically using  last write wins  semantics on a per column basis  So how did this bite us,"[831 1289 1031 563 660 562 668 1281 59 428 743]"
845,training-dataset/engineering/417.txt,engineering,Ditch Your ORMI ve been promoting a functional approach in Ruby for a while now and even though it includes many different techniques and patterns  there s this one idea  one fundamental idea that changes everything   immutability   But what does it even mean in Ruby  To freeze everything  That would be too slow  so no  Immutability oriented design means that you avoid interfaces that can change your objects  Yes  there are plenty of methods in Ruby to mutate something  but when you are designing your object interfaces  you can design them in a way that your objects won t change   Using immutable objects has been an eye opening experience for me  One of the things that it made me understand better is why object relational mapping is such a bad idea and a reason why we have so much unnecessary complexity   I ve been a user of various ORMs for about 10 years  which includes  2 years being on the Data Mapper project core team and  2 more years trying to build its next version  which was meant to solve all the hurdles that comes with the Active Record pattern  I ve seen it from the outside  I ve seen it from the inside  and I don t want to deal with any ORM headaches anymore  Enough is enough   I ditched my ORM   Complexity  When building software  we should be focusing on reducing complexity as much as possible   Object relational mapping is nothing but a layer of additional complexity that exists just because we want to mutate objects and persist them in a database   This stems from the fact that one of the core ideas behind OOD is to introduce abstractions that represent real world concepts  so that it s easier to reason about our code  it s easier to understand it  We ve been stuck with this concept for so long that many people have trouble looking outside of this box   You can still design your objects in such a way that it s easy to understand what s happening without having  a user that changes its email  or  an order that you add products to   Instead of creating objects that represent things like a user or an order  try to think about how to model business transactions  domain specific processeses   Hint  realize that everything can be modeled as a function that simply transforms data  The Impedance Mismatch  The complexity behind object relational mapping is directly caused by something we call the object relational impedance mismatch  It s an insanely complicated problem to solve as there are many different database models and many different ways to represent objects   You have two options here   Agree to have a 1 1 mapping between database model and your domain objects Use some sophisticated ORM system that would know how to map database representation of your objects into their in memory representation in a given programming language  And both options are terrible   First one is especially terrible  and many people in Ruby community know why  because 1 1 mapping tightly couples your application layer with the database  We ve been using Active Record ORMs long enough to know how much it increases complexity of our applications   What about the second option  typically known as the Data Mapper pattern  We don t have that in Ruby  there are still people who are trying to build it and some projects already exist that try to implement this pattern   The reality is that we are not even close to solving the ORM impedance mismatch problem   The worst part is that you can t truly solve it  you can only make it as good as possible  given the constraints a given programming language has  And then people will end up writing hand written SQL anyway   There Is No Database   Except there is  In fact  it s one of the most powerful pieces of your stack  Now think about this for a second   We are going to pretend there is no database so that we can mutate our objects and persist them  I know  it s not what you ve learned  typically we talk about wonderful things   like  proper abstraction so that our database becomes an implementation detail  or  it s going to be easier to switch databases  or  objects are decoupled from the database so it s easier to grow the system  and so on   But what it really  trully boils down to is  I want to mutate my objects and persist them    What if I told you that you can still decouple domain logic from persistence details while  at the same time  avoiding the whole complexity that comes with ORMs and mutable objects   Two magic words  functions and data types   There Is No User  This is no User   there is no Order    There is no ActiveProductShippingContainerHolyMolyManager    There s only a client sending a request to your server  The request contains data that the server uses to transform that data into a response   The closest we can get to model this interaction is a function that takes an input and returns an output  In fact  it s always a series of function calls that eventually returns the response   Once you realize that you will see that there is absolutely no need to come up with awkward abstractions that we ve grown to be completely comfortable with  for some reason   What about the data  Data means complexity so we hide that behind objects  right  Except that it never really worked like advertised   Mixing messy data with messy and unpredictable behavior is what ORM really means  But how  Why   It s messy because we don t define precisely what data types our application is dealing with  We re perfectly happy to pass raw data input straight to our ORM layer and hope for the best  It s unpredictable because objects are mutable  and with mutability come hard to predict side effects  Side effects cause bugs   Imagine you could define a user data type that would guarantee that invalid state is not possible  Imagine you could pass that data type from one place to another to get some response back without having to worry about side effects  You see where this is going  right   Using functions and data types is a much simpler and way more flexible approach to model client server interactions than any typical OO with mutable objects you ve ever seen   There is no User but it s very likely there is SignupUser   There is no Order but you definitely can deal with PlaceOrder   And when you see classes ending with Manager   just run   What Choice Do We Have   I believe one of the biggest misconceptions of our modern OO times is this    I need an ORM because I use an OO language   Do you  You actually don t  What you do need is a way to fetch data from your database and a data transformation layer so that it s easy to transform domain data types to persistence compatible ones   And that removes heaps of unnecessary abstractions that come with typical ORMs   You can still use objects to model interactions between a client and a server but there s no real need to have mutable objects   OO languages that can support that will live longer   My style of programming in Ruby has changed drastically over the last few years  Ditching ORM was one the best decisions I ve ever made  It s the reason why I work on Ruby Object Mapper and talk about this at conferences  It may be against what s common  what s idiomatic  but when what s common and idiomatic has been failing me for so long  I don t see any reason why I should continue going down this rabbit hole   I definitely don t want to see how deep it goes   The truth is  FP communities are ahead of their OO equivalents already  The best we can do is to learn what great paradigms from the FP world we can take and apply in our OO code  so that it serves us well  For me it s moving away from ORMs and mutable objects   Ditch your ORM  Embrace immutability oriented design  It works better,"[845 428 563 668 823 1226 413 397 184 254 1139]"
855,training-dataset/engineering/739.txt,engineering,Statistics for SoftwareSoftware development begins as a quest for capability  doing what could not be done before  Once that what is achieved  the engineer is left with the how  In enterprise software  the most frequently asked questions are   How fast   and more importantly   How reliable    Questions about software performance cannot be answered  or even appropriately articulated  without statistics   Yet most developers can t tell you much about statistics  Much like math  statistics simply don t come up for typical projects  Between coding the new and maintaining the old  who has the time   Engineers must make the time  I understand fifteen minutes can seem like a big commitment these days  so maybe bookmark it  Insistent TLDR seekers can head for our instrumentation section or straight to the summary   For the dedicated few  class is in session  It s time to broach the topic  learn what works  and take the guesswork out of software  A few core practices go a long way in generating meaningful systems analysis  And a few common mistakes set projects way back  This guide aims to lighten software maintenance and speed up future development through answers made possible by the right kinds of applied statistics   To begin  let s consider a target component we want to measure and improve  At PayPal this is often one of our hundreds of HTTP server applications  If you were to look at our internal frameworks  you would find hundreds of code paths instrumented to generate streams of measurements  function execution times  service latencies  request lengths  and response codes  to name a few   We discuss more about instrumentation below  but for now we assume these measurement points exist and focus on numerical measurements like execution times   The correct starting point minimizes bias  We assume the least  treating our measurements as values of random variables  So opens the door to the wide world of descriptive statistics  a whole area devoted to describing the behavior of randomness  While this may sound obvious  remember that much of statistics is inferential  dedicated to modeling future outcomes  They may go hand in hand  but knowing the difference and proper names drastically speeds up future research  Lesson  1 is that it pays to learn the right statistics terminology   Measurement is all about balance  Too little data and you might as well have not bothered  Or worse  you make an uninformed decision that takes your service down with it  Then again  too much data can take down a service  And even if you keep an eye on memory consumption  you need to consider the resources required to manage excess data   Big data  looms large  but it is not a necessary step toward understanding big systems  We want dense  balanced data   So what tools does the engineer have for balanced measurement  When it comes to metadata  there is never a shortage of volume  dimensions  or techniques  To avoid getting lost  we focus on descriptive summary statistics that can be collected with minimal overhead  With that direction  let s start out by covering some familiar territory   Statistical moments may not sound familiar  but virtually everyone uses them  including you  By age 10  most students can compute an average  otherwise known as the arithmetic mean  Our everyday mean is known to statisticians as the first moment  The mean is nice to calculate and can be quite enlightening  but there s a reason this post doesn t end here   The mean is just the most famous of four commonly used moments  These moments are used to create a progression that adds up to a standard data description   Mean   The central tendency of the data Variance   The looseness of the grouping around the mean Skewness   The symmetry or bias toward one end of the scale Kurtosis   The amount of data in  the wings  of the set  These four standardized moments represent the most widely applied metrics for describing the shape of a distribution  Each successive measure adds less practical information than the last  so skewness and kurtosis are often omitted  And while many are just hearing about these measures for the first time  omission may not be a bad thing   The mean  variance  skewness  and kurtosis are almost never the right tools for a performance minded engineer  Moment based measures are not trustworthy messengers of the critical engineering metadata we seek   Moment based measures are not robust  Non robust statistics simultaneously   Bend to skew by outliers  Dilute the meaning of those outliers  An outlier is any data point distant from the rest of the distribution   Outlier  may sound remote and improbable  but they are everywhere  making moment based statistics uninformative and even dangerous  Outliers often represent the most critical data for a troubleshooting engineer   So why do so many still use moments for software  The short answer  if you ll pardon the pun  momentum  The mean and variance have two advantages  easy implementation and broad usage  In reality  that familiarity leads to damaging assumptions that ignore specifics like outliers  For software performance  the mean and variance are useful only in the context of robust statistical measures   So  enough buildup  Lesson  2 is avoid relying solely on non robust statistics like the mean to describe your data  Now  what robust techniques can we actually rely on   If you ve ever looked at census data or gotten standardized test results  you re already familiar with quantiles  Quantiles are points that subdivide a dataset s range into equal parts  Most often  we speak of quartiles  4 parts  and percentiles  100 parts   Measures of central tendency tend to be the most popular  and the same goes for the 50th percentile  the median   While experienced engineers are happier to see the median than the mean  the data doesn t really get interesting until we get into the extremes  For software performance and reliability  that means the 95th  98th  99th  and 99 9th percentiles  We call these the extremes  but as in high traffic systems  they are far from uncommon  We also look for the range formed by the minimum and maximum values  sometimes called the 0th and 100th percentiles   The challenge with quantiles and ranges is efficient computation  For instance  the traditional way to calculate the median is to choose the middle value  or the average of the two middle values  from a sorted set of all the data  Considering all the data at once is the only way to calculate exact quantiles   Keeping all our data in memory would be prohibitively expensive for our use cases  Instead  using the principle of  good enough   an engineer has ways of estimating quantiles much more efficiently  Statistics is founded on utilitarian compromise   As a field  statistics formed to tackle the logistical issue of approximating an unreachable population  Even today  it s still not possible to poll every person  taste every apple  or drive every car  So statistics continues to provide   In the realm of software performance  data collection is automatable  to the point of making measurement too automatic  The problem becomes collation  indexing  and storage  Hard problems  replete with hard working people   Here we re trying to make things easier  We want to avoid those hard problems  The easiest way to avoid too much data is to throw data away  Discarded data may not need storage  but if you re not careful  it will live on  haunting data in the form of biases  We want pristine  meaningful data  indistinguishable from the population  just a whole lot smaller  So statistics provides us random sampling   The twist is that we want to sample from an unknown population  considering only one data point at a time  This use case calls for a special corner of computer science  online algorithms  a subclass of streaming algorithms   Online  implies only individual points are considered in a single pass   Streaming  implies the program can only consider a subset of the data at a time  but can work in batches or run multiple passes  Fortunately  Donald Knuth helped popularize an elegant approach that enables random sampling over a stream  Reservoir sampling  Let s jump right in   First we designate a counter   which will be incremented for every data point seen  The reservoir is generally a list or array of predefined size   Now we can begin adding data  Until we encounter size elements  elements are added directly to reservoir   Once reservoir is full  incoming data points have a size   counter chance to replace an existing sample point  We never look at the value of a data point and the random chance is guaranteed by definition  This way reservoir is always representative of the dataset as a whole  and is just as likely to have a data point from the beginning as it is from the end  All this  with bounded memory requirements  and very little computation  See the instrumentation section below for links to Python implementations   In simpler terms  the reservoir progressively renders a scaled down version of the data  like a fixed size thumbnail  Reservoir sampling s ability to handle populations of unknown size fits perfectly with tracking response latency and other metrics of a long lived server process   Once you have a reservoir  what are the natural next steps  At PayPal  our standard procedure looks like   Look at the min  max  and other quantiles of interest  generally median  95th  99th  99 9th percentiles   Visualize the CDF and histogram to get a sense for the shape of the data  usually by loading the data in a Jupyter notebook and using Pandas  matplotlib  and occasionally bokeh   And that s it really  Beyond this we are usually adding more dimensions  like comparisons over time or between datacenters  Having the range  quantiles  and sampled view of the data really eliminates so much guesswork that we end up saving time  Tighten a timeout  implement a retry  test  and deploy  Maybe add higher level acceptance criteria like an Apdex score  Regardless of the performance problem  we know when we ve fixed it and we have the right numbers and charts to back us up   Reservoir sampling does have its shortcomings  In particular  like an image thumbnail  accuracy is only as good as the resolution configured  In some cases  the data near the edges gets a bit blocky  Good implementations of reservoir sampling will already track the maximum and minimum values  but for engineers interested in the edges  we recommend keeping an increased set of the exact outliers  For example  for critical paths  we sometimes explicitly track the n highest response times observed in the last hour   Depending on your runtime environment  resources may come at a premium  Reservoir sampling requires very little processing power  provided you have an efficient PRNG  Even your Arduino has one of those  But memory costs can pile up  Generally speaking  accuracy scales with the square root of size  Twice as much accuracy will cost you four times as much memory  so there are diminishing returns   At PayPal  the typical reservoir is allocated 16 384 floating point slots  for a total of 64 kilobytes  At this rate  humans run out of memory before the servers do  Tracking 500 variables only takes 8 megabytes  As a developer  remembering what they all are is a different story   Usually  reservoirs get us what we want and we can get on with non statistical development  But sometimes  the situation calls for a more tailored approach   At various points at PayPal  we ve worked with q digests  biased quantile estimators  and plenty of other advanced algorithms for handling performance data  After a lot of experimentation  two approaches remain our go tos  both of which are much simpler than one might presume   The first approach  histogram counters  establishes ranges of interest  called bins or buckets  based on statistics gathered from a particular reservoir  While reservoir counting is data agnostic  looking only at a random value to decide where to put the data  bucketed counting looks at the value  finds the bucket whose range includes that value  and increments the bucket s associated counter  The value itself is not stored  The code is simple  and the memory consumption is even lower  but the key advantage is the execution speed  Bucketed counting is so low overhead that it allows statistics collection to permeate much deeper into our code than other algorithms would allow   The second approach  Piecewise Parabolic Quantile Estimation  P2 for short   is an engineering classic  A product of the 1980s electronics industry  P2 is a pragmatic online algorithm originally designed for simple devices  When we look at a reservoir s distribution and decide we need more resolution for certain quantiles  P2 lets us specify the quantiles ahead of time  and maintains those values on every single observation  The memory consumption is very low  but due to the math involved  P2 uses more CPU than reservoir sampling and bucketed counting  Furthermore  we ve never seen anyone attempt combination of P2 estimators  but we assume it s nontrivial  The good news is that for most distributions we see  our P2 estimators are an order of magnitude more accurate than reservoir sampling   These approaches both take something learned from the reservoir sample and apply it toward doing less  Histograms provide answers in terms of a preconfigured value ranges  P2 provides answers at preconfigured quantile points of interest  Lesson  3 is to choose your statistics to match the questions you need to answer  If you don t know what those questions are  stick to general tools like reservoir sampling   There are a lot of ways to combine statistics and engineering  so it s only fair that we offer some running starts   We focused a lot on statistical fundamentals  but how do we generate relevant datasets in the first place  Our answer is through structured instrumentation of our components  With the right hooks in place  the data will be there when we need it  whether we re staying late to debug an issue or when we have a spare cycle to improve performance   Much of PayPal s Python services  robustness can be credited to a reliable remote logging infrastructure  similar to  but more powerful than  rsyslog  Still  before we can send data upstream  the process must collect internal metrics  We leverage two open source projects  fast approaching major release   faststat   Optimized statistical accumulators  lithoxyl   Next generation logging and application instrumentation  Lithoxyl is a high level library designed for application introspection  It s intended to be the most Pythonic logging framework possible  This includes structured logging and various accumulators  including reservoir sampling  P2  and others  But more importantly  Lithoxyl creates a separate instrumentation aspect for applications  allowing output levels and formats to be managed separately from the instrumentation points themselves   Faststat operates on a lower level  True to its name  Faststat is a compiled Python extension that implements accumulators for the measures described here and many more  This includes everything from geometric harmonic means to Markov like transition tracking to a metametric that tracks the time between stat updates  At just over half a microsecond per point  Faststat s low overhead allows it to permeate into some of the deepest depths of our framework code  Faststat lacks output mechanisms of its own  so our internal framework includes a simple web API and UI for browsing statistics  as well as a greenlet that constantly uploads faststat data to a remote accumulation service for alerting and archiving   One of the many advantages to investing in instrumentation early is that you get a sense for the performance overhead of data collection  Reliability and features far outweigh performance in the enterprise space  Many critical services I ve worked on could be multiple times faster without instrumentation  but removing this aspect would render them unmaintainable  which brings me to my next point   Good work takes cycles  All the methods described here are performance minded  but you have to spend cycles to regain cycles  An airplane could carry more passengers without all those heavy dials and displays up front  It s not hard to imagine why logging and metrics is  for most services  second only to the features themselves  Always remember and communicate that having to choose between features and instrumentation does not bode well for the reliability record of the application or organization   For those who really need to move fast and would prefer to reuse or subscribe  there are several promising choices out there  including New Relic and Prometheus  Obviously we have our own systems  but those offerings do have percentiles and histograms   Without detracting from the main course above  this section remains as dessert  Have your fill  but keep in mind that the concepts above fulfill 95  of the needs of an enterprise SOA environment like PayPal   We focused here on descriptive  non parametric statistics for numerical applications  Statistics is much more than that  Having the vocabulary means the difference between the right answer and no answer  Here are some areas to expand into  and how they have applied to our work   Non parametric statistics gave us quantiles  but offers so much more  Generally  non parametric describes any statistical construct that does not make assumptions about probability distribution  e g  normal or binomial  This means it has the most broadly applicable tools in the descriptive statistics toolbox  This includes everything from the familiar histogram to the sleeker kernel density estimation  KDE   There s also a wide variety of nonparametric tests aimed at quantitatively discovering your data s distribution and expanding into the wide world of parametric methods   Parametric statistics contrast with non parametric statistics in that the data is presumed to follow a given probability distribution  If you ve established or assumed that your data can be modeled as one of the many published distributions  you ve given yourself a powerful set of abstractions with which to reason about your system  We could do a whole article on the probability distributions we expect from different parts of our Python backend services  hint  expect a lot of fish and phones   Teasing apart the curves inherent in your system is quite a feat  but we never drift too far from the real observations  As with any extensive modeling exercise  heed the cautionary song of the black swan   Inferential statistics contrast with descriptive statistics in that the goal is to develop models and predict future performance  Applying predictive modeling  like regression and distribution fitting  can help you assess whether you are collecting sufficient data  or if you re missing some metrics  If you can establish a reliable model for your service and hook it into monitoring and alerting  you ll have reached SRE nirvana  In the meantime  many teams make do with simply overlaying charts with the last week  This is often quite effective  diminishing the need for mathematical inference  but does require constant manual interpretation  doesn t compose well for longer term trend analysis  and really doesn t work when the previous week isn t representative  i e   had an outage or a spike    Categorical statistics contrast with numerical statistics in that the data is not mathematically measurable  Categorical data can be big  such as IPs and phone numbers  or small  like user languages  Our key non numerical metrics are around counts  or cardinality  of categorical data  Some components have used HyperLogLog and Count Min sketches for distributable streaming cardinality estimates  While reservoir sampling is much simpler  and can be used for categorical data as well  HLL and CMS offer increased space efficiency  and more importantly  proven error bounds  After grasping reservoir sampling  but before delving into advanced cardinaltiy structures  you may want to have a look at boltons ThresholdCounter  the heavy hitters counter used extensively in PayPal s Python services  Regardless  be sure to take a look at this ontology of basic statistical data types   Multivariate statistics allow you to analyze multiple output variables at a time  It s easy to go overboard with multiple dimensions  as there s always an extra dimension if you look for it  Nevertheless  a simple  practical exploration of correlations can give you a better sense of your system  as well as inform you as to redundant data collection   Multimodal statistics abound in real world data  multiple peaks or multiple distributions packed into a single dataset  Consider response times from an HTTP service   Successful requests  200s  have a  normal  latency   Client failures  400s  complete quickly  as little work can be done with invalid requests   Server errors  500s  can either be very quick  backend down  or very slow  timeouts    Here we can assume that we have several curves overlaid  with 3 obvious peaks  This exaggerated graph makes it clear that maintaining a single set of summary statistics can do the data great injustice  Two peaks really narrows down the field of effective statistical techniques  and three or more will present a real challenge  There are times when you will want to discover and track datasets separately for more meaningful analysis  Other times it makes more sense to bite the bullet and leave the data mixed   Time series statistics transforms measurements by contextualizing them into a single  near universal dimension  time intervals  At PayPal  time series are used all over  from per minute transaction and error rates sent to OpenTSDB  to the Python team s homegrown  PYPL Pandas stock price analysis  Not all data makes sense as a time series  It may be easy to implement certain algorithms over time series streams  but be careful about overapplication  Time bucketing contorts the data  leading to fewer ways to safely combine samples and more shadows of misleading correlations   Moving metrics  sometimes called rolling or windowed metrics  are another powerful class of calculation that can combine measurement and time  For instance  the exponentially weighted moving average  EWMA   famously used by UNIX for its load averages     uptime 10 53PM up 378 days  1 01  3 users  load average  1 37  0 22  0 14  This output packs a lot of information into a small space  and is very cheap to track  but it takes some knowledge and understanding to interpret correctly  EWMA is simultaneously familiar and nuanced  It s fun to consider whether you want time series style disrcete buckets or the continuous window of a moving statistic  For instance  do you want the counts for yesterday  or the past 24 hours  Do you want the previous hour or the last 60 minutes  Based on the questions people ask about our applications  PayPal Python services keep few moving metrics  and generally use a lot more time series   Survival analysis is used to analyze the lifetimes of system components  and must make an appearance in any engineering article about reliability  Invaluable for simulations and post mortem investigations  even a basic understanding of the bathtub curve can provide insight into lifespans of running processes  Failures are rooted in causes at the beginning  middle  and end of expected lifetime  which when overlaid  create a bathtub aggregate curve  When the software industry gets to a point where it leverages this analysis as much as the hardware industry  the technology world will undoubtedly have become a cleaner place   It s been a journey  but I hope you learned something new  If nothing else  you know how we do it  Remember the three lessons   Statistics is full of very specific terminology and techniques that you may not know that you need  It pays to take time to learn them  Averages and other moment based measures don t cut it for software performance and reliability  Never underestimate the outlier  Use quantiles  counting  and other robust metrics to generate meaningful data suitable for exploring the specifics of your software   If you would like to learn more about statistics  especially branching out from descriptive statistics  I recommend Allen Downey s video course Data Exploration in Python  as well as his free books Think Stats and Think Bayes  If you would like to learn more about enterprise software development  or have developers in need of training  take a look at my new video course  Enterprise Software with Python  It includes even more software fundamentals informed by PayPal Python development  and is out now on oreilly com and Safari   I hope you found this a useful enough guide that you ll save yourself some time  either by applying the techniques described or subscribing and sharing the link with other developers  Everyone needs a reintroduction to statistics after a few years of real work  Even if you studied statistics in school  real work  real data  and real questions have a way of making one wonder who passed those statistics exams  As interconnectivity increases  a rising tide of robust measurement floats all software engineering boats,"[855 397 413 1139 1289 1331 823 563 997 1414 668]"
865,training-dataset/product/438.txt,product,Yesterday s signup flow won t work todayEven if your customer onboarding has a solid foundation and a proven thesis  it s important to continually assess the contexts that informed the design in the first place  Your customers don t stay the same  so neither should your onboarding  Nowhere is this clearer than the signup flow  Getting a savvy  technical engineer to sign up is completely different than getting whole teams of marketers  engineers and customer support reps  all of differing abilities  signed up to your product  To illustrate this  I thought it would be useful to bring you through how our customers have evolved over time  how we adapted the design of the our signup flow to match these changes  and what we learned along the way  1  Moving beyond a technical customer base  By letting people import data via CSV  as well as JavaScript  we minimised the friction of non technical users signing up  When customers sign up  it s almost always better to let people keep moving and exploring  For several years  signup with Intercom centered on one action  installing a snippet of JavaScript on a website or app  At that time  we had an almost entirely technical user base of developers and startups signing up  and JavaScript was the clearest and simplest way to get their data into Intercom  Fast forward a few years  and our customers were changing  They were marketers or product managers  and they found it much harder to install the JavaScript required to create data  So we brought a simple CSV importer upfront in the sign up flow  allowing non technical users to get up and running with Intercom  In the three months following this change  our conversion rate from marketing page to account creation rose dramatically   from 30  to 45   By comparing users who signed up with JavaScript in the same timeframe  we discovered we were getting a totally new audience we d previously been neglecting  Now that we had excellent data to show our CSV importer was working  we added even more ways for users to get started with Intercom   with Mailchimp  Mixpanel  Stripe  Segment etc  with our conversion rate continuing to grow over the following months  Takeaway  Define clear metrics and review them often  Weekly meetings with data analysts allow you to review the performance of the signup flow continually and use the insights to inform your design decisions   Weekly meetings with data analysts allow you to review the performance of the signup flow continually and use the insights to inform your design decisions  But be skeptical about your data  Our data showed we had good results  but we could have missed some really important details   CSV importing could have cannibalised the JavaScript customers  Casting a critical eye over your data helps you understand if your improvements are truly a success   Our data showed we had good results  but we could have missed some really important details   CSV importing could have cannibalised the JavaScript customers  Casting a critical eye over your data helps you understand if your improvements are truly a success  Your customer support team is a great resource to understand your users  Without digging deep into customer conversations  there s no way to find the blind spots in your signup flow  By diving into customer conversations  you ll find find powerful and useful insights to work with  2  Accounting for a more diverse user base  Not everyone has the ability to complete every step  Unblock them by providing ways to loop in colleagues and teammates  Asking a VP of Marketing for a CSV file is unlikely to be a successful signup path Getting a company to sign up for your product is different from getting an individual user to sign up   it requires many people across departments to get set up and to start seeing the value your product can provide  As larger  more complex teams signed up  we needed to design new ways to accommodate their workflow  We redesigned our  Ask a colleague to install  flow  where non engineers could send installation instructions to a colleague  so that new signups could  create a secret link and invite any teammate via any communication channel  Slack  Hipchat  Google Hangouts  mailing lists   open it up to a group  a role  or even the whole company  and didn t have to to know exactly which individual to invite  send teammates directly to a specific setup guide step that needs to be completed instead of letting them figure it out themselves Takeaway  Understand people s behavior with user research  Don t just focus on quantitative data about new signups   understand the  why  too  In one research session  a user would copy and paste the Javascript to a text file  then post to a Trello board for a developer to access  Observing these existing workflows helped us understand the the low success rate of our old  Ask a colleague to install  method   Don t just focus on quantitative data about new signups   understand the  why  too  In one research session  a user would copy and paste the Javascript to a text file  then post to a Trello board for a developer to access  Observing these existing workflows helped us understand the the low success rate of our old  Ask a colleague to install  method  Onboard a whole company instead of just individual users  For big companies  asking a VP of Marketing for a CSV file is unlikely to be a successful signup path  he won t have the CSV file  and he may not even have the company credit card to finish the sign up  By enabling the collaboration of the whole team in signing up  you can unblock people from tasks they can t finish and allow people choose their own ways to collaborate with teammates  3  Showing people value as early as possible,"[865 2 254 657 397 1139 740 668 1331 413 1107]"
921,training-dataset/engineering/110.txt,engineering,Refactoring a JavaScript video storeThe simple example of calculating and formatting a bill for a video store opened my refactoring book in 1999  If done in modern JavaScript  there are several directions you could take the refactoring  I explore four here  refactoring to top level functions  to a nested function with a dispatcher  using classes  and transformation using an intermediate data structure   Many years ago  when I was writing the refactoring book  I opened the book with a  very  simple example of refactoring some code that calculated a customer s bill for renting some videos  in those days we had to go to a store to do that   I was contemplating this refactoring example recently  in particular how it would look if it were written in modern JavaScript   Any refactoring is about improving the code in a particular direction  one that suits a development team s coding style  In the book the example was in Java  and Java  particularly then  suggests a certain style of coding  an object oriented style  With JavaScript  however  there is a lot more options about what kind of style to go for  While you can do a Java like OO style  particularly with ES6  Ecmascript 2015   not all JavaScript pundits favor that style  many indeed consider using classes to be a Bad Thing   This initial video store code To explore further  I need to introduce some code  In this case a JavaScript version of the original example I wrote back at the turn of the century  function statement customer  movies    let totalAmount   0  let frequentRenterPoints   0  let result    Rental Record for   customer name      for  let r of customer rentals    let movie   movies r movieID   let thisAmount   0     determine amount for each movie switch  movie code    case  regular   thisAmount   2  if  r days   2    thisAmount     r days   2    1 5    break  case  new   thisAmount   r days   3  break  case  childrens   thisAmount   1 5  if  r days   3    thisAmount     r days   3    1 5    break      add frequent renter points frequentRenterPoints       add bonus for a two day new release rental if movie code      new     r days   2  frequentRenterPoints      print figures for this rental result      t  movie title  t  thisAmount       totalAmount    thisAmount       add footer lines result     Amount owed is   totalAmount      result     You earned   frequentRenterPoints  frequent renter points     return result    I m using ES6 here  The code operates on two data structures  both of which are just lists of json records  A customer record looks like this    name    martin    rentals       movieID    F001    days   3     movieID    F002    days   1       The movies structure looks like this    F001     title    Ran    code    regular     F002     title    Trois Couleurs  Bleu    code    regular       etc   In the original book  movies were just present as objects in the java object structure  For this essay I prefer passing in the json structure as a parameter  I will assume using some kind of global lookup  such as a Repository  is not appropriate for this application  The statement method prints out a simple text output of a rental statement Rental Record for martin Ran 3 5 Trois Couleurs  Bleu 2 Amount owed is 5 5 You earned 2 frequent renter points This output is crude  even by the standards of example code  Could I not even be bothered to format the numbers decently  Remember  however  that the book was written with Java 1 1  before String format was added to the language  That may partially forgive my laziness  The statement function is an example of the smell Long Method  Just its size is enough to make me suspicious  But just because code smells bad isn t enough of a reason on its own to refactor it  Poorly factored code is a problem because it s hard to understand  Code that s hard to understand is hard to modify  whether to add new features or to debug  So if you don t need to to read and understand some code  then its poor structure won t harm you and you can happily leave it alone for a while  So to trigger our interest in this code fragment  we need a reason for it to change  Our reason  as I used in the book  is to write an HTML version of the statement method  something that prints out something like this   h1 Rental Record for  em martin  em   h1   table   tr  td Ran  td  td 3 5  td   tr   tr  td Trois Couleurs  Bleu  td  td 2  td   tr    table   p Amount owed is  em 5 5  em   p   p You earned  em 2  em  frequent renter points  p  As I indicated earlier  in this essay I m exploring a number of ways in which I can refactor this code to make it easier to add additional output renderings  All of these have the same start  breaking down the single method into a set of functions to capture different parts of the logic  Once I ve done this break down  I ll explore four different ways these functions can be arranged to support alternative renderings   Decomposing into several functions Whenever I work with a overly long function like this  my first thought is to use to look for logical chunks of code and turn them into their own functions using Extract Method   1  The first such chunk that catches my eye is the switch statement  function statement customer  movies    let totalAmount   0  let frequentRenterPoints   0  let result    Rental Record for   customer name      for  let r of customer rentals    let movie   movies r movieID   let thisAmount   0     determine amount for each movie switch  movie code    case  regular   thisAmount   2  if  r days   2    thisAmount     r days   2    1 5    break  case  new   thisAmount   r days   3  break  case  childrens   thisAmount   1 5  if  r days   3    thisAmount     r days   3    1 5    break      add frequent renter points frequentRenterPoints       add bonus for a two day new release rental if movie code      new     r days   2  frequentRenterPoints      print figures for this rental result      t  movie title  t  thisAmount       totalAmount    thisAmount       add footer lines result     Amount owed is   totalAmount      result     You earned   frequentRenterPoints  frequent renter points     return result    My IDE  IntelliJ  offers to do this refactoring automatically for me  but it doesn t do it correctly   its JavaScript abilities aren t as solid or as mature as its Java refactoring  So I do this the manual way  which involves looking at the data used by the candidate extraction  There are three bits of data there  thisAmount is the value being calculated by the extracted code  I can initialize it within the function and return it at the end  is the value being calculated by the extracted code  I can initialize it within the function and return it at the end r is the rental being examined in the loop  I can pass that in as a parameter   is the rental being examined in the loop  I can pass that in as a parameter  movie is the movie for the rental  which is a temp made earlier on  Temporary variables like this usually get in the way when refactoring procedural code  so I prefer to first use Replace Temp with Query to turn them into a function that I can call within any extracted code  Once I ve done the Replace Temp with Query the code looks like this  function statement customer  movies    let totalAmount   0  let frequentRenterPoints   0  let result    Rental Record for   customer name      for  let r of customer rentals    let thisAmount   0     determine amount for each movie switch   movieFor  r  code    case  regular   thisAmount   2  if  r days   2    thisAmount     r days   2    1 5    break  case  new   thisAmount   r days   3  break  case  childrens   thisAmount   1 5  if  r days   3    thisAmount     r days   3    1 5    break      add frequent renter points frequentRenterPoints       add bonus for a two day new release rental if  movieFor  r  code      new     r days   2  frequentRenterPoints      print figures for this rental result      t   movieFor  r  title  t  thisAmount       totalAmount    thisAmount       add footer lines result     Amount owed is   totalAmount      result     You earned   frequentRenterPoints  frequent renter points     return result  function movieFor  rental   return movies rental movieID      Now I extract the switch statement  function statement customer  movies    let totalAmount   0  let frequentRenterPoints   0  let result    Rental Record for   customer name      for  let r of customer rentals    const thisAmount   amountFor  r     add frequent renter points frequentRenterPoints       add bonus for a two day new release rental if movieFor r  code      new     r days   2  frequentRenterPoints      print figures for this rental result      t  movieFor r  title  t  thisAmount       totalAmount    thisAmount       add footer lines result     Amount owed is   totalAmount      result     You earned   frequentRenterPoints  frequent renter points     return result  function movieFor rental   return movies rental movieID    function amountFor  r    let thisAmount   0     determine amount for each movie switch  movieFor r  code    case  regular   thisAmount   2  if  r days   2    thisAmount     r days   2    1 5    break  case  new   thisAmount   r days   3  break  case  childrens   thisAmount   1 5  if  r days   3    thisAmount     r days   3    1 5    break    return thisAmount      I now turn my attention to calculating the frequent renter points  I can do a similar extraction of its code function statement customer  movies    let totalAmount   0  let frequentRenterPoints   0  let result    Rental Record for   customer name      for  let r of customer rentals    const thisAmount   amountFor r   frequentRenterPointsFor  r     print figures for this rental result      t  movieFor r  title  t  thisAmount       totalAmount    thisAmount       add footer lines result     Amount owed is   totalAmount      result     You earned   frequentRenterPoints  frequent renter points     return result    function frequentRenterPointsFor  r      add frequent renter points frequentRenterPoints       add bonus for a two day new release rental if  movieFor r  code      new     r days   2  frequentRenterPoints      Although I ve extracted the function  I don t like the way it works by updating the parent scoped variable  Such side effects make code hard to reason about  so I alter it so that it has no side effects in its body  function statement customer  movies    let totalAmount   0  let frequentRenterPoints   0  let result    Rental Record for   customer name      for  let r of customer rentals    const thisAmount   amountFor r   frequentRenterPoints    frequentRenterPointsFor r     print figures for this rental result      t  movieFor r  title  t  thisAmount       totalAmount    thisAmount       add footer lines result     Amount owed is   totalAmount      result     You earned   frequentRenterPoints  frequent renter points     return result    function frequentRenterPointsFor r    let result   1  if  movieFor r  code      new     r days   2  result     return result     I take the chance to clean up the two extracted functions a bit  while I understand them  function amountFor rental    let result   0  switch  movieFor rental  code    case  regular   result   2  if  rental days   2    result     rental days   2    1 5    return result  case  new   result   rental days   3  return result  case  childrens   result   1 5  if  rental days   3    result     rental days   3    1 5    return result    return result    function frequentRenterPointsFor rental    return  movieFor rental  code      new     rental days   2    2   1    There is more I could do with these functions  especially amountFor   and that was something I did do in the book  But for this essay I won t examine the body of these functions any further  That done  I go back to the body of the function  function statement customer  movies    let totalAmount   0  let frequentRenterPoints   0  let result    Rental Record for   customer name      for  let r of customer rentals    const thisAmount   amountFor r   frequentRenterPoints    frequentRenterPointsFor r     print figures for this rental result      t  movieFor r  title  t  thisAmount       totalAmount    thisAmount       add footer lines result     Amount owed is   totalAmount      result     You earned   frequentRenterPoints  frequent renter points     return result  A general tactic I like to use is getting rid of mutable variables  There are three here  one is collecting up the final string  the other two calculate values that are being used in that string  I m ok with the first  but would like to eradicate the other two  To start doing that I need to split the loop  First I simplify the loop and inline the const  function statement customer  movies    let totalAmount   0  let frequentRenterPoints   0  let result    Rental Record for   customer name      for  let r of customer rentals    frequentRenterPoints    frequentRenterPointsFor r   result      t  movieFor r  title  t  amountFor r        totalAmount    amountFor r        add footer lines result     Amount owed is   totalAmount      result     You earned   frequentRenterPoints  frequent renter points     return result  I then split the loop into three parts  function statement customer  movies    let totalAmount   0  let frequentRenterPoints   0  let result    Rental Record for   customer name      for  let r of customer rentals    frequentRenterPoints    frequentRenterPointsFor r     for  let r of customer rentals    result      t  movieFor r  title  t  amountFor r         for  let r of customer rentals    totalAmount    amountFor r        add footer lines result     Amount owed is   totalAmount      result     You earned   frequentRenterPoints  frequent renter points     return result  Some programmers worry about the performance implications of refactorings like this  in which case take a look of an old but pertinent article on software performance That split allows me to then extract functions for the calculations  function statement customer  movies    let result    Rental Record for   customer name      for  let r of customer rentals    result      t  movieFor r  title  t  amountFor r         result     Amount owed is    totalAmount         result     You earned    totalFrequentRenterPoints     frequent renter points     return result  function totalAmount      let result   0  for  let r of customer rentals    result    amountFor r     return result    function totalFrequentRenterPoints      let result   0  for  let r of customer rentals    result    frequentRenterPointsFor r     return result    Being a fan of collection pipelines  I ll also adjust the loops to use them  function totalFrequentRenterPoints     return customer rentals  map  r     frequentRenterPointsFor r    reduce  a  b     a   b      function totalAmount     return customer rentals  reduce  total  r     total   amountFor r   0     I m not sure which of those two pipeline styles I prefer most   Examining the composed function So now lets look at where we are  Here is all the code  function statement  customer  movies    let result    Rental Record for   customer name      for  let r of customer rentals    result      t  movieFor r  title  t  amountFor r         result     Amount owed is   totalAmount        result     You earned   totalFrequentRenterPoints    frequent renter points     return result  function totalFrequentRenterPoints      return customer rentals  map  r     frequentRenterPointsFor r    reduce  a  b     a   b      function totalAmount      return customer rentals  reduce  total  r     total   amountFor r   0     function movieFor  rental    return movies rental movieID     function amountFor  rental    let result   0  switch  movieFor rental  code    case  regular   result   2  if  rental days   2    result     rental days   2    1 5    return result  case  new   result   rental days   3  return result  case  childrens   result   1 5  if  rental days   3    result     rental days   3    1 5    return result    return result    function frequentRenterPointsFor  rental    return  movieFor rental  code      new     rental days   2    2   1      I now have a nicely composed function  The core code of the function is 7 lines  and is all concerned with formatting the output string  All the calculation code is moved to its own set of nested functions  each of which is small and clearly named to show its purpose  But I m still not quite in a position to write the html emitting function  The decomposed functions are all nested inside the overall statement function  this makes it easier to extract the functions as they can refer to names inside the function scope  which includes each other  such as amountFor calling movieFor   and the supplied parameters customer and movie   But I can t write a simple htmlStatement function that references those functions  To be able to support some different outputs using the same calculations  I need to do some further refactorings  Now I reach a point where I have several options of which refactorings to do depending on how I like to factor my code  I ll run through each of these approaches next  explaining how each one works  and then compare them once I m done with all four   Using a parameter to determine the output One route I could take is to specify the output format as an argument to the statement function  I would begin this refactoring by using Add Parameter  extracting the existing text formatting code  and writing some code at the start to dispatch to the extracted function when the parameter indicates it  function statement customer  movies  format    text      switch  format    case  text   return textStatement      throw new Error  unknown statement format   format     function textStatement     let result    Rental Record for   customer name      for  let r of customer rentals    result      t  movieFor r  title  t  amountFor r         result     Amount owed is   totalAmount        result     You earned   totalFrequentRenterPoints    frequent renter points     return result    I can then write the html generating function and add a clause to the dispatcher  function statement customer  movies  format    text     switch  format    case  text   return textStatement    case  html   return htmlStatement      throw new Error  unknown statement format   format     function htmlStatement     let result     h1 Rental Record for  em   customer name   em   h1      result      table      for  let r of customer rentals    result       tr  td   movieFor r  title   td  td   amountFor r    td   tr        result       table      result      p Amount owed is  em   totalAmount     em   p      result      p You earned  em   totalFrequentRenterPoints     em  frequent renter points  p      return result    I might fancy using a data structure for the dispatcher logic  function statement customer  movies  format    text     const dispatchTable      text   textStatement   html   htmlStatement    if  undefined     dispatchTable format   throw new Error  unknown statement format   format     return dispatchTable format  call     Using top level functions The problem with writing a top level html statement function is that the calculation functions are nested inside the text statement function  So an obvious way to proceed is to move them to the top context  To do this  I begin by looking for function that doesn t refer to any others  in this case movieFor Whenever I move functions around  I like to do it by first copying the function to the new context  fitting it to that context  and then replacing the original function body with a call to the moved function  function topMovieFor rental  movies    return movies rental movieID     function statement customer  movies        snip  function movieFor rental    return topMovieFor rental  movies      function frequentRenterPointsFor rental    return  movieFor rental  code      new     rental days   2    2   1    I can compile and test at this point  which will tell me if the change in context has caused any trouble  Once that s done I can then inline the forwarding function  function movieFor  rental  movies    return movies rental movieID     function statement customer  movies        snip  function frequentRenterPointsFor rental    return  movieFor rental  movies   code      new     rental days   2    2   1    There s a similar change inside amountFor As well as the inline  I also renamed the top level function to match the old name  so the only difference is now the movies parameter  I then do that with all the nested functions function statement  customer  movies    let result    Rental Record for   customer name      for  let r of customer rentals    result      t  movieFor r  movies  title  t  amountFor r  movies         result     Amount owed is   totalAmount customer  movies       result     You earned   totalFrequentRenterPoints customer  movies   frequent renter points     return result    function totalFrequentRenterPoints  customer  movies    return customer rentals  map  r     frequentRenterPointsFor r  movies    reduce  a  b     a   b      function totalAmount  customer  movies    return customer rentals  reduce  total  r     total   amountFor r  movies   0     function movieFor  rental  movies    return movies rental movieID     function amountFor  rental  movies    let result   0  switch  movieFor rental  movies  code    case  regular   result   2  if  rental days   2    result     rental days   2    1 5    return result  case  new   result   rental days   3  return result  case  childrens   result   1 5  if  rental days   3    result     rental days   3    1 5    return result    return result    function frequentRenterPointsFor  rental  movies    return  movieFor rental  movies  code      new     rental days   2    2   1    Now I can easily write the html statement function function htmlStatement customer  movies    let result     h1 Rental Record for  em   customer name   em   h1      result      table      for  let r of customer rentals    result       tr  td   movieFor r  movies  title   td  td   amountFor r  movies    td   tr        result       table      result      p Amount owed is  em   totalAmount customer  movies    em   p      result      p You earned  em   totalFrequentRenterPoints customer  movies    em  frequent renter points  p      return result    Declaring some partially applied local functions When using a global function like this  parameter lists can get rather long  So sometimes it can be useful to declare a local function that calls the global function with some  or all  of the parameters filled in  That local function  which is a partial application of the global function  can then be used later on  There s various ways to do this in JavaScript  One is to assign the local functions to variables  function htmlStatement customer  movies    const amount         totalAmount customer  movies   const frequentRenterPoints         totalFrequentRenterPoints customer  movies   const movie    aRental     movieFor aRental  movies   const rentalAmount    aRental     amountFor aRental  movies   let result     h1 Rental Record for  em   customer name   em   h1      result      table      for  let r of customer rentals    result       tr  td    movie r   title   td  td    rentalAmount r     td   tr        result       table      result      p Amount owed is  em    amount      em   p      result      p You earned  em    frequentRenterPoints      em  frequent renter points  p      return result    Another is to declare them as nested functions  function htmlStatement customer  movies    let result     h1 Rental Record for  em   customer name   em   h1      result      table      for  let r of customer rentals    result       tr  td   movie r  title   td  td   rentalAmount r    td   tr        result       table      result      p Amount owed is  em   amount     em   p      result      p You earned  em   frequentRenterPoints     em  frequent renter points  p      return result  function amount    return totalAmount customer  movies    function frequentRenterPoints    return totalFrequentRenterPoints customer  movies    function rentalAmount aRental   return amountFor aRental  movies    function movie aRental   return movieFor aRental  movies      Yet another approach is to use bind   I ll leave you to look up that one   it s not something I d use here as I find these forms easier to follow   Using classes Object orientation is familiar to me  so it s a not surprise that I m going to consider classes and objects  ES6 introduced good syntax for classical OO  Let s look at how I d apply it to this example  My first step is to wrap the data in objects  starting with the customer  customer es6  export default class Customer   constructor data    this _data   data    get name    return this _data name   get rentals     return this _data rentals     statement es6  import Customer from    customer es6   function statement  customerArg   movies    const customer   new Customer customerArg   let result    Rental Record for   customer name      for  let r of customer rentals    result      t  movieFor r  title  t  amountFor r         result     Amount owed is   totalAmount        result     You earned   totalFrequentRenterPoints    frequent renter points     return result  So far the class is just a simple wrapper over the original JavaScript object  I ll next do a similar wrapper with the rental  rental es6  export default class Rental   constructor data    this _data   data    get days    return this _data days  get movieID    return this _data movieID    customer es6  import Rental from    rental es6  export default class Customer   constructor data    this _data   data    get name    return this _data name   get rentals     return this _data rentals  map r    new Rental r       Now that I have classes wrapped around my simple json objects  I have a target for a Move Method  As with moving functions to the top level  the first function to work with is one that doesn t call any others   movieFor   But this function needs the list of movies as context  which will need to be made available to the newly created rental objects  statement es6  function statement customerArg  movies    const customer   new Customer customerArg  movies    let result    Rental Record for   customer name      for  let r of customer rentals    result      t  movieFor r  title  t  amountFor r         result     Amount owed is   totalAmount        result     You earned   totalFrequentRenterPoints    frequent renter points     return result  class Customer    constructor data  movies     this _data   data  this _movies   movies   get rentals     return this _data rentals map r    new Rental r  this _movies      class Rental    constructor data  movies     this _data   data  this _movies   movies    Once I have the supporting data in place  I can move the function  statement es6  function movieFor rental    return rental movie     class Rental    get movie     return this _movies this movieID     As with the move I did earlier  the first step is to put the core behavior in the new context  fit it into that context  and adjust the original function to call the new one  Once this is working  it s relatively easy to inline the original function calls  statement es6  function statement customerArg  movies    const customer   new Customer customerArg  movies   let result    Rental Record for   customer name      for  let r of customer rentals    result      t   r movie  title  t  amountFor r         result     Amount owed is   totalAmount        result     You earned   totalFrequentRenterPoints    frequent renter points     return result  function amountFor rental    let result   0  switch   rental movie  code    case  regular   result   2  if  rental days   2    result     rental days   2    1 5    return result  case  new   result   rental days   3  return result  case  childrens   result   1 5  if  rental days   3    result     rental days   3    1 5    return result    return result    function frequentRenterPointsFor rental    return   rental movie  code      new     rental days   2    2   1    I can use the same basic sequence to move the two calculations into the rental too  statement es6  function statement customerArg  movies    const customer   new Customer customerArg  movies   let result    Rental Record for   customer name      for  let r of customer rentals    result      t  r movie title  t   r amount         result     Amount owed is   totalAmount        result     You earned   totalFrequentRenterPoints    frequent renter points     return result  function totalFrequentRenterPoints     return customer rentals  map  r     r frequentRenterPoints    reduce  a  b     a   b      function totalAmount     return customer rentals  reduce  total  r     total   r amount   0     class Rental    get frequentRenterPoints     return   this  movie code      new     this days   2    2   1    get amount     let result   0  switch   this  movie code    case  regular   result   2  if   this  days   2    result      this  days   2    1 5    return result  case  new   result   this  days   3  return result  case  childrens   result   1 5  if   this  days   3    result      this  days   3    1 5    return result    return result    I can then move the two totalling functions to the customer statement es6  function statement customerArg  movies    const customer   new Customer customerArg  movies   let result    Rental Record for   customer name      for  let r of customer rentals    result      t  r movie title  t  r amount        result     Amount owed is    customer amount       result     You earned    customer frequentRenterPoints   frequent renter points     return result    class Customer    get frequentRenterPoints     return this  rentals  map  r     r frequentRenterPoints   reduce  a  b     a   b      get amount     return this  rentals  reduce  total  r     total   r amount  0     With the calculation logic moved into the rental and customer objects  writing the html version of the statement is simple  statement es6  function htmlStatement customerArg  movies    const customer   new Customer customerArg  movies   let result     h1 Rental Record for  em   customer name   em   h1      result      table      for  let r of customer rentals    result       tr  td   r movie title   td  td   r amount   td   tr        result       table      result      p Amount owed is  em   customer amount   em   p      result      p You earned  em   customer frequentRenterPoints   em  frequent renter points  p      return result    Classes without the syntax The class syntax in ES2015 is controversial  with some people feeling it isn t needed  often with a side of snark about Java developers   You can take exactly the same series of refactoring steps to come up with a result like this  function statement  customerArg  movies    const customer   createCustomer customerArg  movies   let result    Rental Record for   customer name        for  let r of customer rentals      result      t  r movie   title  t  r amount          result     Amount owed is   customer amount        result     You earned   customer frequentRenterPoints    frequent renter points     return result    function createCustomer  data  movies    return   name        data name  rentals  rentals  amount  amount  frequentRenterPoints  frequentRenterPoints    function rentals      return data rentals map r    createRental r  movies      function frequentRenterPoints      return rentals    map  r     r frequentRenterPoints     reduce  a  b     a   b      function amount      return rentals    reduce  total  r     total   r amount    0       function createRental  data  movies    return   days        data days  movieID        data movieID  movie  movie  amount  amount  frequentRenterPoints  frequentRenterPoints    function movie      return movies data movieID     function amount      let result   0  switch  movie   code    case  regular   result   2  if  data days   2    result     data days   2    1 5    return result  case  new   result   data days   3  return result  case  childrens   result   1 5  if  data days   3    result     data days   3    1 5    return result    return result    function frequentRenterPoints      return  movie   code      new     data days   2    2   1    This approach uses the Function As Object pattern  Constructor functions   createCustomer and createRental   return a JavaScript object  hash  of function references  Each constructor function contains a closure that holds the object s data  Because the returned object of functions are in the same function context they can access this data  I see this as exactly the same pattern as using the class syntax  but implemented a different way  I prefer to use the explicit syntax since it is more explicit   thus making my thinking clearer   Data Transformation All of these approaches have involved the statement printing functions calling other functions to calculate the data they need  Another approach to do this is to pass this data to the statement printing functions in the data structure itself  In this approach the calculation functions are used to transform the customer data structure so that it has all the data the printing functions need  In refactoring terms this is an example of not yet written Split Phase refactoring that Kent Beck described to me last summer  With this refactoring I split the computation into two phases that communicate using an intermediate data structure  I begin this refactoring by introducing the intermediate data structure  function statement customer  movies    const data   createStatementData customer  movies   let result    Rental Record for    data  name      for  let r of data  rentals    result      t  movieFor r  title  t  amountFor r         result     Amount owed is   totalAmount        result     You earned   totalFrequentRenterPoints    frequent renter points     return result  function createStatementData customer  movies    let result   Object assign     customer   return result    For this case I m enriching the original customer data structure with added elements  hence starting with a call to Object assign   I could also make an entirely new data structure  the choice really depends on how different the transformed data structure is to the original  I then do the same thing for each line of rental function statement  function createStatementData customer  movies    let result   Object assign     customer   result rentals   customer rentals map r    createRentalData r    return result  function createRentalData rental    let result   Object assign     rental   return result      Notice that I nest createRentalData inside createStatementData since any caller of createStatementData won t need to know about how the inside is built up  I can then start to populate the transformed data  beginning with the title of the rented movie  function statement customer  movies    const data   createStatementData customer  movies   let result    Rental Record for   data name      for  let r of data rentals    result      t   r title   t  amountFor r         result     Amount owed is   totalAmount        result     You earned   totalFrequentRenterPoints    frequent renter points     return result      function createStatementData customer  movies         function createRentalData rental    let result   Object assign     rental   result title   movieFor rental  title  return result      I follow with the calculation for the amount  followed by the totals  function statement customer  movies    const data   createStatementData customer  movies   let result    Rental Record for   data name      for  let r of data rentals    result      t  r title  t   r amount         result     Amount owed is    data totalAmount       result     You earned    data totalFrequentRenterPoints   frequent renter points     return result  function createStatementData customer  movies    let result   Object assign     customer   result rentals   customer rentals map r    createRentalData r    result totalAmount   totalAmount    result totalFrequentRenterPoints   totalFrequentRenterPoints    return result  function createRentalData rental    let result   Object assign     rental   result title   movieFor rental  title  result amount   amountFor rental   return result      Now that I ve made all the calculation functions put the result of their calculations as data  I can move the functions to separate them from the statement rendering function  First I move all the calculation functions inside createStatementData function statement  customer  movies       body   function createStatementData  customer  movies       body   function createRentalData rental        function totalFrequentRenterPoints         function totalAmount         function movieFor rental        function amountFor rental        function frequentRenterPointsFor rental            Then I move createStatementData outside of statement   function statement  customer  movies        function createStatementData  customer  movies    function createRentalData rental        function totalFrequentRenterPoints         function totalAmount         function movieFor rental        function amountFor rental        function frequentRenterPointsFor rental          Once I ve separated the functions like this  I can write the HTML version of the statement to use the same data structure  function htmlStatement customer  movies    const data   createStatementData customer  movies   let result     h1 Rental Record for  em   data name   em   h1      result      table      for  let r of data rentals    result       tr  td   r title   td  td   r amount   td   tr        result       table      result      p Amount owed is  em   data totalAmount   em   p      result      p You earned  em   data totalFrequentRenterPoints   em  frequent renter points  p      return result    I can also move createStatementData to a separate module to further clarify the boundaries between calculating the data and rendering the statements  statement es6 import createStatementData from    createStatementData es6   function htmlStatement customer  movies        function statement customer  movies        createStatementData es6 export default function createStatementData  customer  movies    function createRentalData rental        function totalFrequentRenterPoints         function totalAmount         function movieFor rental        function amountFor rental        function frequentRenterPointsFor rental           Comparing the approaches So now it s time to step back and take a look at what I ve got  I have an initial body of code  written as a single inline function  I wished to refactor this code to enable an html rendering without duplicating the calculation code  My first step was to break this code up into several functions  living within the original function   From there  I explored four distinct paths  top level functions write all functions as top level functions function htmlStatement customer  movies  function textStatement customer  movies  function totalAmount customer  movies  function totalFrequentRenterPoints customer  movies  function amountFor rental  movies  function frequentRenterPointsFor rental  movies  function movieFor rental  movies  show code parameter dispatch use a parameter to the top level function to state what format of output to emit function statement customer  movies  format  function htmlStatement   function textStatement   function totalAmount   function totalFrequentRenterPoints   function amountFor rental  function frequentRenterPointsFor rental  function movieFor rental  show code classes move calculation logic to classes which are used by rendering functions function textStatement customer  movies  function htmlStatement customer  movies  class Customer get amount   get frequentRenterPoints   get rentals   class Rental get amount   get frequentRenterPoints   get movie   show code transform split calculation logic into separate nested function that produces an intermediate data structure for the rendering functions function statement customer  movies  function htmlStatement customer  movies  function createStatementData customer  movies  function createRentalData   function totalAmount   function totalFrequentRenterPoints   function amountFor rental  function frequentRenterPointsFor rental  function movieFor rental  show code I ll begin with the top level functions example as my baseline for comparison is it s the conceptually simplest alternative  2  It s simple because it divides the work into a set of pure functions  all of which are callable from any point in my code  This is simple to use and simple to test   I can test any individual function easily via either test cases or with a REPL  The downside with top level functions is that there s a lot of repetitive parameter passing  Each function needs to be given the movies data structure and the customer level functions also to be given a customer structure  I m not concerned about the repetitive typing here  but I am concerned about the repetitive reading  Each time I read the parameters I have to figure out what they are and check to see if the parameters are changing  For all these functions  the customer and the movies data are common context   but with top level functions that common context isn t made explicit  I infer it as I read the program and build the model of its execution in my mind  but I prefer things to be as explicit as possible  This factor becomes more important as that context grows  I have only two data items here  but it s not uncommon to find many more  Using top level functions alone I end up with large parameter lists on every call  each one adding to the load on my reading comprehension  This can lead to the trap of bundling all these parameters into a context parameter  which contains all context for many functions  and ends up obscuring what these functions do  I can reduce the pain of all this by defining local partially applied functions  but that s a lot of extra function declaring to throw in the mix   which has to be duplicated with each bit of client code  The advantage of the three other alternatives is that they each make the common context explicit  capturing it within the structure of the program  The parameter dispatch approach does this by capturing the context in the top level parameter list  which is then available as common context to all nested functions  This works particularly well with the original code  making the refactoring from single function to nested functions simpler than a language that lacks nested functions  But the parameter dispatch approach starts to wobble when I need different overall behaviors from my context  such as an html format response  I need to write some kind of dispatcher to decide which function I want to invoke  Specifying a format to a renderer isn t too bad  but such dispatch logic is a distinct smell  However I write it  it s still essentially duplicating the core ability of the language to call a named function  I m heading down a path that can quickly lead me to the nonsense of  function executeFunction  name  args    const dispatchTable           There is a context for this kind of approach  which is when the choice of output format does come to my caller as data  In that case there has to be a dispatch mechanism on that data item  However if my callers are calling the statement function like this  const someValue   statement customer  movieList   text     then there s no way I should be writing dispatch logic in my code  The calling method is the key here  Using literal values to indicate choice of function is a smell  Instead of this API  let the caller say what they want as part of the function name  textStatement or htmlStatement   Then I can use the language s function dispatch mechanism and avoid cobbling together something else myself  So with these two alternatives under my belt  where am I  I want some explicit common context for some logic  but need to call different operations using that logic  When I feel this kind of need  I immediately think of using object orientation   which is in essence a set independently invokable operations on a common context   3  This leads me to the classes version of the example  which allows me to capture the common context of the customer and movies within the customer and rental objects  I set the context once when I instantiate the objects  and then all further logic can use that common context  The object methods are like the partially applied local functions in the top level case  except that here the common context is supplied by the constructor  I thus only write the local functions  not the top level ones  The caller indicates the context with the constructor and then calls the local function directly  I can think of the local methods as partial applications of notional top level functions on the common context of the object instance  Using classes introduces a further notion   that of separating the rendering logic from the calculation logic  One of the faults of the original single function is that it mixes the two together  Splitting into functions separates them to some degree  but they all still exist in the same conceptual space  That s a touch unfair  I could put the calculation functions into one file and the rendering functions into another  linking them by appropriate import statements  But I find that a common context provides a natural hint for how to group logic into modules  I ve described the objects as a set of common partial applications  but there s another way to look at them  The objects are instantiated with an input data structure  but enrich this data with computed data exposed through their calculation functions  I ve reinforced this way of thinking by making these getters  so a client treats them exactly the same as raw data   applying the Uniform Access Principle  I can think of this as a transformation from the constructor argument to this virtual data structure of getters  The transform example is the same idea  but implemented by creating a new data structure that combines the initial data with all the calculated data  Just as the objects encapsulate the calculation logic inside the customer and rental classes  the transform approach encapsulates that logic inside createStatementData and createRentalData   This approach of transforming basic List And Hash data structures is a common feature of much functional thinking  It allows the create Data functions to share the context that they need and the rendering logic to use the multiple outputs in a simple manner  One minor difference between thinking of the class as a transform and transform approach itself is when the transform computation happens  The transform approach here transforms all at once  while the classes make individual transforms with each call  I can easily switch when the computation happens to match the other  In the classes case I can perform all computation at once by doing it in the constructor  For the transform case I can recompute on demand by returning functions in the intermediate data structure  Almost always the performance differences here will be insignificant  if any of these functions is expensive my best bet is usually to use a method function and cache the result after the first call  So there are four approaches   what is my preference  I don t care for writing dispatcher logic  so I wouldn t use the parameter dispatch approach  The top level functions are something I d consider  but my taste for them plummets rapidly as the shared context increases in size  Even with only two arguments  I d be inclined to reach for the other alternatives  Choosing between the classes and transform approach is harder  both provide a good way of making common context explicit and separating concerns well  I don t care for cage fights  so maybe I just make them play tiddlywinks and pick the winner   Further Refactorings In this exploration  I ve explored four ways of arranging the calculation and rendering functions  Software is a very malleable medium and there are more variations that could be done with this  but these are the four that I think are the most interesting to discuss  There are also further refactorings than just the arrangement of these functions  In the book example I decomposed the amount and frequentRenterPoint calculations to support extending the model with new movie types  There are changes I d make to the rendering code  such as pulling out a common pattern of header  lines  and footer  But I think the four paths are enough to think about for this article  My conclusion  if I have one  is that there are different ways to sensibly arrange observably identical computation  Different languages encourage certain styles   the original book refactoring was done in Java  which greatly encourages the classes style  JavaScript handily supports multiple styles  which is good because it provides the programmer with options  and bad because it supplies the programmer with options   One of the difficulties in programming in JavaScript is that there is little consensus on what is good style   It s useful to understand these different styles  but more important to realize what ties them together  Small functions  providing they are well named  can be combined and manipulated to support varied needs both at the same time and over time  Common contexts suggest grouping logic together  while much of the art of programming is deciding how to separate concerns into a clear set of such contexts,"[921 734 823 845 184 428 563 1139 397 413 865]"
934,training-dataset/engineering/1478.txt,engineering,Carrier Payments Big Data Pipeline using Apache StormCarrier payments is a frictionless payment method enabling users to place charges for digital goods directly on their monthly mobile phone bill  There is no account needed  just the phone number  Payment authorization happens by verification of a four digit PIN sent via SMS to a user s mobile phone  After the successful payment transaction  charges will appears on user s monthly mobile phone bill   Historically fraud has been handled on the mobile carrier side through various types of spending caps  daily  weekly  monthly  etc    While these spending caps were able to keep fraud at bay in the early years  as this form of payment has matured  so have the fraud attempts  Over the last year we saw an increase in fraudulent activity based around social engineering  Through social engineering fraudsters are able to trick victims into revealing their PINs   To tackle the increasing fraud activity we decided to build carrier payments risk platform using an hadoop eco system  First step in building the risk platform is to create big data pipeline that will collect data from various sources  clean or filter the data and structure it for data repositories  The accuracy of our decision making directly depends on variety  quantity and speed of data collection   Data comes from various sources  Payment application generates transaction data  Refund job generates data about refunds processed  Customer service department provides customer complaints regarding wrong charge or failed payments  Browser fingerprint can help establish another identity of transacting user  All these sources generate data in different formats and at varying frequencies  Putting together a data pipeline for this is challenging   We wanted to build a data pipeline that processes data reliably and has the ability to replay in case of failure  It should be easy to add or remove new data sources or sinks into pipeline  Data pipeline should be flexible to allow schema changes  We should be able to scale the data pipeline   System Architecture  First challenge was to provide a uniform and flexible entry point to data pipeline  We built a micro service with single REST end point that accept data in JSON format  This makes it easy for any system to send data to pipeline  The schema consists of property  ApplicationRef  and JSON object  data   Sending data as JSON object allows easy schema changes   Micro service writes data into kafka which also acts as buffer for data  There is separate kafka topic for any source that sends data to pipeline  Assigning separate kafka topic brings logical separation to data  makes the kakfa consumer logic simpler and allow us to provide more resources to topics that handle large volume of data   We set up storm cluster to process data after reading it from kafka  To do real time computation on storm  you create  topologies   Topology is a directed acyclic graph or DAG of computation  Topology contains logic for reading data from kafka  transforming it and writing to hive  Typically each topology consists of two nodes of computation   kafkaspout and hivebolt  Some of the topologies contain intermediate node of computation for transforming complex data  We created separate topologies to process data coming from various sources  At the moment we have 4 topologies   purchase topology  customer service topology  browser topology and refund topology   Storm allows deploying and configuring topologies independently  This makes it possible for us to add more data sources in future or change existing ones without much hassle  This also allows us to allocate resources to topologies depending on data volume and processing complexities   Storm cluster set up and parallelism  We set up high availability storm cluster  There are two nimbus nodes with one of them acting as leader node  We have two supervisor nodes each having 2 core Intel  Xeon  CPU ES 2670 0   2 60GHz   Each topology is configured with 2 number workers  4 number executors and 4 number tasks  The number of executors is the number of threads spawned by each worker  JVM process   Together these settings define storm s parallelism  Changing any of the above settings can easily change storm s parallelism   Storm message guarantee set up  Storm s basic abstraction provides an at least once processing guarantee  Messages are replayed only in case of failure  This message semantics are ideal for our use case  The only problem was  we didn t wish to keep trying failed message indeterminately  We added the following settings to our spoutconfig to stop trying failed message if there are at least five successful messages processed after a failed one  We also increased elapsed time between retrials of failed message   spoutConfig maxOffsetBehind   5  spoutConfig retryInitialDelayMs   1000  spoutConfig retryDelayMultiplier   60 0  spoutConfig retryDelayMaxMs   24   60   60   1000  Results  Our data pipeline is currently processing payment transaction  browser  refund and customer service data at  100 tuples min rate  The biggest convenience factor with this architecture was staggered release of topologies  It was very easy to add new topology in storm without disturbing already running topologies  Time taken to release new topology is less than a day  Architecture is proved to be very reliable and we haven t experience any issue or loss of data so far,"[934 397 823 1331 1031 334 743 413 1414 660 0]"
997,training-dataset/engineering/728.txt,engineering,BOSS  Automatically Identifying Performance Bottlenecks through Big DataIntroduction  As the centralized performance team of LinkedIn  our mission is to make LinkedIn pages load faster  We help each engineering team try to hit their page load time goals through various optimization efforts  One common question we need to answer when trying to decrease page load time is  where is the performance bottleneck  In other words  where should the engineers focus their efforts  Usually  to answer this question  a performance engineer will look into performance metrics and check some samples captured by Resource Timing API and Call Graph and locate the hotspots  This approach can be very useful  but had the drawback of  trial and error   Also  many sample waterfalls have to be clicked and analyzed manually to find bottlenecks  We wanted a systematic way that a tool could automatically provide bottleneck details quickly based on existing  large amounts of data   In this blog  we ll discuss BOSS  BOttlenecks for Site Speed   a system we built at LinkedIn that analyzes millions of waterfall samples and automatically identifies bottlenecks for improving performance   Bottleneck analysis is hard  There are a couple of problems with  manual  bottleneck analysis   Dealing with multiple performance data sources  multiple systems serve the user s request  and performance data is tracked separately  We have browser side measurements using Navigation Timing  Resource Timing API  measurements from native applications  iOS  Android   and server side tracking data like Call Graph  Each data source has its own unique schema  which makes it difficult to process all of them in one place  Handling a large volume of performance data  it is important to analyze 100  of our traffic to find the most important bottlenecks  Usually  to find the bottleneck of a page  a performance engineer can look into some samples  and identify some hotspots  but not all  This means that it is easy to miss real bottlenecks and to possibly focus on wrong projects  We wanted to analyze all the data to make sure every LinkedIn member is happy with our site speed  Therefore  we needed to make sure the system could process 100M  records per day  Quantifying paralleled calls  finding a bottleneck is not as easy as simply finding the longest request in a waterfall since if there are other calls in parallel  just fixing the longest request can t reduce the page load time  We needed a model to take both call duration and parallelization into consideration  Interpreting the performance metrics  there is a lot of domain specific terminology in performance data  e g   DNS connection time  redirect time  client render time  etc  It is not easy for developers to understand at first glance why is a page slow  Instead of showing the raw metrics  we wanted to provide actionable items in the result  Examples could include fixing the high response time of your frontend server  removing HTTP redirects in your pages  parallelizing third party request with other calls  etc   In the following sections  we will explain how we addressed these challenges through BOSS   Call tree model to unify performance data sources  The toughest part of automating the analysis is putting various data sources together  We have performance tracking data on both the client side and server side  Those datasets are located separately and have different schemas  To resolve this  we built a generic call tree model to glue data together   One click from the end user will result in multiple requests to multiple systems  As illustrated below  one typical page view contains API requests to our data center  image JS CSS requests to CDNs  and some requests to third parties like Ads  Those requests spread out to multiple systems  and we need a way to trace them in one place,"[997 563 397 855 823 1331 1414 413 2 1289 1155]"
1031,training-dataset/engineering/1461.txt,engineering,Introducing Chaperone  How Uber Engineering Audits Kafka End to Endby Xiaobing Li   Ankur Bansal  As Uber continues to scale  our systems generate continually more events  interservice messages  and logs  Those data needs go through Kafka to get processed  How does our platform audit all these messages in real time   To monitor our Kafka pipeline health and each message passing through  we rely on our auditing system called Chaperone  Since January 2016 Chaperone has been a key piece of Uber Engineering s multi data center infrastructure  currently handling about a trillion messages a day  Here s how it works and why we built it   Uber s Kafka Pipeline Overview  At Uber  services run in multiple data centers in active active mode  Apache Kafka  and specifically uReplicator  is our message bus connecting different parts of the Uber Engineering ecosystem   Operating Kafka at Uber s scale almost instantaneously for many downstream consumers is difficult  We use batching aggressively and rely on asynchronous processing wherever possible for high throughput  Services use in house client libraries to publish messages to Kafka proxies  which batch and forward them to regional Kafka clusters  Some Kafka topics are directly consumed from regional clusters  while many others are combined with data from other data centers into an aggregate Kafka cluster using uReplicator for scalable stream or batch processing   Uber s Kafka pipeline has four tiers spanning a few data centers  The Kafka proxy and its clients are the first two tiers  They act as the gateway to the next tier  the regional Kafka cluster within each data center  Some data may be copied from regional clusters into the aggregate cluster  which is the last tier of the pipeline   Data in the Kafka pipeline follows a path of batching and acking  sending acknowledgments    Uber data flow from proxy client to Kafka brokers goes through several stages   The application sends a message to the proxy client by calling the produce function  The proxy client puts the message into the client buffer and returns to the application  The proxy client batches messages in the buffer and flushes them to the proxy server  The proxy server puts messages into the producer buffer and acks to the proxy client  The batch is then partitioned and placed in corresponding buffers per topic name  The proxy server batches messages in the buffer and flushes to a regional broker  The regional broker appends the messages to local log and acks to the proxy server  with acks 1    uReplicator fetches messages from the regional broker and flushes them to the aggregate broker  The aggregate broker appends messages to local log and acks to the uReplicator  with acks 1     Our Kafka setup optimizes for high throughput  which introduces tradeoffs  Thousands of microservices handling hundreds of thousands of concurrent trips  and growing  using Kafka extensively introduces the potential for problems  The purpose of Chaperone is to ingest each and every message from all the topics and record the counts in a given time period  at each stage in the data pipeline  to detect early and quantify precisely the data loss  lag or duplication along the path that data takes at Uber   An Overview of Chaperone  Chaperone consists of four components  the AuditLibrary  ChaperoneService  ChaperoneCollector  and the WebService   The AuditLibrary implements the audit algorithm and periodically aggregates and outputs time window statistics  This library is thus used for auditing by the other three components  The output module is pluggable  Kafka  HTTP  etc    At proxy client  the auditing metrics are sent to the Kafka proxy  At the other tiers  the metrics are emitted to a dedicated Kafka topic directly   Key to the AuditLibrary is the audit algorithm  Chaperone uses 10 minute tumbling  time  windows to aggregate the messages of each topic continuously  It s the event time inside the message that is used to decide which window to put the message to  For a window of messages  Chaperone calculates statistics like the total count and p99 latency  And periodically  Chaperone wraps the statistics of each window into an auditing message and sends it to the plugged backend  which can be Kafka proxy or Kafka broker as stated   The tier field in auditing message is important for finding where the auditing happened and whether messages have arrived at this location  By comparing the message counts from different tiers for a specific period  we can determine whether messages generated during the query period have been successfully delivered   ChaperoneService is the biggest workhorse component  and is faithfully hungry  It consumes each and every message from Kafka and records a timestamp for audit  ChaperoneService is built using HelixKafkaConsumer from uReplicator  which has already proven itself for better reliability and easier operation than the Kafka high level consumer  as from Kafka 0 8 2   ChaperoneService produces the auditing messages to a dedicated Kafka topic periodically to record the state   ChaperoneCollector listens on the dedicated Kafka topic to fetch all the auditing messages and stores them to the database  Hear  hear  Meanwhile  it also populates multiple dashboards   In the top figure above  we see the total message counts of a topic for each tier by aggregating the counts across all data centers  When there is no data loss  all lines can be coincided perfectly  Gaps show up whenever messages are dropped between tiers  For example  as in the bottom figure  some messages were dropped by Kafka proxy  Yet no loss happened after that tier  With this dashboard  it s easy to determine the loss window so that the relevant action is taken   With a message count at each tier also comes a latency  so we know how fresh messages are and whether a tier is delaying them  Instead of navigating Kafka broker or uReplicator dashboards  users obtain end to end visibility for their topics  health in one single dashboard  as shown below   Finally  WebService is a REST web front end to easily query metrics collected by Chaperone  It can enable us to do things like quantify loss accurately  Once we know the time window for loss  we query Chaperone for exact counts   Our Two Design Requirements for Chaperone  In designing Chaperone  we focused on two must do tasks to achieve accurate auditing results   1  Count each message exactly once  To ensure that each message is audited exactly once  ChaperoneService uses a write ahead log  WAL   Every time ChaperoneService is ready to emit captured Kafka stats  it composes an auditing message and tags it with a UUID  This message  along with the associated offsets  are persisted in the WAL before sending to Kafka  Once acknowledged by Kafka  the entry in the WAL is marked as done  This way  if ChaperoneService crashes  it can use the WAL to resend any unmarked auditing message and find out last audited offset to start consumption  WAL ensures exactly once auditing of each Kafka message and at least once delivery of each auditing message   Next  ChaperoneCollector uses the UUID tagged by ChaperoneService to remove any duplicates  With UUID and WAL together  we ensure exactly once auditing  It s hard to implement similar guarantees in proxy client and server because of low overhead requirements  We rely on their graceful shutdown to flush out state   2  Use a consistent timestamp to audit a message across tiers  Since Chaperone looks at the same Kafka message in multiple tiers  it s important for messages to have embedded timestamps  Without them  we would see a time shift in counting  At Uber  most of the data produced to Kafka is either encoded with avro like schema or sent as JSON  For messages encoded with schema  the event time can be extracted in constant time  But JSON messages have to be decoded to extract event time  To speed this up  we implemented a stream based JSON parser that can scan for timestamps without paying the upfront cost of decoding the whole message  This efficient parser is used in ChaperoneService but is still too expensive to use in proxy client and server  Therefore  we use processing timestamp in those two tiers  But the discrepancy of message counts across tiers due to inconsistent timestamps may trigger false positive alert on data loss  We re working on addressing timestamp inconsistency and plan to publish a follow up article on our solution   Chaperone s Two Main Uses at Uber  1  Detect data loss  Before Chaperone was built  the first indicator for data loss was consumers of data complaining about loss  By that time  it was already too late and we also didn t know which part of pipeline incurred loss  With Chaperone  we built a loss detection job that periodically polls metrics from Chaperone and alerts as soon as it sees discrepancy in counts between tiers  The alert provides end to end coverage for the Kafka pipeline  uncovering issues that system metrics of each pipeline component can hardly expose  The job automatically discovers new topics  and you can configure different alert rules based on data importance and loss thresholds  Loss notification is sent out via various channels such a paging system  enterprise chat  or email to make you aware fast   2  Read data beyond offsets available in Kafka  In most of our clusters in production  we still use Kafka 8  which doesn t have timestamp to offset index support natively  Thus  we built our own with Chaperone  The index powers our time range query for Kafka messages so you are not limited to reading by offset  you can read data using the timestamps provided by Chaperone   Even though Kafka has limited retention  we back up older data and keep the offsets of messages intact  The backed up topics paired with the index created by Chaperone lets users read data well beyond what currently exists in Kafka using time range query on the same interface  With this feature  Kafka users can inspect the messages within any period of the topic lifetime to debug issues of their service and backfill the messages if necessary  When there is discrepancy between the auditing results from downstream systems and those from Chaperone  the specific set of messages can be dumped out for fine grained comparison to locate the root cause   Summary  We built Chaperone to answers the following types of questions   Is there any data loss happening  If so  how much and where in pipeline is it dropped   What is the end to end latency  If there is lag  where is the it originating   Is there any data duplication   Chaperone not only gives us a good picture of system health  it also alerts us in events of data loss  For instance  when our uReplicator had a dead loop bug when brokers responded with unexpected errors  Neither uReplicator nor Kafka broker had alerts triggered  but the data loss detection job kicked in to quickly expose the bug   If you re interested in learning more  try it out for yourself   we ve open sourced Chaperone and the source code is available on Github   Xiaobing Li wrote this article with Ankur Bansal  They are software engineers at Uber on the streaming platform within our Core Infrastructure org,"[1031 743 1281 831 934 638 0 660 563 1414 397]"
1107,training-dataset/product/723.txt,product,Five Reasons Not to Trust Your Analytics Data146 Flares 146 Flares    Don t get me wrong  I m an advocate of data driven decisions and I prefer my opinions and ideas to be grounded in a robust number backed case  I can have jubilatory moments when analysing figures and seeing the positive impact of what I do  as a product manager  This is also why I continue working out the numbers myself even if there are teams of analysts who could do it for me  And this is also why I have learned to not blindly trust the numbers   Numbers are wrong more often than you think  What can possibly go wrong  Well  a lot  Take a look at this article on Google Analytics common mistakes or this discussion on trusting GA data  Your tracking set up may be incorrect  the knowledge and documentation may be missing important information   for example  events may not be named in an explanatory fashion or the tracking start date not taken into account   the analyser may make errors in querying the data and then calculating results   You re missing the full picture  Numbers can be misinterpreted if the full context is not understood  For example I ve sometimes scratched my head wondering why my conversion rate was not going up after making improvements to a purchase funnel  and then realised that the marketing team had started an acquisition campaign that meant a higher volume of our visitors were less  qualified  than before  and consequently were less prone to convert on average   If the reverse had happened and my conversion rate had benefited from a drop in marketing acquisition volumes  I may not have questioned it and prided myself for the full extent of the conversion rate hike   The rule of one key success metric for all devices does not work  Your user behaviours may vary depending on the device used  and so should the metrics  Having worked with publishers  who do not have the  easy  task of measuring against a purchase funnel  it can be challenging to find the right KPIs to worry about  And content consumption  especially  will most likely show lower KPIs on mobile  making it easy to deduce there is a problem   However  sometimes the metric is just not adapted  I have once seen a same scroll depth target of 75  applied to desktop users and to mobile users  When inspecting the layout of the pages on each device  I realised that on mobile all the left hand and right hand elements of the desktop page were stacked underneath the core content  meaning that a user was only 50  down the page once they had read the full article    Good  numbers may hide big UX issues  It is common practice to check user experience when you realise a figure does not match expectations  for example if you identify a huge drop of conversion from page 2 to 3 in your purchase flow   But what if the conversion goes up between those 2 pages because the user has missed a crucial piece of information and all they saw was to press  next   Sometimes  numbers will make you think all is fine  when it is not   Stats cannot replace the value of regularly watching customers use your service either through face to face or recorded sessions   When KPIs do not say anything useful  One of the big culprits   thankfully slowly becoming irrelevant   is page views  If set as a target  someone will surely find ways to grow this KPI without any improvement in customer behaviour  And this is probably how these endless galleries of images were born  where each picture counts as a page view  or articles broken down in multiple pages  with no benefit to the users   For all of these reasons  I would not take numbers for granted  and would bear the following in mind,"[1107 740 1155 2 865 1331 1139 385 997 254 374]"
1139,training-dataset/product/185.txt,product,If you don t pay attention  data can drive you off a cliffYou re a hotshot manager  You love your dashboards and you keep your finger on the beating pulse of the business  You take pride in using data to drive your decisions rather than shooting from the hip like one of those old school 1950s bosses  This is the 21st century  and data is king  You even hired a sexy statistician or data scientist  though you don t really understand what they do  Never mind  you can proudly tell all your friends that you are leading a modern data driven team  Nothing can go wrong  right  Incorrect  If you don t pay attention  data can drive you off a cliff  This article discusses seven of the ways this can happen  Read on to ensure it doesn t happen to you   1  Pretending uncertainty doesn t exist  Last month  your favourite metric was 5 2   This month  it s 5 5   Looks like things are getting better   you must be doing something right  But is 5 5  really different from 5 2   All things being equal  you should expect some variability in most of your metrics  The values you see are drawn from a distribution of possible values  which means you can t be certain what value you ll be seeing next  Fortunately  with more data you would be able to quantify this uncertainty and know which values are more likely  Don t fear or ignore uncertainty  Embrace and study it  and you ll be on the right track   2  Confusing observed and unobserved quantities  Everyone agrees that the future is uncertain  We can generate forecasts with varying degrees of confidence  but we never know for sure what s going to happen  However  some people tend to ignore uncertainty in forecasts  treating the unobserved future values as comparable to observed present values  For example  marketers often compare customer lifetime value with the cost of acquiring a customer  The problem is that customer lifetime value relies on a prediction of the net profit from a customer  so it s largely unobserved and uncertain   while the business has much more control and certainty around the cost of acquiring a customer  though it s not completely known   Treating the two values as if they re observed and known is risky  as it can lead to major financial losses   3  Thinking that your data is correct  Ask anyone who works with data  and they ll tell you that it s always messy  A well known saying among data scientists is that 80  of the work is data cleaning and the other 20  is complaining about data cleaning  Hence  it s likely that at least some of the figures you re relying on to make decisions are somewhat inaccurate  However  it s important to remember that this doesn t make the data completely useless  But if something looks too good to be true  it probably isn t true  Finally  it s highly unlikely that the data is always correct when you like the results and always incorrect when the results aren t favourable  so don t use the  guy on the internet said our data isn t 100  correct  excuse to push back on inconvenient truths   4  Believing that your data is complete  No matter how big you are  your data doesn t capture everything your customers do  Even Google and the NSA don t have a full view of what people are up to in the non digital world  and they can t completely read our minds  yet   Most businesses have much less data than the big tech companies  and they look a bit silly trying to explain customer behaviour using only the data they have  At the end of the day  you have to work with the data you can access  but never underestimate the effectiveness of obtaining more  relevant  data   5  Measuring the wrong thing  Maybe you recently read an article emphasising the importance of real metrics  like daily active users  as opposed to vanity metrics like number of signups to your service  You therefore decide to track the daily active users of your product  But have you thought about whether this metric is relevant to what you re trying to achieve  If you run a business like Airbnb  where transactions are inherently infrequent  do you really care if people don t regularly log in  You probably don t  as long as they use the product when they actually need it  Measuring and trying to optimise the wrong thing can be very risky  Indeed  deciding on metrics and their measurement can be seen as the hardest parts of data science   6  Not recognising your unconscious incompetence  To quote Bertrand Russell   One of the painful things about our time is that those who feel certainty are stupid  and those with any imagination and understanding are filled with doubt and indecision   Not recognising the extent of your ignorance when it comes to data is pretty common among those with no training in the field  which may lead to illusory superiority  This may be exacerbated by the fact that those who do know what they re doing tend to talk a lot about uncertainty and how there are many things that are simply unknowable  My hope is that this short article would help people graduate from unconscious incompetence  where you don t even recognise the importance of what you don t know  to conscious incompetence  where you recognise the need to learn and rely on expert advice   7  Ignoring expert advice  Once you ve recognised your skill gaps  you may decide to hire a data scientist to help you get more value out of your data  However  despite the hype  data scientists are not magicians  In fact  because of the hype  the definition of data science is so diluted that some people say that the term itself has become useless  The truth is that dealing with data is hard  every organisation is somewhat different  and it takes time and commitment to get value out of data  The worst thing you can do is to hire an expensive expert to help you  and then ignore their advice when their findings are hard to digest  If you re not ready to work with a data scientist  you might as well save yourself some money and remain in a state of blissful ignorance   Note  This article is not a portrayal of how things are with my current employer  Car Next Door  Views expressed are my own  In fact  if you want to work at a place where expert advice is acted on and uncertainty is seen as something to be studied rather than ignored  we re hiring   Advertisements,"[1139 397 2 1331 823 413 334 855 668 254 374]"
1155,training-dataset/engineering/654.txt,engineering,Building Conversion Tracking at LinkedInOverview of system components  Conversion tracking can be broken down into three main components   Insight Tag Endpoint  When a potential customer hits the advertiser s site  the page makes a call out to the Insight Tag Endpoint  which hits a cached datastore derived from MySQL  Its purpose is to determine whether the page the user lands on is eligible for a conversion which campaigns are associated to the Insight Tag  whether the campaigns are active  whether the URL fits the conversion action match rule  etc  Eligible conversion  fires   events that trigger our attention  only occur if a user is found to be a LinkedIn member  and this will emit a Kafka event which is loaded into our offline HDFS  Hadoop file system  storage medium  Data and reporting  Reading in the HDFS data  an offline process takes in each individual conversion fire to determine the potential impressions  ad views  and clicks for each conversion event  Conversion fires without associated impressions and clicks are dropped  and the remaining ones are aggregated into meaningful metrics broken down by conversion type  campaign type  etc  The results are stored in Pinot  an in house online analytical processing  OLAP  data store  which provides an interface for very high performant data queries and reporting  The user interface  API partners and UI   This system includes the conversion action definitions  reading the Insight Tag  the reporting dashboard  and general management of conversion tracking   We chose to use a dual AWS LinkedIn stack in our architecture  The AWS stack is part of the previous LinkedIn Lead Accelerator product offering  and already contains various integrations to support Insight Tag management  The LinkedIn stack allows us to use the LinkedIn domain cookie to extract tracking information  as well as included widely adopted systems like Pinot and the Campaign Manager UI  to build conversion tracking on LinkedIn  More importantly  because of our members first policy  it allows us to respect our members  opt out preferences   Data modeling  There are three primary data entities in Conversion Tracking,"[1155 1107 997 88 397 1281 374 2 638 1414 563]"
1226,training-dataset/engineering/165.txt,engineering,Invalid Object Is An Anti PatternThe idea of an object that validates its own state has been made very popular by Rails  ActiveRecord  We can see this pattern in many places  not only in ORM libraries but in many other gems whenever some sort of validation is needed   Have you ever thought about why we re allowing invalid state just to validate data  It doesn t seem to be a good idea  in fact  it feels like a huge anti pattern   Let s think about this for a second  Why do we validate data  Typically  to make sure that invalid state doesn t leak into our systems  If it s so essential to make sure that invalid state is not allowed then why do we allow instantiating objects with invalid state  Especially when we re dealing with such core objects like an Active Record model which deals with so called business logic  This sounds like a really bad idea   When it s possible to instantiate an invalid object  chances are it s going to happen when you don t really expect it   Consider this   class User   ActiveRecord    Base validates  email    name   presence  true end user   User   select    id    first  We state that a user must have an email and a name but then it s possible to create its instance by projecting only  id attribute  When something is possible  it means it s going to happen  Especially in big code bases  ActiveRecord is at least kind enough to raise a meaningful error when you try to access an attribute that was not loaded from the database   This kind of errors are probably not very common  since in most of the cases you rely on default behavior which is to load all attributes  but the fact that it s possible to load an object into memory that in some contexts could crash your app feels like a bad strategy   Using same objects as your  wall of defense  against untrusted input and for implementing core application logic is a mistake  Since these objects accept invalid state  their lack of type safety makes them a shaky foundation for building complex systems  You can t treat them as values as they are mutable  You can t really rely on their state  because it can be invalid  You can t treat them as canonical sources of information as their state depends on the query logic which can be dynamic as the example above shows   You may think that validations help here  You are almost right  Validations reduce the risk that your system will crash due to invalid state but they are no guarantee  Not to mention that in complex domains validation logic is just damn difficult to implement and despite your great efforts your database is being filled with invalid data  At some point you will see it once some new feature was added that happens to rely on that data but now you need to fix the data that you already have persisted  and often it is a troublesome process   Type safety is important  Properly validating data at the boundaries of our system is one thing  making sure that core  foundational objects are always valid is another thing  Which has inspired me to create dry data and dry validation   Type Safety Using dry data and dry validation  With dry data and dry validation it s possible to implement precise validation of an untrusted input and define  domain objects  with constrained types  which is probably a unique and  unpopular  approach  Both libraries are using each other  which is a cool synergy   dry validation uses coercion system from dry data and dry data uses predicates from dry validation for constrained types   UPDATE  yes  circular deps are not a good idea  dry validation depends on dry data but not the other way around  The common part used by dry data  the rule predicate system  will be extracted soon into a shared gem   Here s an example of our User model using dry data struct with constrained types   module Types Email   Strict    String   constrained   format    A  w        a z d        a z       a z   z i   Name   Strict    String   constrained   size  3     64   end class User   Dry    Data    Struct attribute  id   Types    Int attribute  email   Types    Email attribute  name   Types    Name end   this will raise a type error since name is too short User   new   id  1   email   jane doe org    name   J     I ve already benefited from simple type checks like checking if a given value has correct class  which helped me to spot silly bugs before an app hit the production  Now with dry data   dry validation it s possible to be even more strict and define constrained types   Both libraries are very young but I encourage you to try them out  I believe it s going to help in building more robust applications  If you re worried about performance check out this gist which shows how ROM loads 3 type safe user entities slightly faster than non type safe ActiveRecord models   If you are interested in dry validation  check out my previous post and for more information about dry data please refer to its README,"[1226 845 397 413 823 1139 334 2 1281 1331 563]"
1274,training-dataset/engineering/1105.txt,engineering,Net Promoter Score and Survey Analysis in SQLWant to make data analysis fast for everyone  Join Us   Introduced by Fred Reichheld in 2003  Net Promoter Score  NPS  is a simple method for measuring the likelihood your customers will recommend your product or service  It requires that you survey respondents and ask one simple question    How likely are you to recommend  Company Product Service  to a friend or colleague    Typically respondents are presented with a scale from 0 to 10 to answer this question  Here s how the NPS question might appear on one of our own customer surveys   After collecting results  calculating NPS is easy  A response of 0 through 6 categorizes respondents as  Detractors   a 7 or 8 as  Passives   and a 9 or 10 as  Promoters   Your Net Promoter Score is a measure of the ratio of Detractors to Promoters  This is calculated by subtracting the number of Detractors from the number of Promoters  dividing the result by the total number of survey responses  and then multiplying by 100 to turn the result into an integer  NPS scores have a range of  100 to 100   That s a little wordy  here s how we like to think about it     promoters   detractors     count         100 as nps  This gives us a clear quantitative measure of the likelihood to recommend  but what about the  why   Most NPS surveys also collect qualitative data to pair with the quantitative measure  Something along the lines of a question like this   The majority of the Fortune 500 measure NPS for a very good reason  moving people from Detractors and Passives to Promoters improves business outcomes  It provides a simple mechanism for companies to bring the voice of the customer into product and business decisions  leading to reduced churn and increased customer satisfaction   In this post we re going to talk about how you can analyze numeric and text NPS results using SQL  The latter can be especially difficult   text analytics software is expensive  often difficult to implement in existing ETL processes  and can be challenging to understand  We ll provide clear approaches to successfully tackling both   NPS at the Hotel California  We collected 2 700 hotel reviews for the Hotel California from TripAdvisor for our sample dataset  Each review rates the hotel between 1 and 5  We map these reviews into NPS terminology using 5 as a Promoter  4 as a Passive  and 1 3 as a Detractor  A bit different from above  but the same principles apply   The table we will work from has 4 columns  Name  Date  Verbatim  and Review  It is called hotel_reviews and one row looks like   Simple Charts  The first thing you need to do is partition your data by the review score to take a look  All of our further analysis will be based around our segments  but we re analysts and we like raw data   select review   count       from hotel_reviews group by 1  And voila   We can see that our hotel has pretty good scores  Our NPS will certainly be high  but to obtain the final score we ll need to bucket our data into the NPS segments   Promoters  Passives  and Detractors  We can see our segments with a case statement   select case when review   5 then  Promoter  when review   4 then  Passive  when review in   1   2   3   then  Detractor  else  Error  end as np_segment   count       from hotel_reviews group by 1  In order to calculate our NPS we will need to do some math  This is a simple operation that can be done in SQL   First bucket the surveys and assign a value of 1 to Promoters  0 to Passives and  1 to Detractors  This can appear to be an odd skew  but treating Detractors and Promoters as equal but opposite is implicit to the metric  We can now calculate NPS on the fly like so   with nps_segments as   select case when review   5 then 1 when review   4 then 0 when review in   1   2   3   then   1 else null end as np_score from hotel_reviews      Here we will use a float conversion to avoid integer division     This is Redshift specific  Adjust accordingly  select round   sum   np_score     count         float   100   0   as nps from nps_segments  We can now see our hotel s NPS is a respectable 48  NPS should be compared carefully across verticals  but a quick look at the NPS of some common companies can provide some frame of reference  Apple has an NPS of 72 and Comcast an NPS of  3  Those are reasonable boundaries for our expectations   If we add a date column to our query we can examine our scores over time  We have also added a count column so we can see how many survey responses we have been collecting  As you can see  NPS appears to be trending slightly upwards  although 2005 and 2006 were rough years   with nps_segments as   select date   case when review   5 then 1 when review   4 then 0 when review in   1   2   3   then   1 else 0 end as np_score from hotel_reviews   select year   date   as dte   count       as surveys   round   sum   np_score     count         float   100   2   as nps from nps_segments group by 1  A similar query can show you how your relative proportions of Promoters and Detractors are changing over time  NPS changes as people change segments  You can improve NPS by moving your customers from Detractors to Passives  or Passives to Promoters  Both have value and understanding the mix shift can help craft a complete NPS strategy   We know that our score is reasonably good  and that it seems to be increasing  Everyone has patted themselves on the back  but now it s time to focus on next steps  Luckily Hotel California has asked customers why they provided the scores they did   Text Analysis  Now that we have a grasp on our NPS numbers  it s time to examine the wealth of text data we have collected  This is much more challenging  It requires more data preparation and more human interpretation  Survey response volume also matters   the more survey responses the better   First  it s important to clean your text responses  For our purposes this means   Making the text lower case  This will reduce work and computation later on by allowing simpler joins across tables  Removing grammar and numbers  These characters provide context for humans but for our analysis will cause errors  For instance  we want  food  and  food   to be treated in the same fashion  This will allow us to provide accurate counts of word frequency in our next step  Correcting spacing  so that multiple consecutive spaces become a single space  We will use spaces to count words below  Wait for it  it s clever   We used Redshift s regex functionality to achieve each of these goals  The inner statement makes the verbatim lower case and removes everything but letter characters  The second takes consecutive spaces and reduces them to a single space  If you would like a refresher on regex syntax  we have a blog post to help you  The code used for this exercise was   regexp_replace   regexp_replace   lower   verbatim       a zA Z                   1             The iterative process of refining your selection is made much easier using SQL Snippets with Periscope Data by maintaining a standardized set of code which can be improved over time   I put this into a SQL snippet called  verbatims_clean   so I could reuse it quickly and easily  Here is a row  pre and post cleanup  The first is easier to read for us  but the second is easier to work with in SQL   The first thing you can do is add the ability to filter based on words  Filters can be applied to the clean verbatim  and the original verbatim can be displayed  In the incredibly flexible Periscope Data Filters  this looks like      This is a Snippet inside a Filter     Look how nice that snippet works     You can even add parameters  Select date   verbatim   review from hotel_reviews where    verbatim_clean     Words    This will be done differently in different visualization systems  but the core principle remains the same  By filtering based on a cleaned verbatim set  you can return comprehensive results to the user   Once you have these cleaned verbatims  you can break them down into an even more interesting table  with one row per word per verbatim  We can now perform operations based on individual words rather than on sentences  This can help create powerful visualizations and can even be used for semantic analysis and categorization  topics we plan to cover in a future post   The table should then look like   The SQL to perform this operation looks like so  and will be further explained below     Here we will use the length function to count the number of words in each text response  After our regex clean up step  each text responses will have words equal to the number of spaces  plus one  For example   live long and prosper  has 3 spaces and 4 words   with word_count as   select name   date   verbatim   review     verbatim_clean   as verbatim_clean   length    verbatim_clean      length   regexp_replace    verbatim_clean         1              1 as num_words from hotel_reviews      By using our num_words column in a range join can can split_part    each text responses into its individual words   word_parse as   select name   date   review   split_part   verbatim_clean         nmbr    integer   as word from word_count join numbers on nmbr    num_words and nmbr    0 order by 1   2      The stop words table is important  See point 3 below  select   from word_parse where word not in   select   from stopwords    You ll note a few interesting things about this query   In word_parse we are looking to count the number of words in each text response  We subtract the length of the cleaned verbatim with spaces removed from the length of the cleaned verbatim with spaces  plus one  This gives us the number of words in the sentence  To do this  it is important you use regex to strip all whitespace down to a single character  as double spaces would lead to inaccurate word counts  We join to a table called  numbers   This is a sequence of all numbers from 1 to 1 million  used in interesting cases like this  Here  it allows us to split the verbatims based on the number of words per sentence  Make sure this table is properly indexed  or sub select a much smaller set of numbers  Read more about sequence tables here  In the final part of the query I remove every word defined as a  stop word   A stop word is any word that is frequent but provides little value to semantic analysis  They are essentially grammar  A list can be found here  http   www lextek com manuals onix stopwords1 html  This list should be kept up to date with words related to your own business  For instance  your company name is a stop word which will be in every verbatim but does not provide any information   Now that you have all your text broken into individual words  you can create more interesting visualizations  For example  the following query will give you the word ranked by frequency of appearance among people who give you different scores   with word_count as   select review   word   count       as cnt from hotel_words group by 1   2     ranked_words as   select     rank    over   partition by review order by cnt desc   as word_rank from word_count      I define pain and delight points below to limit the number of    lines returned  I am only interested in the top ten things    among the 1 s and the 5 s to begin  I will dig in more later  select   from ranked_words left join   select word   min   word_rank   as min_rank   max   case when review in   1   5   and word_rank    10 then 1 else 0 end   as pain_and_delight_points from ranked_words group by 1   important_words on ranked_words   word   important_words   word where pain_and_delight_points   1  Keeping in mind that a low rank means a high frequency of occurrence  this chart can quickly show great insights and tell you where you need to focus attention  In the following chart  we have the frequency rank along the Y axis and the review score on the X axis  Here we can see that  food  is highly ranked among low scores  whereas  staff  is the opposite  The staff are doing great  but it s time to begin working on the kitchen   Wrap Up  In a few minutes we calculated Hotel California s Net Promoter Score  examined their history  and began to explore some of the deeper reasons why they aren t beating the Ritz  This is valuable insight  and will help drive the goals of potential Circle back Projects  Rewards Programs  Tactical Churn Groups  or Voice of Consumer Product Development Task Forces  some of which are certain to follow   Bring this together and you will have an awesome  filterable dashboard to explore your survey data  Have fun  and Happy Periscoping   Subscribe  Thank you,"[1274 657 563 2 1289 865 855 397 0 740 428]"
1281,training-dataset/engineering/1222.txt,engineering,Building Scalable Applications Using Event Sourcing and CQRSBuilding Scalable Applications Using Event Sourcing and CQRS  About a year ago  I came across the terms event sourcing and CQRS  I have been fascinated by it ever since  Right now  I am in the middle of building out event driven microservice infrastructure at Andela using event sourcing  Check out Building Out an Antifragile Microservice Architecture   Andela   Design Consideration  my previous blog post to find out some of the design consideration we had to make   In this blog post  I will be discussing event sourcing  cqrs  kafka and how we are using it to build scalable data driven infrastructure  I am by no means an expert on the subject  So  if you have experience building an event sourced application  please do me and anyone else reading this a favor by pointing out things I get wrong   Traditional System  Most applications work with data  and the typical approach is for the application to maintain the current state of the data by updating it as users work with the data  For example  in the traditional create  read  update  and delete  CRUD  model  a typical data process will be to read data from the store  make some modifications to it  and update the current state of the data with the new values   often by using transactions that lock the data  This approach has many limitations   It requires 2 phase commit 2PC  when working with event driven systems by using a distributed transaction involving the database and the Message Broker  2PC commit reduce the throughput of transactions  To know more about 2PC  check out this video   In a collaborative domain with many concurrent users  data update conflicts are more likely to occur because the update operations take place on a single item of data   Unless there is an additional auditing mechanism  which records the details of each operation in a separate log  history is lost   Event sourcing  Event sourcing achieves atomicity without 2PC by using a radically different  event centric approach to persisting business entities  Rather than store the current state of an entity  the application stores a sequence of state changing events  The application reconstructs an entity s current state by replaying the events  Since saving an event is a single operation  it is inherently atomic  The diagram below shows how an event stream is used to reconstruct current state   In an event sourced application  there is an eventstore that persist events  The Event Store also behaves like the Message Broker  It provides an API that enables services to subscribe to events  The Event Store delivers events to all interested subscribers  The Event Store is the backbone of an event sourced microservices architecture  For more in depth description of event sourcing  check out introducing event sourcing and this post by Martin Fowler   Why should I use event sourcing   Event sourcing provides a lot of benefits besides having a history of events  They include  but not limited to   Audit trail   Events are immutable and store the full history of the state of the system  As such  they can provide a detailed audit trail of what has taken place within the system     Events are immutable and store the full history of the state of the system  As such  they can provide a detailed audit trail of what has taken place within the system  Integration with other subsystems   Event store can publish events to notify other interested subsystems of changes to the application s state  Again  the event store provides a complete record of all the events that it published to other systems     Event store can publish events to notify other interested subsystems of changes to the application s state  Again  the event store provides a complete record of all the events that it published to other systems  Time Travel  By storing events  you have the ability to determine the state of the system at any previous point in time by querying the events associated with a domain object up to that point in time  This enables you to answer historical questions from the business about the system   Command and Query Responsibility Segregation CQRS   CQRS is a simple design pattern for separating concerns  Here  object s methods should be either commands or queries but not both  A query returns data and does not alter the state of the object  a command changes the state of an object but does not return any data  The benefit is that you have a better understanding of what does  and what does not change the state in your system   CQRS pattern comes in handy when building an event sourced application  Since event sourcing stores current state as a series of events  it becomes time consuming when you need to retrieve the current state  This is because to get the current state of an entity  we will need to rebuild state within the system by replaying the series of events for that entity  What we can do to alleviate this program is to maintain two separate model write model  eventstore and read model a normal database   Once an event is persisted in an event store  a different handler will be in charge of updating the read store  We can read data directly from the read store but not write to it  By using CQRS  we have completely separated read operations via readstore  from write operations via eventstore    Kafka as an EventStore  Kafka is typically a message broker or message queue comparable to AMQP  JMS  NATS  RabbitMQ   It has two types of clients   producers  send messages to Kafka  consumers  subscribe to streams of messages in Kafka  What makes Kafka interesting and why it s suitable for use as an eventstore is that it is structured as a log  The way kafka works can be summarized using the diagram below   Anatomy of  Producers  write messages to kafka topics eg a topic called users  Every message that is sent to a Kafka topic eg user s topic  by a producer is appended to the end of a partition  Only write operation is supported by Kafka   Topics are split into multiple partitions and each partition is a log a totally ordered sequence of events   Partitions in a topic are independent from each other  so there is no ordering guarantee across partitions  Each partition is stored on disk and replicated across several machines based on the replication factor of the partition s topic   so it is durable and can tolerate machine failure without data loss   Within each partition  messages have a monotonically increasing offset  log position   To consume messages from Kafka  a client reads messages sequentially  starting from a particular offset  That offset is managed by the consumer   Applying event sourcing with Kafka  At Andela  each kafka topic maps to a bounded context  which in turn maps to a microservice  I will be using our user management service microservice as a case study  user management service is responsible for managing all users of our platform  We started with identifying all the types of event that will be published to users topic  We were able to come up with the following list of domain events   UserCreatedEvent  UserUpdatedEvent  UserDeletedEvent  UserLoggedInEvent  RoleAssignedToUserEvent  RoleUnassignedFromUserEvent  RoleCreatedEvent  RoleDeletedEvent  RoleUpdatedEvent  These domain events can be published by any microservice even by user management service  and all interested microservice will consume this event  The diagram below is a simplified flow of a user request,"[1281 638 1031 563 743 1289 413 660 831 0 934]"
1289,training-dataset/engineering/647.txt,engineering,a Survey and Decision Guidance   Baqend BlogNoSQL Databases  a Survey and Decision Guidance Together with our colleagues at the University of Hamburg  we   that is Felix Gessert  Wolfram Wingerath  Steffen Friedrich and Norbert Ritter   presented an overview over the NoSQL landscape at SummerSOC 16 last month  Here is the written gist  We give our best to convey the condensed NoSQL knowledge we gathered building Baqend  TL DR Today  data is generated and consumed at unprecedented scale  This has lead to novel approaches for scalable data management subsumed under the term  NoSQL  database systems to handle the ever increasing data volume and request loads  However  the heterogeneity and diversity of the numerous existing systems impede the well informed selection of a data store appropriate for a given application context  Therefore  this article gives a top down overview of the field  Instead of contrasting the implementation specifics of individual representatives  we propose a comparative classification model that relates functional and non functional requirements to techniques and algorithms employed in NoSQL databases  This NoSQL Toolbox allows us to derive a simple decision tree to help practitioners and researchers filter potential system candidates based on central application requirements  1  Introduction Traditional relational database management systems  RDBMSs  provide powerful mechanisms to store and query structured data under strong consistency and transaction guarantees and have reached an unmatched level of reliability  stability and support through decades of development  In recent years  however  the amount of useful data in some application areas has become so vast that it cannot be stored or processed by traditional database solutions  User generated content in social networks or data retrieved from large sensor networks are only two examples of this phenomenon commonly referred to as Big Data  A class of novel data storage systems able to cope with Big Data are subsumed under the term NoSQL databases  many of which offer horizontal scalability and higher availability than relational databases by sacrificing querying capabilities and consistency guarantees  These trade offs are pivotal for service oriented computing and as a service models  since any stateful service can only be as scalable and fault tolerant as its underlying data store  There are dozens of NoSQL database systems and it is hard to keep track of where they excel  where they fail or even where they differ  as implementation details change quickly and feature sets evolve over time  In this article  we therefore aim to provide an overview of the NoSQL landscape by discussing employed concepts rather than system specificities and explore the requirements typically posed to NoSQL database systems  the techniques used to fulfil these requirements and the trade offs that have to be made in the process  Our focus lies on key value  document and wide column stores  since these NoSQL categories cover the most relevant techniques and design decisions in the space of scalable data management  In Section 2  we describe the most common high level approaches towards categorizing NoSQL database systems either by their data model into key value stores  document stores and wide column stores or by the safety liveness trade offs in their design  CAP and PACELC   We then survey commonly used techniques in more detail and discuss our model of how requirements and techniques are related in Section 3   before we give a broad overview of prominent database systems by applying our model to them in Section 4   A simple and abstract decision model for restricting the choice of appropriate NoSQL systems based on application requirements concludes the paper in Section 5  2  High Level System Classification In order to abstract from implementation details of individual NoSQL systems  high level classification criteria can be used to group similar data stores into categories  In this section  we introduce the two most prominent approaches  data models and CAP theorem classes  2 1 Different Data Models The most commonly employed distinction between NoSQL databases is the way they store and allow access to data  Each system covered in this paper can be categorised as either key value store  document store or wide column store  Figure 1  Key value stores offer efficient storage and retrieval of arbitrary values  2 1 1 Key Value Stores  A key value store consists of a set of key value pairs with unique keys  Due to this simple structure  it only supports get and put operations  As the nature of the stored value is transparent to the database  pure key value stores do not support operations beyond simple CRUD  Create  Read  Update  Delete   Key value stores are therefore often referred to as schemaless  Any assumptions about the structure of stored data are implicitly encoded in the application logic  schema on read  and not explicitly defined through a data definition language  schema on write   The obvious advantages of this data model lie in its simplicity  The very simple abstraction makes it easy to partition and query the data  so that the database system can achieve low latency as well as high throughput  However  if an application demands more complex operations  e g  range queries  this data model is not powerful enough  Figure 1 illustrates how user account data and settings might be stored in a key value store  Since queries more complex than simple lookups are not supported  data has to be analyzed inefficiently in application code to extract information like whether cookies are supported or not  cookies  false   2 1 2 Document Stores  A document store is a key value store that restricts values to semi structured formats such as JSON documents  This restriction in comparison to key value stores brings great flexibility in accessing the data  It is not only possible to fetch an entire document by its ID  but also to retrieve only parts of a document  e g  the age of a customer  and to execute queries like aggregation  query by example or even full text search  Figure 2  Document stores are aware of the internal structure of the stored entity and thus can support queries  3 1 3 Wide Column Stores Wide column stores inherit their name from the image that is often used to explain the underlying data model  a relational table with many sparse columns  Technically  however  a wide column store is closer to a distributed multi level sorted map  The first level keys identify rows which themselves consist of key value pairs  The first level keys are called row keys  the second level keys are called column keys  This storage scheme makes tables with arbitrarily many columns feasible  because there is no column key without a corresponding value  Hence  null values can be stored without any space overhead  The set of all columns is partitioned into so called column families to colocate columns on disk that are usually accessed together  On disk  wide column stores do not colocate all data from each row  but instead values of the same column family and from the same row  Hence  an entity  a row  cannot be retrieved by one single lookup as in a document store  but has to be joined together from the columns of all column families  However  this storage layout usually enables highly efficient data compression and makes retrieving only a portion of an entity very efficient  The data are stored in lexicographic order of their keys  so that data that are accessed together are physically co located  given a careful key design  As all rows are distributed into contiguous ranges  so called tablets  among different tablet servers  row scans only involve few servers and thus are very efficient  Figure 3  Data in a wide column store  Bigtable  which pioneered the wide column model  was specifically developed to store a large collection of webpages as illustrated in Figure 3  Every row in the webpages table corresponds to a single webpage  The row key is a concatenation of the URL components in reversed order and every column key is composed of the column family name and a column qualifier  separated by a colon  There are two column families  the  contents  column family with only one column holding the actual webpage and the  anchor  column family holding links to each webpage  each in a separate column  Every cell in the table  i e  every value accessible by the combination of row and column key  can be versioned by timestamps or version numbers  It is important to note that much of the information of an entity lies in the keys and not only in the values   2 2 Consistency Availability Trade Offs  CAP and PACELC Another defining property of a database apart from how the data are stored and how they can be accessed is the level of consistency that is provided  Some databases are built to guarantee strong consistency and serializability  ACID   while others favour availability  BASE   This trade off is inherent to every distributed database system and the huge number of different NoSQL systems shows that there is a wide spectrum between the two paradigms  In the following  we explain the two theorems CAP and PACELC according to which database systems can be categorised by their respective positions in this spectrum  CAP  Like the famous FLP Theorem  the CAP Theorem  presented by Eric Brewer at PODC 2000 and later proven by Gilbert and Lynch  is one of the truly influential impossibility results in the field of distributed computing  because it places an ultimate upper bound on what can possibly be accomplished by a distributed system  It states that a sequentially consistent read write register that eventually responds to every request cannot be realised in an asynchronous system that is prone to network partitions  In other words  it can guarantee at most two of the following three properties at the same time  Consistency  C    Reads and writes are always executed atomically and are strictly consistent  linearizable   Put differently  all clients have the same view on the data at all times     Reads and writes are always executed atomically and are strictly consistent  linearizable   Put differently  all clients have the same view on the data at all times  Availability  A    Every non failing node in the system can always accept read and write requests by clients and will eventually return with a meaningful response  i e  not with an error message     Every non failing node in the system can always accept read and write requests by clients and will eventually return with a meaningful response  i e  not with an error message  Partition tolerance  P   The system upholds the previously displayed consistency guarantees and availability in the presence of message loss between the nodes or partial system failure  Brewer argues that a system can be both available and consistent in normal operation  but in the presence of a system partition  this is not possible  If the system continues to work in spite of the partition  there is some non failing node that has lost contact to the other nodes and thus has to decide to either continue processing client requests to preserve availability  AP  eventual consistent systems  or to reject client requests in order to uphold consistency guarantees  CP   The first option violates consistency  because it might lead to stale reads and conflicting writes  while the second option obviously sacrifices availability  There are also systems that usually are available and consistent  but fail completely when there is a partition  CA   for example single node systems  It has been shown that the CAP theorem holds for any consistency property that is at least as strong as causal consistency  which also includes any recency bounds on the permissible staleness of data    atomicity   Serializability as the correctness criterion of transactional isolation does not require strong consistency  However  similar to consistency  serializability can also not be achieved under network partitions  The classification of NoSQL systems as either AP  CP or CA vaguely reflects the individual systems  capabilities and hence is widely accepted as a means for high level comparisons  However  it is important to note that the CAP Theorem actually does not state anything on normal operation  it merely tells us whether a system favors availability or consistency in the face of a network partition  In contrast to the FLP Theorem  the CAP theorem assumes a failure model that allows arbitrary messages to be dropped  reordered or delayed indefinitely  Under the weaker assumption of reliable communication channels  i e  messages always arrive but asynchronously and possibly reordered  a CAP system is in fact possible using the Attiya  Bar Noy  Dolev algorithm  as long as a majority of nodes are up   Therefore  consensus as used for coordination in many NoSQL systems either natively  e g  in Megastore  or through coordination services like Chubby and Zookeeper is even harder to achieve with high availability than strong consistency  see FLP Theorem   PACELC  This lack of the CAP Theorem is addressed in an article by Daniel Abadi in which he points out that the CAP Theorem fails to capture the trade off between latency and consistency during normal operation  even though it has proven to be much more influential on the design of distributed systems than the availability consistency trade off in failure scenarios  He formulates PACELC which unifies both trade offs and thus portrays the design space of distributed systems more accurately  From PACELC  we learn that in case of a Partition  there is an Availability Consistency trade off  Else  i e  in normal operation  there is a Latency Consistency trade off  This classification basically offers two possible choices for the partition scenario  A C  and also two for normal operation  L C  and thus appears more fine grained than the CAP classification  However  many systems cannot be assigned exclusively to one single PACELC class and one of the four PACELC classes  namely PC EL  can hardly be assigned to any system  3  Techniques Every significantly successful database is designed for a particular class of applications  or to achieve a specific combination of desirable system properties  The simple reason why there are so many different database systems is that it is not possible for any system to achieve all desirable properties at once  Traditional SQL databases such as PostgreSQL have been built to provide the full functional package  a very flexible data model  sophisticated querying capabilities including joins  global integrity constraints and transactional guarantees  On the other end of the design spectrum  there are key value stores like Dynamo that scale with data and request volume and offer high read and write throughput as well as low latency  but barely any functionality apart from simple lookups  In this section  we highlight the design space of distributed database systems  concentrating on sharding  replication  storage management and query processing  We survey the available techniques and discuss how they are related to different functional and non functional properties  goals  of data management systems  In order to illustrate what techniques are suitable to achieve which system properties  we provide the NoSQL Toolbox  Figure 4  where each technique is connected to the functional and non functional properties it enables  positive edges only    Figure 4  The NoSQL Toolbox  It connects the techniques of NoSQL databases with the desired functional and non functional system properties they support   3 1 Sharding Several distributed relational database systems such as Oracle RAC or IBM DB2 pureScale rely on a shared disk architecture where all database nodes access the same central data repository  e g  a NAS or SAN   Thus  these systems provide consistent data at all times  but are also inherently difficult to scale  In contrast  the  NoSQL  database systems focused in this paper are built upon a shared nothing architecture  meaning each system consists of many servers with private memory and private disks that are connected through a network  Thus  high scalability in throughput and data volume is achieved by sharding  partitioning  data across different nodes  shards  in the system  There are three basic distribution techniques  range sharding  hash sharding and entity group sharding  To make efficient scans possible  the data can be partitioned into ordered and contiguous value ranges by range sharding  However  this approach requires some coordination through a master that manages assignments  To ensure elasticity  the system has to be able to detect and resolve hotspots automatically by further splitting an overburdened shard  Range sharding is supported by wide column stores like BigTable  HBase or Hypertable and document stores  e g  MongoDB  RethinkDB  Espresso and DocumentDB  Another way to partition data over several machines is hash sharding where every data item is assigned to a shard server according to some hash value built from the primary key  This approach does not require a coordinator and also guarantees the data to be evenly distributed across the shards  as long as the used hash function produces an even distribution  The obvious disadvantage  though  is that it only allows lookups and makes scans unfeasible  Hash sharding is used in key value stores and is also available in some wide coloumn stores like Cassandra or Azure Tables  The shard server that is responsible for a record can be determined as serverid   hash id  servers  for example  However  this hashing scheme requires all records to be reassigned every time a new server joins or leaves  because it changes with the number of shard servers  servers   Consequently  it infeasible to use in elastic systems like Dynamo  Riak or Cassandra  which allow additional resources to be added on demand and again be removed when dispensable  For increased flexibility  elastic systems typically use consistent hashing where records are not directly assigned to servers  but instead to logical partitions which are then distributed across all shard servers  Thus  only a fraction of the data have to be reassigned upon changes in the system topology  For example  an elastic system can be downsized by offloading all logical partitions residing on a particular server to other servers and then shutting down the now idle machine  For details on how consistent hashing is used in NoSQL systems  see the Dynamo paper  Entity group sharding is a data partitioning scheme with the goal of enabling single partition transactions on co located data  The partitions are called entity groups and either explicitly declared by the application  e g  in G Store andMegaStore  or derived from transactions  access patterns  e g  in Relational Cloud and Cloud SQL Server   If a transaction accesses data that spans more than one group  data ownership can be transferred between entity groups or the transaction manager has to fallback to more expensive multi node transaction protocols  3 2 Replication In terms of CAP  conventional RDBMSs are often CA systems run in single server mode  The entire system becomes unavailable on machine failure  And so system operators secure data integrity and availability through expensive  but reliable high end hardware  In contrast  NoSQL systems like Dynamo  BigTable or Cassandra are designed for data and request volumes that cannot possibly be handled by one single machine  and therefore they run on clusters consisting of thousands of servers   Low end hardware is used  because it is substantially more cost efficient than high end hardware   Since failures are inevitable and will occur frequently in any large scale distributed system  the software has to cope with them on a daily basis   In 2009  Google fellow Jeff Dean stated that a typical new cluster at Google encounters thousands of hard drive failures  1 000 single machine failures  20 rack failures and several network partitions due to expected and unexpected circumstances in its first year alone  Many more recent cases of network partitions and outages in large cloud data centers have been reported   Replication allows the system to maintain availability and durability in the face of such errors  But storing the same records on different machines  replica servers  in the cluster introduces the problem of synchronization between them and thus a trade off between consistency on the one hand and latency and availability on the other  Gray et al  propose a two tier classification of different replication strategies according to when updates are propagated to replicas and where updates are accepted  There are two possible choices on tier one   when    Eager synchronous  replication propagates incoming changes synchronously to all replicas before a commit can be returned to the client  whereas lazy  asynchronous  replication applies changes only at the receiving replica and passes them on asynchronously  The great advantage of eager replication is consistency among replicas  but it comes at the cost of higher write latency due to the need to wait for other replicas and impaired availability  Lazy replication is faster  because it allows replicas to diverge  as a consequence  stale data might be served  On the second tier   where    again  two different approaches are possible  Either a master slave  primary copy  scheme is pursued where changes can only be accepted by one replica  the master  or  in a update anywhere  multi master  approach  every replica can accept writes  In master slave protocols  concurrency control is not more complex than in a distributed system without replicas  but the entire replica set becomes unavailable  as soon as the master fails  Multi master protocols require complex mechanisms for prevention or detection and reconciliation of conflicting changes  Techniques typically used for these purposes are versioning  vector clocks  gossiping and read repair  e g  in Dynamo  and convergent or commutative datatypes  e g  in Riak   Basically  all four combinations of the two tier classification are possible  Distributed relational systems usually perform eager master slave replication to maintain strong consistency  Eager update anywhere replication as for example featured in Google s Megastore suffers from a heavy communication overhead generated by synchronisation and can cause distributed deadlocks which are expensive to detect  NoSQL database systems typically rely on lazyreplication  either in combination with the master slave  CP systems  e g  HBase and MongoDB  or the update anywhere approach  AP systems  e g  Dynamo and Cassandra   Many NoSQL systems leave the choice between latency and consistency to the client  i e  for every request  the client decides whether to wait for a response from any replica to achieve minimal latency or for a certainly consistent response  by a majority of the replicas or the master  to prevent stale data  An aspect of replication that is not covered by the two tier scheme is the distance between replicas  The obvious advantage of placing replicas near one another is low latency  but close proximity of replicas might also reduce the positive effects on availability  for example  if two replicas of the the same data item are placed in the same rack  the data item is not available on rack failure in spite of replication  But more than the possibility of mere temporary unavailability  placing replicas nearby also bears the peril of losing all copies at once in a disaster scenario  An alternative technique for latency reduction is used in Orestes  where data is cached close to applications using web caching infrastructure and cache coherence protocols  Geo replication can protect the system against complete data loss and improve read latency for distributed access from clients  Eager geo replication  as implemented in Megastore  Spanner  MDCC and Mencius achieve strong consistency at the cost of higher write latencies  typically 100ms to 600ms   With lazy geo replication as in Dynamo  PNUTS  Walter  COPS  Cassandra and BigTable recent changes may be lost  but the system performs better and remains available during partitions  Charron Bost et al   Chapter 12  and  szu and Valduriez  Chapter 13  provide a comprehensive discussion of database replication  3 3 Storage Management For best performance  database systems need to be optimized for the storage media they employ to serve and persist data  These are typically main memory  RAM   solid state drives  SSDs  and spinning disk drives  HDDs  that can be used in any combination  Unlike RDBMSs in enterprise setups  distributed NoSQL databases avoid specialized shared disk architectures in favor of shared nothing clusters based on commodity servers  employing commodity storage media   Storage devices are typically visualized as a  storage pyramid   see Figure 5 or Hellerstein et al    There is also a set of transparent caches  e g  L1 L3 CPU caches and disk buffers  not shown in the Figure   that are only implicitly leveraged through well engineered database algorithms that promote data locality  The very different cost and performance characteristics of RAM  SSD and HDD storage and the different strategies to leverage their strengths  storage management  are one reason for the diversity of NoSQL databases  Storage management has a spatial dimension  where to store data  and a temporal dimension  when to store data   Update in place and append only IO are two complementary spatial techniques of organizing data  in memory prescribes RAM as the location of data  whereas logging is a temporal technique that decouples main memory and persistent storage and thus provides control over when data is actually persisted  Figure 5  The storage pyramid and its role in NoSQL systems  In their seminal paper  the end of an architectural era   Stonebraker et al  have found that in typical RDBMSs  only 6 8  of the execution time is spent on  useful work   while the rest is spent on  buffer management  34 6    i e  caching to mitigate slower disk access  latching  14 2    to protect shared data structures from race conditions caused by multi threading  locking  16 3    to guarantee logical isolation of transactions  logging  1 9    to ensure durability in the face of failures  hand coded optimizations  16 2   This motivates that large performance improvements can be expected if RAM is used as primary storage  in memory databases   The downside are high storage costs and lack of durability   a small power outage can destroy the database state  This can be solved in two ways  The state can be replicated over n in memory server nodes protecting against n 1 single node failures  e g  HStore  VoltDB  or by logging to durable storage  e g  Redis or SAP Hana   Through logging  a random write access pattern can be transformed to a sequential one comprised of received operations and their associated properties  e g  redo information   In most NoSQL systems  the commit rule for logging is respected  which demands every write operation that is confirmed as successful to be logged and the log to be flushed to persistent storage  In order to avoid the rotational latency of HDDs incurred by logging each operation individually  log flushes can be batched together  group commit  which slightly increases the latency of individual writes  but drastically improves throughput  SSDs and more generally all storage devices based on NAND flash memory differ substantially from HDDs in various aspects    1  asymmetric speed of read and write operations   2  no in place overwrite   the whole block must be erased before overwriting any page in that block  and  3  limited program erase cycles   Min et al   2012   Thus  a database system s storage management must not treat SSDs and HDDs as slightly slower  persistent RAM  since random writes to an SSD are roughly an order of magnitude slower than sequential writes  Random reads  on the other hand  can be performed without any performance penalties  There are some database systems  e g  Oracle Exadata  Aerospike  that are explicitly engineered for these performance characteristics of SSDs  In HDDs  both random reads and writes are 10 100 times slower than sequential access  Logging hence suits the strengths of SSDs and HDDs which both offer a significantly higher throughput for sequential writes  For in memory databases  an update in place access pattern is ideal  It simplifies the implementation and random writes to RAM are essentially equally fast as sequential ones  with small differences being hidden by pipelining and the CPU cache hierarchy  However  RDBMSs and many NoSQL systems  e g  MongoDB  employ an update in place update pattern for persistent storage  too  To mitigate the slow random access to persistent storage  main memory is usually used as a cache and complemented by logging to guarantee durability  In RDBMSs  this is achieved through a complex buffer pool which not only employs cache replace algorithms appropriate for typical SQL based access patterns  but also ensures ACID semantics  NoSQL databases have simpler buffer pools that profit from simpler queries and the lack of ACID transactions  The alternative to the buffer pool model is to leave caching to the OS through virtual memory  e g  employed in MongoDB s MMAP storage engine   This simplifies the database architecture  but has the downside of giving less control over which data items or pages reside in memory and when they get evicted  Also read ahead  speculative reads  and write behind  write buffering  transparently performed with OS buffering lack sophistication as they are based on file system logics instead of database queries  Append only storage  also referred to as log structuring  tries to maximize throughput by writing sequentially  Although log structured file systems have a long research history  append only I O has only recently been popularized for databases by BigTable s use of Log Structured Merge  LSM  trees consisting of an in memory cache  a persistent log and immutable  periodically written storage files  LSM trees and variants like Sorted Array Merge Trees  SAMT  and Cache Oblivious Look ahead Arrays  COLA  have been applied in many NoSQL systems  Cassandra  CouchDB  LevelDB  Bitcask  RethinkDB  WiredTiger  RocksDB  InfluxDB  TokuDB   Designing a database to achieve maximum write performance by always writing to a log is rather simple  the difficulty lies in providing fast random and sequential reads  This requires an appropriate index structure that is either permanently updated as a copy on write  COW  data structure  e g  CouchDB s COW B trees  or only periodically persisted as an immutable data structure  e g  in BigTable style systems   An issue of all log structured storage approaches is costly garbage collection  compaction  to reclaim space of updated or deleted items  In virtualized environments like Infrastructure as a Service clouds many of the discussed characteristics of the underlying storage layer are hidden   Table 1  A direct comparison of functional requirements  non functional requirements and techniques among MongoDB  Redis  HBase  Riak  Cassandra and MySQL according to our NoSQL Toolbox   3 4 Query Processing The querying capabilities of a NoSQL database mainly follow from its distribution model  consistency guarantees and data model  Primary key lookup  i e  retrieving data items by a unique ID  is supported by every NoSQL system  since it is compatible to range  as well as hash partitioning  Filter queries return all items  or projections  that meet a predicate specified over the properties of data items from a single table  In their simplest form  they can be performed as filtered full table scans  For hash partitioned databases this implies a scatter gather pattern where each partition performs the predicated scan and results are merged  For range partitioned systems  any conditions on the range attribute can be exploited to select partitions  To circumvent the inefficiencies of O n  scans  secondary indexes can be employed  These can either be local secondary indexes that are managed in each partition or global secondary indexes that index data over all partitions  As the global index itself has to be distributed over partitions  consistent secondary index maintenance would necessitate slow and potentially unavailable commit protocols  Therefore in practice  most systems only offer eventual consistency for these indexes  e g  Megastore  Google AppEngine Datastore  DynamoDB  or do not support them at all  e g  HBase  Azure Tables   When executing global queries over local secondary indexes the query can only be targeted to a subset of partitions if the query predicate and the partitioning rules intersect  Otherwise  results have to be assembled through scatter gather  For example  a user table with range partitioning over an age field can service queries that have an equality condition on age from one partition whereas queries over names need to be evaluated at each partition  A special case of global secondary indexing is full text search  where selected fields or complete data items are fed into either a database internal inverted index  e g  MongoDB  or to an external search platform such as ElasticSearch or Solr  Riak Search  DataStax Cassandra   Query planning is the task of optimizing a query plan to minimize execution costs  For aggregations and joins  query planning is essential as these queries are very inefficient and hard to implement in application code  The wealth of literature and results on relational query processing is largely disregarded in current NoSQL systems for two reasons  First  the key value and wide column model are centered around CRUD and scan operations on primary keys which leave little room for query optimization  Second  most work on distributed query processing focuses on OLAP  online analytical processing  workloads that favor throughput over latency whereas single node query optimization is not easily applicable for partitioned and replicated databases  However  it remains an open research challenge to generalize the large body of applicable query optimization techniques especially in the context of document databases   Currently only RethinkDB can perform general   joins  MongoDB s aggregation framework has support for left outer equi joins in its aggregation framework and CouchDB allows joins for pre declared map reduce views   In database analytics can be performed either natively  e g  in MongoDB  Riak  CouchDB  or through external analytics platforms such as Hadoop  Spark and Flink  e g  in Cassandra and HBase   The prevalent native batch analytics abstraction exposed by NoSQL systems is MapReduce   An alternative to MapReduce are generalized data processing pipelines  where the database tries to optimize the flow of data and locality of computation based on a more declarative query language  e g  MongoDB s aggregation framework   Due to I O  communication overhead and limited execution plan optimization  these batch  and micro batch oriented approaches have high response times Materialized views are an alternative with lower query response times  They are declared at design time and continuously updated on change operations  e g  in CouchDB and Cassandra   However  similar to global secondary indexing  view consistency is usually relaxed in favor of fast  highly available writes  when the system is distributed  As only few database systems come with built in support for ingesting and querying unbounded streams of data near real time analytics pipelines commonly implement either the Lambda Architecture or the Kappa Architecture  The former complements a batch processing framework like Hadoop MapReduce with a stream processor such as Storm  see for example Summingbird  and the latter exclusively relies on stream processing and forgoes batch processing altogether  4  System Case Studies In this section  we provide a qualitative comparison of some of the most prominent key value  document and wide column stores  We present the results in strongly condensed comparisons and refer to the documentations of the individual systems for in detail information  The proposed NoSQL Toolbox  see Figure 4  is a means of abstraction that can be used to classify database systems along three dimensions  functional requirements  non functional requirements and the techniques used to implement them  We argue that this classification characterizes many database systems well and thus can be used to meaningfully contrast different database systems  Table 1 shows a direct comparison of MongoDB  Redis  HBase  Riak  Cassandra and MySQL in their respective default configurations  A more verbose comparison of central system properties is presented in the large comparison Table 2 at the end of this article  The methodology used to identify the specific system properties consists of an in depth analysis of publicly available documentation and literature on the systems  Furthermore  some properties had to be evaluated by researching the open source code bases  personal communication with the developers as well as a meta analysis of reports and benchmarks by practitioners  For detailed descriptions see the slides from our ICDE 2016 Tutorial  which goes over many details of the different NoSQL systems  The comparison elucidates how SQL and NoSQL databases are designed to fulfill very different needs  RDBMSs provide an unmatched level of functionality whereas NoSQL databases excel on the non functional side through scalability  availability  low latency and or high throughput  However  there are also large differences among the NoSQL databases  Riak and Cassandra  for example  can be configured to fulfill many non functional requirements  but are only eventually consistent and do not feature many functional capabilities apart from data analytics and  in case of Cassandra  conditional updates  MongoDB and HBase  on the other hand  offer stronger consistency and more sophisticated functional capabilities such as scan queries and   only MongoDB    filter queries  but do not maintain read and write availability during partitions and tend to display higher read latencies  Redis  as the only non partitioned system in this comparison apart from MySQL  shows a special set of trade offs centered around the ability to maintain extremely high throughput at low latency using in memory data structures and asynchronous master slave replication   Figure 6  A decision tree for mapping requirements to  NoSQL  database systems   5  Conclusions Choosing a database system always means to choose one set of desirable properties over another  To break down the complexity of this choice  we present a binary decision tree in Figure 6 that maps trade off decisions to example applications and potentially suitable database systems  The leaf nodes cover applications ranging from simple caching  left  to Big Data analytics  right   Naturally  this view on the problem space is not complete  but it vaguely points towards a solution for a particular data management problem  The first split in the tree is along the access pattern of applications  They either rely on fast lookups only  left half  or require more complex querying capabilities  right half   The fast lookup applications can be distinguished further by the data volume they process  If the main memory of one single machine can hold all the data  a single node system like Redis or Memcache probably is the best choice  depending on whether functionality  Redis  or simplicity  Memcache  is favored  If the data volume is or might grow beyond RAM capacity or is even unbounded  a multi node system that scales horizontally might be more appropriate  The most important decision in this case is whether to favor availability  AP  or consistency  CP  as described by the CAP theorem  Systems like Cassandra and Riak can deliver an always on experience  while systems like HBase  MongoDB and DynamoDB deliver strong consistency  The right half of the tree covers applications requiring more complex queries than simple lookups  Here  too  we first distinguish the systems by the data volume they have to handle according to whether single node systems are feasible  HDD size  or distribution is required  unbounded volume   For common OLTP  online transaction processing  workloads on moderately large data volumes  traditional RDBMSs or graph databases like Neo4J are optimal  because they offer ACID semantics  If  however  availability is of the essence  distributed systems like MongoDB  CouchDB or DocumentDB are preferrable  If the data volume exceeds the limits of a single machine  the choice of the right system depends on the prevalent query pattern  When complex queries have to be optimised for latency  as for example in social networking applications  MongoDB is very attractive  because it facilitates expressive ad hoc queries  HBase and Cassandra are also useful in such a scenario  but excel at throughput optimised Big Data analytics  when combined with Hadoop  In summary  we are convinced that the proposed top down model is an effective decision support to filter the vast amount of NoSQL database systems based on central requirements  The NoSQL Toolbox furthermore provides a mapping from functional and non functional requirements to common implementation techniques to categorize the constantly evolving NoSQL space,"[1289 563 428 831 0 562 668 59 413 334 397]"
1331,training-dataset/product/716.txt,product,A Foundation for Data Driven UX76 Flares 76 Flares    At ProductTank NYC  Lindsay Silver talked about the challenges of blending the inspiration of fashion and taste with the guidance offered by huge amounts of hard data at Conde Nast  where he is the Head of Data Technology  Specifically  he walked through an internal data product for that leverages UX data to support creative and commercial decisions  making it easier to build digital products  This talk highlights some of the technical specs of their product  as well as the  data usability  factors they discovered and the requirements for an effective UX data infrastructure   Conde Naste has a staggeringly huge number of users  and a huge number of interactions with each of those users  which obviously generates an overflow of data  Different teams in the organisation have used various 3rd party analytics tools  and they generally work okay   However  those tools are ultimately general tools  and having data in 3rd party repositories slows down the process of getting the data the teams need and doing the exact analysis they need  So  the team decided to built a custom data framework to solve the specific media product challenges they were facing  Going into this project  they knew the data systems would need to be SMART   Scalable   able to expand gracefully  able to expand gracefully Malleable   flexible enough to grow and fit future needs  flexible enough to grow and fit future needs Accessible   Simple enough to be used operationally  and granular enough to answer specific questions  Simple enough to be used operationally  and granular enough to answer specific questions Reliable   Exactly what it sounds like   Exactly what it sounds like  Timely   Provide data in a reasonable time frame  for a reasonable history  at a reasonable granularity   The team were trying to satisfy varied demands   the data needs of engineers and CXOs  and being able to cope with a huge  varied and changing ecosystem that Conde Naste is pushing data around  The good news is that they succeeded in building a real time system allowing them to push event data about their products to any system they need  make custom metrics  define custom meta data  and build a data infrastructure that helps them build and rapidly improve their digital products   Lessons Learned Building a Custom Data Infrastructure  It is crucial to understand that your product is not a category  and that you are building it to fit a specific need  So  in the case of Conde Naste  they weren t simply building  a web app   they were building  Vogue Online   and so Sparrow needed to reflect that   Crucially  when you re dealing with vast amounts of data and thinking about usability  you re like to arrive at an idea something like   Accessibility of data   Granularity of data   Age of data  What Does the Right UX Data Make Possible   Sparrow takes in over 10 000 events per second  and access to that data is devolved out to individual teams  empowering them to make the decisions that matter to their specific challenges  So  for example  their editors are working on a system to dynamically adjust publishing times of new articles to maximum impact  based on the entities in each article and historical data about what s worked well   They re also working on targeting content syndication based on user behaviour  and intelligent content resurfacing based on trends and surges in topic interest  The data is enabling smarter ad placement based on trend spotting  and behavioural targeting   Basically  having the right data framework in place is enabling Conde Naste to embrace digital publishing as a truly distinct phase of the media industry  understanding how their users are interacting with their products in an incredibly fine grained way  and then blending that data with creative intuition  The result is that they can build new digital products faster and more effectively than ever before   If it s enabling them to change the way they build media products so dramatically  then it s worth taking a look at your own data infrastructure and analytics options  and asking yourself if you re getting the right data  enough data  and whether you re able to use the data in the ways you need  It probably isn t necessary to build your own infrastructure for it  but you can at least make some smart decisions about how your existing one is set up  and what analytics tools you re using,"[1331 397 2 823 334 254 1139 413 934 740 668]"
1348,training-dataset/business/345.txt,business,What casinos and crime teach us about new tech use casesOne of the best ways to understand the future is to study industries that have artificials constraints and work their way on figuring out a way around these constraints  These industries often tend to be most innovative and define new technology use cases way before those use cases get mainstream adoption  There are four industries that regularly demonstrate these characteristics   1  Terrorism and related crime  2  Adult entertainment  3  Drug trafficking  4  Gambling  I am  of course  using the term  industry  loosely here  These industries have always stayed a few steps ahead of the rest  For example  the primary use case of drug trafficking on Silk Road was one of the first large scale implementations of the blockchain  much more before it fascinated the financial servcies industry  The adult entertainment industry was the first to monetize content effectively on the internet  something that the traditional media still struggles to do  Terrorism used technology to manage remote decentralized teams using mobile networks way before fleet managers and distributed sales teams figured out the mechanics of doing that  We ve repeatedly seen these industries leverage technology to figure out use cases and discover mechanics way before traditional industries do   Gambling  in particular is a very interesting industry  Casinos  themselves  have always served as multi sided interaction environments where the platform  casino  always wins by knowing more about the interactions than any individual participant   The gambling industry was the first to formalized and implement behaviour design  Casinos have repeatedly worked on creating behavior design schedules so that users keep coming back and participating further  These principles subsequently found their way into other industries like advertising and gaming  and subsequently into social media   The gambling industry also instituted one of the first large scale implementations of mass personalization  using data  Casinos have always built specific strategies to target whales   gamblers who spend extraordinarily high amounts and are accordingly treated with different incentives  With a wealth of data  casinos have been working on scaling whale type incentives across a larger base of players  and on automating their experience journey with the casino  Data driven mass personalization is increasingly entering other industries  Personalised experiences that were once served only to the top 1  are moving onto a larger base  Wealth management is one such industry where a handful of high net worth individuals get highly personalized advice but the middle layer of investors is served with products and services  not necessarily with personalized investment advice  With the advent of robo advisors  we will see technology augment human workers enabling a larger base of advisors to scale their investment advisory with lower skill requirements and on better economics  allowing them to cater to a much larger group of investors   I believe we will see something similar in the early adoption of VR and automation  and will be keeping an eye out for unexpected use cases in these spaces,"[1348 397 2 668 1331 413 254 563 0 1139 660]"
1382,training-dataset/engineering/243.txt,engineering,Stack Overflow s Bosun Architecture   Kyle BrandtBosun is a significant piece of our monitoring infrastructure at Stack Overflow  In forums such as Bosun s Slack Chat Room  we are frequently asked to describe how Bosun is set up in our environment   This is a detailed post about our architecture surrounding Bosun  This is not the only way to set things up and may not always be the best way either   but this is our current setup   The Pieces  There are a lot of components we use with Bosun  but they all serve their purposes in our monitoring infrastructure  The following table outlines them   Bosun Alerting System Expression language for evaluating time series from OpenTSDB  Graphite  Elastic  and InfluxDB Templates for rich notifications  HTML  Graphs  Tables  CSS inlining Web interface for viewing alerts  writing expressions  graphing  creating alerts and templates  and testing alerts over history  A store for metric metadata and string data about tags  for example  host IPs  Serial numbers  etc  scollector Collection Agent Runs on Windows and Linux  Polls local OS and applications via APIs  also polls external services via SNMP  ICMP  etc  With no configuration  monitors anything it auto discovers  IIS  Redis  Elastic  Etc   configuration rarely needed  BosunReporter NET App Metrics Sends application metrics to bosun  OpenTSDB Time Series Database Daemon Takes time series data and stores it in HBase Time series are tagged  so it is a multi dimensional time series database Opserver Monitoring dashboard Pulls from SolarWinds Orion or Bosun Also includes indepth independent monitoring and control of Redis  SQL Server  HAProxy  and Elastic Grafana User Dashboards Uses the  Grafana Bosun plugin  https   github com grafana grafana plugins tree master datasources bosun  to create graphs and tables via Bosun expressions TPS Parses Web Logs Sends raw logs to SQL and summaries in time series format to Bosun  tsdbrelay Relay Time Series and Meta Data Cross replicates time series and meta data across data centers Creates denormalized OpenTSDB metrics for faster querying speed HBase Where OpenTSDB Stores Data Hadoop  The distributed filed system  HDFS   MapReduce framework  and Cluster Management  Zookeeper   HBase  A clone of Google s Bigtable runs on top of Hadoop Cloudera Manager  What we use to manage HBase HAProxy Load Balancer Used at Stack We use it in front of Bosun  OpenTSDB daemons  and tsdbrelay Redis In memory Store  Optionally  used by Bosun to store state  metadata  metric indexes  etc  If Redis is not used  a built in implementation in Go called LedisDB is used Elastic Document Searching Where we send system logs  Can be queried with Bosun expressions  The Bosun Grafana plugin can use this method to graph time series info about the logs as well   Metric and Query Flow  Metrics are collected many ways   Our homegrown applications calculate their own metrics and send them to Bosun  C  programs use BosunReporter NET and Go programs use the Go collect package   scollector gathers stats from the OS  services it auto discovers   Network equipment and other devices must be polled from a third party machine since they can not run scollector  We designate 2 hosts to run scollector with polling mode enabled  They collect data via SNMP  network devices   ICMP pings  and can be expanded via plug ins   No matter how data is collected  it is not send it directly to Bosun  They send to tsdbrelays which have two purposes   They relay the data to two different bosun clusters   more on that later   They denormalize certain datapoints   They actually do not send directly to tsdbrelay  We front end tsdbrelay through a load balancer  HAProxy  so that we can scale horizontally   We also front end the Bosun service itself for the same reason  One Bosun server is in read only mode  and flipping between the two requires us to sync the state and take the other bosun out of read only mode  OpenTSDB performance is improved if all write requests go to the same node and our HAProxy setup allows us to funnel all write requests to a single node when all nodes are up   Many different systems query the time series database  We call this the  query flow    Grafana  Queries OpenTSDB using Bosun expressions via  api expr   It can also query Elastic directly  as well as query Elastic via  Grafana    Bosun    Elastic     It can also query Elastic directly  as well as query Elastic via  Grafana    Bosun    Elastic  Opserver  Queries Bosun s  api host endpoint and also queries OpenTSDB directly   endpoint and also queries OpenTSDB directly  Bosun itself  when running alerts  or users interact with the UI  it queries OpenTSDB or Elastic  if it exists   Our OpenTSDB and HBase Setup  In our main datacenter in NY to we have two HBase clusters  One is a three node cluster running on NY TSDB0 1 2 3   The other is a local replica running on NY BOSUN01  The local replica is for backups  See how we Backup below    In our secondary datacenter  named  CO   we have a three node cluster that is a mirror setup of the NY TSDB cluster  The HBase clusters don t replicate across datacenters  rather tsdbrelay relays the datapoints to each independent data store   We use a HDFS replication factor of 3 in the two main HBase clusters  The HBase replica is currently a single machine  so it has no HDFS replication   We use Cloudera Manager to manage our OpenTSDB clusters  We have found that HBase can be stable  but it is difficult for a shop that doesn t happen to use it anywhere else   The Numbers  3 7 Billion datapoints a day  43 000 Datapoints a second  per cluster   8 Gigabytes a day of HDFS unreplicated growth per cluster  24GB replicated   Currently   7TB of replicated storage consumed  OpenTSDB Appends vs Compaction  HBase was very unstable when we first started using it in 2015  The main thing that improved our stability was to switch to OpenTSDB s  appends model  instead of the  hourly compactions model    Note  These are unrelated to HBase compactions    OpenTSDB stores time series as metric tags in hourly rows  By storing them as per hour  OpenTSDB only needs to store the delta  the offset from the base hour  for each datapoint  This is more space efficient   In the appends model  storing new datapoints only requires appending to the row  However this requires HBase to read the existing data block  then write it out again with the new datapoint appended  This generates a lot of network traffic   In the compaction model  it writes the data in a less efficient form but one that requires less network bandwidth  Once each hour it then rewrites the data in the more compact  delta  format  As a result the network activity is reduced to an hourly burst instead of constant reading and rewriting   In this graph you can see the Append Model s network traffic load versus Hourly Compaction s greatly reduced network load   This network traffic was unexpectedly huge considering that our input into OpenTSDB  gzip compressed HTTP JSON  is on average 4 Mbit sec  Appends can be seen in the hourly seasonality of the above graph  As the row grows over the hour  so do the size of the rows being re written and and replicated across HDFS and HBase replication   Zookeeper  We found that for stability we have had to greatly increase the timeouts around zookeeper   Zookeeper   tickTime  30000 maxSessionTimeout  600000  HBase   zookeeper session timeout  600000  With low zookeeper timeouts  if operations took a long time or spent a long time in garbage collection  then HBase servers would eject themselves taking the cluster down   Denormalization  Query Speed  and Last  HBase only has one index  OpenTSDB is optimized around the metric  So for example  querying all the metrics about a host  cpu  mem  etc  is slower than a query against a single metric for all hosts  The HBase schema for OpenTSDB reflects this   00000150E22700000001000001                            metric time tagk tagv  Since the metric and time fields are the start of the key  and the key is the only thing indexed  the more tags you have on a metric  the more rows that have to be scanned to find your particular host  This reduces query speed   However  host based views are common  In this schema  they have to multiple metrics  and if those metrics have the host something tagset  the query gets slower as you add more hosts  Combine that with the number of rows that need to be scanned for longer durations and the queries become untenable   As an Example  os cpu has only one tag key host  and about 500 possible values in our environment  To query one year of data  downsampled  to a single host in this metric takes about 20 seconds  If you add more tags it only gets worse  i e  os net bytes which has host  interface  and direction   We work around this query speed issue to generate views in two ways   Redis stores the last value  the last two for counters  for all series  When generating these views  we can get current values for anything nearly instantly  This data is used in Boson s  api host endpoints and can also be queried directly at  api last Denormalize the metrics  To denormalize metrics we put the host name in the metric  and only give it a single tag key   value pair  at least one tag key with a value is required   We use the following argument to tsdbrelay to do this    denormalize os cpu__host os mem used__host os net bytes__host os net bond bytes__host os net other bytes__host os net tunnel bytes__host os net virtual bytes__host  This results in a metric like __ny web01 os cpu   A year s worth of data  also downsampled  is queried in  100 milliseconds as compared to 20 seconds for the normalized metric   HBase Replication  We have struggled some with HBase replication  The secondary cluster has had to be rebuilt when we found corruption  and also when we upgraded to SSDs  With our most recent case of replication breaking we found that force splitting some of the unbalanced regions on the secondary cluster seems to have fixed in  You can see the unbalance in size via the HBase Web GUI    When replication breaks  the logs grow rapidly  and drain at a much slower rate then they grow   Since replication backing up uses so much disk space  we have to factor in head room for this when sizing the HBase clusters   Hardware  NY TSDB0 1 2 3   Per Sever   Area Details Utilization Model Dell R620 CPU 2x Intel R  Xeon R  CPU E5 2650 v2   2 60GHz  15  Ram 128 GB Ram  8x 16GB Chips  34 GB RSS  Rest Used in File Cache Disk 2 Spinny OS Disks  8x SSDs in JBOD  INTEL SSDSC2BB480G4  Avg Read  1 6 MByte sec  Avg Write  57 MByte sec Network Redundant 10Gigabit  500 MBit sec  NY BOSUN02  Also HBase Replica  see previous diagram   Area Details Utilization Model Dell R620 CPU 2x Intel R  Xeon R  CPU E5 2643 v2   3 50GHz  15  Ram 64 GB Ram  4x 16GB Chips  20 GB RSS  Rest Used in File Cache Disk 2 Spinny OS Disks  8x SSDs in JBOD  INTEL SSDSC2BB480G4  Avg Read  1 1 MByte sec  Avg Write  33 MByte sec Network Redundant 10Gigabit  300 MBit sec  Reference Configs  How we deploy  The CI framework we use is called TeamCity  We use it to build packages for Deploy scollector  tsdbrelay  to directly deploy Bosun binaries and configurations into production  TeamCity calls each configuration a  build    scollector and tsdbrelay  Linux   We make RPM packages using fpm combined with this script to easily wrap binaries into RPMs  The script reads mk_rpm_fpmdir   txt to manufacture input to FPM   to manufacture input to FPM  scollector  Windows   A TeamCity build produces a binary and does the right thing so that it is distributed to our Windows hosts   Bosun is deployed directly by TeamCity  We do this because we tend to build pretty often and use branches  so the lag of RPM would be annoying to our developers   Our Bosun configuration is kept in a Git repository  TeamCity triggers on changes to this repo  runs tests  pushes the new config to production  then restarts Bosun   Bosun currently doesn t have live configuration reloading    How we Backup  The are three things in and around Bosun that are Backed up   Bosun s Configuration File Bosun s State and Metadata Storage The Time Series Data  The configuration file is stored in Git which is backed up independently  we use Gitlab internally   We now use redis to store bosun s state and metadata  Bosun will use ledis if no redis server is provided  but we haven t explored how this would be backed up as it is there for small test setups  In redis we use both RDB and AOF for redis persistence   RDB allows us to have snapshots of the datastore  These can be restored easily  even for local development    AOF allows us persistence across restarts of redis   Backing up the time series database  in our case OpenTSDB  is one of our pain points  HBase is designed to scale to petabytes  With that much data  a standalone full backup becomes impractical  So HBase isn t really designed for these sort of backups  See Approaches to Backup and Disaster Recovery in Hbase  This is a shame for us  as we don t have petabytes of data in another system standalone backups might be an option  Therefore we do the following in place of traditional standalone backups   Datapoint level replication  tsdbrelay sends all datapoints to its local and remote cluster  one in NY  one in CO   Although the two HBase clusters are not exactly consistent with each other  different IDs in the database  missing data when a datacenter is out  they are  good enough  copies of each other  Restore  Rotating hourly HBase snaphosts Backup cluster in the main datacenter  An HBase replica  This is distinct from HDFS replication   that is within a cluster  whereas HBase replication replicates to another cluster    Without standalone full backups we are frankly a bit afraid of HBase  but we haven t had any significant data losses since we started using it   Possible Future Changes  Our largest pain point is using OpenTSDB and HBase  Since we don t use HBase anywhere else  the administrative overhead eats up a lot of time  Also  OpenTSDB has had some long standing issues that have been difficult  for example counters don t downsample correctly and our data is in counter format when possible so we don t lose resolution  That being said we still feel for us it is currently our best option  We will be expanding our two main clusters with two more nodes each  running tsdbrelay  opentsdb  and region servers  to increase space   We are very interested in InfluxDB  It is written in Go which we are familiar with and has a rapid devlopment cycle  Bosun has it as a backend  and scollector can send to it since it has a OpenTSDB api style endpoint  However  we are waiting for InfluxBD to have a more stable clustering story before we could consider switching to it in production,"[1382 59 1289 563 638 660 562 334 668 1414 413]"
1414,training-dataset/engineering/1070.txt,engineering,From Big Data to Fast Data in Four Weeks or How Reactive Programming is Changing the World   Part 1Part 1  Reactive Manifesto s Invisible Hand  Let me first setup the context for my story  I ve been with PayPal for 5 years  I m an architect  I m part of the team responsible for PayPal Tracking domain  Tracking is commonly and historically understood as the measurement of customer visits to web pages  With the customer s permission our platform collects all kinds of signals from PayPal web pages  mobile apps and services  for variety of reasons  Most prominent among them are measuring new product adoptions  A B testing  and fraud analysis   We collect several terabytes of data on our Hadoop systems every day  This is not an extremely huge amount  but it is big enough to be one of the primary motivations for adopting Hadoop at PayPal four years ago   Story  This particular story starts in the middle of December 2015  a week or so before holidays  I was pulled into a meeting to discuss a couple of new projects   At the time  PayPal was feverishly preparing to release a brand spanking new mobile wallet app  It had been in development for a long time and promised a complete change in PayPal experience on smartphones  Lots of attention was being paid to this roll out from all levels of leadership   basically  a Big Deal   Measuring new product adoption happens to be one of the main  raison d etre  of the tracking platform  so following roll outs is nothing new for us  This time  however  we were ready to provide adoption metrics in near real time  By the launch date in the end of February  Product managers were able to report the new app adoption progress  as of 30 min ago  instead of the customary  as of yesterday    This was our first full production Fast Data application  You might be wondering why the title says four weeks  though from mid December to the end of February it is ten weeks   It did in fact take a small team   Sudhir R   Loga V   Shunmugavel C   Krishnaprasad K    only four weeks to make it happen   The first full volume live test happened during the Super Bowl  when PayPal aired its ad during the game  And small volume tests happened even earlier in January when the ad was first showcased in some news channels   Just one year back there would have been no chance we could deliver on something like that in four weeks  time  but in December 2015 we were on a roll  We had a production quality prototype running within two weeks and a full volume live test happened two weeks later  In another six weeks  PayPal s largest release of the last several years was measured by this analytical workflow   So what changed during a single year   Allow me to get really nerdy for a second  On a fundamental level  what happened is that we embraced  at first without even realizing it  the Reactive Manifesto in two key components of our platform   What has changed between December 2014 and December 2015  The original Tracking platform design followed common industry standard from 4 5 years ago  We collected data in real time  buffered it in memory and small files  and finally uploaded consolidated files onto Hadoop cluster  At this point a variety of MapReduce jobs would crunch the data and publish the  analyst friendly  version to Teradata repository  from which our end users could query the data using SQL   Sounds straightforward  It is anything but  There are several pitfalls and bottlenecks to consider   First of all is data movement  We are a large company  with lots of services and lots of different types of processing distributed across multiple datacenters  On top of all of this we are also paranoid about security of anything and everything   We ended up keeping a dedicated Hadoop cluster in a live environment for the sole purpose of collecting data  Then we would copy files to our main analytical Hadoop cluster in a different data center   It worked  but     All this waiting for big enough files and scheduling of file transfers added up to hours  We have tinkered with different tools  open source and commercial  looking for the goldilocks solution to address all the constraints and challenges  At the end of the day most of those tools failed to account for the reality on the ground   Until  that is  we discovered Kafka  That was the first reactive domino to fall   Kafka and moving data across data centers  There are three brilliant design decisions which made Kafka the indispensable tool it is today   multi datacenter topology out of the box   short circuiting internal data movement from network socket to OS file system cache and back to network socket   sticking with strictly sequential I O reads and writes  It fits our needs almost perfectly   Throughput  Moving data between datacenters  Check   Can configure topology to satisfy our complicated environment  Flexible persisted buffer between real time stream and Hadoop  Check   Price performance  High availability  Check   Challenges  Few   The only real trouble we ve had with Kafka itself is the level of maturity of its producer API and MirrorMaker  There are lots of knobs and dials to figure out  It changes from release to release  Some producer API behaviors are finicky  We had to invest some time into making sure we are not losing data between our client and Kafka broker when the broker goes down or connection is lost   MirrorMaker requires some careful configuration to make sure we are not double compressing events  It took us some experimentation in a live environment to get to a proper capacity for the MirrorMaker instances  Even with all the hiccups  considering we were using pre GA release open source tool  it was a pretty rewarding and successful endeavor   Most of our troubles came not from Kafka  Our network infrastructure and especially firewall capacities were not ready for the awesome throughput Kafka has unleashed  We were able to move very high volume of data at sub second latency between geographically removed data centers  but every so many hours the whole setup would slow down to a crawl   It took us some time to understand that it was not a Kafka or MirrorMaker problem  as we naturally suspected at first   As it happened we were the first team to attempt that kind of volume of streaming traffic between our data centers  Prevailing traffic patterns at the time were all bulk oriented  file transfer  DB import export etc   Sustained real time traffic has immediately uncovered bottlenecks and temporary saturations  which were hidden before  Good learning experience for us and our Network engineers   Personally I was very impressed with Kafka capabilities and naturally wanted to know how it was written  That led me to Scala  Long story short  I had to rediscover functional programming and with it a totally different way of thinking about concurrency  That led me to Akka and Actors model   Hello Reactive Programming   From MapReduce to Spark  Another major challenge that plagued us at the time was MapReduce inefficiency  both in physical sense and in a developer s productivity sense  There are  literally  books written on MapReduce optimization  We know why it is inefficient and how to deal with it   in theory   In practice however  developers tend to use MapReduce in the simplest way possible  splitting workflows into multiple digestible steps  saving intermediate results to HDFS  and incurring horrifying amount of I O in the process   MapReduce is just too low level and too dumb  Mixing complex business logic with MapReduce low level optimization techniques is asking too much  In architectural lingo  the buildability of such system is very problematic  Or in English  it takes way too long to code and test  Predictable results are tons of long running  brittle  hard to manage  and inefficient workflows   Plus it is Java  Who in their right mind would use Java for analytics  Analytics is a synonym for SQL  right  We ended up writing convoluted pipes in MapReduce to cleanup and reorganize data and then dumped it to Teradata so somebody else can do agile analytical work using SQL  while we were super busy figuring out MapReduce  Pig and Hive helped to a degree  but only just   We have tried to explore alternatives like  SQL on Hadoop  etc  Lots of cool tools  lots of vendors  The Hadoop ecosystem is rich  but hard to navigate  None has addressed our needs comprehensively  Goldilocks solution for BigData was out of reach it seemed   Until  that is  we stumbled into Spark  And that was the second reactive domino to fall   By now  it has become clear to us that using MapReduce is a dead end  But Spark  Switching to Scala  What about the existing  hard earned skill set  People just started to get comfortable with MapReduce and Pig and Hive  it is very risky to switch  All valid arguments  Nevertheless  we jumped   What makes Spark such a compelling system for the big data   RDD  Resilient Distributed Dataset  concept has addressed most of the MapReduce inefficiency issues  both physical and productivity   Resilient   resiliency is a key  The main reason behind MapReduce excessive I O disorder is the need to preserve intermediate data to protect against node crashes  Spark solved the same with minimum I O by keeping a history of transformations  functions  instead of actual transformed data chunks  Intermediate data can still be check pointed to HDFS if the process is very long and the developer can decide if a checkpoint is needed case by case  Once a dataset is read into cache  it can stay there as long as needed or as long as the cache size allows it  And again  the developer can control the behavior case by case   Distributed Dataset  One thing that always bugged me in MapReduce is its inability to reason about my data as a dataset  Instead you are forced to think in single key value pair  small chunk  block  split  or file  Coming from SQL  it felt like going backwards 20 years  Spark has solved this perfectly  A developer can now reason about his data as a Scala immutable collection  RDD API  or data frame  DataFrame API  or both interchangeably  DataSet API   This flexibility and abstraction from the distributed nature of the data leads to a huge productivity gains for developers   And finally  no more Map   Reduce shackles  You can chain whatever functions you need  Spark will optimize your logic into DAG  dynamic acyclic graph   similar to how SQL is interpreted  and run it in one swoop keeping data in memory as much as physically possible    Minor  detail   nobody knew Scala here  So to get all the efficiency and productivity benefits we had to train people in Scala and do it quickly   It is a bit more than just a language though  Scala  at the first glance  has enough of a similarity to Java that any Java coder can fake it   Plus  however cumbersome  Java can be used as Spark language and so can Python    The real problem was to change the mindset of the Java MapReduce developer to a functional way of thinking  We needed a clean break from old Java Python MR habits  What habits  Most of all mutability  Java beans  mutable collections and variables  loops etc  We were going to switch to Scala and do it right   It has started slow  We have designed a curriculum for our Java MapReduce developers  We started with couple of two hour sessions  one for Functional programming Scala basics and one for Spark basics  After some trial and error the FP Scala part was extended to a four hour learning by example session  while Spark barely needed two hours if at all  As it happens  once you understand the functional side of Scala  especially immutable collections  collection functions  and lambda expressions  Spark comes naturally   It snowballed very quickly  within a couple of months we had the majority of our MapReduce engineers re trained and sworn off MapReduce forever  Spark was in   Spark  naturally  is written in Scala and Akka   2 0 for team Reactive   Stay tuned for Part 2   Lambda Architecture meets reality  coming soon  Part 2 Preview,"[1414 334 0 397 823 413 743 563 934 855 1289]"
1416,training-dataset/product/1349.txt,product,Creating design driven data visualizationIn our first DesignTalk of 2017  Hayley Hughes shares examples and insights from her work at IBM on data visualization  As we continue to increase the quantity and kinds of data collected  we need to experiment more with how to make it unique and personal   Learn how to correctly use color  how to be mindful of accessibility issues  and how to include your company s visual history  After watching this DesignTalk  you ll be inspired to bring your work to its highest fidelity,"[1416 2 397 1331 734 254 823 334 1139 563 740]"
